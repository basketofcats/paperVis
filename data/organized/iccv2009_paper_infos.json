[
    {
        "title": "Decomposing a scene into geometric and semantically consistent regions",
        "authors": [
            "Stephen Gould",
            "Richard Fulton",
            "Daphne Koller"
        ],
        "abstract": "High-level, or holistic, scene understanding involves reasoning about objects, regions, and the 3D relationships between them. This requires a representation above the level of pixels that can be endowed with high-level attributes such as class of object/region, its orientation, and (rough 3D) location within the scene. Towards this goal, we propose a region-based model which combines appearance and scene geometry to automatically decompose a scene into semantically meaningful regions. Our model is defined in terms of a unified energy function over scene appearance and structure. We show how this energy function can be learned from data and present an efficient inference technique that makes use of multiple over-segmentations of the image to propose moves in the energy-space. We show, experimentally, that our method achieves state-of-the-art performance on the tasks of both multi-class image segmentation and geometric reasoning. Finally, by understanding region classes and geometry, we show how our model can be used as the basis for 3D reconstruction of the scene.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459211",
        "reference_list": [
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 192,
            "other": 110,
            "total": 302
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image segmentation",
                "Solid modeling",
                "Geometry",
                "Image reconstruction",
                "Computer science",
                "Pixel",
                "Roads",
                "Image decomposition",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "functions",
                "image reconstruction",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene decomposition",
                "geometric consistent regions",
                "semantically consistent regions",
                "pixels",
                "region-based model",
                "unified energy function",
                "scene appearance",
                "scene geometry",
                "scene structure",
                "inference technique",
                "image oversegmentation",
                "multiclass image segmentation",
                "scene reconstruction"
            ]
        },
        "id": 0,
        "cited_by": [
            {
                "year": "2015",
                "id": 288
            },
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2013",
                "id": 104
            },
            {
                "year": "2011",
                "id": 42
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 211
            },
            {
                "year": "2011",
                "id": 228
            },
            {
                "year": "2011",
                "id": 268
            },
            {
                "year": "2011",
                "id": 272
            },
            {
                "year": "2011",
                "id": 328
            }
        ]
    },
    {
        "title": "Boundary ownership by lifting to 2.1D",
        "authors": [
            "Ido Leichter",
            "Michael Lindenbaum"
        ],
        "abstract": "This paper addresses the \u201cboundary ownership\u201d problem, also known as the figure/ground assignment problem. Estimating boundary ownerships is a key step in perceptual organization: it allows higher-level processing to be applied on non-accidental shapes corresponding to figural regions. Existing methods for estimating the boundary ownerships for a given set of boundary curves model the probability distribution function (PDF) of the binary figure/ground random variables associated with the curves. Instead of modeling this PDF directly, the proposed method uses the 2.1D model: it models the PDF of the ordinal depths of the image segments enclosed by the curves. After this PDF is maximized, the boundary ownership of a curve is determined according to the ordinal depths of the two image segments it abuts. This method has two advantages: first, boundary ownership configurations inconsistent with every depth ordering (and thus very likely to be incorrect) are eliminated from consideration; second, it allows for the integration of cues related to image segments (not necessarily adjacent) in addition to those related to the curves. The proposed method models the PDF as a conditional random field (CRF) conditioned on cues related to the curves, T-junctions, and image segments. The CRF is formulated using learnt non-parametric distributions of the cues. The method significantly improves the currently achieved figure/ground assignment accuracy, with 20.7% fewer errors in the Berkeley Segmentation Dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459208",
        "reference_list": [
            {
                "year": "2007",
                "id": 144
            },
            {
                "year": "2005",
                "id": 159
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 8,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Humans",
                "Shape",
                "Level set",
                "Computer science",
                "Probability distribution",
                "Random variables",
                "Layout",
                "Parallel processing",
                "Performance evaluation"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "boundary ownership",
                "2.1D",
                "figure-ground assignment problem",
                "high level processing",
                "nonaccidental shape",
                "probability distribution function",
                "binary figure-ground random variable",
                "image segmentation",
                "conditional random field"
            ]
        },
        "id": 1,
        "cited_by": [
            {
                "year": "2015",
                "id": 43
            },
            {
                "year": "2011",
                "id": 268
            }
        ]
    },
    {
        "title": "Curvature regularity for region-based image segmentation and inpainting: A linear programming relaxation",
        "authors": [
            "Thomas Schoenemann",
            "Fredrik Kahl",
            "Daniel Cremers"
        ],
        "abstract": "We consider a class of region-based energies for image segmentation and inpainting which combine region integrals with curvature regularity of the region boundary. To minimize such energies, we formulate an integer linear program which jointly estimates regions and their boundaries. Curvature regularity is imposed by respective costs on pairs of adjacent boundary segments. By solving the associated linear programming relaxation and thresholding the solution one obtains an approximate solution to the original integer problem. To our knowledge this is the first approach to impose curvature regularity in region-based formulations in a manner that is independent of initialization and allows to compute a bound on the optimal energy. In a variety of experiments on segmentation and inpainting, we demonstrate the advantages of higher-order regularity. Moreover, we demonstrate that for most experiments the optimality gap is smaller than 2% of the global optimum. For many instances we are even able to compute the global optimum.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459209",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2007",
                "id": 132
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 26,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Linear programming",
                "Computer science",
                "Integral equations",
                "Optimization methods",
                "Costs",
                "Psychology",
                "Humans",
                "Computer vision",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "image segmentation",
                "integer programming",
                "linear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curvature regularity",
                "region-based image segmentation",
                "inpainting",
                "linear programming relaxation",
                "region-based energies",
                "region integrals",
                "region boundary",
                "integer linear program",
                "boundary segments",
                "optimal energy",
                "higher-order regularity",
                "optimality gap",
                "global optimum"
            ]
        },
        "id": 2,
        "cited_by": [
            {
                "year": "2015",
                "id": 44
            },
            {
                "year": "2011",
                "id": 32
            },
            {
                "year": "2011",
                "id": 160
            },
            {
                "year": "2011",
                "id": 223
            },
            {
                "year": "2011",
                "id": 271
            },
            {
                "year": "2011",
                "id": 310
            }
        ]
    },
    {
        "title": "Human detection using partial least squares analysis",
        "authors": [
            "William Robson Schwartz",
            "Aniruddha Kembhavi",
            "David Harwood",
            "Larry S. Davis"
        ],
        "abstract": "Significant research has been devoted to detecting people in images and videos. In this paper we describe a human detection method that augments widely used edge-based features with texture and color information, providing us with a much richer descriptor set. This augmentation results in an extremely high-dimensional feature space (more than 170,000 dimensions). In such high-dimensional spaces, classical machine learning algorithms such as SVMs are nearly intractable with respect to training. Furthermore, the number of training samples is much smaller than the dimensionality of the feature space, by at least an order of magnitude. Finally, the extraction of features from a densely sampled grid structure leads to a high degree of multicollinearity. To circumvent these data characteristics, we employ Partial Least Squares (PLS) analysis, an efficient dimensionality reduction technique, one which preserves significant discriminative information, to project the data onto a much lower dimensional subspace (20 dimensions, reduced from the original 170,000). Our human detection system, employing PLS analysis over the enriched descriptor set, is shown to outperform state-of-the-art techniques on three varied datasets including the popular INRIA pedestrian dataset, the low-resolution gray-scale DaimlerChrysler pedestrian dataset, and the ETHZ pedestrian dataset consisting of full-length videos of crowded scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459205",
        "reference_list": [
            {
                "year": "2001",
                "id": 60
            },
            {
                "year": "2007",
                "id": 252
            },
            {
                "year": "2007",
                "id": 217
            }
        ],
        "citation": {
            "ieee": 169,
            "other": 119,
            "total": 288
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Least squares methods",
                "Videos",
                "Image edge detection",
                "Machine learning algorithms",
                "Data mining",
                "Feature extraction",
                "Information analysis",
                "Gray-scale",
                "Layout"
            ]
        },
        "id": 3,
        "cited_by": [
            {
                "year": "2015",
                "id": 475
            },
            {
                "year": "2013",
                "id": 15
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2013",
                "id": 261
            },
            {
                "year": "2011",
                "id": 117
            }
        ]
    },
    {
        "title": "An HOG-LBP human detector with partial occlusion handling",
        "authors": [
            "Xiaoyu Wang",
            "Tony X. Han",
            "Shuicheng Yan"
        ],
        "abstract": "By combining Histograms of Oriented Gradients (HOG) and Local Binary Pattern (LBP) as the feature set, we propose a novel human detection approach capable of handling partial occlusion. Two kinds of detectors, i.e., global detector for whole scanning windows and part detectors for local regions, are learned from the training data using linear SVM. For each ambiguous scanning window, we construct an occlusion likelihood map by using the response of each block of the HOG feature to the global detector. The occlusion likelihood map is then segmented by Mean-shift approach. The segmented portion of the window with a majority of negative response is inferred as an occluded region. If partial occlusion is indicated with high likelihood in a certain scanning window, part detectors are applied on the unoccluded regions to achieve the final classification on the current scanning window. With the help of the augmented HOG-LBP feature and the global-part occlusion handling method, we achieve a detection rate of 91.3% with FPPW= 10 \u22126 , 94.7% with FPPW= 10 \u22125 , and 97.9% with FPPW= 10 \u22124 on the INRIA dataset, which, to our best knowledge, is the best human detection performance on the INRIA dataset. The global-part occlusion handling method is further validated using synthesized occlusion data constructed from the INRIA and Pascal dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459207",
        "reference_list": [
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 521,
            "other": 313,
            "total": 834
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Detectors",
                "Support vector machines",
                "Image segmentation",
                "Histograms",
                "Support vector machine classification",
                "Training data",
                "Object detection",
                "Pixel",
                "Testing"
            ]
        },
        "id": 4,
        "cited_by": [
            {
                "year": "2017",
                "id": 367
            },
            {
                "year": "2015",
                "id": 5
            },
            {
                "year": "2015",
                "id": 220
            },
            {
                "year": "2015",
                "id": 253
            },
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2013",
                "id": 15
            },
            {
                "year": "2013",
                "id": 100
            },
            {
                "year": "2013",
                "id": 130
            },
            {
                "year": "2013",
                "id": 131
            },
            {
                "year": "2013",
                "id": 257
            },
            {
                "year": "2013",
                "id": 261
            },
            {
                "year": "2013",
                "id": 319
            },
            {
                "year": "2013",
                "id": 323
            },
            {
                "year": "2013",
                "id": 330
            }
        ]
    },
    {
        "title": "Max-margin additive classifiers for detection",
        "authors": [
            "Subhransu Maji",
            "Alexander C. Berg"
        ],
        "abstract": "We present methods for training high quality object detectors very quickly. The core contribution is a pair of fast training algorithms for piece-wise linear classifiers, which can approximate arbitrary additive models. The classifiers are trained in a max-margin framework and significantly outperform linear classifiers on a variety of vision datasets. We report experimental results quantifying training time and accuracy on image classification tasks and pedestrian detection, including detection results better than the best previous on the INRIA dataset with faster training.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459203",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2007",
                "id": 172
            },
            {
                "year": "2007",
                "id": 259
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 58,
            "other": 35,
            "total": 93
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Computer vision",
                "Support vector machines",
                "Support vector machine classification",
                "Object detection",
                "Image classification",
                "Object recognition",
                "Kernel",
                "Piecewise linear techniques",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object detection",
                "piecewise linear techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "max margin additive classifiers",
                "object detection",
                "training algorithm",
                "piecewise linear classifiers",
                "image classification tasks",
                "pedestrian detection",
                "optimization"
            ]
        },
        "id": 5,
        "cited_by": [
            {
                "year": "2015",
                "id": 455
            },
            {
                "year": "2013",
                "id": 147
            },
            {
                "year": "2013",
                "id": 266
            },
            {
                "year": "2009",
                "id": 54
            }
        ]
    },
    {
        "title": "Kernel methods for weakly supervised mean shift clustering",
        "authors": [
            "Oncel Tuzel",
            "Fatih Porikli",
            "Peter Meer"
        ],
        "abstract": "Mean shift clustering is a powerful unsupervised data analysis technique which does not require prior knowledge of the number of clusters, and does not constrain the shape of the clusters. The data association criteria is based on the underlying probability distribution of the data points which is defined in advance via the employed distance metric. In many problem domains, the initially designed distance metric fails to resolve the ambiguities in the clustering process. We present a novel semi-supervised kernel mean shift algorithm where the inherent structure of the data points is learned with a few user supplied constraints in addition to the original metric. The constraints we consider are the pairs of points that should be clustered together. The data points are implicitly mapped to a higher dimensional space induced by the kernel function where the constraints can be effectively enforced. The mode seeking is then performed on the embedded space and the approach preserves all the advantages of the original mean shift algorithm. Experiments on challenging synthetic and real data clearly demonstrate that significant improvements in clustering accuracy can be achieved by employing only a few constraints.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459204",
        "reference_list": [
            {
                "year": "2005",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 11,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Clustering algorithms",
                "Density functional theory",
                "Computer vision",
                "Image segmentation",
                "Face detection",
                "Layout",
                "Machine learning algorithms",
                "Laboratories",
                "Power engineering computing"
            ],
            "INSPEC: Controlled Indexing": [
                "data analysis",
                "learning (artificial intelligence)",
                "pattern clustering",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "kernel methods",
                "weakly supervised mean shift clustering",
                "unsupervised data analysis technique",
                "probability distribution",
                "employed distance metric",
                "semisupervised kernel mean shift algorithm"
            ]
        },
        "id": 6,
        "cited_by": []
    },
    {
        "title": "Finding shareable informative patterns and optimal coding matrix for multiclass boosting",
        "authors": [
            "Bang Zhang",
            "Getian Ye",
            "Yang Wang",
            "Jie Xu",
            "Gunawan Herman"
        ],
        "abstract": "A multiclass classification problem can be reduced to a collection of binary problems using an error-correcting coding matrix that specifies the binary partitions of the classes. The final classifier is an ensemble of base classifiers learned on binary problems and its performance is affected by two major factors: the qualities of the base classifiers and the coding matrix. Previous studies either focus on one of these factors or consider two factors separately. In this paper, we propose a new multiclass boosting algorithm called AdaBoost.SIP that considers both two factors simultaneously. In this algorithm, informative patterns, which are shareable by different classes rather than only discriminative on specific single class, are generated at first. Then the binary partition preferred by each pattern is found by performing stage-wise functional gradient descent on a margin-based cost function. Finally, base classifiers and coding matrix are optimized simultaneously by maximizing the negative gradient of such cost function. The proposed algorithm is applied to scene and event recognition and experimental results show its effectiveness in multiclass classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459146",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Partitioning algorithms",
                "Computer vision",
                "Frequency",
                "Australia",
                "Computer errors",
                "Cost function",
                "Decision trees",
                "Computer science",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "encoding",
                "error correction codes",
                "matrix algebra",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shareable informative patterns",
                "multiclass classification",
                "error-correcting coding matrix",
                "AdaBoost.SIP",
                "informative patterns",
                "margin-based cost function"
            ]
        },
        "id": 7,
        "cited_by": []
    },
    {
        "title": "Learning with dynamic group sparsity",
        "authors": [
            "Junzhou Huang",
            "Xiaolei Huang",
            "Dimitris Metaxas"
        ],
        "abstract": "This paper investigates a new learning formulation called dynamic group sparsity. It is a natural extension of the standard sparsity concept in compressive sensing, and is motivated by the observation that in some practical sparse data the nonzero coefficients are often not random but tend to be clustered. Intuitively, better results can be achieved in these cases by reasonably utilizing both clustering and sparsity priors. Motivated by this idea, we have developed a new greedy sparse recovery algorithm, which prunes data residues in the iterative process according to both sparsity and group clustering priors rather than only sparsity as in previous methods. The proposed algorithm can recover stably sparse data with clustering trends using far fewer measurements and computations than current state-of-the-art algorithms with provable guarantees. Moreover, our algorithm can adaptively learn the dynamic group structure and the sparsity number if they are not available in the practical applications. We have applied the algorithm to sparse recovery and background subtraction in videos. Numerous experiments with improved performance over previous methods further validate our theoretical proofs and the effectiveness of the proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459202",
        "reference_list": [
            {
                "year": "2003",
                "id": 170
            },
            {
                "year": "2003",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 8,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Clustering algorithms",
                "Iterative algorithms",
                "Matching pursuit algorithms",
                "Noise measurement",
                "Pursuit algorithms",
                "Minimization methods",
                "Computer vision",
                "Iterative methods",
                "Current measurement",
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "greedy algorithms",
                "iterative methods",
                "learning (artificial intelligence)",
                "pattern clustering",
                "sparse matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic group sparsity",
                "learning formulation",
                "standard sparsity concept",
                "compressive sensing",
                "greedy sparse recovery algorithm",
                "iterative process",
                "group clustering",
                "background subtraction"
            ]
        },
        "id": 8,
        "cited_by": [
            {
                "year": "2013",
                "id": 421
            }
        ]
    },
    {
        "title": "Building Rome in a day",
        "authors": [
            "Sameer Agarwal",
            "Noah Snavely",
            "Ian Simon",
            "Steven M. Seitz",
            "Richard Szeliski"
        ],
        "abstract": "We present a system that can match and reconstruct 3D scenes from extremely large collections of photographs such as those found by searching for a given city (e.g., Rome) on Internet photo sharing sites. Our system uses a collection of novel parallel distributed matching and reconstruction algorithms, designed to maximize parallelism at each stage in the pipeline and minimize serialization bottlenecks. It is designed to scale gracefully with both the size of the problem and the amount of available computation. We have experimented with a variety of alternative algorithms at each stage of the pipeline and report on which ones work best in a parallel computing environment. Our experimental results demonstrate that it is now possible to reconstruct cities consisting of 150 K images in less than a day on a cluster with 500 compute cores.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459148",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2007",
                "id": 24
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 267,
            "other": 235,
            "total": 502
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Pipelines",
                "Cities and towns",
                "Parallel processing",
                "Computer vision",
                "Layout",
                "Clustering algorithms",
                "Painting",
                "Lighting",
                "Earth"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image reconstruction",
                "Internet",
                "peer-to-peer computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photographs collections",
                "Internet photo sharing sites",
                "parallel distributed reconstruction algorithms",
                "parallel distributed matching algorithms",
                "serialization bottlenecks",
                "parallel computing",
                "3D scene matching",
                "3D scene reconstruction"
            ]
        },
        "id": 9,
        "cited_by": [
            {
                "year": "2017",
                "id": 249
            },
            {
                "year": "2017",
                "id": 410
            },
            {
                "year": "2015",
                "id": 96
            },
            {
                "year": "2015",
                "id": 237
            },
            {
                "year": "2013",
                "id": 34
            },
            {
                "year": "2013",
                "id": 59
            },
            {
                "year": "2013",
                "id": 63
            },
            {
                "year": "2013",
                "id": 94
            },
            {
                "year": "2013",
                "id": 118
            },
            {
                "year": "2013",
                "id": 407
            },
            {
                "year": "2011",
                "id": 37
            },
            {
                "year": "2011",
                "id": 110
            },
            {
                "year": "2011",
                "id": 135
            },
            {
                "year": "2011",
                "id": 143
            },
            {
                "year": "2011",
                "id": 221
            },
            {
                "year": "2011",
                "id": 235
            },
            {
                "year": "2011",
                "id": 297
            }
        ]
    },
    {
        "title": "Reconstructing building interiors from images",
        "authors": [
            "Yasutaka Furukawa",
            "Brian Curless",
            "Steven M. Seitz",
            "Richard Szeliski"
        ],
        "abstract": "This paper proposes a fully automated 3D reconstruction and visualization system for architectural scenes (interiors and exteriors). The reconstruction of indoor environments from photographs is particularly challenging due to texture-poor planar surfaces such as uniformly-painted walls. Our system first uses structure-from-motion, multi-view stereo, and a stereo algorithm specifically designed for Manhattan-world scenes (scenes consisting predominantly of piece-wise planar surfaces with dominant directions) to calibrate the cameras and to recover initial 3D geometry in the form of oriented points and depth maps. Next, the initial geometry is fused into a 3D model with a novel depth-map integration algorithm that, again, makes use of Manhattan-world assumptions and produces simplified 3D models. Finally, the system enables the exploration of reconstructed environments with an interactive, image-based 3D viewer. We demonstrate results on several challenging datasets, including a 3D reconstruction and image-based walk-through of an entire floor of a house, the first result of this kind from an automated computer vision system.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459145",
        "reference_list": [],
        "citation": {
            "ieee": 85,
            "other": 69,
            "total": 154
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Layout",
                "Surface reconstruction",
                "Stereo vision",
                "Geometry",
                "Solid modeling",
                "Buildings",
                "Visualization",
                "Indoor environments",
                "Surface texture"
            ]
        },
        "id": 10,
        "cited_by": [
            {
                "year": "2015",
                "id": 147
            },
            {
                "year": "2015",
                "id": 300
            },
            {
                "year": "2013",
                "id": 4
            },
            {
                "year": "2013",
                "id": 267
            },
            {
                "year": "2011",
                "id": 111
            },
            {
                "year": "2011",
                "id": 119
            },
            {
                "year": "2011",
                "id": 235
            }
        ]
    },
    {
        "title": "Is dual linear self-calibration artificially ambiguous?",
        "authors": [
            "Pierre Gurdjos",
            "Adrien Bartoli",
            "Peter Sturm"
        ],
        "abstract": "This purely theoretical work investigates the problem of artificial singularities in camera self-calibration. Self-calibration allows one to upgrade a projective reconstruction to metric and has a concise and well-understood formulation based on the Dual Absolute Quadric (DAQ), a rank-3 quadric envelope satisfying (nonlinear) \u2018spectral constraints\u2019: it must be positive of rank 3. The practical scenario we consider is the one of square pixels, known principal point and varying unknown focal length, for which generic Critical Motion Sequences (CMS) have been thoroughly derived. The standard linear self-calibration algorithm uses the DAQ paradigm but ignores the spectral constraints. It thus has artificial CMSs, which have barely been studied so far. We propose an algebraic model of singularities based on the confocal quadric theory. It allows to easily derive all types of CMSs. We first review the already known generic CMSs, for which any self-calibration algorithm fails. We then describe all CMSs for the standard linear self-calibration algorithm; among those are artificial CMSs caused by the above spectral constraints being neglected. We then show how to detect CMSs. If this is the case it is actually possible to uniquely identify the correct self-calibration solution, based on a notion of signature of quadrics. The main conclusion of this paper is that a posteriori enforcing the spectral constraints in linear self-calibration is discriminant enough to resolve all artificial CMSs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459152",
        "reference_list": [
            {
                "year": "2007",
                "id": 274
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Data acquisition",
                "Cameras",
                "Image reconstruction",
                "Collision mitigation",
                "Image resolution",
                "Pixel",
                "Constraint optimization",
                "Layout"
            ]
        },
        "id": 11,
        "cited_by": [
            {
                "year": "2013",
                "id": 122
            }
        ]
    },
    {
        "title": "Globally optimal affine epipolar geometry from apparent contours",
        "authors": [
            "Gang Li",
            "Yanghai Tsin"
        ],
        "abstract": "We study the problem of estimating the epipolar geometry from apparent contours of smooth curved surfaces with affine camera models. Since apparent contours are viewpoint dependent, the only true image correspondences are projections of the frontier points, i.e., surface points whose tangent planes are also their epipolar planes. However, frontier points are unknown a priori and must be estimated simultaneously with epipolar geometry. Previous approaches to this problem adopt local greedy search methods which are sensitive to initialization, and may get trapped in local minima. We propose the first algorithm that guarantees global optimality for this problem. We first reformulate the problem using a separable form that allows us to search effectively in a 2D space, instead of on a 5D hypersphere in the classical formulation. Next, in a branch-and-bound algorithm we introduce a novel lower bounding function through interval matrix analysis. Experimental results on both synthetic and real scenes demonstrate that the proposed method is able to quickly obtain the optimal solution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459147",
        "reference_list": [
            {
                "year": "2007",
                "id": 57
            },
            {
                "year": "2005",
                "id": 128
            },
            {
                "year": "2007",
                "id": 114
            },
            {
                "year": "2001",
                "id": 132
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Cameras",
                "Cost function",
                "Layout",
                "Computational geometry",
                "Computer vision",
                "Educational institutions",
                "Search methods",
                "Transmission line matrix methods",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "matrix algebra",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "globally optimal affine epipolar geometry",
                "apparent contours",
                "smooth curved surfaces",
                "affine camera models",
                "image correspondences",
                "surface points",
                "epipolar planes",
                "local greedy search methods",
                "2D space",
                "branch-and-bound algorithm",
                "interval matrix analysis",
                "computer vision"
            ]
        },
        "id": 12,
        "cited_by": []
    },
    {
        "title": "Activity recognition using the velocity histories of tracked keypoints",
        "authors": [
            "Ross Messing",
            "Chris Pal",
            "Henry Kautz"
        ],
        "abstract": "We present an activity recognition feature inspired by human psychophysical performance. This feature is based on the velocity history of tracked keypoints. We present a generative mixture model for video sequences using this feature, and show that it performs comparably to local spatio-temporal features on the KTH activity recognition dataset. In addition, we contribute a new activity recognition dataset, focusing on activities of daily living, with high resolution video sequences of complex actions. We demonstrate the superiority of our velocity history feature on high resolution video sequences of complicated activities. Further, we show how the velocity history feature can be extended, both with a more sophisticated latent velocity model, and by combining the velocity history feature with other useful information, like appearance, position, and high level semantic information. Our approach performs comparably to established and state of the art methods on the KTH dataset, and significantly outperforms all other methods on our challenging new dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459154",
        "reference_list": [],
        "citation": {
            "ieee": 149,
            "other": 109,
            "total": 258
        },
        "keywords": {
            "IEEE Keywords": [
                "History",
                "Humans",
                "Video sequences",
                "Computer vision",
                "Patient monitoring",
                "Tracking",
                "Cognition",
                "Face recognition",
                "Detectors",
                "Computerized monitoring"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human psychophysical performance",
                "tracked keypoint velocity history",
                "generative mixture model",
                "local spatio-temporal features",
                "KTH activity recognition dataset",
                "high resolution video sequences",
                "latent velocity model"
            ]
        },
        "id": 13,
        "cited_by": [
            {
                "year": "2013",
                "id": 169
            },
            {
                "year": "2013",
                "id": 444
            },
            {
                "year": "2011",
                "id": 14
            },
            {
                "year": "2011",
                "id": 51
            },
            {
                "year": "2011",
                "id": 179
            },
            {
                "year": "2011",
                "id": 217
            }
        ]
    },
    {
        "title": "Quasi-periodic event analysis for social game retrieval",
        "authors": [
            "Ping Wang",
            "Gregory D. Abowd",
            "James M. Rehg"
        ],
        "abstract": "A new problem of retrieving social games from unstructured videos is proposed. Social games are characterized by repetitions (with variations) of alternating turns between two players. We define games as quasi-periodic motion patterns in video based on their repetitiveness property. We have developed an algorithm to extract such patterns from video. The patterns extracted by our method, from video clips of social games taken from YouTube, are shown to correspond to meaningful stages of the games. We demonstrate promising results in retrieving social games from unstructured, lab-recorded footage of children's play, and identifying social interactions in a dataset of approximately 3.75 hours of home movies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459151",
        "reference_list": [
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2005",
                "id": 105
            },
            {
                "year": "2007",
                "id": 209
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 6,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Games",
                "Videos",
                "Motion pictures",
                "Information retrieval",
                "Computer vision",
                "Data mining",
                "Pediatrics",
                "Cameras",
                "Motion analysis",
                "Motion estimation"
            ]
        },
        "id": 14,
        "cited_by": [
            {
                "year": "2013",
                "id": 437
            },
            {
                "year": "2011",
                "id": 88
            }
        ]
    },
    {
        "title": "Modelling activity global temporal dependencies using Time Delayed Probabilistic Graphical Model",
        "authors": [
            "Chen Change Loy",
            "Tao Xiang",
            "Shaogang Gong"
        ],
        "abstract": "We present a novel approach for detecting global behaviour anomalies in multiple disjoint cameras by learning time delayed dependencies between activities cross camera views. Specifically, we propose to model multi-camera activities using a Time Delayed Probabilistic Graphical Model (TD-PGM) with different nodes representing activities in different semantically decomposed regions from different camera views, and the directed links between nodes encoding causal relationships between the activities. A novel two-stage structure learning algorithm is formulated to learn globally optimised time-delayed dependencies. A new cumulative abnormality score is also introduced to replace the conventional log-likelihood score for gaining significantly more robust and reliable real-time anomaly detection. The effectiveness of the proposed approach is validated using a camera network installed at a busy underground station.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459156",
        "reference_list": [
            {
                "year": "2005",
                "id": 240
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 4,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Delay effects",
                "Graphical models",
                "Cameras",
                "Optimization methods",
                "Learning systems",
                "Layout",
                "Noise robustness",
                "Road vehicles",
                "Content addressable storage",
                "Computer science"
            ]
        },
        "id": 15,
        "cited_by": []
    },
    {
        "title": "Action detection in complex scenes with spatial and temporal ambiguities",
        "authors": [
            "Yuxiao Hu",
            "Liangliang Cao",
            "Fengjun Lv",
            "Shuicheng Yan",
            "Yihong Gong",
            "Thomas S. Huang"
        ],
        "abstract": "In this paper, we investigate the detection of semantic human actions in complex scenes. Unlike conventional action recognition in well-controlled environments, action detection in complex scenes suffers from cluttered backgrounds, heavy crowds, occluded bodies, and spatial-temporal boundary ambiguities caused by imperfect human detection and tracking. Conventional algorithms are likely to fail with such spatial-temporal ambiguities. In this work, the candidate regions of an action are treated as a bag of instances. Then a novel multiple-instance learning framework, named SMILE-SVM (Simulated annealing Multiple Instance LEarning Support Vector Machines), is presented for learning human action detector based on imprecise action locations. SMILE-SVM is extensively evaluated with satisfactory performances on two tasks: (1) human action detection on a public video action database with cluttered backgrounds, and (2) a real world problem of detecting whether the customers in a shopping mall show an intention to purchase the merchandise on shelf (even if they didn't buy it eventually). In addition, the complementary nature of motion and appearance features in action detection are also validated, demonstrating a boosted performance in our experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459153",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2005",
                "id": 59
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2007",
                "id": 26
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 3,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Humans",
                "Machine learning",
                "Simulated annealing",
                "Support vector machines",
                "Detectors",
                "Performance evaluation",
                "Spatial databases",
                "Merchandise",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "semantic networks",
                "simulated annealing",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human action detection",
                "semantic human actions detection",
                "cluttered backgrounds",
                "occluded bodies",
                "spatial-temporal boundary ambiguities",
                "SMILE-SVM",
                "simulated annealing multiple instance learning support vector machines",
                "video action database",
                "real world problem"
            ]
        },
        "id": 16,
        "cited_by": [
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2015",
                "id": 366
            },
            {
                "year": "2011",
                "id": 14
            },
            {
                "year": "2011",
                "id": 307
            }
        ]
    },
    {
        "title": "Semi-automatic stereo extraction from video footage",
        "authors": [
            "Moshe Guttmann",
            "Lior Wolf",
            "Daniel Cohen-Or"
        ],
        "abstract": "We present a semi-automatic system that converts conventional video shots to stereoscopic video pairs. The system requires just a few user-scribbles in a sparse set of frames. The system combines a diffusion scheme, which takes into account the local saliency and the local motion at each video location, coupled with a classification scheme that assigns depth to image patches. The system tolerates both scene motion and camera motion. In typical shots, containing hundreds of frames, even in the face of significant motion, it is enough to mark scribbles on the first and last frames of the shot. Once marked, plausible stereo results are obtained in a matter of seconds, leading to a scalable video conversion system. Finally, we validate our results with ground truth stereo video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459158",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2007",
                "id": 291
            },
            {
                "year": "2007",
                "id": 170
            }
        ],
        "citation": {
            "ieee": 61,
            "other": 27,
            "total": 88
        },
        "keywords": {
            "IEEE Keywords": [
                "Image converters",
                "Cameras",
                "Layout",
                "Production",
                "Motion pictures",
                "Data mining",
                "Stereo vision",
                "Transmission line matrix methods",
                "Computer science",
                "Three dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image motion analysis",
                "stereo image processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semi automatic stereo extraction",
                "video footage",
                "conventional video shots",
                "stereoscopic video pairs",
                "user-scribbles",
                "local saliency",
                "local motion",
                "camera motion",
                "scalable video conversion system",
                "ground truth stereo video",
                "classification scheme",
                "diffusion scheme"
            ]
        },
        "id": 17,
        "cited_by": []
    },
    {
        "title": "Filter flow",
        "authors": [
            "Steven M. Seitz",
            "Simon Baker"
        ],
        "abstract": "The filter flow problem is to compute a space-variant linear filter that transforms one image into another. This framework encompasses a broad range of transformations including stereo, optical flow, lighting changes, blur, and combinations of these effects. Parametric models such as affine motion, vignetting, and radial distortion can also be modeled within the same framework. All such transformations are modeled by selecting a number of constraints and objectives on the filter entries from a catalog which we enumerate. Most of the constraints are linear, leading to globally optimal solutions (via linear programming) for affine transformations, depth-from-defocus, and other problems. Adding a (non-convex) compactness objective enables solutions for optical flow with illumination changes, space-variant defocus, and higher-order smoothness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459155",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 16,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical filters",
                "Nonlinear filters",
                "Image motion analysis",
                "Optical distortion",
                "Lighting",
                "Nonlinear optics",
                "Cameras",
                "Kernel",
                "Linear programming",
                "Layout"
            ]
        },
        "id": 18,
        "cited_by": [
            {
                "year": "2015",
                "id": 95
            },
            {
                "year": "2015",
                "id": 449
            }
        ]
    },
    {
        "title": "Shift-map image editing",
        "authors": [
            "Yael Pritch",
            "Eitam Kav-Venaki",
            "Shmuel Peleg"
        ],
        "abstract": "Geometric rearrangement of images includes operations such as image retargeting, inpainting, or object rearrangement. Each such operation can be characterized by a shiftmap: the relative shift of every pixel in the output image from its source in an input image. We describe a new representation of these operations as an optimal graph labeling, where the shift-map represents the selected label for each output pixel. Two terms are used in computing the optimal shift-map: (i) A data term which indicates constraints such as the change in image size, object rearrangement, a possible saliency map, etc. (ii) A smoothness term, minimizing the new discontinuities in the output image caused by discontinuities in the shift-map. This graph labeling problem can be solved using graph cuts. Since the optimization is global and discrete, it outperforms state of the art methods in most cases. Efficient hierarchical solutions for graph-cuts are presented, and operations on 1M images can take only a few seconds.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459159",
        "reference_list": [],
        "citation": {
            "ieee": 42,
            "other": 82,
            "total": 124
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Optimization methods",
                "Pixel",
                "Computer vision",
                "Image segmentation",
                "Computer science",
                "Crops",
                "Greedy algorithms",
                "Face detection",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "graph theory",
                "image restoration",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shift map",
                "optimal graph labeling",
                "optimization",
                "graph-cuts",
                "image editing",
                "geometric image rearrangement"
            ]
        },
        "id": 19,
        "cited_by": [
            {
                "year": "2017",
                "id": 478
            },
            {
                "year": "2011",
                "id": 191
            }
        ]
    },
    {
        "title": "Looking around the corner using transient imaging",
        "authors": [
            "Ahmed Kirmani",
            "Tyler Hutchison",
            "James Davis",
            "Ramesh Raskar"
        ],
        "abstract": "We show that multi-path analysis using images from a timeof-flight (ToF) camera provides a tantalizing opportunity to infer about 3D geometry of not only visible but hidden parts of a scene. We provide a novel framework for reconstructing scene geometry from a single viewpoint using a camera that captures a 3D time-image I(x, y, t) for each pixel. We propose a framework that uses the time-image and transient reasoning to expose scene properties that may be beyond the reach of traditional computer vision. We corroborate our theory with free space hardware experiments using a femtosecond laser and an ultrafast photo detector array. The ability to compute the geometry of hidden elements, unobservable by both the camera and illumination source, will create a range of new computer vision opportunities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459160",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 17,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Computer vision",
                "Geometry",
                "Optical imaging",
                "Image analysis",
                "Image reconstruction",
                "Hardware",
                "Laser theory",
                "Optical arrays"
            ]
        },
        "id": 20,
        "cited_by": []
    },
    {
        "title": "Modeling deformable objects from a single depth camera",
        "authors": [
            "Miao Liao",
            "Qing Zhang",
            "Huamin Wang",
            "Ruigang Yang",
            "Minglun Gong"
        ],
        "abstract": "We propose a novel approach to reconstruct complete 3D deformable models over time by a single depth camera, provided that most parts of the models are observed by the camera at least once. The core of this algorithm is based on the assumption that the deformation is continuous and predictable in a short temporal interval. While the camera can only capture part of a whole surface at any time instant, partial surfaces reconstructed from different times are assembled together to form a complete 3D surface for each time instant, even when the shape is under severe deformation. A mesh warping algorithm based on linear mesh deformation is used to align different partial surfaces. A volumetric method is then used to combine partial surfaces, fix missing holes, and smooth alignment errors. Our experiment shows that this approach is able to reconstruct visually plausible 3D surface deformation results with a single camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459161",
        "reference_list": [
            {
                "year": "2007",
                "id": 94
            },
            {
                "year": "2007",
                "id": 94
            },
            {
                "year": "2005",
                "id": 165
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 9,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Cameras",
                "Surface reconstruction",
                "Layout",
                "Assembly",
                "Shape",
                "Stereo vision",
                "Fuses",
                "Image sequences",
                "Focusing"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D deformable object modeling",
                "single depth camera",
                "partial surface reconstruction",
                "mesh warping algorithm",
                "linear mesh deformation",
                "volumetric method",
                "alignment errors",
                "visually plausible 3D surface deformation"
            ]
        },
        "id": 21,
        "cited_by": [
            {
                "year": "2017",
                "id": 94
            },
            {
                "year": "2015",
                "id": 243
            },
            {
                "year": "2015",
                "id": 256
            },
            {
                "year": "2015",
                "id": 344
            },
            {
                "year": "2011",
                "id": 92
            }
        ]
    },
    {
        "title": "A prism-based system for multispectral video acquisition",
        "authors": [
            "Hao Du",
            "Xin Tong",
            "Xun Cao",
            "Stephen Lin"
        ],
        "abstract": "In this paper, we propose a prism-based system for capturing multispectral videos. The system consists of a triangular prism, a monochrome camera, and an occlusion mask. Incoming light beams from the scene are sampled by the occlusion mask, dispersed into their constituent spectra by the triangular prism, and then captured by the monochrome camera. Our system is capable of capturing videos of high spectral resolution. It also allows for different tradeoffs between spectral and spatial resolution by adjusting the focal length of the camera. We demonstrate the effectiveness of our system with several applications, including human skin detection, physical material recognition, and RGB video generation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459162",
        "reference_list": [
            {
                "year": "2007",
                "id": 250
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Multispectral imaging",
                "Spatial resolution",
                "Lighting",
                "Humans",
                "Asia",
                "Skin",
                "Image color analysis",
                "Band pass filters"
            ]
        },
        "id": 22,
        "cited_by": [
            {
                "year": "2017",
                "id": 332
            }
        ]
    },
    {
        "title": "Estimating natural illumination from a single outdoor image",
        "authors": [
            "Jean-Fran\u00e7ois Lalonde",
            "Alexei A. Efros",
            "Srinivasa G. Narasimhan"
        ],
        "abstract": "Given a single outdoor image, we present a method for estimating the likely illumination conditions of the scene. In particular, we compute the probability distribution over the sun position and visibility. The method relies on a combination of weak cues that can be extracted from different portions of the image: the sky, the vertical surfaces, and the ground. While no single cue can reliably estimate illumination by itself, each one can reinforce the others to yield a more robust estimate. This is combined with a data-driven prior computed over a dataset of 6 million Internet photos. We present quantitative results on a webcam dataset with annotated sun positions, as well as qualitative results on consumer-grade photographs downloaded from Internet. Based on the estimated illumination, we show how to realistically insert synthetic 3-D objects into the scene.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459163",
        "reference_list": [
            {
                "year": "2005",
                "id": 34
            },
            {
                "year": "2001",
                "id": 112
            },
            {
                "year": "2005",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 30,
            "other": 19,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Sun",
                "Layout",
                "Yield estimation",
                "Internet",
                "Atmosphere",
                "Humans",
                "Computer science",
                "Distributed computing",
                "Probability distribution"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "lighting",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "natural illumination estimation",
                "single outdoor image",
                "probability distribution",
                "Internet photos",
                "synthetic 3-D objects",
                "Webcam dataset"
            ]
        },
        "id": 23,
        "cited_by": []
    },
    {
        "title": "A linear formulation of shape from specular flow",
        "authors": [
            "Guillermo D. Canas",
            "Yuriy Vasilyev",
            "Yair Adato",
            "Todd Zickler",
            "Steven Gortler",
            "Ohad Ben-Shahar"
        ],
        "abstract": "When a curved mirror-like surface moves relative to its environment, it induces a motion field-or specular flow- on the image plane that observes it. This specular flow is related to the mirror's shape through a non-linear partial differential equation, and there is interest in understanding when and how this equation can be solved for surface shape. Existing analyses of this `shape from specular flow equation' have focused on closed-form solutions, and while they have yielded insight, their critical reliance on externally-provided initial conditions and/or specific motions makes them difficult to apply in practice. This paper resolves these issues. We show that a suitable reparameterization leads to a linear formulation of the shape from specular flow equation. This formulation radically simplifies the reconstruction process and allows, for example, both motion and shape to be recovered from as few as two specular flows even when no externally-provided initial conditions are available. Our analysis moves us closer to a practical method for recovering shape from specular flow that operates under arbitrary, unknown motions in unknown illumination environments and does not require additional shape information from other sources.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459164",
        "reference_list": [
            {
                "year": "2007",
                "id": 44
            },
            {
                "year": "2005",
                "id": 189
            },
            {
                "year": "2005",
                "id": 15
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 4,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Partial differential equations",
                "Nonlinear equations",
                "Motion analysis",
                "Differential equations",
                "Image motion analysis",
                "Closed-form solution",
                "Image reconstruction",
                "Information analysis",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "nonlinear differential equations",
                "partial differential equations",
                "shapes (structures)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear shape formulation",
                "specular flow equation",
                "nonlinear partial differential equation",
                "reconstruction process"
            ]
        },
        "id": 24,
        "cited_by": [
            {
                "year": "2011",
                "id": 73
            }
        ]
    },
    {
        "title": "Landmark-based sparse color representations for color transfer",
        "authors": [
            "Tzu-Wei Huang",
            "Hwann-Tzong Chen"
        ],
        "abstract": "We present a novel image representation that characterizes a color image by an intensity image and a small number of color pixels. Our idea is based on solving an inverse problem of colorization: Given a color image, we seek to obtain an intensity image and a small subset of color pixels, which are called landmark pixels, so that the input color image can be recovered faithfully using the intensity image and the color cues provided by the selected landmark pixels. We develop an algorithm to derive the landmark-based sparse color representations from color images, and use the representations in the applications of color transfer and color correction. The computational cost for these applications is low owing to the sparsity of the proposed representation. The landmark-based representation is also preferable to statistics-based representations, e.g. color histograms and Gaussian mixture models, when we need to reconstruct the color image from a given representation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459165",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            },
            {
                "year": "2005",
                "id": 187
            },
            {
                "year": "2001",
                "id": 32
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Color",
                "Pixel",
                "Histograms",
                "Gray-scale",
                "Image representation",
                "Inverse problems",
                "Image reconstruction",
                "Image retrieval",
                "Computer vision",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image representation",
                "inverse problems",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "landmark based sparse color representation",
                "color transfer",
                "image representation",
                "intensity image",
                "colorization inverse problem",
                "color correction",
                "statistics based representation"
            ]
        },
        "id": 25,
        "cited_by": []
    },
    {
        "title": "Stereo from flickering caustics",
        "authors": [
            "Yohay Swirski",
            "Yoav Y. Schechner",
            "Ben Herzberg",
            "Shahriar Negahdaripour"
        ],
        "abstract": "Underwater, natural illumination typically varies strongly temporally and spatially. The reason is that waves on the water surface refract light into the water in a spatiotemporally varying manner. The resulting underwater illumination field is known as underwater caustics or flicker. In past studies, flicker has often been considered to be an undesired effect, which degrades the quality of images. In contrast, in this work, we show that flicker can actually be useful for vision in the underwater domain. Specifically, it solves very simply, accurately, and densely the stereo correspondence problem, irrespective of the object's texture. The temporal radiance variations due to flicker are unique to each object point, thus disambiguating the correspondence, with very simple calculations. This process is further enhanced by compounding the spatial variability in the flicker field. The method is demonstrated by underwater in-situ experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459166",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 1,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Stereo vision",
                "Computer vision",
                "Lighting",
                "Light scattering",
                "Optical refraction",
                "Surface waves",
                "Spatiotemporal phenomena",
                "Degradation",
                "Robustness",
                "Biological system modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "flickering caustics",
                "surface refract light",
                "underwater illumination field",
                "image quality",
                "object texture",
                "temporal radiance variations"
            ]
        },
        "id": 26,
        "cited_by": [
            {
                "year": "2011",
                "id": 142
            }
        ]
    },
    {
        "title": "Learning a dense multi-view representation for detection, viewpoint classification and synthesis of object categories",
        "authors": [
            "Hao Su",
            "Min Sun",
            "Li Fei-Fei",
            "Silvio Savarese"
        ],
        "abstract": "Recognizing object classes and their 3D viewpoints is an important problem in computer vision. Based on a part-based probabilistic representation [31], we propose a new 3D object class model that is capable of recognizing unseen views by pose estimation and synthesis. We achieve this by using a dense, multiview representation of the viewing sphere parameterized by a triangular mesh of viewpoints. Each triangle of viewpoints can be morphed to synthesize new viewpoints. By incorporating 3D geometrical constraints, our model establishes explicit correspondences among object parts across viewpoints. We propose an incremental learning algorithm to train the generative model. A cellphone video clip of an object is first used to initialize model learning. Then the model is updated by a set of unsorted training images without viewpoint labels. We demonstrate the robustness of our model on object detection, viewpoint classification and synthesis tasks. Our model performs superiorly to and on par with state-of-the-art algorithms on the Savarese et al. 2007 and PASCAL datasets in object detection. It outperforms all previous work in viewpoint classification and offers promising results in viewpoint synthesis.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459168",
        "reference_list": [
            {
                "year": "2007",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 7,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Computer science",
                "Computer vision",
                "Cellular phones",
                "Robustness",
                "Mesh generation",
                "Solid modeling",
                "Image segmentation",
                "Image recognition",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object detection",
                "object recognition",
                "pose estimation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense multiview representation Learning",
                "3D viewpoint classification",
                "viewpoint detection",
                "object categories synthesis",
                "object classes recognition",
                "computer vision",
                "part based probabilistic representation",
                "3D object class model",
                "pose estimation",
                "pose synthesis",
                "viewpoint triangular mesh",
                "3D geometrical constraints",
                "incremental learning algorithm",
                "cellphone video clip"
            ]
        },
        "id": 27,
        "cited_by": [
            {
                "year": "2011",
                "id": 122
            },
            {
                "year": "2011",
                "id": 124
            },
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2011",
                "id": 331
            }
        ]
    },
    {
        "title": "On feature combination for multiclass object classification",
        "authors": [
            "Peter Gehler",
            "Sebastian Nowozin"
        ],
        "abstract": "A key ingredient in the design of visual object classification systems is the identification of relevant class specific aspects while being robust to intra-class variations. While this is a necessity in order to generalize beyond a given set of training images, it is also a very difficult problem due to the high variability of visual appearance within each class. In the last years substantial performance gains on challenging benchmark datasets have been reported in the literature. This progress can be attributed to two developments: the design of highly discriminative and robust image features and the combination of multiple complementary features based on different aspects such as shape, color or texture. In this paper we study several models that aim at learning the correct weighting of different features from training data. These include multiple kernel learning as well as simple baseline methods. Furthermore we derive ensemble methods inspired by Boosting which are easily extendable to several multiclass setting. All methods are thoroughly evaluated on object classification datasets using a multitude of feature descriptors. The key results are that even very simple baseline methods, that are orders of magnitude faster than learning techniques are highly competitive with multiple kernel learning. Furthermore the Boosting type methods are found to produce consistently better results in all experiments. We provide insight of when combination methods can be expected to work and how the benefit of complementary features can be exploited most efficiently.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459169",
        "reference_list": [
            {
                "year": "2007",
                "id": 225
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 258,
            "other": 189,
            "total": 447
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Computer vision",
                "Robustness",
                "Shape",
                "Training data",
                "Boosting",
                "Cybernetics",
                "Performance gain",
                "Object recognition",
                "Image classification"
            ]
        },
        "id": 28,
        "cited_by": [
            {
                "year": "2015",
                "id": 112
            },
            {
                "year": "2015",
                "id": 339
            },
            {
                "year": "2013",
                "id": 35
            },
            {
                "year": "2013",
                "id": 100
            },
            {
                "year": "2013",
                "id": 102
            },
            {
                "year": "2013",
                "id": 208
            },
            {
                "year": "2013",
                "id": 320
            },
            {
                "year": "2013",
                "id": 336
            },
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2011",
                "id": 236
            },
            {
                "year": "2011",
                "id": 259
            },
            {
                "year": "2011",
                "id": 328
            },
            {
                "year": "2011",
                "id": 337
            },
            {
                "year": "2011",
                "id": 338
            }
        ]
    },
    {
        "title": "Discriminative models for multi-class object layout",
        "authors": [
            "Chaitanya Desai",
            "Deva Ramanan",
            "Charless Fowlkes"
        ],
        "abstract": "Many state-of-the-art approaches for object recognition reduce the problem to a 0-1 classification task. Such reductions allow one to leverage sophisticated classifiers for learning. These models are typically trained independently for each class using positive and negative examples cropped from images. At test-time, various post-processing heuristics such as non-maxima suppression (NMS) are required to reconcile multiple detections within and between different classes for each image. Though crucial to good performance on benchmarks, this post-processing is usually defined heuristically. We introduce a unified model for multi-class object recognition that casts the problem as a structured prediction task. Rather than predicting a binary label for each image window independently, our model simultaneously predicts a structured labeling of the entire image. Our model learns statistics that capture the spatial arrangements of various object classes in real images, both in terms of which arrangements to suppress through NMS and which arrangements to favor through spatial co-occurrence statistics. We formulate parameter estimation in our model as a max-margin learning problem. Given training images with ground-truth object locations, we show how to formulate learning as a convex optimization problem. We employ a cutting plane algorithm similar to efficiently learn a model from thousands of training images. We show state-of-the-art results on the PASCAL VOC benchmark that indicate the benefits of learning a global model encapsulating the spatial layout of multiple object classes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459256",
        "reference_list": [
            {
                "year": "2005",
                "id": 168
            }
        ],
        "citation": {
            "ieee": 116,
            "other": 54,
            "total": 170
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Object recognition",
                "Detectors",
                "Benchmark testing",
                "Predictive models",
                "Statistics",
                "Face detection",
                "Image edge detection",
                "Computer science",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image classification",
                "learning (artificial intelligence)",
                "object recognition",
                "parameter estimation",
                "statistics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiclass object recognition",
                "nonmaxima suppression",
                "spatial co-occurrence statistics",
                "parameter estimation",
                "max-margin learning",
                "convex optimization",
                "cutting plane algorithm",
                "PASCAL VOC benchmark"
            ]
        },
        "id": 29,
        "cited_by": [
            {
                "year": "2015",
                "id": 223
            },
            {
                "year": "2015",
                "id": 293
            },
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2013",
                "id": 46
            },
            {
                "year": "2013",
                "id": 270
            },
            {
                "year": "2013",
                "id": 453
            },
            {
                "year": "2011",
                "id": 64
            },
            {
                "year": "2011",
                "id": 71
            },
            {
                "year": "2011",
                "id": 105
            },
            {
                "year": "2011",
                "id": 232
            },
            {
                "year": "2011",
                "id": 308
            }
        ]
    },
    {
        "title": "Combining efficient object localization and image classification",
        "authors": [
            "Hedi Harzallah",
            "Fr\u00e9d\u00e9ric Jurie",
            "Cordelia Schmid"
        ],
        "abstract": "In this paper we present a combined approach for object localization and classification. Our contribution is twofold. (a) A contextual combination of localization and classification which shows that classification can improve detection and vice versa. (b) An efficient two stage sliding window object localization method that combines the efficiency of a linear classifier with the robustness of a sophisticated non-linear one. Experimental results evaluate the parameters of our two stage sliding window approach and show that our combined object localization and classification methods outperform the state-of-the-art on the PASCAL VOC 2007 and 2008 datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459257",
        "reference_list": [
            {
                "year": "2005",
                "id": 168
            }
        ],
        "citation": {
            "ieee": 94,
            "other": 58,
            "total": 152
        },
        "keywords": {
            "IEEE Keywords": [
                "Image classification",
                "Object detection",
                "Detectors",
                "Image segmentation",
                "Support vector machines",
                "Support vector machine classification",
                "Layout",
                "Robustness",
                "Machine learning algorithms",
                "Machine learning"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object localization method",
                "image classification",
                "two stage sliding window approach",
                "linear classifier",
                "PASCAL VOC 2007 datasets",
                "PASCAL VOC 2008 datasets"
            ]
        },
        "id": 30,
        "cited_by": [
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2013",
                "id": 266
            },
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2011",
                "id": 39
            },
            {
                "year": "2011",
                "id": 226
            },
            {
                "year": "2011",
                "id": 238
            }
        ]
    },
    {
        "title": "Storyboard sketches for Content Based Video Retrieval",
        "authors": [
            "J. P. Collomosse",
            "G. McNeill",
            "Y. Qian"
        ],
        "abstract": "We present a novel Content Based Video Retrieval (CBVR) system, driven by free-hand sketch queries depicting both objects and their movement (via dynamic cues; streak-lines and arrows). Our main contribution is a probabilistic model of video clips (based on Linear Dynamical Systems), leading to an algorithm for matching descriptions of sketched objects to video. We demonstrate our model fitting to clips under static and moving camera conditions, exhibiting linear and oscillatory motion. We evaluate retrieval on two real video data sets, and on a video data set exhibiting controlled variation in shape, color, motion and clutter.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459258",
        "reference_list": [
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2003",
                "id": 1
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Content based retrieval",
                "Shape control",
                "Signal processing algorithms",
                "Cameras",
                "Image retrieval",
                "Layout",
                "Computer vision",
                "Speech processing",
                "Video signal processing",
                "Information retrieval"
            ]
        },
        "id": 31,
        "cited_by": [
            {
                "year": "2017",
                "id": 280
            }
        ]
    },
    {
        "title": "Image sequence geolocation with human travel priors",
        "authors": [
            "Evangelos Kalogerakis",
            "Olga Vesselova",
            "James Hays",
            "Alexei A. Efros",
            "Aaron Hertzmann"
        ],
        "abstract": "This paper presents a method for estimating geographic location for sequences of time-stamped photographs. A prior distribution over travel describes the likelihood of traveling from one location to another during a given time interval. This distribution is based on a training database of 6 million photographs from Flickr.com. An image likelihood for each location is defined by matching a test photograph against the training database. Inferring location for images in a test sequence is then performed using the Forward-Backward algorithm, and the model can be adapted to individual users as well. Using temporal constraints allows our method to geolocate images without recognizable landmarks, and images with no geographic cues whatsoever. This method achieves a substantial performance improvement over the best-available baseline, and geolocates some users' images with near-perfect accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459259",
        "reference_list": [],
        "citation": {
            "ieee": 38,
            "other": 46,
            "total": 84
        },
        "keywords": {
            "IEEE Keywords": [
                "Image sequences",
                "Humans",
                "Image databases",
                "Hidden Markov models",
                "Testing",
                "Computer vision",
                "Image recognition",
                "Earth",
                "Application software",
                "Performance evaluation"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image sequence geolocation",
                "human travel priors",
                "geographic location estimation",
                "training database",
                "image likelihood",
                "forward-backward algorithm",
                "test sequence",
                "temporal constraints",
                "time-stamped photographs sequence"
            ]
        },
        "id": 32,
        "cited_by": [
            {
                "year": "2015",
                "id": 188
            }
        ]
    },
    {
        "title": "You'll never walk alone: Modeling social behavior for multi-target tracking",
        "authors": [
            "S. Pellegrini",
            "A. Ess",
            "K. Schindler",
            "L. van Gool"
        ],
        "abstract": "Object tracking typically relies on a dynamic model to predict the object's location from its past trajectory. In crowded scenarios a strong dynamic model is particularly important, because more accurate predictions allow for smaller search regions, which greatly simplifies data association. Traditional dynamic models predict the location for each target solely based on its own history, without taking into account the remaining scene objects. Collisions are resolved only when they happen. Such an approach ignores important aspects of human behavior: people are driven by their future destination, take into account their environment, anticipate collisions, and adjust their trajectories at an early stage in order to avoid them. In this work, we introduce a model of dynamic social behavior, inspired by models developed for crowd simulation. The model is trained with videos recorded from birds-eye view at busy locations, and applied as a motion model for multi-people tracking from a vehicle-mounted camera. Experiments on real sequences show that accounting for social interactions and scene knowledge improves tracking performance, especially during occlusions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459260",
        "reference_list": [
            {
                "year": "2009",
                "id": 194
            }
        ],
        "citation": {
            "ieee": 180,
            "other": 129,
            "total": 309
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Vehicle dynamics",
                "Layout",
                "Humans",
                "Computer vision",
                "Trajectory",
                "Cameras",
                "Legged locomotion",
                "Path planning",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic social behavior",
                "multitarget tracking",
                "object tracking",
                "crowd simulation",
                "motion model",
                "vehicle-mounted camera",
                "social interaction",
                "scene knowledge"
            ]
        },
        "id": 33,
        "cited_by": [
            {
                "year": "2017",
                "id": 268
            },
            {
                "year": "2015",
                "id": 338
            },
            {
                "year": "2015",
                "id": 350
            },
            {
                "year": "2015",
                "id": 485
            },
            {
                "year": "2013",
                "id": 287
            },
            {
                "year": "2013",
                "id": 381
            },
            {
                "year": "2013",
                "id": 437
            },
            {
                "year": "2011",
                "id": 78
            },
            {
                "year": "2011",
                "id": 94
            }
        ]
    },
    {
        "title": "An efficient algorithm for Co-segmentation",
        "authors": [
            "Dorit S. Hochbaum",
            "Vikas Singh"
        ],
        "abstract": "This paper is focused on the Co-segmentation problem [1] - where the objective is to segment a similar object from a pair of images. The background in the two images may be arbitrary; therefore, simultaneous segmentation of both images must be performed with a requirement that the appearance of the two sets of foreground pixels in the respective images are consistent. Existing approaches [1, 2] cast this problem as a Markov Random Field (MRF) based segmentation of the image pair with a regularized difference of the two histograms - assuming a Gaussian prior on the foreground appearance [1] or by calculating the sum of squared differences [2]. Both are interesting formulations but lead to difficult optimization problems, due to the presence of the second (histogram difference) term. The model proposed here bypasses measurement of the histogram differences in a direct fashion; we show that this enables obtaining efficient solutions to the underlying optimization model. Our new algorithm is similar to the existing methods in spirit, but differs substantially in that it can be solved to optimality in polynomial time using a maximum flow procedure on an appropriately constructed graph. We discuss our ideas and present promising experimental results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459261",
        "reference_list": [
            {
                "year": "2005",
                "id": 97
            },
            {
                "year": "2007",
                "id": 71
            }
        ],
        "citation": {
            "ieee": 85,
            "other": 47,
            "total": 132
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Histograms",
                "Pixel",
                "Computer vision",
                "Hydrogen",
                "Biomedical informatics",
                "Computer industry",
                "Operations research",
                "Markov random fields",
                "Polynomials"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "graph theory",
                "image segmentation",
                "Markov processes",
                "object detection",
                "optimisation",
                "polynomial approximation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "co-segmentation algorithm",
                "Markov random field",
                "image segmentation",
                "histograms",
                "Gaussian process",
                "optimization problems",
                "polynomial time",
                "maximum flow procedure",
                "graph",
                "object segmentation"
            ]
        },
        "id": 34,
        "cited_by": [
            {
                "year": "2013",
                "id": 49
            },
            {
                "year": "2013",
                "id": 105
            },
            {
                "year": "2013",
                "id": 290
            },
            {
                "year": "2013",
                "id": 329
            },
            {
                "year": "2011",
                "id": 21
            },
            {
                "year": "2011",
                "id": 96
            },
            {
                "year": "2011",
                "id": 328
            }
        ]
    },
    {
        "title": "Image segmentation with a bounding box prior",
        "authors": [
            "Victor Lempitsky",
            "Pushmeet Kohli",
            "Carsten Rother",
            "Toby Sharp"
        ],
        "abstract": "User-provided object bounding box is a simple and popular interaction paradigm considered by many existing interactive image segmentation frameworks. However, these frameworks tend to exploit the provided bounding box merely to exclude its exterior from consideration and sometimes to initialize the energy minimization. In this paper, we discuss how the bounding box can be further used to impose a powerful topological prior, which prevents the solution from excessive shrinking and ensures that the user-provided box bounds the segmentation in a sufficiently tight way. The prior is expressed using hard constraints incorporated into the global energy minimization framework leading to an NP-hard integer program. We then investigate the possible optimization strategies including linear relaxation as well as a new graph cut algorithm called pinpointing. The latter can be used either as a rounding method for the fractional LP solution, which is provably better than thresholding-based rounding, or as a fast standalone heuristic. We evaluate the proposed algorithms on a publicly available dataset, and demonstrate the practical benefits of the new prior both qualitatively and quantitatively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459262",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2005",
                "id": 120
            }
        ],
        "citation": {
            "ieee": 118,
            "other": 65,
            "total": 183
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Iterative algorithms",
                "Power generation economics",
                "Mice",
                "Active contours",
                "Computer vision",
                "Linear programming",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "graph theory",
                "image segmentation",
                "integer programming",
                "linear programming",
                "relaxation theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "bounding box prior",
                "object bounding box",
                "interaction paradigm",
                "topological prior",
                "global energy minimization framework",
                "NP-hard integer program",
                "optimization strategy",
                "linear relaxation",
                "graph cut algorithm",
                "pinpointing",
                "rounding method",
                "fractional LP solution",
                "standalone heuristic"
            ]
        },
        "id": 35,
        "cited_by": [
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2015",
                "id": 173
            },
            {
                "year": "2015",
                "id": 194
            },
            {
                "year": "2013",
                "id": 163
            },
            {
                "year": "2013",
                "id": 271
            },
            {
                "year": "2011",
                "id": 46
            },
            {
                "year": "2011",
                "id": 228
            },
            {
                "year": "2011",
                "id": 284
            }
        ]
    },
    {
        "title": "Globally optimal segmentation of multi-region objects",
        "authors": [
            "Andrew Delong",
            "Yuri Boykov"
        ],
        "abstract": "Many objects contain spatially distinct regions, each with a unique colour/texture model. Mixture models ignore the spatial distribution of colours within an object, and thus cannot distinguish between coherent parts versus randomly distributed colours. We show how to encode geometric interactions between distinct region+boundary models, such as regions being interior/exterior to each other along with preferred distances between their boundaries. With a single graph cut, our method extracts only those multi-region objects that satisfy such a combined model. We show applications in medical segmentation and scene layout estimation. Unlike Li et al. we do not need \u201cdomain unwrapping\u201d nor do we have topological limits on shapes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459263",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2007",
                "id": 71
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 38,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image segmentation",
                "Solid modeling",
                "Robustness",
                "Layout",
                "Constraint optimization",
                "Dynamic programming",
                "Tree graphs",
                "Active contours",
                "Level set"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "graph theory",
                "image colour analysis",
                "image segmentation",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "globally optimal segmentation",
                "multiregion objects",
                "colour-texture model",
                "randomly distributed colours",
                "distinct region-boundary models",
                "geometric interactions",
                "graph cut",
                "scene layout estimation",
                "medical segmentation",
                "domain unwrapping"
            ]
        },
        "id": 36,
        "cited_by": [
            {
                "year": "2015",
                "id": 197
            },
            {
                "year": "2013",
                "id": 253
            },
            {
                "year": "2011",
                "id": 266
            }
        ]
    },
    {
        "title": "Similarity metrics for categorization: From monolithic to category specific",
        "authors": [
            "Boris Babenko",
            "Steve Branson",
            "Serge Belongie"
        ],
        "abstract": "Similarity metrics that are learned from labeled training data can be advantageous in terms of performance and/or efficiency. These learned metrics can then be used in conjunction with a nearest neighbor classifier, or can be plugged in as kernels to an SVM. For the task of categorization two scenarios have thus far been explored. The first is to train a single \u201cmonolithic\u201d similarity metric that is then used for all examples. The other is to train a metric for each category in a 1-vs-all manner. While the former approach seems to be at a disadvantage in terms of performance, the latter is not practical for large numbers of categories. In this paper we explore the space in between these two extremes. We present an algorithm that learns a few similarity metrics, while simultaneously grouping categories together and assigning one of these metrics to each group. We present promising results and show how the learned metrics generalize to novel categories.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459264",
        "reference_list": [
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 9,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Machine learning",
                "Training data",
                "Kernel",
                "Computer vision",
                "Nearest neighbor searches",
                "Support vector machines",
                "Support vector machine classification",
                "Space exploration",
                "Object recognition",
                "Focusing"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object recognition",
                "pattern classification",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nearest neighbor classifier",
                "SVM",
                "single monolithic similarity metric",
                "support vector machine",
                "object recognition"
            ]
        },
        "id": 37,
        "cited_by": [
            {
                "year": "2011",
                "id": 20
            }
        ]
    },
    {
        "title": "Local distance functions: A taxonomy, new algorithms, and an evaluation",
        "authors": [
            "Deva Ramanan",
            "Simon Baker"
        ],
        "abstract": "We present a taxonomy for local distance functions where most existing algorithms can be regarded as approximations of the geodesic distance defined by a metric tensor. We categorize existing algorithms by how, where and when they estimate the metric tensor. We also extend the taxonomy along each axis. How: We introduce hybrid algorithms that use a combination of dimensionality reduction and metric learning to ameliorate over-fitting. Where: We present an exact polynomial time algorithm to integrate the metric tensor along the lines between the test and training points under the assumption that the metric tensor is piecewise constant. When: We propose an interpolation algorithm where the metric tensor is sampled at a number of references points during the offline phase, which are then interpolated during online classification. We also present a comprehensive evaluation of all the algorithms on tasks in face recognition, object recognition, and digit recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459265",
        "reference_list": [
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2007",
                "id": 201
            },
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Taxonomy",
                "Tensile stress",
                "Testing",
                "Polynomials",
                "Face recognition",
                "Training data",
                "Linear discriminant analysis",
                "Phase estimation",
                "Interpolation",
                "Object recognition"
            ]
        },
        "id": 38,
        "cited_by": []
    },
    {
        "title": "TagProp: Discriminative metric learning in nearest neighbor models for image auto-annotation",
        "authors": [
            "Matthieu Guillaumin",
            "Thomas Mensink",
            "Jakob Verbeek",
            "Cordelia Schmid"
        ],
        "abstract": "Image auto-annotation is an important open problem in computer vision. For this task we propose TagProp, a discriminatively trained nearest neighbor model. Tags of test images are predicted using a weighted nearest-neighbor model to exploit labeled training images. Neighbor weights are based on neighbor rank or distance. TagProp allows the integration of metric learning by directly maximizing the log-likelihood of the tag predictions in the training set. In this manner, we can optimally combine a collection of image similarity metrics that cover different aspects of image content, such as local shape descriptors, or global color histograms. We also introduce a word specific sigmoidal modulation of the weighted neighbor tag predictions to boost the recall of rare words. We investigate the performance of different variants of our model and compare to existing work. We present experimental results for three challenging data sets. On all three, TagProp makes a marked improvement as compared to the current state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459266",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 167,
            "other": 161,
            "total": 328
        },
        "keywords": {
            "IEEE Keywords": [
                "Nearest neighbor searches",
                "Predictive models",
                "Computer vision",
                "Testing",
                "Shape",
                "Histograms",
                "Vocabulary",
                "Video sharing",
                "Content management",
                "Large-scale systems"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "TagProp",
                "discriminative metric learning",
                "image auto-annotation",
                "computer vision",
                "weighted nearest-neighbor model",
                "tag predictions",
                "image similarity metrics",
                "word specific sigmoidal modulation"
            ]
        },
        "id": 39,
        "cited_by": [
            {
                "year": "2017",
                "id": 55
            },
            {
                "year": "2015",
                "id": 464
            },
            {
                "year": "2015",
                "id": 516
            },
            {
                "year": "2013",
                "id": 200
            },
            {
                "year": "2011",
                "id": 77
            },
            {
                "year": "2011",
                "id": 81
            },
            {
                "year": "2011",
                "id": 82
            }
        ]
    },
    {
        "title": "Sparsity induced similarity measure for label propagation",
        "authors": [
            "Hong Cheng",
            "Zicheng Liu",
            "Jie Yang"
        ],
        "abstract": "Graph-based semi-supervised learning has gained considerable interests in the past several years thanks to its effectiveness in combining labeled and unlabeled data through label propagation for better object modeling and classification. A critical issue in constructing a graph is the weight assignment where the weight of an edge specifies the similarity between two data points. In this paper, we present a novel technique to measure the similarities among data points by decomposing each data point as an L1 sparse linear combination of the rest of the data points. The main idea is that the coefficients in such a sparse decomposition reflect the point's neighborhood structure thus providing better similarity measures among the decomposed data point and the rest of the data points. The proposed approach is evaluated on four commonly-used data sets and the experimental results show that the proposed Sparsity Induced Similarity (SIS) measure significantly improves label propagation performance. As an application of the SIS-based label propagation, we show that the SIS measure can be used to improve the Bag-of-Words approach for scene classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459267",
        "reference_list": [],
        "citation": {
            "ieee": 41,
            "other": 29,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Semisupervised learning",
                "Nearest neighbor searches",
                "Euclidean distance",
                "Training data",
                "Gain measurement",
                "Layout",
                "Kernel",
                "Phase estimation",
                "Phase measurement",
                "Pattern recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "pattern recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparsity induced similarity measure",
                "label propagation",
                "graph-based semi-supervised learning",
                "sparse linear combination",
                "sparse decomposition",
                "bag-of-words approach",
                "scene classification"
            ]
        },
        "id": 40,
        "cited_by": [
            {
                "year": "2015",
                "id": 308
            },
            {
                "year": "2011",
                "id": 30
            }
        ]
    },
    {
        "title": "Coded aperture pairs for depth from defocus",
        "authors": [
            "Changyin Zhou",
            "Stephen Lin",
            "Shree Nayar"
        ],
        "abstract": "The classical approach to depth from defocus uses two images taken with circular apertures of different sizes. We show in this paper that the use of a circular aperture severely restricts the accuracy of depth from defocus. We derive a criterion for evaluating a pair of apertures with respect to the precision of depth recovery. This criterion is optimized using a genetic algorithm and gradient descent search to arrive at a pair of high resolution apertures. The two coded apertures are found to complement each other in the scene frequencies they preserve. This property enables them to not only recover depth with greater fidelity but also obtain a high quality all-focused image from the two captured images. Extensive simulations as well as experiments on a variety of scenes demonstrate the benefits of using the coded apertures over conventional circular apertures.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459268",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Apertures",
                "Semisupervised learning",
                "Nearest neighbor searches",
                "Euclidean distance",
                "Training data",
                "Gain measurement",
                "Layout",
                "Kernel",
                "Phase estimation",
                "Phase measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "genetic algorithms",
                "gradient methods",
                "image coding",
                "image resolution",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coded aperture pairs",
                "circular apertures",
                "depth recovery",
                "genetic algorithm",
                "gradient descent search",
                "high resolution apertures"
            ]
        },
        "id": 41,
        "cited_by": [
            {
                "year": "2015",
                "id": 398
            },
            {
                "year": "2011",
                "id": 80
            }
        ]
    },
    {
        "title": "Time-constrained photography",
        "authors": [
            "Samuel W. Hasinoff",
            "Kiriakos N. Kutulakos",
            "Fr\u00e9do Durand",
            "William T. Freeman"
        ],
        "abstract": "Capturing multiple photos at different focus settings is a powerful approach for reducing optical blur, but how many photos should we capture within a fixed time budget? We develop a framework to analyze optimal capture strategies balancing the tradeoff between defocus and sensor noise, incorporating uncertainty in resolving scene depth. We derive analytic formulas for restoration error and use Monte Carlo integration over depth to derive optimal capture strategies for different camera designs, under a wide range of photographic scenarios. We also derive a new upper bound on how well spatial frequencies can be preserved over the depth of field. Our results show that by capturing the optimal number of photos, a standard camera can achieve performance at the level of more complex computational cameras, in all but the most demanding of cases. We also show that computational cameras, although specifically designed to improve one-shot performance, generally benefit from capturing multiple photos as well.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459269",
        "reference_list": [
            {
                "year": "2007",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 16,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Photography",
                "Cameras",
                "Optical noise",
                "Optical sensors",
                "Uncertainty",
                "Spatial resolution",
                "Layout",
                "Monte Carlo methods",
                "Upper bound",
                "Frequency"
            ]
        },
        "id": 42,
        "cited_by": [
            {
                "year": "2017",
                "id": 24
            },
            {
                "year": "2017",
                "id": 169
            }
        ]
    },
    {
        "title": "Light field video stabilization",
        "authors": [
            "Brandon M. Smith",
            "Li Zhang",
            "Hailin Jin",
            "Aseem Agarwala"
        ],
        "abstract": "We describe a method for producing a smooth, stabilized video from the shaky input of a hand-held light field video camera\u2014specifically, a small camera array. Traditional stabilization techniques dampen shake with 2D warps, and thus have limited ability to stabilize a significantly shaky camera motion through a 3D scene. Other recent stabilization techniques synthesize novel views as they would have been seen along a virtual, smooth 3D camera path, but are limited to static scenes. We show that video camera arrays enable much more powerful video stabilization, since they allow changes in viewpoint for a single time instant. Furthermore, we point out that the straightforward approach to light field video stabilization requires computing structure-from-motion, which can be brittle for typical consumer-level video of general dynamic scenes. We present a more robust approach that avoids input camera path reconstruction. Instead, we employ a spacetime optimization that directly computes a sequence of relative poses between the virtual camera and the camera array, while minimizing acceleration of salient visual features in the virtual image plane. We validate our novel method by comparing it to state-of-the-art stabilization software, such as Apple iMovie and 2d3 SteadyMove Pro, on a number of challenging scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459270",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            },
            {
                "year": "2007",
                "id": 143
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 13,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Rendering (computer graphics)",
                "Computer vision",
                "Optical arrays",
                "Robustness",
                "Image reconstruction",
                "Acceleration",
                "Motion pictures",
                "Signal design"
            ]
        },
        "id": 43,
        "cited_by": [
            {
                "year": "2013",
                "id": 9
            }
        ]
    },
    {
        "title": "Super-resolution from a single image",
        "authors": [
            "Daniel Glasner",
            "Shai Bagon",
            "Michal Irani"
        ],
        "abstract": "Methods for super-resolution can be broadly classified into two families of methods: (i) The classical multi-image super-resolution (combining images obtained at subpixel misalignments), and (ii) Example-Based super-resolution (learning correspondence between low and high resolution image patches from a database). In this paper we propose a unified framework for combining these two families of methods. We further show how this combined approach can be applied to obtain super resolution from as little as a single image (with no database or prior examples). Our approach is based on the observation that patches in a natural image tend to redundantly recur many times inside the image, both within the same scale, as well as across different scales. Recurrence of patches within the same image scale (at subpixel misalignments) gives rise to the classical super-resolution, whereas recurrence of patches across different scales of the same image gives rise to example-based super-resolution. Our approach attempts to recover at each pixel its best possible resolution increase based on its patch redundancy within and across scales.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459271",
        "reference_list": [],
        "citation": {
            "ieee": 530,
            "other": 262,
            "total": 792
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Strontium",
                "Image databases",
                "Computer science",
                "Mathematics",
                "Layout",
                "Equations",
                "Image reconstruction",
                "Computer vision",
                "Frequency"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single image",
                "multiimage super-resolution",
                "subpixel misalignment",
                "example-based super-resolution",
                "low resolution image patch",
                "high resolution image patch",
                "natural image",
                "image scale"
            ]
        },
        "id": 44,
        "cited_by": [
            {
                "year": "2017",
                "id": 26
            },
            {
                "year": "2017",
                "id": 346
            },
            {
                "year": "2017",
                "id": 471
            },
            {
                "year": "2015",
                "id": 36
            },
            {
                "year": "2015",
                "id": 38
            },
            {
                "year": "2015",
                "id": 41
            },
            {
                "year": "2015",
                "id": 58
            },
            {
                "year": "2015",
                "id": 203
            },
            {
                "year": "2013",
                "id": 69
            },
            {
                "year": "2013",
                "id": 117
            },
            {
                "year": "2013",
                "id": 239
            },
            {
                "year": "2013",
                "id": 311
            },
            {
                "year": "2013",
                "id": 353
            },
            {
                "year": "2013",
                "id": 416
            },
            {
                "year": "2011",
                "id": 112
            },
            {
                "year": "2011",
                "id": 274
            }
        ]
    },
    {
        "title": "Compact signatures for high-speed interest point description and matching",
        "authors": [
            "Michael Calonder",
            "Vincent Lepetit",
            "Pascal Fua",
            "Kurt Konolige",
            "James Bowman",
            "Patrick Mihelich"
        ],
        "abstract": "Prominent feature point descriptors such as SIFT and SURF allow reliable real-time matching but at a computational cost that limits the number of points that can be handled on PCs, and even more on less powerful mobile devices. A recently proposed technique that relies on statistical classification to compute signatures has the potential to be much faster but at the cost of using very large amounts of memory, which makes it impractical for implementation on low-memory devices. In this paper, we show that we can exploit the sparseness of these signatures to compact them, speed up the computation, and drastically reduce memory usage. We base our approach on Compressive Sensing theory. We also highlight its effectiveness by incorporating it into two very different SLAM packages and demonstrating substantial performance increases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459272",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 13,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis",
                "Personal communication networks",
                "Simultaneous localization and mapping",
                "Distributed computing",
                "Handheld computers",
                "Computational efficiency",
                "Mobile computing",
                "Costs",
                "Packaging",
                "Sparse matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "compact signatures",
                "high-speed interest point description",
                "feature point descriptors",
                "reliable real-time matching",
                "computational cost",
                "PCs",
                "mobile devices",
                "statistical classification",
                "memory amounts",
                "low-memory devices",
                "signature sparseness",
                "memory usage",
                "compressive sensing theory",
                "SLAM packages"
            ]
        },
        "id": 45,
        "cited_by": [
            {
                "year": "2015",
                "id": 458
            },
            {
                "year": "2011",
                "id": 12
            }
        ]
    },
    {
        "title": "Attribute and simile classifiers for face verification",
        "authors": [
            "Neeraj Kumar",
            "Alexander C. Berg",
            "Peter N. Belhumeur",
            "Shree K. Nayar"
        ],
        "abstract": "We present two novel methods for face verification. Our first method - \u201cattribute\u201d classifiers - uses binary classifiers trained to recognize the presence or absence of describable aspects of visual appearance (e.g., gender, race, and age). Our second method - \u201csimile\u201d classifiers - removes the manual labeling required for attribute classification and instead learns the similarity of faces, or regions of faces, to specific reference people. Neither method requires costly, often brittle, alignment between image pairs; yet, both methods produce compact visual descriptions, and work on real-world images. Furthermore, both the attribute and simile classifiers improve on the current state-of-the-art for the LFW data set, reducing the error rates compared to the current best by 23.92% and 26.34%, respectively, and 31.68% when combined. For further testing across pose, illumination, and expression, we introduce a new data set - termed PubFig - of real-world images of public figures (celebrities and politicians) acquired from the internet. This data set is both larger (60,000 images) and deeper (300 images per individual) than existing data sets of its kind. Finally, we present an evaluation of human performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459250",
        "reference_list": [
            {
                "year": "2007",
                "id": 19
            },
            {
                "year": "2007",
                "id": 229
            }
        ],
        "citation": {
            "ieee": 443,
            "other": 243,
            "total": 686
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Error analysis",
                "Face detection",
                "Face recognition",
                "Labeling",
                "Humans",
                "Skin",
                "Cameras",
                "Computer vision",
                "Nose"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "simile classifier method",
                "face verification",
                "attribute classifier method",
                "LFW data set",
                "Internet"
            ]
        },
        "id": 46,
        "cited_by": [
            {
                "year": "2017",
                "id": 55
            },
            {
                "year": "2017",
                "id": 153
            },
            {
                "year": "2017",
                "id": 199
            },
            {
                "year": "2017",
                "id": 394
            },
            {
                "year": "2017",
                "id": 584
            },
            {
                "year": "2015",
                "id": 162
            },
            {
                "year": "2015",
                "id": 416
            },
            {
                "year": "2015",
                "id": 417
            },
            {
                "year": "2015",
                "id": 426
            },
            {
                "year": "2013",
                "id": 31
            },
            {
                "year": "2013",
                "id": 45
            },
            {
                "year": "2013",
                "id": 85
            },
            {
                "year": "2013",
                "id": 90
            },
            {
                "year": "2013",
                "id": 91
            },
            {
                "year": "2013",
                "id": 92
            },
            {
                "year": "2013",
                "id": 152
            },
            {
                "year": "2013",
                "id": 185
            },
            {
                "year": "2013",
                "id": 208
            },
            {
                "year": "2013",
                "id": 244
            },
            {
                "year": "2013",
                "id": 264
            },
            {
                "year": "2013",
                "id": 269
            },
            {
                "year": "2013",
                "id": 324
            },
            {
                "year": "2013",
                "id": 388
            },
            {
                "year": "2013",
                "id": 399
            },
            {
                "year": "2013",
                "id": 400
            },
            {
                "year": "2013",
                "id": 410
            },
            {
                "year": "2013",
                "id": 453
            },
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 137
            },
            {
                "year": "2011",
                "id": 155
            },
            {
                "year": "2011",
                "id": 177
            },
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2011",
                "id": 286
            },
            {
                "year": "2011",
                "id": 317
            },
            {
                "year": "2011",
                "id": 321
            }
        ]
    },
    {
        "title": "A shape-based object class model for knowledge transfer",
        "authors": [
            "Michael Stark",
            "Michael Goesele",
            "Bernt Schiele"
        ],
        "abstract": "Object class models trained on hundreds or thousands of images have shown to enable robust detection. Transferring knowledge from such models to new object classes trained from a few or even as little as one training instance however is still in its infancy. This paper designs a shape-based model that allows to easily and explicitly transfer knowledge on three different levels: transfer of individual parts' shape and appearance information, transfer of local symmetry between parts, and transfer of part topology. Due to the factorized form of the model, knowledge can either be transferred for the complete model or just partial knowledge corresponding to certain aspects of the model. The experiments clearly demonstrate that the proposed model is competitive with the state-of-the-art and enables both full and partial knowledge transfer.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459231",
        "reference_list": [
            {
                "year": "2007",
                "id": 39
            },
            {
                "year": "2007",
                "id": 224
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 22,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Knowledge transfer",
                "Shape",
                "Object detection",
                "Training data",
                "Machine learning",
                "Robustness",
                "Computer vision",
                "Computer science",
                "Topology",
                "Animals"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape-based object class model",
                "knowledge transfer",
                "robust detection",
                "part topology"
            ]
        },
        "id": 47,
        "cited_by": [
            {
                "year": "2017",
                "id": 356
            },
            {
                "year": "2013",
                "id": 428
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 236
            },
            {
                "year": "2011",
                "id": 286
            }
        ]
    },
    {
        "title": "Learning pedestrian dynamics from the real world",
        "authors": [
            "Paul Scovanner",
            "Marshall F. Tappen"
        ],
        "abstract": "In this paper we describe a method to learn parameters which govern pedestrian motion by observing video data. Our learning framework is based on variational mode learning and allows us to efficiently optimize a continuous pedestrian cost model. We show that this model can be trained on automatic tracking results, and provides realistic and accurate pedestrian motions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459224",
        "reference_list": [],
        "citation": {
            "ieee": 39,
            "other": 21,
            "total": 60
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Tracking",
                "Layout",
                "Cost function",
                "Virtual environment",
                "Event detection",
                "Learning systems",
                "Computer vision",
                "Videoconference",
                "Large-scale systems"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pedestrian dynamics",
                "video data obseravtion",
                "variational mode learning"
            ]
        },
        "id": 48,
        "cited_by": [
            {
                "year": "2015",
                "id": 350
            }
        ]
    },
    {
        "title": "Resilient Subclass Discriminant Analysis",
        "authors": [
            "Dijia Wu",
            "Kim L. Boyer"
        ],
        "abstract": "We propose a dimension reduction technique named Resilient Subclass Discriminant Analysis (RSDA) for high dimensional classification problems. The technique iteratively estimates the subclass division by embedding the Fisher Discriminant Analysis (FDA) with Expectation-Maximization (EM) in Gaussian Mixture Models (GMM). The new method maintains the adaptability of SDA to a wide range of data distributions by approximating the distribution of each class as a mixture of Gaussians, and provides superior feature selection performance to SDA with modified EM clustering that estimates a posteriori probability of latent variables in lower-dimensional Fisher's discriminant space, which also improves the robustness in problems of small training datasets compared with conventional EM algorithm. Extensive experiments and comparison results against other well-known Discriminant Analysis (DA) methods are presented using synthetic data, benchmark datasets as well as a real computational vision problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459212",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Linear discriminant analysis",
                "Clustering algorithms",
                "Neural networks",
                "Robustness",
                "Feature extraction",
                "Covariance matrix",
                "Kernel",
                "Clustering methods",
                "Computer vision",
                "Pattern recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "feature extraction",
                "Gaussian processes",
                "iterative methods",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "resilient subclass discriminant analysis",
                "dimension reduction technique",
                "high dimensional classification problems",
                "iterative estimation",
                "Fisher discriminant analysis",
                "expectation maximization method",
                "Gaussian mixture model",
                "data distribution",
                "feature selection",
                "variables posteriori probability",
                "computational vision problem"
            ]
        },
        "id": 49,
        "cited_by": []
    },
    {
        "title": "Fast Ray features for learning irregular shapes",
        "authors": [
            "Kevin Smith",
            "Alan Carleton",
            "Vincent Lepetit"
        ],
        "abstract": "We introduce a new class of image features, the Ray feature set, that consider image characteristics at distant contour points, capturing information which is difficult to represent with standard feature sets. This property allows Ray features to efficiently and robustly recognize deformable or irregular shapes, such as cells in microscopic imagery. Experiments show Ray features clearly outperform other powerful features including Haar-like features and Histograms of Oriented Gradients when applied to detecting irregularly shaped neuron nuclei and mitochondria. Ray features can also provide important complementary information to Haar features for other tasks such as face detection, reducing the number of weak learners and computational cost. Ray features can be efficiently precomputed to reduce cost, just as precomputing integral images reduces the overall cost of Haar features. While Rays are slightly more expensive to precompute, their computational cost is less than that of Haar features for scanning an AdaBoost-based detector window across an image at run-time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459210",
        "reference_list": [
            {
                "year": "2005",
                "id": 211
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 16,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Computational efficiency",
                "Costs",
                "Robustness",
                "Image recognition",
                "Microscopy",
                "Histograms",
                "Neurons",
                "Face detection",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Haar transforms",
                "object detection",
                "ray tracing",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image features",
                "Ray feature set",
                "Haar features",
                "face detection",
                "AdaBoost"
            ]
        },
        "id": 50,
        "cited_by": []
    },
    {
        "title": "Extending continuous cuts: Anisotropic metrics and expansion moves",
        "authors": [
            "Carl Olsson",
            "Martin Byr\u00f6d",
            "Niels Chr. Overgaard",
            "Fredrik Kahl"
        ],
        "abstract": "The concept of graph cuts is by now a standard method for all sorts of low level vision problems. Its popularity is largely due to the fact that globally or near globally optimal solutions can be computed using efficient max flow algorithms. On the other hand it has been observed that this method may suffer from metrication errors. Recent work has begun studying continuous versions of graph cuts, which give smaller metrication errors. Another advantage is that continuous cuts are straightforward to parallelize. In this paper we extend the class of functionals that can be optimized in the continuous setting to include anisotropic TV-norms. We show that there is a so called coarea formula for these functionals making it possible to minimize them by solving a convex problem. We also show that the concept of a-expansion moves can be reformulated to fit the continuous formulation, and we derive approximation bounds in analogy with the discrete case. A continuous version of the Potts model for multi-class segmentation problems is presented, and it is shown how to obtain provably good solutions using continuous \u03b1-expansions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459206",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 11,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Anisotropic magnetoresistance",
                "Application software",
                "Computer errors",
                "Computer vision",
                "Image segmentation",
                "Image restoration",
                "Stereo vision",
                "TV"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "computer vision",
                "convex programming",
                "graph theory",
                "image segmentation",
                "minimisation",
                "Potts model"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "anisotropic metrics",
                "continuous graph cuts",
                "low level vision problems",
                "max flow algorithms",
                "metrication errors",
                "anisotropic TV-norms",
                "coarea formula",
                "convex problem",
                "\u03b1-expansion moves",
                "approximation bounds",
                "Potts model",
                "multiclass segmentation problems"
            ]
        },
        "id": 51,
        "cited_by": []
    },
    {
        "title": "Robust fitting of multiple structures: The statistical learning approach",
        "authors": [
            "Tat-Jun Chin",
            "Hanzi Wang",
            "David Suter"
        ],
        "abstract": "We propose an unconventional but highly effective approach to robust fitting of multiple structures by using statistical learning concepts. We design a novel Mercer kernel for the robust estimation problem which elicits the potential of two points to have emerged from the same underlying structure. The Mercer kernel permits the application of well-grounded statistical learning methods, among which nonlinear dimensionality reduction, principal component analysis and spectral clustering are applied for robust fitting. Our method can remove gross outliers and in parallel discover the multiple structures present. It functions well under severe outliers (more than 90% of the data) and considerable inlier noise without requiring elaborate manual tuning or unrealistic prior information. Experiments on synthetic and real problems illustrate the superiority of the proposed idea over previous methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459150",
        "reference_list": [
            {
                "year": "2003",
                "id": 115
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 1,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistical learning",
                "Kernel",
                "Statistics",
                "Noise robustness",
                "Computer vision",
                "Computer science",
                "Australia",
                "Principal component analysis",
                "Feature extraction",
                "Pipelines"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "pattern clustering",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "statistical learning approach",
                "multiple structures",
                "Mercer kernel",
                "robust estimation problem",
                "principal component analysis",
                "nonlinear dimensionality reduction",
                "spectral clustering",
                "inlier noise"
            ]
        },
        "id": 52,
        "cited_by": [
            {
                "year": "2015",
                "id": 323
            },
            {
                "year": "2013",
                "id": 5
            },
            {
                "year": "2013",
                "id": 170
            },
            {
                "year": "2011",
                "id": 164
            }
        ]
    },
    {
        "title": "Constrained clustering by spectral kernel learning",
        "authors": [
            "Zhenguo Li",
            "Jianzhuang Liu"
        ],
        "abstract": "Clustering performance can often be greatly improved by leveraging side information. In this paper, we consider constrained clustering with pairwise constraints, which specify some pairs of objects from the same cluster or not. The main idea is to design a kernel to respect both the proximity structure of the data and the given pairwise constraints. We propose a spectral kernel learning framework and formulate it as a convex quadratic program, which can be optimally solved efficiently. Our framework enjoys several desirable features: 1) it is applicable to multi-class problems; 2) it can handle both must-link and cannot-link constraints; 3) it can propagate pairwise constraints effectively; 4) it is scalable to large-scale problems; and 5) it can handle weighted pairwise constraints. Extensive experiments have demonstrated the superiority of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459157",
        "reference_list": [
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2007",
                "id": 221
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Clustering algorithms",
                "Partitioning algorithms",
                "Couplings",
                "Large-scale systems",
                "Learning systems",
                "Pattern recognition",
                "Glass",
                "Application software",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "constraint theory",
                "convex programming",
                "pattern clustering",
                "quadratic programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "constrained clustering",
                "spectral kernel learning",
                "pairwise constraints",
                "convex quadratic program",
                "multiclass problems",
                "must link constraints",
                "cannot link constraints",
                "weighted pairwise constraints"
            ]
        },
        "id": 53,
        "cited_by": []
    },
    {
        "title": "Learning image similarity from Flickr groups using Stochastic Intersection Kernel MAchines",
        "authors": [
            "Gang Wang",
            "Derek Hoiem",
            "David Forsyth"
        ],
        "abstract": "Measuring image similarity is a central topic in computer vision. In this paper, we learn similarity from Flickr groups and use it to organize photos. Two images are similar if they are likely to belong to the same Flickr groups. Our approach is enabled by a fast Stochastic Intersection Kernel MAchine (SIKMA) training algorithm, which we propose. This proposed training method will be useful for many vision problems, as it can produce a classifier that is more accurate than a linear classifier, trained on tens of thousands of examples in two minutes. The experimental results show our approach performs better on image matching, retrieval, and classification than using conventional visual features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459167",
        "reference_list": [
            {
                "year": "2001",
                "id": 159
            },
            {
                "year": "2009",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 12,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Machine learning",
                "Stochastic processes",
                "Kernel",
                "Support vector machines",
                "Histograms",
                "Computer vision",
                "Support vector machine classification",
                "Large-scale systems",
                "Feedback",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image similarity",
                "flickr groups",
                "computer vision",
                "fast stochastic intersection kernel machine training algorithm",
                "linear classifier",
                "image matching",
                "image retrieval",
                "image classification"
            ]
        },
        "id": 54,
        "cited_by": [
            {
                "year": "2015",
                "id": 516
            }
        ]
    },
    {
        "title": "Group-sensitive multiple kernel learning for object categorization",
        "authors": [
            "Jingjing Yang",
            "Yuanning Li",
            "Yonghong Tian",
            "Lingyu Duan",
            "Wen Gao"
        ],
        "abstract": "In this paper, we propose a group-sensitive multiple kernel learning (GS-MKL) method to accommodate the intra-class diversity and the inter-class correlation for object categorization. By introducing an intermediate representation \"group\" between images and object categories, GS-MKL attempts to find appropriate kernel combination for each group to get a finer depiction of object categories. For each category, images within a group share a set of kernel weights while images from different groups may employ distinct sets of kernel weights. In GS-MKL, such group-sensitive kernel combinations together with the multi-kernels based classifier are optimized in a joint manner to seek a trade-off between capturing the diversity and keeping the invariance for each category. Extensive experiments show that our proposed GS-MKL method has achieved encouraging performance over three challenging datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459172",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2007",
                "id": 36
            },
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2007",
                "id": 20
            },
            {
                "year": "2007",
                "id": 225
            },
            {
                "year": "2009",
                "id": 125
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 5,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Bridges",
                "Robustness",
                "Support vector machines",
                "Support vector machine classification",
                "Learning systems",
                "Information processing",
                "Computers",
                "Laboratories",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object categorization",
                "group sensitive multiple kernel learning",
                "intraclass diversity",
                "interclass correlation",
                "multikernels based classifier"
            ]
        },
        "id": 55,
        "cited_by": [
            {
                "year": "2013",
                "id": 35
            },
            {
                "year": "2011",
                "id": 39
            },
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2011",
                "id": 337
            }
        ]
    },
    {
        "title": "Recognizing actions by shape-motion prototype trees",
        "authors": [
            "Zhe Lin",
            "Zhuolin Jiang",
            "Larry S. Davis"
        ],
        "abstract": "A prototype-based approach is introduced for action recognition. The approach represents an action as a sequence of prototypes for efficient and flexible action matching in long video sequences. During training, first, an action prototype tree is learned in a joint shape and motion space via hierarchical k-means clustering; then a lookup table of prototype-to-prototype distances is generated. During testing, based on a joint likelihood model of the actor location and action prototype, the actor is tracked while a frame-to-prototype correspondence is established by maximizing the joint likelihood, which is efficiently performed by searching the learned prototype tree; then actions are recognized using dynamic prototype sequence matching. Distance matrices used for sequence matching are rapidly obtained by look-up table indexing, which is an order of magnitude faster than brute-force computation of frame-to-frame distances. Our approach enables robust action matching in very challenging situations (such as moving cameras, dynamic backgrounds) and allows automatic alignment of action sequences. Experimental results demonstrate that our approach achieves recognition rates of 91.07% on a large gesture dataset (with dynamic backgrounds), 100% on the Weizmann action dataset and 95.77% on the KTH action dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459184",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2007",
                "id": 209
            }
        ],
        "citation": {
            "ieee": 58,
            "other": 35,
            "total": 93
        },
        "keywords": {
            "IEEE Keywords": [
                "Prototypes",
                "Object recognition",
                "Face detection",
                "Training data",
                "Image resolution",
                "Boosting",
                "Airplanes",
                "Machine learning",
                "Linear discriminant analysis",
                "Uncertainty"
            ]
        },
        "id": 56,
        "cited_by": [
            {
                "year": "2013",
                "id": 396
            },
            {
                "year": "2011",
                "id": 5
            },
            {
                "year": "2011",
                "id": 89
            },
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2011",
                "id": 99
            },
            {
                "year": "2011",
                "id": 225
            }
        ]
    },
    {
        "title": "Simultaneous and orthogonal decomposition of data using Multimodal Discriminant Analysis",
        "authors": [
            "Terence Sim",
            "Sheng Zhang",
            "Jianran Li",
            "Yan Chen"
        ],
        "abstract": "We present Multimodal Discriminant Analysis (MMDA), a novel method for decomposing variations in a dataset into independent factors (modes). For face images, MMDA effectively separates personal identity, illumination and pose into orthogonal subspaces. MMDA is based on maximizing the Fisher Criterion on all modes at the same time, and is therefore well-suited for multimodal and mode-invariant pattern recognition. We also show that MMDA may be used for dimension reduction, and for synthesizing images under novel illumination and even novel personal identity.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459189",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 1,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Lighting",
                "Least squares methods",
                "Light sources",
                "Least squares approximation",
                "Automation",
                "Educational institutions",
                "Information science",
                "Geometry",
                "Jacobian matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "biometrics (access control)",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "data orthogonal decomposition",
                "data simultaneous decomposition",
                "multimodal discriminant analysis",
                "Fisher Criterion",
                "multimodal pattern recognition",
                "mode-invariant pattern recognition",
                "personal identity"
            ]
        },
        "id": 57,
        "cited_by": []
    },
    {
        "title": "Fast and robust Earth Mover's Distances",
        "authors": [
            "Ofir Pele",
            "Michael Werman"
        ],
        "abstract": "We present a new algorithm for a robust family of Earth Mover's Distances - EMDs with thresholded ground distances. The algorithm transforms the flow-network of the EMD so that the number of edges is reduced by an order of magnitude. As a result, we compute the EMD by an order of magnitude faster than the original algorithm, which makes it possible to compute the EMD on large histograms and databases. In addition, we show that EMDs with thresholded ground distances have many desirable properties. First, they correspond to the way humans perceive distances. Second, they are robust to outlier noise and quantization effects. Third, they are metrics. Finally, experimental results on image retrieval show that thresholding the ground distance of the EMD improves both accuracy and speed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459199",
        "reference_list": [],
        "citation": {
            "ieee": 89,
            "other": 116,
            "total": 205
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Earth",
                "Histograms",
                "Costs",
                "Humans",
                "Image retrieval",
                "Image edge detection",
                "Quantization",
                "Computer vision",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "thresholded ground distances",
                "flow-network",
                "histograms",
                "outlier noise",
                "quantization effects",
                "robust earth mover's distances"
            ]
        },
        "id": 58,
        "cited_by": [
            {
                "year": "2015",
                "id": 69
            },
            {
                "year": "2015",
                "id": 364
            },
            {
                "year": "2013",
                "id": 210
            }
        ]
    },
    {
        "title": "Active segmentation with fixation",
        "authors": [
            "Ajay Mishra",
            "Yiannis Aloimonos",
            "Cheong Loong Fah"
        ],
        "abstract": "The human visual system observes and understands a scene/image by making a series of fixations. Every \u201cfixation point\u201d lies inside a particular region of arbitrary shape and size in the scene which can either be an object or just a part of it. We define as a basic segmentation problem the task of segmenting that region containing the \u201cfixation point\u201d. Segmenting this region is equivalent to finding the enclosing contour - a connected set of boundary edge fragments in the edge map of the scene - around the fixation. We present here a novel algorithm that finds this bounding contour and achieves the segmentation of one object, given the fixation. The proposed segmentation framework combines monocular cues (color/intensity/texture) with stereo and/or motion, in a cue independent manner. We evaluate the performance of the proposed algorithm on challenging videos and stereo pairs. Although the proposed algorithm is more suitable for an active observer capable of fixating at different locations in the scene, it applies to a single image as well. In fact, we show that even with monocular cues alone, the introduced algorithm performs as well or better than a number of image segmentation algorithms, when applied to challenging inputs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459254",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2007",
                "id": 88
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 26,
            "total": 66
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Layout",
                "Humans",
                "Computer vision",
                "Visual system",
                "Stereo vision",
                "Videos",
                "Retina",
                "Laboratories",
                "Educational institutions"
            ]
        },
        "id": 59,
        "cited_by": [
            {
                "year": "2013",
                "id": 77
            },
            {
                "year": "2011",
                "id": 96
            }
        ]
    },
    {
        "title": "Directional statistics BRDF model",
        "authors": [
            "Ko Nishino"
        ],
        "abstract": "We introduce a novel parametric BRDF model that can accurately encode a wide variety of real-world isotropic BRDFs with a small number of parameters. The key observation we make is that a BRDF may be viewed as a statistical distribution on a unit hemisphere. We derive a novel directional statistics distribution, which we refer to as the hemispherical exponential power distribution, and model an isotropic BRDF with a mixture of it. The novel directional statistics BRDF model allows us to derive a canonical probabilistic method for estimating its parameters including the number of components. We show that the model captures the full spectrum of real-world isotropic BRDFs with accuracy comparable to non-parametric models but with a much more compact representation. We also experimentally show that the model achieves better accuracy with less measurements compared with such non-parametric models. We further demonstrate the advantages of the novel BRDF model by showing its use for reflection component separation and for exploring the space of isotropic BRDFs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459255",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 6,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistical distributions",
                "Optical reflection",
                "Solid modeling",
                "Brain modeling",
                "Parametric statistics",
                "Parameter estimation",
                "Space exploration",
                "Computer vision",
                "Inverse problems",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "parameter estimation",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "directional statistics BRDF model",
                "directional statistics distribution",
                "hemispherical exponential power distribution",
                "canonical probabilistic method",
                "parameter estimation",
                "bidirectional reflectance distribution function"
            ]
        },
        "id": 60,
        "cited_by": [
            {
                "year": "2017",
                "id": 238
            },
            {
                "year": "2015",
                "id": 397
            },
            {
                "year": "2011",
                "id": 136
            }
        ]
    },
    {
        "title": "Multi-scale object detection by clustering lines",
        "authors": [
            "Bj\u00f6rn Ommer",
            "Jitendra Malik"
        ],
        "abstract": "Object detection in cluttered, natural scenes has a high complexity since many local observations compete for object hypotheses. Voting methods provide an efficient solution to this problem. When Hough voting is extended to location and scale, votes naturally become lines through scale space due to the local scale-location-ambiguity. In contrast to this, current voting methods stick to the location-only setting and cast point votes, which require local estimates of scale. Rather than searching for object hypotheses in the Hough accumulator, we propose a weighted, pairwise clustering of voting lines to obtain globally consistent hypotheses directly. In essence, we propose a hierarchical approach that is based on a sparse representation of object boundary shape. Clustering of voting lines (CVL) condenses the information from these edge points in few, globally consistent candidate hypotheses. A final verification stage concludes by refining the candidates. Experiments on the ETHZ shape dataset show that clustering voting lines significantly improves state-of-the-art Hough voting techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459200",
        "reference_list": [
            {
                "year": "2001",
                "id": 58
            },
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2005",
                "id": 64
            },
            {
                "year": "2005",
                "id": 48
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 32,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Hough transforms",
                "object detection",
                "pattern clustering",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale object detection",
                "Hough voting technique",
                "local scale location-ambiguity",
                "object hypotheses",
                "Hough accumulator",
                "clustering voting lines"
            ]
        },
        "id": 61,
        "cited_by": []
    },
    {
        "title": "Local Trinary Patterns for human action recognition",
        "authors": [
            "Lahav Yeffet",
            "Lior Wolf"
        ],
        "abstract": "We present a novel action recognition method which is based on combining the effective description properties of Local Binary Patterns with the appearance invariance and adaptability of patch matching based methods. The resulting method is extremely efficient, and thus is suitable for real-time uses of simultaneous recovery of human action of several lengths and starting points. Tested on all publicity available datasets in the literature known to us, our system repeatedly achieves state of the art performance. Lastly, we present a new benchmark that focuses on uncut motion recognition in broadcast sports video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459201",
        "reference_list": [
            {
                "year": "2007",
                "id": 206
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2007",
                "id": 147
            }
        ],
        "citation": {
            "ieee": 107,
            "other": 79,
            "total": 186
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Pattern recognition",
                "Histograms",
                "Optical computing",
                "Atom optics",
                "Concatenated codes",
                "Computer science",
                "Pattern matching",
                "System testing",
                "Benchmark testing"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern matching",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local trinary patterns",
                "human action recognition method",
                "local binary patterns",
                "appearance invariance",
                "appearance adaptability",
                "patch matching based methods",
                "human action recovery",
                "datasets",
                "motion recognition",
                "broadcast sports video"
            ]
        },
        "id": 62,
        "cited_by": [
            {
                "year": "2013",
                "id": 443
            },
            {
                "year": "2011",
                "id": 254
            }
        ]
    },
    {
        "title": "Is that you? Metric learning approaches for face identification",
        "authors": [
            "Matthieu Guillaumin",
            "Jakob Verbeek",
            "Cordelia Schmid"
        ],
        "abstract": "Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. In this paper we present two methods for learning robust distance measures: (a) a logistic discriminant approach which learns the metric from a set of labelled image pairs (LDML) and (b) a nearest neighbour approach which computes the probability for two images to belong to the same class (MkNN). We evaluate our approaches on the Labeled Faces in the Wild data set, a large and very challenging data set of faces from Yahoo! News. The evaluation protocol for this data set defines a restricted setting, where a fixed set of positive and negative image pairs is given, as well as an unrestricted one, where faces are labelled by their identity. We are the first to present results for the unrestricted setting, and show that our methods benefit from this richer training data, much more so than the current state-of-the-art method. Our results of 79.3% and 87.5% correct for the restricted and unrestricted setting respectively, significantly improve over the current state-of-the-art result of 78.5%. Confidence scores obtained for face identification can be used for many applications e.g. clustering or recognition from a single training example. We show that our learned metrics also improve performance for these tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459197",
        "reference_list": [
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2007",
                "id": 215
            },
            {
                "year": "2007",
                "id": 201
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 252,
            "other": 152,
            "total": 404
        },
        "keywords": {
            "IEEE Keywords": [
                "Level set",
                "Image segmentation",
                "Kernel",
                "Active contours",
                "Computational complexity",
                "Pixel",
                "Biomedical computing",
                "Computer science",
                "Graph theory",
                "Optimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "metric learning approach",
                "face identification",
                "logistic discriminant approach",
                "nearest neighbour approach",
                "LDML",
                "MkNN",
                "state-of-the-art method",
                "data set",
                "evaluation protocol"
            ]
        },
        "id": 63,
        "cited_by": [
            {
                "year": "2017",
                "id": 398
            },
            {
                "year": "2017",
                "id": 567
            },
            {
                "year": "2015",
                "id": 144
            },
            {
                "year": "2015",
                "id": 177
            },
            {
                "year": "2015",
                "id": 411
            },
            {
                "year": "2015",
                "id": 431
            },
            {
                "year": "2015",
                "id": 471
            },
            {
                "year": "2015",
                "id": 476
            },
            {
                "year": "2013",
                "id": 31
            },
            {
                "year": "2013",
                "id": 41
            },
            {
                "year": "2013",
                "id": 185
            },
            {
                "year": "2013",
                "id": 200
            },
            {
                "year": "2013",
                "id": 244
            },
            {
                "year": "2013",
                "id": 300
            },
            {
                "year": "2013",
                "id": 332
            },
            {
                "year": "2013",
                "id": 388
            },
            {
                "year": "2011",
                "id": 197
            },
            {
                "year": "2011",
                "id": 248
            },
            {
                "year": "2009",
                "id": 39
            }
        ]
    },
    {
        "title": "Semi-Supervised Random Forests",
        "authors": [
            "Christian Leistner",
            "Amir Saffari",
            "Jakob Santner",
            "Horst Bischof"
        ],
        "abstract": "Random Forests (RFs) have become commonplace in many computer vision applications. Their popularity is mainly driven by their high computational efficiency during both training and evaluation while still being able to achieve state-of-the-art accuracy. This work extends the usage of Random Forests to Semi-Supervised Learning (SSL) problems. We show that traditional decision trees are optimizing multi-class margin maximizing loss functions. From this intuition, we develop a novel multi-class margin definition for the unlabeled data, and an iterative deterministic annealing-style training algorithm maximizing both the multi-class margin of labeled and unlabeled samples. In particular, this allows us to use the predicted labels of the unlabeled data as additional optimization variables. Furthermore, we propose a control mechanism based on the out-of-bag error, which prevents the algorithm from degradation if the unlabeled data is not useful for the task. Our experiments demonstrate state-of-the-art semi-supervised learning performance in typical machine learning problems and constant improvement using unlabeled data for the Caltech-101 object categorization task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459198",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 25,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Semisupervised learning",
                "Iterative algorithms",
                "Computer vision",
                "Application software",
                "Computational efficiency",
                "Decision trees",
                "Annealing",
                "Error correction",
                "Machine learning algorithms",
                "Degradation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision applications",
                "iterative deterministic annealing style training algorithm",
                "multiclass margin",
                "control mechanism",
                "out-of-bag error",
                "machine learning problems",
                "unlabeled data",
                "Caltech-101 object categorization task",
                "semisupervised random forests",
                "semisupervised learning problems"
            ]
        },
        "id": 64,
        "cited_by": [
            {
                "year": "2013",
                "id": 52
            },
            {
                "year": "2013",
                "id": 258
            },
            {
                "year": "2013",
                "id": 402
            },
            {
                "year": "2011",
                "id": 278
            }
        ]
    },
    {
        "title": "Graph cuts using a Riemannian metric induced by tensor voting",
        "authors": [
            "Hyung Il Koo",
            "Nam Ik Cho"
        ],
        "abstract": "In this paper, we present a new algorithm that combines the advantages of tensor voting into graph cuts. Tensor voting has been a popular tool for a number of early vision problems since it can use principles of perceptual grouping, which are not well considered in graph cuts. We attempt to encode the power of tensor voting into an energy minimization framework. For this, we assume that the tensor map obtained by tensor voting induces a Riemannian metric in image domain, and the metric is constructed according to the conventional ways of tensor interpretation. Finally, by embedding the induced Riemannian metric into the graph via edge weights, the graph cuts algorithm can have priors considering principles of perceptual grouping. The proposed method can be used in the labeling of occluded regions, object segmentation using only edge information, and boundary regularization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459195",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2007",
                "id": 132
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Voting",
                "Image segmentation",
                "Clustering algorithms",
                "Inference algorithms",
                "Labeling",
                "Computer vision",
                "Extrapolation",
                "Application software",
                "Object segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "image segmentation",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph cuts",
                "Riemannian metric",
                "tensor voting",
                "perceptual grouping principle",
                "energy minimization",
                "object segmentation",
                "edge information",
                "boundary regularization",
                "occluded region"
            ]
        },
        "id": 65,
        "cited_by": []
    },
    {
        "title": "Kernel active contour",
        "authors": [
            "Shan Tan",
            "Ioannis A. Kakadiaris"
        ],
        "abstract": "Level sets and graph cuts are two state-of-the-art image segmentation methods in use today. The two methods are apparently different from each other not only because they originate from different theory foundations but also because they employ image information in different ways - level sets typically use image information in a point-wise way, whereas graph cuts use image information in a pairwise way. In this paper, we derive an equivalence relationship between the two methods through kernel technology. In particular, we show that the kernelization of the Chan-Vese (CV) functional - a functional widely used in the level set community - is exactly the energy optimized in the average association - a well-known graph cut criterion. We refer to the level sets method using the kernelized version of the CV functional as kernel active contour. The kernel active contour has computational complexity O(n2) due to the involved kernel technology. We propose a fast implementation for kernel active contour with computational complexity only O(n) using random projection. The kernel active contour is evaluated on synthetic and real images and compared with several existing level set and graph cut methods for image segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459196",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Active contours",
                "Level set",
                "Image segmentation",
                "Computational complexity",
                "Pixel",
                "Biomedical computing",
                "Computer science",
                "Graph theory",
                "Optimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "graph theory",
                "image segmentation",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "kernel active contour",
                "level sets",
                "graph cuts",
                "image segmentation method",
                "image information",
                "pairwise way",
                "Chan-Vese functional",
                "graph cut criterion",
                "computational complexity",
                "random projection"
            ]
        },
        "id": 66,
        "cited_by": []
    },
    {
        "title": "Dimensionality reduction and principal surfaces via Kernel Map Manifolds",
        "authors": [
            "Samuel Gerber",
            "Tolga Tasdizen",
            "Ross Whitaker"
        ],
        "abstract": "We present a manifold learning approach to dimensionality reduction that explicitly models the manifold as a mapping from low to high dimensional space. The manifold is represented as a parametrized surface represented by a set of parameters that are defined on the input samples. The representation also provides a natural mapping from high to low dimensional space, and a concatenation of these two mappings induces a projection operator onto the manifold. The explicit projection operator allows for a clearly defined objective function in terms of projection distance and reconstruction error. A formulation of the mappings in terms of kernel regression permits a direct optimization of the objective function and the extremal points converge to principal surfaces as the number of data to learn from increases. Principal surfaces have the desirable property that they, informally speaking, pass through the middle of a distribution. We provide a proof on the convergence to principal surfaces and illustrate the effectiveness of the proposed approach on synthetic and real data sets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459193",
        "reference_list": [
            {
                "year": "2007",
                "id": 200
            },
            {
                "year": "2007",
                "id": 190
            },
            {
                "year": "2003",
                "id": 187
            },
            {
                "year": "2005",
                "id": 83
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 6,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image reconstruction",
                "Surface reconstruction",
                "Laplace equations",
                "Application software",
                "Computer vision",
                "Principal component analysis",
                "Scientific computing",
                "Cities and towns",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "learning (artificial intelligence)",
                "optimisation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dimensionality reduction",
                "kernel map manifolds",
                "manifold learning approach",
                "parametrized surface",
                "projection operator",
                "objective function",
                "projection distance",
                "reconstruction error",
                "kernel regression",
                "optimization",
                "principal surfaces"
            ]
        },
        "id": 67,
        "cited_by": []
    },
    {
        "title": "Joint learning of visual attributes, object classes and visual saliency",
        "authors": [
            "Gang Wang",
            "David Forsyth"
        ],
        "abstract": "We present a method to learn visual attributes (eg.\"red\", \"metal\", \"spotted\") and object classes (eg. \"car\", \"dress\", \"umbrella\") together. We assume images are labeled with category, but not location, of an instance. We estimate models with an iterative procedure: the current model is used to produce a saliency score, which, together with a homogeneity cue, identifies likely locations for the object (resp. attribute); then those locations are used to produce better models with multiple instance learning. Crucially, the object and attribute models must agree on the potential locations of an object. This means that the more accurate of the two models can guide the improvement of the less accurate model. Our method is evaluated on two data sets of images of real scenes, one in which the attribute is color and the other in which it is material. We show that our joint learning produces improved detectors. We demonstrate generalization by detecting attribute-object pairs which do not appear in our training data. The iteration gives significant improvement in performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459194",
        "reference_list": [],
        "citation": {
            "ieee": 70,
            "other": 11,
            "total": 81
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Object detection",
                "Training data",
                "Computer science",
                "Layout",
                "Learning systems",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "iterative methods",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual attributes joint learning",
                "object class",
                "visual saliency",
                "iterative procedure",
                "saliency score",
                "homogeneity cue",
                "multiple instance learning",
                "attribute object pairs detection"
            ]
        },
        "id": 68,
        "cited_by": [
            {
                "year": "2013",
                "id": 45
            },
            {
                "year": "2013",
                "id": 152
            },
            {
                "year": "2013",
                "id": 269
            },
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2011",
                "id": 177
            },
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2011",
                "id": 321
            }
        ]
    },
    {
        "title": "Bayesian Poisson regression for crowd counting",
        "authors": [
            "Antoni B. Chan",
            "Nuno Vasconcelos"
        ],
        "abstract": "Poisson regression models the noisy output of a counting function as a Poisson random variable, with a log-mean parameter that is a linear function of the input vector. In this work, we analyze Poisson regression in a Bayesian setting, by introducing a prior distribution on the weights of the linear function. Since exact inference is analytically unobtainable, we derive a closed-form approximation to the predictive distribution of the model. We show that the predictive distribution can be kernelized, enabling the representation of non-linear log-mean functions. We also derive an approximate marginal likelihood that can be optimized to learn the hyperparameters of the kernel. We then relate the proposed approximate Bayesian Poisson regression to Gaussian processes. Finally, we present experimental results using Bayesian Poisson regression for crowd counting from low-level features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459191",
        "reference_list": [],
        "citation": {
            "ieee": 56,
            "other": 35,
            "total": 91
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Layout",
                "Lighting",
                "Least squares methods",
                "Light sources",
                "Least squares approximation",
                "Automation",
                "Educational institutions",
                "Information science",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "Bayes methods",
                "Gaussian processes",
                "image processing",
                "nonlinear functions",
                "regression analysis",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian Poisson regression",
                "crowd counting",
                "counting function",
                "Poisson random variable",
                "linear functions",
                "input vector",
                "closed form approximation",
                "predictive distribution",
                "nonlinear log-mean functions",
                "marginal likelihood optimization",
                "hyperparameters",
                "kernels",
                "Gaussian processes",
                "low level features"
            ]
        },
        "id": 69,
        "cited_by": [
            {
                "year": "2017",
                "id": 196
            },
            {
                "year": "2017",
                "id": 567
            },
            {
                "year": "2017",
                "id": 618
            }
        ]
    },
    {
        "title": "Efficient discriminative learning of parts-based models",
        "authors": [
            "M. Pawan Kumar",
            "Andrew Zisserman",
            "Philip H.S. Torr"
        ],
        "abstract": "Supervised learning of a parts-based model can be formulated as an optimization problem with a large (exponential in the number of parts) set of constraints. We show how this seemingly difficult problem can be solved by (i) reducing it to an equivalent convex problem with a small, polynomial number of constraints (taking advantage of the fact that the model is tree-structured and the potentials have a special form); and (ii) obtaining the globally optimal model using an efficient dual decomposition strategy. Each component of the dual decomposition is solved by a modified version of the highly optimized SVM-Light algorithm. To demonstrate the effectiveness of our approach, we learn human upper body models using two challenging, publicly available datasets. Our model accounts for the articulation of humans as well as the occlusion of parts. We compare our method with a baseline iterative strategy as well as a state of the art algorithm and show significant efficiency improvements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459192",
        "reference_list": [
            {
                "year": "2007",
                "id": 51
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 20,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Lighting",
                "Least squares methods",
                "Light sources",
                "Least squares approximation",
                "Automation",
                "Educational institutions",
                "Information science",
                "Geometry",
                "Jacobian matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "learning (artificial intelligence)",
                "object recognition",
                "polynomials",
                "pose estimation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "efficient discriminative learning",
                "parts based models",
                "supervised learning",
                "optimization problem",
                "constraint set",
                "convex problem",
                "optimal model",
                "dual decomposition strategy",
                "optimized SVM light algorithm",
                "human upper body models",
                "part occlusion",
                "polynomial"
            ]
        },
        "id": 70,
        "cited_by": []
    },
    {
        "title": "Probabilistic occlusion boundary detection on spatio-temporal lattices",
        "authors": [
            "M.E. Sargin",
            "L. Bertelli",
            "B.S. Manjunath",
            "K. Rose"
        ],
        "abstract": "In this paper, we present an algorithm for occlusion boundary detection. The main contribution is a probabilistic detection framework defined on spatio-temporal lattices, which enables joint analysis of image frames. For this purpose, we introduce two complementary cost functions for creating the spatio-temporal lattice and for performing global inference of the occlusion boundaries, respectively. In addition, a novel combination of low-level occlusion features is discriminatively learnt in the detection framework. Simulations on the CMU Motion Dataset provide ample evidence that proposed algorithm outperforms the leading existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459190",
        "reference_list": [
            {
                "year": "2007",
                "id": 144
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2007",
                "id": 291
            },
            {
                "year": "2007",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 7,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Lattices",
                "Layout",
                "Lighting",
                "Least squares methods",
                "Light sources",
                "Least squares approximation",
                "Automation",
                "Educational institutions",
                "Information science",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "object detection",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic occlusion boundary detection",
                "spatio-temporal lattices",
                "image frames",
                "CMU motion dataset",
                "low-level occlusion features",
                "computer vision"
            ]
        },
        "id": 71,
        "cited_by": [
            {
                "year": "2015",
                "id": 54
            },
            {
                "year": "2013",
                "id": 214
            }
        ]
    },
    {
        "title": "Higher-order gradient descent by fusion-move graph cut",
        "authors": [
            "Hiroshi Ishikawa"
        ],
        "abstract": "Markov Random Field is now ubiquitous in many formulations of various vision problems. Recently, optimization of higher-order potentials became practical using higher-order graph cuts: the combination of i) the fusion move algorithm, ii) the reduction of higher-order binary energy minimization to first-order, and iii) the QPBO algorithm. In the fusion move, it is crucial for the success and efficiency of the optimization to provide proposals that fits the energies being optimized. For higher-order energies, it is even more so because they have richer class of null potentials. In this paper, we focus on the efficiency of the higher-order graph cuts and present a simple technique for generating proposal labelings that makes the algorithm much more efficient, which we empirically show using examples in stereo and image denoising.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459187",
        "reference_list": [
            {
                "year": "2007",
                "id": 68
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2005",
                "id": 55
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 9,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Lighting",
                "Least squares methods",
                "Light sources",
                "Least squares approximation",
                "Automation",
                "Educational institutions",
                "Information science",
                "Geometry",
                "Jacobian matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "gradient methods",
                "image denoising",
                "image fusion",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "higher-order gradient descent",
                "fusion-move graph cut",
                "Markov random field",
                "optimization",
                "QPBO algorithm",
                "higher-order graph cuts",
                "stereo denoising",
                "image denoising",
                "computer vision"
            ]
        },
        "id": 72,
        "cited_by": []
    },
    {
        "title": "Active skeleton for non-rigid object detection",
        "authors": [
            "Xiang Bai",
            "Xinggang Wang",
            "Longin Jan Latecki",
            "Wenyu Liu",
            "Zhuowen Tu"
        ],
        "abstract": "We present a shape-based algorithm for detecting and recognizing non-rigid objects from natural images. The existing literature in this domain often cannot model the objects very well. In this paper, we use the skeleton (medial axis) information to capture the main structure of an object, which has the particular advantage in modeling articulation and non-rigid deformation. Given a set of training samples, a tree-union structure is learned on the extracted skeletons to model the variation in configuration. Each branch on the skeleton is associated with a few part-based templates, modeling the object boundary information. We then apply sum-and-max algorithm to perform rapid object detection by matching the skeleton-based active template to the edge map extracted from a test image. The algorithm reports the detection result by a composition of the local maximum responses. Compared with the alternatives on this topic, our algorithm requires less training samples. It is simple, yet efficient and effective. We show encouraging results on two widely used benchmark image sets: the Weizmann horse dataset [7] and the ETHZ dataset [16].",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459188",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 1,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Skeleton",
                "Object detection",
                "Layout",
                "Lighting",
                "Least squares methods",
                "Light sources",
                "Least squares approximation",
                "Automation",
                "Educational institutions",
                "Information science"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object detection",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active skeleton",
                "shape based algorithm",
                "nonrigid objects detection",
                "objects recognition",
                "medial axis information",
                "nonrigid deformation",
                "tree union structure",
                "part based template",
                "object boundary information",
                "sum-and-max algorithm",
                "rapid object detection",
                "skeleton based active template",
                "edge map extraction",
                "local maximum responses"
            ]
        },
        "id": 73,
        "cited_by": []
    },
    {
        "title": "Sparse representation of cast shadows via \u21131-regularized least squares",
        "authors": [
            "Xue Mei",
            "Haibin Ling",
            "David W. Jacobs"
        ],
        "abstract": "Scenes with cast shadows can produce complex sets of images. These images cannot be well approximated by low-dimensional linear subspaces. However, in this paper we show that the set of images produced by a Lambertian scene with cast shadows can be efficiently represented by a sparse set of images generated by directional light sources. We first model an image with cast shadows as composed of a diffusive part (without cast shadows) and a residual part that captures cast shadows. Then, we express the problem in an \u2113 1 -regularized least squares formulation, with nonnegativity constraints. This sparse representation enjoys an effective and fast solution, thanks to recent advances in compressive sensing. In experiments on both synthetic and real data, our approach performs favorably in comparison to several previously proposed methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459185",
        "reference_list": [
            {
                "year": "2003",
                "id": 74
            },
            {
                "year": "2001",
                "id": 80
            },
            {
                "year": "2005",
                "id": 173
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Least squares methods",
                "Layout",
                "Lighting",
                "Light sources",
                "Least squares approximation",
                "Automation",
                "Educational institutions",
                "Information science",
                "Geometry",
                "Jacobian matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "least squares approximations",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparse representation",
                "cast shadows",
                "\u21131-regularized least squares formulation",
                "Lambertian scene",
                "compressive sensing"
            ]
        },
        "id": 74,
        "cited_by": [
            {
                "year": "2013",
                "id": 116
            }
        ]
    },
    {
        "title": "Robust multilinear principal component analysis",
        "authors": [
            "Kohei Inoue",
            "Kenji Hara",
            "Kiichi Urahama"
        ],
        "abstract": "We propose two methods for robustifying multilinear principal component analysis (MPCA) which is an extension of the conventional PCA for reducing the dimensions of vectors to higher-order tensors. For two kinds of outliers, i.e., sample outliers and intra-sample outliers, we derive iterative algorithms on the basis of the Lagrange multipliers. We also demonstrate that the proposed methods outperform the original MPCA when datasets contain such outliers experimentally.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459186",
        "reference_list": [
            {
                "year": "2001",
                "id": 49
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Principal component analysis",
                "Layout",
                "Lighting",
                "Least squares methods",
                "Light sources",
                "Least squares approximation",
                "Automation",
                "Educational institutions",
                "Information science"
            ],
            "INSPEC: Controlled Indexing": [
                "image sampling",
                "iterative methods",
                "principal component analysis",
                "tensors",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust multilinear principal component analysis",
                "higher-order tensors",
                "intra-sample outliers",
                "iterative algorithms",
                "Lagrange multipliers",
                "vector dimension reduction"
            ]
        },
        "id": 75,
        "cited_by": []
    },
    {
        "title": "Efficient discriminative local learning for object recognition",
        "authors": [
            "Yen-Yu Lin",
            "Jyun-Fan Tsai",
            "Tyng-Luh Liu"
        ],
        "abstract": "Although object recognition methods based on local learning can reasonably resolve the difficulties caused by the large variations in images from the same category, the high risk of overfitting and the heavy computational cost in training numerous local models (classifiers or distance functions) often limit their applicability. To address these two unpleasant issues, we cast the multiple, independent training processes of local models as a correlative multi-task learning problem, and design a new boosting algorithm to accomplish it. Specifically, we establish a parametric space where these local models lie and spread as a manifold-like structure, and use boosting to perform local model training by completing the manifold embedding. Via sharing the common embedding space, the learning of each local model can be properly regularized by the extra knowledge from other models, while the training time is also significantly reduced. Experimental results on two benchmark datasets, Caltech-101 and VOC 2007, support that our approach not only achieves promising recognition rates but also gives a two order speed-up in realizing local learning.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459182",
        "reference_list": [
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Face detection",
                "Training data",
                "Image resolution",
                "Boosting",
                "Airplanes",
                "Machine learning",
                "Linear discriminant analysis",
                "Uncertainty",
                "Information science"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative local learning",
                "object recognition methods",
                "boosting algorithm",
                "image variations",
                "manifold-like structure",
                "correlative multitask learning problem"
            ]
        },
        "id": 76,
        "cited_by": []
    },
    {
        "title": "Multiple kernels for object detection",
        "authors": [
            "Andrea Vedaldi",
            "Varun Gulshan",
            "Manik Varma",
            "Andrew Zisserman"
        ],
        "abstract": "Our objective is to obtain a state-of-the art object category detector by employing a state-of-the-art image classifier to search for the object in all possible image sub-windows. We use multiple kernel learning of Varma and Ray (ICCV 2007) to learn an optimal combination of exponential \u03c7 2 kernels, each of which captures a different feature channel. Our features include the distribution of edges, dense and sparse visual words, and feature descriptors at different levels of spatial organization. Such a powerful classifier cannot be tested on all image sub-windows in a reasonable amount of time. Thus we propose a novel three-stage classifier, which combines linear, quasi-linear, and non-linear kernel SVMs. We show that increasing the non-linearity of the kernels increases their discriminative power, at the cost of an increased computational complexity. Our contributions include (i) showing that a linear classifier can be evaluated with a complexity proportional to the number of sub-windows (independent of the sub-window area and descriptor dimension); (ii) a comparison of three efficient methods of proposing candidate regions (including the jumping window classifier of Chum and Zisserman (CVPR 2007) based on proposing windows from scale invariant features); and (Hi) introducing overlap-recall curves as a mean to compare and optimize the performance of the intermediate pipeline stages. The method is evaluated on the PASCAL Visual Object Detection Challenge, and exceeds the performances of previously published methods for most of the classes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459183",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 258,
            "other": 171,
            "total": 429
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Object detection",
                "Art",
                "Detectors",
                "Testing",
                "Costs",
                "Computational complexity",
                "Optimization methods",
                "Pipelines",
                "Performance evaluation"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object detection",
                "object category detector",
                "image classifier",
                "multiple kernel learning",
                "exponential \u03c72kernels",
                "feature descriptors",
                "image subwindows",
                "three-stage classifier",
                "nonlinear kernel SVM",
                "quasi-linear SVM",
                "linear kernel SVM",
                "support vector machine",
                "PASCAL visual object detection"
            ]
        },
        "id": 77,
        "cited_by": [
            {
                "year": "2015",
                "id": 128
            },
            {
                "year": "2015",
                "id": 293
            },
            {
                "year": "2015",
                "id": 339
            },
            {
                "year": "2013",
                "id": 15
            },
            {
                "year": "2013",
                "id": 35
            },
            {
                "year": "2013",
                "id": 91
            },
            {
                "year": "2013",
                "id": 102
            },
            {
                "year": "2013",
                "id": 147
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2013",
                "id": 257
            },
            {
                "year": "2013",
                "id": 336
            },
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2013",
                "id": 425
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2011",
                "id": 64
            },
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 231
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2011",
                "id": 272
            },
            {
                "year": "2011",
                "id": 286
            },
            {
                "year": "2011",
                "id": 337
            },
            {
                "year": "2009",
                "id": 85
            }
        ]
    },
    {
        "title": "I know what you did last summer: object-level auto-annotation of holiday snaps",
        "authors": [
            "Stephan Gammeter",
            "Lukas Bossard",
            "Till Quack",
            "Luc Van Gool"
        ],
        "abstract": "The state-of-the art in visual object retrieval from large databases allows to search millions of images on the object level. Recently, complementary works have proposed systems to crawl large object databases from community photo collections on the Internet. We combine these two lines of work to a large-scale system for auto-annotation of holiday snaps. The resulting method allows for automatic labeling objects such as landmark buildings, scenes, pieces of art etc. at the object level in a fully automatic manner. The labeling is multi-modal and consists of textual tags, geographic location, and related content on the Internet. Furthermore, the efficiency of the retrieval process is optimized by creating more compact and precise indices for visual vocabularies using background information obtained in the crawling stage of the system. We demonstrate the scalability and precision of the proposed method by conducting experiments on millions of images downloaded from community photo collections on the Internet.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459180",
        "reference_list": [
            {
                "year": "2007",
                "id": 24
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 29,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Internet",
                "Art",
                "Information retrieval",
                "Image databases",
                "Visual databases",
                "Labeling",
                "Image retrieval",
                "Large-scale systems",
                "Layout",
                "Vocabulary"
            ]
        },
        "id": 78,
        "cited_by": [
            {
                "year": "2013",
                "id": 434
            },
            {
                "year": "2011",
                "id": 84
            },
            {
                "year": "2011",
                "id": 143
            }
        ]
    },
    {
        "title": "Learning long term face aging patterns from partially dense aging databases",
        "authors": [
            "Jinli Suo",
            "Xilin Chen",
            "Shiguang Shan",
            "Wen Gao"
        ],
        "abstract": "Studies on face aging are handicapped by lack of long term dense aging sequences for model training. To handle this problem, we propose a new face aging model, which learns long term face aging patterns from partially dense aging databases. The learning strategy is based on two assumptions: (i) short term face aging pattern is relatively simple and is possible to be learned from currently available databases; (ii) long term face aging is a continuous and smooth Markov process. Adopting a compositional face representation, our aging algorithm learns a function-based short term aging model from real aging sequences to infer facial parameters within a short age span. Based on the predefined smoothness criteria between two overlapping short term aging patterns, we concatenate these learned short term aging patterns to build the long term aging patterns. Both the subjective assessment and objective evaluations of synthetic aging sequences validate the effectiveness of the proposed model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459181",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 0,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Aging",
                "Databases",
                "Biological system modeling",
                "Prototypes",
                "Face detection",
                "Skin",
                "Content addressable storage",
                "Computer vision",
                "Muscles",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "image sequences",
                "learning (artificial intelligence)",
                "Markov processes",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "handicapped",
                "model training",
                "face aging model",
                "learning strategy",
                "Markov process",
                "face representation",
                "real aging sequences",
                "smoothness criteria",
                "subjective assessment",
                "objective evaluations"
            ]
        },
        "id": 79,
        "cited_by": [
            {
                "year": "2013",
                "id": 358
            }
        ]
    },
    {
        "title": "Beyond the Euclidean distance: Creating effective visual codebooks using the Histogram Intersection Kernel",
        "authors": [
            "Jianxin Wu",
            "James M. Rehg"
        ],
        "abstract": "Common visual codebook generation methods used in a Bag of Visual words model, e.g. k-means or Gaussian Mixture Model, use the Euclidean distance to cluster features into visual code words. However, most popular visual descriptors are histograms of image measurements. It has been shown that the Histogram Intersection Kernel (HIK) is more effective than the Euclidean distance in supervised learning tasks with histogram features. In this paper, we demonstrate that HIK can also be used in an unsupervised manner to significantly improve the generation of visual codebooks. We propose a histogram kernel k-means algorithm which is easy to implement and runs almost as fast as k-means. The HIK codebook has consistently higher recognition accuracy over k-means codebooks by 2-4%. In addition, we propose a one-class SVM formulation to create more effective visual code words which can achieve even higher accuracy. The proposed method has established new state-of-the-art performance numbers for 3 popular benchmark datasets on object and scene recognition. In addition, we show that the standard k-median clustering method can be used for visual codebook generation and can act as a compromise between HIK and k-means approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459178",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2007",
                "id": 27
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2007",
                "id": 198
            },
            {
                "year": "2007",
                "id": 36
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 6,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Euclidean distance",
                "Histograms",
                "Kernel",
                "Layout",
                "Support vector machines",
                "Data mining",
                "Intelligent robots",
                "Code standards",
                "Unsupervised learning",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Gaussian processes",
                "learning (artificial intelligence)",
                "pattern clustering",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Euclidean distance",
                "histogram intersection kernel",
                "visual codebook generation methods",
                "Gaussian mixture model",
                "supervised learning tasks",
                "SVM formulation",
                "k-median clustering method"
            ]
        },
        "id": 80,
        "cited_by": [
            {
                "year": "2013",
                "id": 35
            }
        ]
    },
    {
        "title": "Incremental Multiple Kernel Learning for object recognition",
        "authors": [
            "Aniruddha Kembhavi",
            "Behjat Siddiquie",
            "Roland Miezianko",
            "Scott McCloskey",
            "Larry S. Davis"
        ],
        "abstract": "A good training dataset, representative of the test images expected in a given application, is critical for ensuring good performance of a visual categorization system. Obtaining task specific datasets of visual categories is, however, far more tedious than obtaining a generic dataset of the same classes. We propose an Incremental Multiple Kernel Learning (IMKL) approach to object recognition that initializes on a generic training database and then tunes itself to the classification task at hand. Our system simultaneously updates the training dataset as well as the weights used to combine multiple information sources. We demonstrate our system on a vehicle classification problem in a video stream overlooking a traffic intersection. Our system updates itself with images of vehicles in poses more commonly observed in the scene, as well as with image patches of the background, leading to an increase in performance. A considerable change in the kernel combination weights is observed as the system gathers scene specific training data over time. The system is also seen to adapt itself to the illumination change in the scene as day transitions to night.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459179",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 8,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Object recognition",
                "Layout",
                "Image databases",
                "Lighting",
                "Vehicles",
                "Support vector machines",
                "Support vector machine classification",
                "Cameras",
                "Visual databases"
            ]
        },
        "id": 81,
        "cited_by": []
    },
    {
        "title": "Convex optimization for multi-class image labeling with a novel family of total variation based regularizers",
        "authors": [
            "J. Lellmann",
            "F. Becker",
            "C. Schn\u00f6rr"
        ],
        "abstract": "We introduce a linearly weighted variant of the total variation for vector fields in order to formulate regularizers for multi-class labeling problems with non-trivial interclass distances. We characterize the possible distances, show that Euclidean distances can be exactly represented, and review some methods to approximate non-Euclidean distances in order to define novel total variation based regularizers. We show that the convex relaxed problem can be efficiently optimized to a prescribed accuracy with optimality certificates using Nesterov's method, and evaluate and compare our approach on several synthetical and real-world examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459176",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 19,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Image segmentation",
                "Costs",
                "Spatial coherence",
                "TV",
                "Jacobian matrices",
                "Pattern analysis",
                "Human computer interaction",
                "Mathematics",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "geometry",
                "image colour analysis",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convex optimization",
                "multiclass image labeling",
                "total variation based regularizers",
                "Euclidean distances",
                "nonEuclidean distances",
                "Nesterov method",
                "color segmentation"
            ]
        },
        "id": 82,
        "cited_by": [
            {
                "year": "2013",
                "id": 367
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2011",
                "id": 266
            },
            {
                "year": "2011",
                "id": 296
            },
            {
                "year": "2011",
                "id": 333
            }
        ]
    },
    {
        "title": "The infinite Hidden Markov random field model",
        "authors": [
            "Sotirios P. Chatzis",
            "Gabriel Tsechpenakis"
        ],
        "abstract": "Dirichlet process (DP) mixture models have recently emerged in the cornerstone of nonparametric Bayesian statistics as promising candidates for clustering applications where the number of clusters is unknown a priori. Hidden Markov random field (HMRF) models are parametric statistical models widely used for image segmentation, as they appear naturally in problems where a spatially-constrained clustering scheme is asked for. A major limitation of HMRF models concerns the automatic selection of the proper number of their states, i.e. the number of segments derived by the image segmentation procedure. Typically, for this purpose, various likelihood based criteria are employed. Nevertheless, such methods often fail to yield satisfactory results, exhibiting significant overfitting proneness. Recently, higher order conditional random field models using potentials defined on superpixels have been considered as alternatives tackling these issues. Still, these models are in general computationally inefficient, a fact that limits their widespread adoption in practical applications. To resolve these issues, in this paper we introduce a novel, nonparametric Bayesian formulation for the HMRF model, the infinite HMRF model. We describe an efficient variational Bayesian inference algorithm for the proposed model, and we apply it to a series of image segmentation problems, demonstrating its advantages over existing methodologies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459177",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Image segmentation",
                "Bayesian methods",
                "Inference algorithms",
                "Belief propagation",
                "Markov random fields",
                "Parametric statistics",
                "Pixel",
                "Yield estimation",
                "Approximation methods"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "hidden Markov models",
                "image segmentation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "infinite hidden Markov random field model",
                "Dirichlet process mixture models",
                "nonparametric Bayesian statistics",
                "image segmentation",
                "parametric statistical models",
                "spatially-constrained clustering scheme",
                "likelihood based criteria",
                "higher order conditional random field models"
            ]
        },
        "id": 83,
        "cited_by": []
    },
    {
        "title": "Convex multi-region segmentation on manifolds",
        "authors": [
            "Ama\u00ebl Delaunoy",
            "Ketut Fundana",
            "Emmanuel Prados",
            "Anders Heyden"
        ],
        "abstract": "In this paper, we address the problem of segmenting data defined on a manifold into a set of regions with uniform properties. In particular, we propose a numerical method when the manifold is represented by a triangular mesh. Based on recent image segmentation models, our method minimizes a convex energy and then enjoys significant favorable properties: it is robust to initialization and avoid the problem of the existence of local minima present in many variational models. The contributions of this paper are threefold: firstly we adapt the convex image labeling model to manifolds; in particular the total variation formulation. Secondly we show how to implement the proposed method on triangular meshes, and finally we show how to use and combine the method in other computer vision problems, such as 3D reconstruction. We demonstrate the efficiency of our method by testing it on various data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459174",
        "reference_list": [
            {
                "year": "2007",
                "id": 176
            },
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2001",
                "id": 7
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 6,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image reconstruction",
                "Surface reconstruction",
                "Surface texture",
                "Computer vision",
                "Labeling",
                "Layout",
                "Mathematics",
                "Robustness",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "image reconstruction",
                "image segmentation",
                "mesh generation",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convex multiregion segmentation",
                "manifold",
                "triangular mesh",
                "image segmentation",
                "convex energy minimization",
                "convex image labeling model",
                "total variation formulation",
                "computer vision problem",
                "3D reconstruction"
            ]
        },
        "id": 84,
        "cited_by": [
            {
                "year": "2011",
                "id": 310
            }
        ]
    },
    {
        "title": "Class segmentation and object localization with superpixel neighborhoods",
        "authors": [
            "Brian Fulkerson",
            "Andrea Vedaldi",
            "Stefano Soatto"
        ],
        "abstract": "We propose a method to identify and localize object classes in images. Instead of operating at the pixel level, we advocate the use of superpixels as the basic unit of a class segmentation or pixel localization scheme. To this end, we construct a classifier on the histogram of local features found in each superpixel. We regularize this classifier by aggregating histograms in the neighborhood of each superpixel and then refine our results further by using the classifier in a conditional random field operating on the superpixel graph. Our proposed method exceeds the previously published state-of-the-art on two challenging datasets: Graz-02 and the PASCAL VOC 2007 Segmentation Challenge.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459175",
        "reference_list": [
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2009",
                "id": 275
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 199,
            "other": 123,
            "total": 322
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Histograms",
                "Pixel",
                "Object detection",
                "Computer science",
                "Detectors",
                "Grid computing",
                "Merging",
                "Time measurement",
                "Statistics"
            ]
        },
        "id": 85,
        "cited_by": [
            {
                "year": "2015",
                "id": 153
            },
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2013",
                "id": 106
            },
            {
                "year": "2013",
                "id": 232
            },
            {
                "year": "2013",
                "id": 288
            },
            {
                "year": "2011",
                "id": 1
            },
            {
                "year": "2011",
                "id": 56
            },
            {
                "year": "2011",
                "id": 328
            }
        ]
    },
    {
        "title": "Spectral clustering of linear subspaces for motion segmentation",
        "authors": [
            "Fabien Lauer",
            "Christoph Schn\u00f6rr"
        ],
        "abstract": "This paper studies automatic segmentation of multiple motions from tracked feature points through spectral embedding and clustering of linear subspaces. We show that the dimension of the ambient space is crucial for separability, and that low dimensions chosen in prior work are not optimal. We suggest lower and upper bounds together with a data-driven procedure for choosing the optimal ambient dimension. Application of our approach to the Hopkins155 video benchmark database uniformly outperforms a range of state-of-the-art methods both in terms of segmentation accuracy and computational speed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459173",
        "reference_list": [
            {
                "year": "2001",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 50,
            "other": 28,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Computer vision",
                "Clustering algorithms",
                "Image segmentation",
                "Motion analysis",
                "Tracking",
                "Image processing",
                "Upper bound",
                "Spatial databases",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "image sequences",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spectral clustering",
                "motion segmentation",
                "spectral embedding",
                "linear subspace spectral clustering",
                "upper bounds",
                "lower bounds",
                "Hopkins155 video benchmark database",
                "video sequences"
            ]
        },
        "id": 86,
        "cited_by": [
            {
                "year": "2015",
                "id": 523
            },
            {
                "year": "2013",
                "id": 197
            },
            {
                "year": "2011",
                "id": 204
            }
        ]
    },
    {
        "title": "Multiple view semantic segmentation for street view images",
        "authors": [
            "Jianxiong Xiao",
            "Long Quan"
        ],
        "abstract": "We propose a simple but powerful multi-view semantic segmentation framework for images captured by a camera mounted on a car driving along streets. In our approach, a pair-wise Markov Random Field (MRF) is laid out across multiple views. Both 2D and 3D features are extracted at a super-pixel level to train classifiers for the unary data terms of MRF. For smoothness terms, our approach makes use of color differences in the same image to identify accurate segmentation boundaries, and dense pixel-to-pixel correspondences to enforce consistency across different views. To speed up training and to improve the recognition quality, our approach adaptively selects the most similar training data for each scene from the label pool. Furthermore, we also propose a powerful approach within the same framework to enable large-scale labeling in both the 3D space and 2D images. We demonstrate our approach on more than 10,000 images from Google Maps Street View.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459249",
        "reference_list": [
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2005",
                "id": 235
            },
            {
                "year": "2007",
                "id": 89
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 5,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Cameras",
                "Markov random fields",
                "Feature extraction",
                "Data mining",
                "Pixel",
                "Training data",
                "Layout",
                "Large-scale systems",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image colour analysis",
                "image recognition",
                "image segmentation",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "street view image",
                "multiview semantic segmentation",
                "pair-wise Markov random field",
                "feature extraction",
                "super-pixel level",
                "smoothness term",
                "image color difference",
                "image segmentation",
                "dense pixel-to-pixel correspondence",
                "recognition quality",
                "large-scale labeling",
                "3D space",
                "2D image"
            ]
        },
        "id": 87,
        "cited_by": [
            {
                "year": "2015",
                "id": 132
            },
            {
                "year": "2013",
                "id": 202
            },
            {
                "year": "2011",
                "id": 15
            },
            {
                "year": "2011",
                "id": 56
            }
        ]
    },
    {
        "title": "Gradient domain layer separation under independent motion",
        "authors": [
            "Yunqiang Chen",
            "Ti-Chiun Chang",
            "Chunxiao Zhou",
            "Tong Fang"
        ],
        "abstract": "Multi-exposure X-ray imaging can see through objects and separate different material into transparent layers. However, layer motion makes the separation task under-determined. Instead of aligning the non-rigid motion, we address the layer separation problem in gradient domain and propose an energy optimization framework to regularize it by explicitly enforcing independence constraint. It is shown that gradient domain allows more accurate and robust independence analysis between non-stationary signal using mutual information (MI) and hence achieves better separation. Furthermore, gradient fields contain sufficient information for full reconstruction of separated layers by solving the Poisson Equation. For efficient regularization of the gradient separation, energy terms based on the Taylor expansion of MI is further derived. Evaluation on both synthesized and real datasets proves the effectiveness of our algorithm and its robustness to complex tissue motion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459171",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "X-ray imaging",
                "Constraint optimization",
                "Information analysis",
                "Signal analysis",
                "Mutual information",
                "Image reconstruction",
                "Poisson equations",
                "Taylor series",
                "Signal synthesis"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image motion analysis",
                "medical image processing",
                "Poisson equation",
                "X-ray imaging"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gradient domain layer separation",
                "multiexposure X-ray imaging",
                "nonrigid motion",
                "energy optimization framework",
                "nonstationary signal",
                "mutual information",
                "Poisson equation",
                "tissue motion"
            ]
        },
        "id": 88,
        "cited_by": []
    },
    {
        "title": "Image segmentation with simultaneous illumination and reflectance estimation: An energy minimization approach",
        "authors": [
            "Chunming Li",
            "Fang Li",
            "Chiu-Yen Kao",
            "Chenyang Xu"
        ],
        "abstract": "Spatial intensity variations caused by illumination changes have been a challenge for image segmentation and many other computer vision tasks. This paper presents a novel method for image segmentation with simultaneous estimation of illumination and reflectance images. The proposed method is based on the composition of an observed scene image with an illumination component and a reflectance component, known as intrinsic images. We define an energy functional in terms of an illumination image, the membership functions of the regions, and the corresponding reflectance constants of the regions in the scene. This energy is convex in each of its variables. By minimizing the energy, image segmentation result is obtained in the form of the membership functions of the regions. The illumination and reflectance components of the observed image are estimated simultaneously as the result of energy minimization. With illumination taken into account, the proposed method is able to segment images with non-uniform intensities caused by spatial variations in illumination. Comparisons with the state-of-the-art piecewise smooth model demonstrate the superior performance of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459239",
        "reference_list": [
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Lighting",
                "Reflectivity",
                "Layout",
                "Computer vision",
                "Mathematics",
                "State estimation",
                "Geometry",
                "Object recognition",
                "Smoothing methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "reflectance estimation",
                "energy minimization approach",
                "computer vision",
                "reflectance images",
                "illumination images",
                "intrinsic images",
                "membership functions",
                "spatial variations",
                "piecewise smooth model"
            ]
        },
        "id": 89,
        "cited_by": []
    },
    {
        "title": "Robust dynamical model for simultaneous registration and segmentation in a variational framework: A Bayesian approach",
        "authors": [
            "Pratim Ghosh",
            "Mehmet Emre Sargin",
            "B.S. Manjunath"
        ],
        "abstract": "We introduce a dynamical model for simultaneous registration and segmentation in a variational framework for image sequences, where the dynamics is incorporated using a Bayesian formulation. A linear stochastic equation relating the tracked object (or a region of interest) is first derived under the assumption that the successive images in the sequence are related by a dense and possibly non-linear displacement field. This derivation allows for the use of a computationally efficient and recursive implementation of the Bayesian formulation in this framework. The contour of the tracked object returned by the dynamical model is not only close to the previously detected shape but is also consistent with the temporal statistics of the tracked object. The performance of the proposed approach is evaluated on real image sequences. It is shown that, with respect to a variety of error metrics such as F-measure, mean absolute deviation and Hausdorff distance, the proposed approach outperforms the state-of-the art approach without the dynamical model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459170",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Bayesian methods",
                "Image segmentation",
                "Image sequences",
                "Stochastic processes",
                "Nonlinear equations",
                "Object detection",
                "Shape",
                "Statistics",
                "Art"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image registration",
                "image segmentation",
                "image sequences",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust dynamical model",
                "simultaneous registration",
                "simultaneous segmentation",
                "variational framework",
                "Bayesian formulation",
                "image sequences",
                "linear stochastic equation",
                "error metrics",
                "mean absolute deviation",
                "Hausdorff distance"
            ]
        },
        "id": 90,
        "cited_by": []
    },
    {
        "title": "Beyond connecting the dots: A polynomial-time algorithm for segmentation and boundary estimation with imprecise user input",
        "authors": [
            "Thomas Windheuser",
            "Thomas Schoenemann",
            "Daniel Cremers"
        ],
        "abstract": "We propose a polynomial-time algorithm for segmentation and (open) boundary estimation which takes into account a series of user-specified attraction points. In contrast to existing algorithms which impose that the segmenting boundary passes through these points, our algorithm allows an imprecision in the user input. An energy minimization approach imposes that the segmenting boundary optimally passes along high-contrast edges in such a way that at least one point along the computed boundary is as close as possible to any given attraction point. In this sense, the user input can be seen as a soft constraint. We prove that the resulting optimization problem is NP-hard. We prove that in the case that the user attraction points are ordered, then optimal solutions can be computed in polynomial time using a shortest path formulation in an appropriately constructed four-dimensional graph spanned by the image pixels, a set of tangent angles and the user attraction points. Experimental results on a variety of images demonstrate that good quality segmentations can be obtained with a few imprecise user clicks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459281",
        "reference_list": [
            {
                "year": "2007",
                "id": 131
            },
            {
                "year": "2007",
                "id": 88
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 6,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Joining processes",
                "Polynomials",
                "Image segmentation",
                "Pixel",
                "Computer vision",
                "Computer science",
                "Layout",
                "Level set",
                "Labeling",
                "Degradation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "polynomial-time algorithm",
                "imprecise user input",
                "image segmentation",
                "open boundary estimation",
                "user-specified attraction points",
                "energy minimization approach",
                "soft constraint",
                "optimization",
                "NP-hard problem",
                "polynomial time",
                "shortest path formulation",
                "four-dimensional graph",
                "image pixels"
            ]
        },
        "id": 91,
        "cited_by": []
    },
    {
        "title": "Implicit color segmentation features for pedestrian and object detection",
        "authors": [
            "Patrick Ott",
            "Mark Everingham"
        ],
        "abstract": "We investigate the problem of pedestrian detection in still images. Sliding window classifiers, notably using the Histogram-of-Gradient (HOG) features proposed by Dalal and Triggs are the state-of-the-art for this task, and we base our method on this approach. We propose a novel feature extraction scheme which computes implicit `soft segmentations' of image regions into foreground/background. The method yields stronger object/background edges than gray-scale gradient alone, suppresses textural and shading variations, and captures local coherence of object appearance. The main contributions of our work are: (i) incorporation of segmentation cues into object detection; (ii) integration with classifier learning cf. a post-processing filter; (iii) high computational efficiency. We report results on the INRIA person detection dataset, achieving state-of-the-art results considerably exceeding those of the original HOG detector. Preliminary results for generic object detection on the PASCAL VOC2006 dataset also show substantial improvements in accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459238",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 46
            },
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 25,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Image segmentation",
                "Feature extraction",
                "Detectors",
                "Support vector machines",
                "Support vector machine classification",
                "Histograms",
                "Image edge detection",
                "Filters",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "filtering theory",
                "image colour analysis",
                "image segmentation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "implicit color segmentation features",
                "pedestrian-object detection",
                "histogram-of-gradient features",
                "object-background edges",
                "textural-shading variations",
                "computational efficiency",
                "post-processing filter",
                "HOG detector",
                "PASCAL VOC2006 dataset"
            ]
        },
        "id": 92,
        "cited_by": [
            {
                "year": "2017",
                "id": 529
            },
            {
                "year": "2013",
                "id": 261
            }
        ]
    },
    {
        "title": "Power watersheds: A new image segmentation framework extending graph cuts, random walker and optimal spanning forest",
        "authors": [
            "Camille Couprie",
            "Leo Grady",
            "Laurent Najman",
            "Hugues Talbot"
        ],
        "abstract": "In this work, we extend a common framework for seeded image segmentation that includes the graph cuts, random walker, and shortest path optimization algorithms. Viewing an image as a weighted graph, these algorithms can be expressed by means of a common energy function with differing choices of a parameter q acting as an exponent on the differences between neighboring nodes. Introducing a new parameter p that fixes a power for the edge weights allows us to also include the optimal spanning forest algorithm for watersheds in this same framework. We then propose a new family of segmentation algorithms that fixes p to produce an optimal spanning forest but varies the power q beyond the usual watershed algorithm, which we term power watersheds. Placing the watershed algorithm in this energy minimization framework also opens new possibilities for using unary terms in traditional watershed segmentation and using watersheds to optimize more general models of use in application beyond image segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459284",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2007",
                "id": 88
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 33,
            "total": 69
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Minimization methods",
                "Tree graphs",
                "Computer vision",
                "Surface topography",
                "User interfaces",
                "Computer errors",
                "Visualization",
                "Benchmark testing",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "power watersheds",
                "image segmentation framework",
                "graph cuts",
                "random walker",
                "optimal spanning forest",
                "shortest path optimization algorithms",
                "energy minimization framework"
            ]
        },
        "id": 93,
        "cited_by": [
            {
                "year": "2013",
                "id": 230
            }
        ]
    },
    {
        "title": "Associative hierarchical CRFs for object class image segmentation",
        "authors": [
            "L'ubor Ladick\u00fd",
            "Chris Russell",
            "Pushmeet Kohli",
            "Philip H.S. Torr"
        ],
        "abstract": "Most methods for object class segmentation are formulated as a labelling problem over a single choice of quantisation of an image space - pixels, segments or group of segments. It is well known that each quantisation has its fair share of pros and cons; and the existence of a common optimal quantisation level suitable for all object categories is highly unlikely. Motivated by this observation, we propose a hierarchical random field model, that allows integration of features computed at different levels of the quantisation hierarchy. MAP inference in this model can be performed efficiently using powerful graph cut based move making algorithms. Our framework generalises much of the previous work based on pixels or segments. We evaluate its efficiency on some of the most challenging data-sets for object class segmentation, and show it obtains state-of-the-art results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459248",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2001",
                "id": 70
            }
        ],
        "citation": {
            "ieee": 183,
            "other": 87,
            "total": 270
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Quantization",
                "Pixel",
                "Labeling",
                "Object segmentation",
                "Inference algorithms",
                "Computer vision",
                "Object recognition",
                "Optimization methods",
                "Color"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "quantisation (signal)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "associative hierarchical conditional random field",
                "object class image segmentation",
                "labelling problem",
                "image space",
                "optimal quantisation level",
                "hierarchical random field model",
                "quantisation hierarchy",
                "MAP inference",
                "powerful graph cut",
                "pixels",
                "data sets"
            ]
        },
        "id": 94,
        "cited_by": [
            {
                "year": "2017",
                "id": 410
            },
            {
                "year": "2017",
                "id": 523
            },
            {
                "year": "2015",
                "id": 170
            },
            {
                "year": "2015",
                "id": 288
            },
            {
                "year": "2013",
                "id": 38
            },
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2013",
                "id": 106
            },
            {
                "year": "2013",
                "id": 177
            },
            {
                "year": "2013",
                "id": 215
            },
            {
                "year": "2013",
                "id": 232
            },
            {
                "year": "2013",
                "id": 271
            },
            {
                "year": "2013",
                "id": 384
            },
            {
                "year": "2013",
                "id": 387
            },
            {
                "year": "2011",
                "id": 1
            },
            {
                "year": "2011",
                "id": 228
            }
        ]
    },
    {
        "title": "Segmentation, ordering and multi-object tracking using graphical models",
        "authors": [
            "Chaohui Wang",
            "Martin de La Gorce",
            "Nikos Paragios"
        ],
        "abstract": "In this paper, we propose a unified graphical-model framework to interpret a scene composed of multiple objects in monocular video sequences. Using a single pairwise Markov random field (MRF), all the observed and hidden variables of interest such as image intensities, pixels' states (associated object's index and relative depth), objects' states (model motion parameters and relative depth) are jointly considered. Particular attention is given to occlusion handling by introducing a rigorous visibility modeling within the MRF formulation. Through minimizing the MRF's energy, we simultaneously segment, track and sort by depth the objects. Promising experimental results demonstrate the potential of this framework and its robustness to image noise, cluttered background, moving camera and background, and even complete occlusions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459247",
        "reference_list": [
            {
                "year": "2005",
                "id": 4
            },
            {
                "year": "2007",
                "id": 370
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 0,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Graphical models",
                "Image segmentation",
                "Shape",
                "Layout",
                "Chaos",
                "Video sequences",
                "Pixel",
                "Background noise",
                "Computer vision",
                "Active contours"
            ]
        },
        "id": 95,
        "cited_by": [
            {
                "year": "2013",
                "id": 365
            }
        ]
    },
    {
        "title": "Joint optimization of segmentation and appearance models",
        "authors": [
            "Sara Vicente",
            "Vladimir Kolmogorov",
            "Carsten Rother"
        ],
        "abstract": "Many interactive image segmentation approaches use an objective function which includes appearance models as an unknown variable. Since the resulting optimization problem is NP-hard the segmentation and appearance are typically optimized separately, in an EM-style fashion. One contribution of this paper is to express the objective function purely in terms of the unknown segmentation, using higher-order cliques. This formulation reveals an interesting bias of the model towards balanced segmentations. Furthermore, it enables us to develop a new dual decomposition optimization procedure, which provides additionally a lower bound. Hence, we are able to improve on existing optimizers, and verify that for a considerable number of real world examples we even achieve global optimality. This is important since we are able, for the first time, to analyze the deficiencies of the model. Another contribution is to establish a property of a particular dual decomposition approach which involves convex functions depending on foreground area. As a consequence, we show that the optimal decomposition for our problem can be computed efficiently via a parametric maxflow algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459287",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2009",
                "id": 298
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 18,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Educational institutions",
                "Object recognition",
                "Training data",
                "Packaging",
                "Biomedical imaging",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "expectation-maximisation algorithm",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "segmentation model",
                "appearance model",
                "interactive image segmentation",
                "EM-style fashion",
                "higher-order clique",
                "dual decomposition optimization",
                "global optimality",
                "convex function",
                "optimal decomposition",
                "parametric maxflow algorithm"
            ]
        },
        "id": 96,
        "cited_by": [
            {
                "year": "2015",
                "id": 173
            },
            {
                "year": "2015",
                "id": 181
            },
            {
                "year": "2015",
                "id": 196
            },
            {
                "year": "2015",
                "id": 197
            },
            {
                "year": "2013",
                "id": 220
            },
            {
                "year": "2009",
                "id": 298
            }
        ]
    },
    {
        "title": "Level set segmentation with both shape and intensity priors",
        "authors": [
            "Siqi Chen",
            "Richard J. Radke"
        ],
        "abstract": "We present a new variational level-set-based segmentation formulation that uses both shape and intensity prior information learned from a training set. By applying Bayes' rule to the segmentation problem, the cost function decomposes into shape and image energy parts. The shape energy is based on recently proposed nonparametric shape distributions, and we propose a new image energy model that incorporates learned intensity information from both foreground and background objects. The proposed variational level set segmentation framework has two main advantages. First, by characterizing image information with regional intensity distributions, there is no need to balance image energy and shape energy using a heuristic weighting factor. Second, by incorporating learned intensity information into the image model using a nonparametric density estimation method and an appropriate distance measure, our segmentation framework can handle problems where the interior/exterior of the shape has a highly inhomogeneous intensity distribution. We demonstrate our segmentation algorithm using challenging pelvis CT scans.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459290",
        "reference_list": [
            {
                "year": "2003",
                "id": 54
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 7,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Level set",
                "Image segmentation",
                "Active contours",
                "Shape measurement",
                "Solid modeling",
                "Power engineering and energy",
                "Systems engineering and theory",
                "Cost function",
                "Density measurement",
                "Pelvis"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image segmentation",
                "set theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational level-set-based segmentation formulation",
                "intensity prior information",
                "training set",
                "Bayes rule",
                "image energy parts",
                "shape energy",
                "nonparametric shape distributions",
                "heuristic weighting factor",
                "nonparametric density estimation method",
                "appropriate distance measure",
                "intensity distribution"
            ]
        },
        "id": 97,
        "cited_by": []
    },
    {
        "title": "Scene shape priors for superpixel segmentation",
        "authors": [
            "Alastair P. Moore",
            "Simon J. D. Prince",
            "Jonathan Warrell",
            "Umar Mohammed",
            "Graham Jones"
        ],
        "abstract": "Unsupervised over-segmentation of an image into super-pixels is a common preprocessing step for image parsing algorithms. Superpixels are used as both regions of support for feature vectors and as a starting point for the final segmentation. In this paper we investigate incorporating a priori information into superpixel segmentations. We learn a probabilistic model that describes the spatial density of the object boundaries in the image. We then describe an over-segmentation algorithm that partitions this density roughly equally between superpixels whilst still attempting to capture local object boundaries. We demonstrate this approach using road scenes where objects in the center of the image tend to be more distant and smaller than those at the edge. We show that our algorithm successfully learns this foveated spatial distribution and can exploit this knowledge to improve the segmentation. Lastly, we introduce a new metric for evaluating vision labeling problems. We measure performance on a challenging real-world dataset and illustrate the limitations of conventional evaluation metrics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459246",
        "reference_list": [
            {
                "year": "2007",
                "id": 144
            },
            {
                "year": "2005",
                "id": 185
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Shape",
                "Image segmentation",
                "Object detection",
                "Partitioning algorithms",
                "Laboratories",
                "Europe",
                "Educational institutions",
                "Roads",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "probability",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene shape priors",
                "superpixel segmentation",
                "unsupervised over-segmentation",
                "image parsing algorithm",
                "feature vectors",
                "probabilistic model",
                "road scenes",
                "spatial distribution",
                "vision labeling problem",
                "evaluation metrics"
            ]
        },
        "id": 98,
        "cited_by": [
            {
                "year": "2011",
                "id": 56
            }
        ]
    },
    {
        "title": "LIVEcut: Learning-based interactive video segmentation by evaluation of multiple propagated cues",
        "authors": [
            "Brian L. Price",
            "Bryan S. Morse",
            "Scott Cohen"
        ],
        "abstract": "Video sequences contain many cues that may be used to segment objects in them, such as color, gradient, color adjacency, shape, temporal coherence, camera and object motion, and easily-trackable points. This paper introduces LIVEcut, a novel method for interactively selecting objects in video sequences by extracting and leveraging as much of this information as possible. Using a graph-cut optimization framework, LIVEcut propagates the selection forward frame by frame, allowing the user to correct any mistakes along the way if needed. Enhanced methods of extracting many of the features are provided. In order to use the most accurate information from the various potentially-conflicting features, each feature is automatically weighted locally based on its estimated accuracy using the previous implicitly-validated frame. Feature weights are further updated by learning from the user corrections required in the previous frame. The effectiveness of LIVEcut is shown through timing comparisons to other interactive methods, accuracy comparisons to unsupervised methods, and qualitatively through selections on various video sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459293",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 60,
            "other": 26,
            "total": 86
        },
        "keywords": {
            "IEEE Keywords": [
                "Video compression",
                "Data mining",
                "Shape",
                "Video sequences",
                "Video sharing",
                "Cameras",
                "Image segmentation",
                "Feature extraction",
                "Image coding",
                "Spatiotemporal phenomena"
            ]
        },
        "id": 99,
        "cited_by": [
            {
                "year": "2017",
                "id": 228
            },
            {
                "year": "2017",
                "id": 458
            },
            {
                "year": "2017",
                "id": 467
            },
            {
                "year": "2015",
                "id": 192
            },
            {
                "year": "2015",
                "id": 361
            },
            {
                "year": "2013",
                "id": 228
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2013",
                "id": 449
            },
            {
                "year": "2011",
                "id": 145
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2011",
                "id": 253
            }
        ]
    },
    {
        "title": "Non-Euclidean image-adaptive Radial Basis Functions for 3D interactive segmentation",
        "authors": [
            "Benoit Mory",
            "Roberto Ardon",
            "Anthony J. Yezzi",
            "Jean-Philippe Thiran"
        ],
        "abstract": "In the context of variational image segmentation, we propose a new finite-dimensional implicit surface representation. The key idea is to span a subset of implicit functions with linear combinations of spatially-localized kernels that follow image features. This is achieved by replacing the Euclidean distance in conventional Radial Basis Functions with non-Euclidean, image-dependent distances. For the minimization of an objective region-based criterion, this representation yields more accurate results with fewer control points than its Euclidean counterpart. If the user positions these control points, the non-Euclidean distance enables to further specify our localized kernels for a target object in the image. Moreover, an intuitive control of the result of the segmentation is obtained by casting inside/outside labels as linear inequality constraints. Finally, we discuss several algorithmic aspects needed for a responsive interactive workflow. We have applied this framework to 3D medical imaging and built a real-time prototype with which the segmentation of whole organs is only a few clicks away.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459245",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2001",
                "id": 187
            },
            {
                "year": "2003",
                "id": 165
            },
            {
                "year": "2007",
                "id": 120
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 12,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Target tracking",
                "Pixel",
                "Principal component analysis",
                "Testing",
                "Data mining",
                "Kernel",
                "Noise robustness",
                "Parameter estimation",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "curve fitting",
                "image representation",
                "image segmentation",
                "interactive systems",
                "radial basis function networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonEuclidean image-adaptive radial basis functions",
                "3D interactive segmentation",
                "variational image segmentation",
                "finite-dimensional implicit surface representation",
                "spatially-localized kernels",
                "image features",
                "image-dependent distances",
                "3D medical imaging"
            ]
        },
        "id": 100,
        "cited_by": [
            {
                "year": "2013",
                "id": 155
            }
        ]
    },
    {
        "title": "Automatic ovarian follicle quantification from 3D ultrasound data using global/local context with database guided segmentation",
        "authors": [
            "Terrence Chen",
            "Wei Zhang",
            "Sara Good",
            "Kevin S. Zhou",
            "Dorin Comaniciu"
        ],
        "abstract": "In this paper, we present a novel probabilistic framework for automatic follicle quantification in 3D ultrasound data. The proposed framework robustly estimates size and location of each individual ovarian follicle by fusing the information from both global and local context. Follicle candidates at detected locations are then segmented by a novel database guided segmentation method. To efficiently search hypothesis in a high dimensional space for multiple object detection, a clustered marginal space learning approach is introduced. Extensive evaluations conducted on 501 volumes containing 8108 follicles showed that our method is able to detect and segment ovarian follicles with high robustness and accuracy. It is also much faster than the current ultrasound manual workflow. The proposed method is able to streamline the clinical workflow and improve the accuracy of existing follicular measurements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459243",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2007",
                "id": 85
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 7,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Ultrasonic imaging",
                "Databases",
                "Ultrasonic variables measurement",
                "Shape measurement",
                "Volume measurement",
                "Robustness",
                "In vitro fertilization",
                "Object detection",
                "Monitoring",
                "Measurement standards"
            ],
            "INSPEC: Controlled Indexing": [
                "biological organs",
                "biomedical ultrasonics",
                "image segmentation",
                "object detection",
                "probability",
                "sensor fusion",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic ovarian follicle quantification",
                "3D ultrasound data",
                "global-local context",
                "database guided segmentation",
                "clustered marginal space learning approach",
                "probabilistic framework",
                "search hypothesis",
                "object detection",
                "information fusion"
            ]
        },
        "id": 101,
        "cited_by": []
    },
    {
        "title": "Non-rigid object localization and segmentation using eigenspace representation",
        "authors": [
            "Omar Arif",
            "Patricio Antonio Vela"
        ],
        "abstract": "This paper presents a novel non-rigid object localization and segmentation algorithm using an eigenspace representation. Previous approaches to eigenspace methods for object tracking use vectorized image regions as observations, whereas the proposed method uses each individual pixel as an observation. Localization using the pixel-wise eigenspace representation is robust to noise and occlusions. A unique feature of the approach is that it permits segmentation in addition to localization. Localization and segmentation are carried out by deriving a similarity function in the eigenspace. The algorithm is tested on synthetic and real world tracking examples to demonstrate the performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459244",
        "reference_list": [
            {
                "year": "2009",
                "id": 143
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 7,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Image segmentation",
                "Pixel",
                "Principal component analysis",
                "Testing",
                "Data mining",
                "Kernel",
                "Noise robustness",
                "Parameter estimation",
                "Feature extraction"
            ]
        },
        "id": 102,
        "cited_by": []
    },
    {
        "title": "Robust graph-cut scene segmentation and reconstruction for free-viewpoint video of complex dynamic scenes",
        "authors": [
            "Jean-Yves Guillemaut",
            "Joe Kilner",
            "Adrian Hilton"
        ],
        "abstract": "Current state-of-the-art image-based scene reconstruction techniques are capable of generating high-fidelity 3D models when used under controlled capture conditions. However, they are often inadequate when used in more challenging outdoor environments with moving cameras. In this case, algorithms must be able to cope with relatively large calibration and segmentation errors as well as input images separated by a wide-baseline and possibly captured at different resolutions. In this paper, we propose a technique which, under these challenging conditions, is able to efficiently compute a high-quality scene representation via graph-cut optimisation of an energy function combining multiple image cues with strong priors. Robustness is achieved by jointly optimising scene segmentation and multiple view reconstruction in a view-dependent manner with respect to each input camera. Joint optimisation prevents propagation of errors from segmentation to reconstruction as is often the case with sequential approaches. View-dependent processing increases tolerance to errors in on-the-fly calibration compared to global approaches. We evaluate our technique in the case of challenging outdoor sports scenes captured with manually operated broadcast cameras and demonstrate its suitability for high-quality free-viewpoint video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459299",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 12,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Robustness",
                "Image segmentation",
                "Image reconstruction",
                "Cameras",
                "Calibration",
                "Energy resolution",
                "Image resolution",
                "Multimedia communication",
                "Broadcasting"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image reconstruction",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene segmentation",
                "complex dynamic scenes",
                "graph cut optimisation",
                "energy function",
                "multiple view reconstruction",
                "robustness",
                "free viewpoint video"
            ]
        },
        "id": 103,
        "cited_by": [
            {
                "year": "2017",
                "id": 326
            },
            {
                "year": "2011",
                "id": 257
            }
        ]
    },
    {
        "title": "Saliency driven total variation segmentation",
        "authors": [
            "Michael Donoser",
            "Martin Urschler",
            "Martin Hirzer",
            "Horst Bischof"
        ],
        "abstract": "This paper introduces an unsupervised color segmentation method. The underlying idea is to segment the input image several times, each time focussing on a different salient part of the image and to subsequently merge all obtained results into one composite segmentation. We identify salient parts of the image by applying affinity propagation clustering to efficiently calculated local color and texture models. Each salient region then serves as an independent initialization for a figure/ground segmentation. Segmentation is done by minimizing a convex energy functional based on weighted total variation leading to a global optimal solution. Each salient region provides an accurate figure/ ground segmentation highlighting different parts of the image. These highly redundant results are combined into one composite segmentation by analyzing local segmentation certainty. Our formulation is quite general, and other salient region detection algorithms in combination with any semi-supervised figure/ground segmentation approach can be used. We demonstrate the high quality of our method on the well-known Berkeley segmentation database. Furthermore we show that our method can be used to provide good spatial support for recognition frameworks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459296",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2007",
                "id": 88
            },
            {
                "year": "2003",
                "id": 42
            }
        ],
        "citation": {
            "ieee": 64,
            "other": 43,
            "total": 107
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Spatial databases",
                "Training data",
                "Image color analysis",
                "Image databases",
                "Computer vision",
                "Labeling",
                "Computer graphics",
                "Detection algorithms",
                "Level set"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image recognition",
                "image segmentation",
                "image texture",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "saliency driven total variation segmentation",
                "unsupervised color segmentation method",
                "affinity propagation clustering",
                "local color model",
                "texture model",
                "figure segmentation",
                "ground segmentation",
                "convex energy",
                "weighted total variation",
                "composite segmentation",
                "salient region detection algorithm",
                "Berkeley segmentation database",
                "recognition framework"
            ]
        },
        "id": 104,
        "cited_by": [
            {
                "year": "2017",
                "id": 21
            },
            {
                "year": "2015",
                "id": 180
            },
            {
                "year": "2013",
                "id": 190
            }
        ]
    },
    {
        "title": "FLoSS: Facility location for subspace segmentation",
        "authors": [
            "Nevena Lazic",
            "Inmar Givoni",
            "Brendan Frey",
            "Parham Aarabi"
        ],
        "abstract": "Subspace segmentation is the task of segmenting data lying on multiple linear subspaces. Its applications in computer vision include motion segmentation in video, structure-from-motion, and image clustering. In this work, we describe a novel approach for subspace segmentation that uses probabilistic inference via a message-passing algorithm. We cast the subspace segmentation problem as that of choosing the best subset of linear subspaces from a set of candidate subspaces constructed from the data. Under this formulation, subspace segmentation corresponds to facility location, a well studied operational research problem. Approximate solutions to this NP-hard optimization problem can be found by performing maximum-a-posteriori (MAP) inference in a probabilistic graphical model. We describe the graphical model and a message-passing inference algorithm. We demonstrate the performance of Facility Location for Subspace Segmentation, or FLoSS, on synthetic data as well as on 3D multi-body video motion segmentation from point correspondences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459302",
        "reference_list": [
            {
                "year": "2001",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 14,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Computer vision",
                "Motion segmentation",
                "Clustering algorithms",
                "Costs",
                "Application software",
                "Inference algorithms",
                "Graphical models",
                "Principal component analysis",
                "Independent component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "facility location",
                "image segmentation",
                "inference mechanisms",
                "maximum likelihood estimation",
                "message passing",
                "motion estimation",
                "operations research",
                "optimisation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "FLoSS",
                "facility location",
                "subspace segmentation",
                "multiple linear subspaces",
                "computer vision",
                "motion segmentation",
                "image clustering",
                "probabilistic inference",
                "operational research problem",
                "NP-hard optimization problem",
                "maximum-a-posteriori inference",
                "probabilistic graphical model",
                "message passing inference algorithm",
                "3D multibody video motion segmentation",
                "data segmentation"
            ]
        },
        "id": 105,
        "cited_by": [
            {
                "year": "2013",
                "id": 66
            },
            {
                "year": "2013",
                "id": 197
            }
        ]
    },
    {
        "title": "Video object segmentation by tracking regions",
        "authors": [
            "William Brendel",
            "Sinisa Todorovic"
        ],
        "abstract": "This paper presents an approach to unsupervised segmentation of moving and static objects occurring in a video. Objects are, in general, spatially cohesive and characterized by locally smooth motion trajectories. Therefore, they occupy regions within each frame, while the shape and location of these regions vary slowly from frame to frame. Thus, video segmentation can be done by tracking regions across the frames such that the resulting tracks are locally smooth. To this end, we use a low-level segmentation to extract regions in all frames, and then we transitively match and cluster the similar regions across the video. The similarity is defined with respect to the region photometric, geometric, and motion properties. We formulate a new circular dynamic-time warping (CDTW) algorithm that generalizes DTW to match closed boundaries of two regions, without compromising DTW's guarantees of achieving the optimal solution with linear complexity. Our quantitative evaluation and comparison with the state of the art suggest that the proposed approach is a competitive alternative to currently prevailing point-based methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459242",
        "reference_list": [],
        "citation": {
            "ieee": 75,
            "other": 29,
            "total": 104
        },
        "keywords": {
            "IEEE Keywords": [
                "Object segmentation",
                "Shape",
                "Computer vision",
                "Photometry",
                "Motion segmentation",
                "Clustering algorithms",
                "Heuristic algorithms",
                "Optical variables control",
                "Image motion analysis",
                "Brightness"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image segmentation",
                "tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video object segmentation",
                "tracking regions",
                "smooth motion trajectories",
                "region photometric properties",
                "region geometric properties",
                "region motion properties",
                "circular dynamic-time warping algorithm",
                "linear complexity"
            ]
        },
        "id": 106,
        "cited_by": [
            {
                "year": "2017",
                "id": 175
            },
            {
                "year": "2015",
                "id": 360
            },
            {
                "year": "2015",
                "id": 361
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2013",
                "id": 279
            },
            {
                "year": "2013",
                "id": 440
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2011",
                "id": 253
            }
        ]
    },
    {
        "title": "Texel-based texture segmentation",
        "authors": [
            "Sinisa Todorovic",
            "Narendra Ahuja"
        ],
        "abstract": "Given an arbitrary image, our goal is to segment all distinct texture subimages. This is done by discovering distinct, cohesive groups of spatially repeating patterns, called texels, in the image, where each group defines the corresponding texture. Texels occupy image regions, whose photometric, geometric, structural, and spatial-layout properties are samples from an unknown pdf. If the image contains texture, by definition, the image will also contain a large number of statistically similar texels. This, in turn, will give rise to modes in the pdf of region properties. Texture segmentation can thus be formulated as identifying modes of this pdf. To this end, first, we use a low-level, multiscale segmentation to extract image regions at all scales present. Then, we use the meanshift with a new, variable-bandwidth, hierarchical kernel to identify modes of the pdf defined over the extracted hierarchy of image regions. The hierarchical kernel is aimed at capturing texel substructure. Experiments demonstrate that accounting for the structural properties of texels is critical for texture segmentation, leading to competitive performance vs. the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459308",
        "reference_list": [
            {
                "year": "2007",
                "id": 87
            },
            {
                "year": "2001",
                "id": 58
            },
            {
                "year": "2003",
                "id": 95
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 6,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Layout",
                "Photometry",
                "Kernel",
                "Surface texture",
                "Shape",
                "Optical materials",
                "Lighting",
                "Bandwidth",
                "Optical variables control"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "feature extraction",
                "image segmentation",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "texel-based texture segmentation",
                "arbitrary image",
                "repeating patterns",
                "photometric properties",
                "geometric properties",
                "structural properties",
                "spatial-layout properties",
                "pdf",
                "multiscale segmentation",
                "image region extraction"
            ]
        },
        "id": 107,
        "cited_by": []
    },
    {
        "title": "Label set perturbation for MRF based neuroimaging segmentation",
        "authors": [
            "Dylan Hower",
            "Vikas Singh",
            "Sterling C. Johnson"
        ],
        "abstract": "Graph-cuts based algorithms are effective for a variety of segmentation tasks in computer vision. Ongoing research is focused toward making the algorithms even more general, as well as to better understand their behavior with respect to issues such as the choice of the weighting function and sensitivity to placement of seeds. In this paper, we investigate in the context of neuroimaging segmentation, the sensitivity/stability of the solution with respect to the input \u201clabels\u201d or \u201cseeds\u201d. In particular, as a form of parameter learning, we are interested in the effect of allowing the given set of labels (and consequently, the response/statistics of the weighting function) to vary for obtaining lower energy segmentation solutions. This perturbation leads to a \u201crefined\u201d label set (or parameters) better suited to the input image, yielding segmentations that are less sensitive to the set of labels or seeds provided. Our proposed algorithm (using Parametric Pseudoflow) yields improvements over graph-cuts based segmentation with a fixed set of labels. We present experiments on about 450 3-D brain image volumes demonstrating the efficacy of the algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459305",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2007",
                "id": 88
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Neuroimaging",
                "Image segmentation",
                "Labeling",
                "Costs",
                "Biomedical imaging",
                "Brain",
                "Stability",
                "Computer vision",
                "Focusing",
                "Parametric statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "brain",
                "graph theory",
                "image segmentation",
                "Markov processes",
                "medical image processing",
                "neurophysiology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "MRF",
                "label set perturbation",
                "graph-cuts based algorithms",
                "computer vision",
                "weighting function",
                "neuroimaging segmentation",
                "lower energy segmentation solutions",
                "3-D brain image volumes"
            ]
        },
        "id": 108,
        "cited_by": []
    },
    {
        "title": "Robust image segmentation using learned priors",
        "authors": [
            "Ayman El-Baz",
            "Georgy Gimel'farb"
        ],
        "abstract": "A novel parametric deformable model of a goal object controlled by shape and appearance priors learned from co-aligned training images is introduced. The shape prior is built in a linear space of vectors of distances to the training boundaries from their common centroid. The appearance prior is modeled with a spatially homogeneous 2nd-order Markov-Gibbs random field (MGRF) of gray levels within each training boundary. Geometric structure of the MGRF and Gibbs potentials are analytically estimated from the training data. To accurately separate goal objects from arbitrary background, the deformable model is evolved by solving an Eikonal partial differential equation with a speed function combining the shape and appearance priors and the current appearance model. The latter represents empirical gray level marginals inside and outside an evolving boundary with adaptive linear combinations of discrete Gaussians (LCDG). The analytical shape and appearance priors and a simple Expectation-Maximization procedure for getting the object and background LCDGs, make our segmentation considerably faster than most of the known counterparts. Experiments with various images confirm robustness, accuracy, and speed of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459314",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Image segmentation",
                "Deformable models",
                "Solid modeling",
                "Shape control",
                "Gray-scale",
                "Biomedical imaging",
                "Principal component analysis",
                "Robust control",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "Gaussian processes",
                "image segmentation",
                "partial differential equations",
                "random processes",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "shape prior",
                "appearance prior",
                "vector",
                "Markov-Gibbs random field",
                "geometric structure",
                "Eikonal partial differential equation",
                "discrete Gaussians",
                "expectation-maximization procedure"
            ]
        },
        "id": 109,
        "cited_by": []
    },
    {
        "title": "Non-negative matrix factorization of partial track data for motion segmentation",
        "authors": [
            "Anil M. Cheriyadat",
            "Richard J. Radke"
        ],
        "abstract": "This paper addresses the problem of segmenting low-level partial feature point tracks belonging to multiple motions. We show that the local velocity vectors at each instant of the trajectory are an effective basis for motion segmentation. We decompose the velocity profiles of point tracks into different motion components and corresponding non-negative weights using non-negative matrix factorization (NNMF). We then segment the different motions using spectral clustering on the derived weights. We test our algorithm on the Hopkins 155 benchmarking database and several new sequences, demonstrating that the proposed algorithm can accurately segment multiple motions at a speed of a few seconds per frame. We show that our algorithm is particularly successful on low-level tracks from real-world video that are fragmented, noisy and inaccurate.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459311",
        "reference_list": [
            {
                "year": "2001",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 13,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Motion segmentation",
                "Computer vision",
                "Clustering algorithms",
                "Videoconference",
                "Data engineering",
                "Benchmark testing",
                "Motion analysis",
                "Cameras",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "matrix decomposition",
                "statistical analysis",
                "tracking",
                "vectors",
                "video databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonnegative matrix factorization",
                "partial track data",
                "low-level partial feature point tracks",
                "local velocity vectors",
                "trajectory",
                "NNMF",
                "spectral clustering",
                "Hopkins 155 benchmarking database",
                "multiple motion segmentation"
            ]
        },
        "id": 110,
        "cited_by": [
            {
                "year": "2015",
                "id": 365
            },
            {
                "year": "2013",
                "id": 385
            },
            {
                "year": "2011",
                "id": 220
            }
        ]
    },
    {
        "title": "Efficient segmentation using feature-based graph partitioning active contours",
        "authors": [
            "Filiz Bunyak",
            "Kannappan Palaniappan"
        ],
        "abstract": "Graph partitioning active contours (GPAC) is a recently introduced approach that elegantly embeds the graph-based image segmentation problem within a continuous optimization framework. GPAC can be used within parametric snake-based or implicit level set-based active contour continuous paradigms for image partitioning. However, GPAC similar to many other graph-based approaches has quadratic memory requirements which severely limits the scalability of the algorithm to practical problem domains. An N\u00d7N image requires O(N 4 ) computation and memory to create and store the full graph of pixel inter-relationships even before the start of the contour optimization process. For example, an 1024\u00d71024 grayscale image needs over one terabyte of memory. Approximations using tile/block-based or superpixel-based multiscale grouping of the pixels reduces this complexity by trading off accuracy. This paper describes a new algorithm that implements the exact GPAC algorithm using a constant memory requirement of a few kilobytes, independent of image size.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459320",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Active contours",
                "Image segmentation",
                "Partitioning algorithms",
                "Pixel",
                "Cost function",
                "Level set",
                "Scalability",
                "Computer science",
                "Gray-scale",
                "Tiles"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph partitioning active contours",
                "graph-based image segmentation problem",
                "continuous optimization",
                "parametric snake-based active contour continuous paradigms",
                "implicit level set-based active contour continuous paradigms",
                "image partitioning",
                "quadratic memory requirements",
                "pixel inter-relationships",
                "contour optimization process",
                "grayscale image",
                "tile-based multiscale grouping",
                "block-based multiscale grouping",
                "superpixel-based multiscale grouping"
            ]
        },
        "id": 111,
        "cited_by": []
    },
    {
        "title": "Analysis of orientation and scale in smoothly varying textures",
        "authors": [
            "Jason Chang",
            "John W. Fisher"
        ],
        "abstract": "We present a novel representation for modeling textured regions subject to smooth variations in orientation and scale. Utilizing the steerable pyramid of Simoncelli and Freeman as a basis, we decompose textured regions of natural images into explicit local attributes of contrast, bias, scale, and orientation. Additionally, we impose smoothness on these attributes via Markov random fields. The combination allows for demonstrable improvements in common scene analysis applications including unsupervised segmentation, reflectance and shading estimation, and estimation of the radiometric response function from a single image.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459317",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Gabor filters",
                "Image analysis",
                "Computer vision",
                "Image texture analysis",
                "Markov random fields",
                "Application software",
                "Reflectivity",
                "Radiometry",
                "Robustness"
            ]
        },
        "id": 112,
        "cited_by": []
    },
    {
        "title": "Learning based digital matting",
        "authors": [
            "Yuanjie Zheng",
            "Chandra Kambhamettu"
        ],
        "abstract": "We cast some new insights into solving the digital matting problem by treating it as a semi-supervised learning task in machine learning. A local learning based approach and a global learning based approach are then produced, to fit better the scribble based matting and the trimap based matting, respectively. Our approaches are easy to implement because only some simple matrix operations are needed. They are also extremely accurate because they can efficiently handle the nonlinear local color distributions by incorporating the kernel trick, that are beyond the ability of many previous works. Our approaches can outperform many recent matting methods, as shown by the theoretical analysis and comprehensive experiments. The new insights may also inspire several more works.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459326",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2005",
                "id": 122
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 7,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Machine learning",
                "Pixel",
                "Semisupervised learning",
                "Kernel",
                "Motion pictures",
                "Computer science",
                "Digital images",
                "Production",
                "Labeling",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "digital matting",
                "semi-supervised learning task",
                "machine learning",
                "kernel trick"
            ]
        },
        "id": 113,
        "cited_by": [
            {
                "year": "2013",
                "id": 359
            }
        ]
    },
    {
        "title": "The One-Shot similarity kernel",
        "authors": [
            "Lior Wolf",
            "Tal Hassner",
            "Yaniv Taigman"
        ],
        "abstract": "The One-Shot similarity measure has recently been introduced in the context of face recognition where it was used to produce state-of-the-art results. Given two vectors, their One-Shot similarity score reflects the likelihood of each vector belonging in the same class as the other vector and not in a class defined by a fixed set of \u201cnegative\u201d examples. The potential of this approach has thus far been largely unexplored. In this paper we analyze the One-Shot score and show that: (1) when using a version of LDA as the underlying classifier, this score is a Conditionally Positive Definite kernel and may be used within kernel-methods (e.g., SVM), (2) it can be efficiently computed, and (3) that it is effective as an underlying mechanism for image representation. We further demonstrate the effectiveness of the One-Shot similarity score in a number of applications including multiclass identification and descriptor generation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459323",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 45,
            "other": 24,
            "total": 69
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Computer science",
                "Linear discriminant analysis",
                "Support vector machines",
                "Support vector machine classification",
                "Image representation",
                "Training data",
                "Computer vision",
                "Face recognition",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image representation",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "one-shot similarity kernel",
                "face recognition",
                "vectors",
                "one-shot similarity score",
                "LDA",
                "classifier",
                "conditionally positive definite kernel",
                "image representation",
                "multiclass identification",
                "descriptor generation"
            ]
        },
        "id": 114,
        "cited_by": []
    },
    {
        "title": "A theory of active object localization",
        "authors": [
            "Alexander Andreopoulos",
            "John K. Tsotsos"
        ],
        "abstract": "We present some theoretical results related to the problem of actively searching for a target in a 3D environment, under the constraint of a maximum search time. We define the object localization problem as the maximization over the search region of the Lebesgue integral of the scene structure probabilities. We study variants of the problem as they relate to actively selecting a finite set of optimal viewpoints of the scene for detecting and localizing an object. We do a complexity-level analysis and show that the problem variants are NP-Complete or NP-Hard. We study the tradeoffs of localizing vs. detecting a target object, using single-view and multiple-view recognition, under imperfect dead-reckoning and an imperfect recognition algorithm. These results motivate a set of properties that efficient and reliable active object localization algorithms should satisfy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459332",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 8,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Target recognition",
                "Layout",
                "Spatial resolution",
                "Cameras",
                "Machine vision",
                "Costs",
                "Computer science",
                "Constraint theory",
                "Data acquisition"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "object detection",
                "object recognition",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active object localization",
                "3D environment",
                "maximum search time",
                "maximum search time",
                "scene structure probabilities",
                "complexity-level analysis",
                "NP-Complete problem",
                "multiple-view recognition",
                "imperfect dead-reckoning",
                "NP-hard problem",
                "Lebesgue integral",
                "single-view recognition"
            ]
        },
        "id": 115,
        "cited_by": []
    },
    {
        "title": "Active subspace learning",
        "authors": [
            "Xiaofei He",
            "Deng Cai"
        ],
        "abstract": "Many previous studies have shown that naturally occurring data cannot possibly fill up the high dimensional space uniformly, rather it must concentrate around lower dimensional structure. The typical supervised subspace learning algorithms to discover this low dimensional structure include Linear Discriminant Analysis (LDA). For LDA, the training data points are usually pre-given. However, in some real world applications like relevance feedback image retrieval, there is opportunity to interact with the user and actively select the training points for labeling. In this paper, we propose a novel active subspace learning algorithm which selects the most informative data points and uses them for learning an optimal subspace. Using techniques from experimental design, we discuss how to perform data selection in supervised or semi-supervised subspace learning by minimizing the expected error. Experiments on image retrieval show improvement over state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459329",
        "reference_list": [
            {
                "year": "2007",
                "id": 17
            },
            {
                "year": "2007",
                "id": 16
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Linear discriminant analysis",
                "Image retrieval",
                "Labeling",
                "Feedback",
                "Design for experiments",
                "Vectors",
                "Helium",
                "Educational institutions",
                "Computer science",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active subspace learning",
                "supervised subspace learning algorithm",
                "linear discriminant analysis",
                "relevance feedback image retrieval"
            ]
        },
        "id": 116,
        "cited_by": []
    },
    {
        "title": "Automatic learning and extraction of multi-local features",
        "authors": [
            "Oscar Danielsson",
            "Stefan Carlsson",
            "Josephine Sullivan"
        ],
        "abstract": "In this paper we introduce a new kind of feature - the multi-local feature, so named as each one is a collection of local features, such as oriented edgels, in a very specific spatial arrangement. A multi-local feature has the ability to capture underlying constant shape properties of exemplars from an object class. Thus it is particularly suited to representing and detecting visual classes that lack distinctive local structures and are mainly defined by their global shape. We present algorithms to automatically learn an ensemble of these features to represent an object class from weakly labelled training images of that class, as well as procedures to detect these features efficiently in novel images. The power of multi-local features is demonstrated by using the ensemble in a simple voting scheme to perform object category detection on a standard database. Despite its simplicity, this scheme yields detection rates matching state-of-the-art object detection systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459338",
        "reference_list": [
            {
                "year": "2007",
                "id": 139
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 7,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Shape",
                "Object detection",
                "Robustness",
                "Computer vision",
                "Computer science",
                "Voting",
                "Image databases",
                "Spatial databases",
                "Visual databases"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic learning",
                "oriented edgels",
                "spatial arrangement",
                "multilocal feature extraction",
                "object category detection"
            ]
        },
        "id": 117,
        "cited_by": []
    },
    {
        "title": "Fast realistic multi-action recognition using mined dense spatio-temporal features",
        "authors": [
            "Andrew Gilbert",
            "John Illingworth",
            "Richard Bowden"
        ],
        "abstract": "Within the field of action recognition, features and descriptors are often engineered to be sparse and invariant to transformation. While sparsity makes the problem tractable, it is not necessarily optimal in terms of class separability and classification. This paper proposes a novel approach that uses very dense corner features that are spatially and temporally grouped in a hierarchical process to produce an overcomplete compound feature set. Frequently reoccurring patterns of features are then found through data mining, designed for use with large data sets. The novel use of the hierarchical classifier allows real time operation while the approach is demonstrated to handle camera motion, scale, human appearance variations, occlusions and background clutter. The performance of classification, outperforms other state-of-the-art action recognition algorithms on the three datasets; KTH, multi-KTH, and Hollywood. Multiple action localisation is performed, though no groundtruth localisation data is required, using only weak supervision of class labels for each training sequence. The Hollywood dataset contain complex realistic actions from movies, the approach outperforms the published accuracy on this dataset and also achieves real time performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459335",
        "reference_list": [
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2007",
                "id": 209
            },
            {
                "year": "2007",
                "id": 67
            },
            {
                "year": "2007",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 50,
            "other": 42,
            "total": 92
        },
        "keywords": {
            "IEEE Keywords": [
                "Data mining",
                "Motion pictures",
                "Optical films",
                "Object recognition",
                "Cameras",
                "Testing",
                "Image motion analysis",
                "Fires",
                "Detectors",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "image recognition",
                "very large databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast realistic multi-action recognition",
                "mined dense spatio-temporal features",
                "class separability",
                "class classification",
                "very dense corner features",
                "data mining",
                "large data sets",
                "camera motion",
                "human appearance variations",
                "background clutter",
                "Hollywood dataset",
                "multi-KTH dataset"
            ]
        },
        "id": 118,
        "cited_by": [
            {
                "year": "2013",
                "id": 317
            },
            {
                "year": "2013",
                "id": 342
            },
            {
                "year": "2011",
                "id": 225
            }
        ]
    },
    {
        "title": "Quantifying contextual information for object detection",
        "authors": [
            "Wei-Shi Zheng",
            "Shaogang Gong",
            "Tao Xiang"
        ],
        "abstract": "Context is critical for minimising ambiguity in object detection. In this work, a novel context modelling framework is proposed without the need of any prior scene segmentation or context annotation. This is achieved by exploring a new polar geometric histogram descriptor for context representation. In order to quantify context, we formulate a new context risk function and a maximum margin context (MMC) model to solve the minimization problem of the risk function. Crucially, the usefulness and goodness of contextual information is evaluated directly and explicitly through a discriminant context inference method and a context confidence function, so that only reliable contextual information that is relevant to object detection is utilised. Experiments on PASCAL VOC2005 and i-LIDS datasets demonstrate that the proposed context modelling approach improves object detection significantly and outperforms a state-of-the-art alternative context model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459344",
        "reference_list": [
            {
                "year": "2005",
                "id": 168
            },
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2003",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Context modeling",
                "Layout",
                "Solid modeling",
                "Histograms",
                "Labeling",
                "Computer science",
                "Robustness",
                "Terminology",
                "Roads"
            ],
            "INSPEC: Controlled Indexing": [
                "minimisation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contextual information quantification",
                "object detection",
                "ambiguity minimisation",
                "context modelling framework",
                "polar geometric histogram descriptor",
                "context representation",
                "context risk function",
                "maximum margin context",
                "discriminant context inference method",
                "context confidence function",
                "PASCAL VOC2005 dataset",
                "i-LIDS dataset"
            ]
        },
        "id": 119,
        "cited_by": [
            {
                "year": "2011",
                "id": 137
            }
        ]
    },
    {
        "title": "Feature-centric Efficient Subwindow Search",
        "authors": [
            "Alain Lehmann",
            "Bastian Leibe",
            "Luc van Gool"
        ],
        "abstract": "Many object detection systems rely on linear classifiers embedded in a sliding-window scheme. Such exhaustive search involves massive computation. Efficient Subwindow Search (ESS) avoids this by means of branch and bound. However, ESS makes an unfavourable memory tradeoff. Memory usage scales with both image size and overall object model size. This risks becoming prohibitive in a multiclass system. In this paper, we make the connection between sliding-window and Hough-based object detection explicit. Then, we show that the feature-centric view of the latter also nicely fits with the branch and bound paradigm, while it avoids the ESS memory tradeoff. Moreover, on-line integral image calculations are not needed. Both theoretical and quantitative comparisons with the ESS bound are provided, showing that none of this comes at the expense of performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459341",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 12,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Electronic switching systems",
                "Object detection",
                "Feature extraction",
                "Computer vision",
                "Detectors",
                "Voting",
                "Shape",
                "Histograms",
                "Laboratories",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "Hough transforms",
                "image classification",
                "object detection",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature-centric efficient subwindow search",
                "linear classifier",
                "sliding-window scheme",
                "branch-and- bound paradigm",
                "memory usage",
                "image size",
                "overall object model size",
                "Hough-based object detection",
                "feature-centric view",
                "ESS memory tradeoff",
                "ESS bound"
            ]
        },
        "id": 120,
        "cited_by": []
    },
    {
        "title": "A latent model of discriminative aspect",
        "authors": [
            "Ali Farhadi",
            "Mostafa Kamali Tabrizi",
            "Ian Endres",
            "David Forsyth"
        ],
        "abstract": "Recognition using appearance features is confounded by phenomena that cause images of the same object to look different, or images of different objects to look the same. This may occur because the same object looks different from different viewing directions, or because two generally different objects have views from which they look similar. In this paper, we introduce the idea of discriminative aspect, a set of latent variables that encode these phenomena. Changes in view direction are one cause of changes in discriminative aspect, but others include changes in texture or lighting. However, images are not labelled with relevant discriminative aspect parameters. We describe a method to improve discrimination by inferring and then using latent discriminative aspect parameters. We apply our method to two parallel problems: object category recognition and human activity recognition. In each case, appearance features are powerful given appropriate training data, but traditionally fail badly under large changes in view. Our method can recognize an object quite reliably in a view for which it possesses no training example. Our method also reweights features to discount accidental similarities in appearance. We demonstrate that our method produces a significant improvement on the state of the art for both object and activity recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459350",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2007",
                "id": 265
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 19,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Bicycles",
                "Computer science",
                "Image recognition",
                "Humans",
                "Training data",
                "Object recognition",
                "Solid modeling",
                "Costs",
                "Mice"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "features recognition",
                "object category recognition",
                "human activity recognition",
                "training data"
            ]
        },
        "id": 121,
        "cited_by": [
            {
                "year": "2013",
                "id": 390
            },
            {
                "year": "2013",
                "id": 396
            },
            {
                "year": "2011",
                "id": 122
            }
        ]
    },
    {
        "title": "Unlabeled data improvesword prediction",
        "authors": [
            "Nicolas Loeff",
            "Ali Farhadi",
            "Ian Endres",
            "David A. Forsyth"
        ],
        "abstract": "Labeling image collections is a tedious task, especially when multiple labels have to be chosen for each image. In this paper we introduce a new framework that extends state of the art models in word prediction to incorporate information from unlabeled examples, using manifold regularization. To the best of our knowledge this is the first semi-supervised multi-task model used in vision problems. The new model can be solved using gradient descent and is fast and efficient. We show remarkable improvements for cases with few labeled examples for challenging multi-task learning problems in vision (predicting words for images and attributes for objects).",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459347",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Semisupervised learning",
                "Machine learning algorithms",
                "Labeling",
                "Clustering algorithms",
                "Predictive models",
                "Geometry",
                "Computer science",
                "Tagging",
                "Explosions",
                "Search engines"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image processing",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unlabeled data",
                "word prediction improvement",
                "image collections labeling",
                "manifold regularization",
                "semisupervised multitask model",
                "vision problems",
                "gradient descent",
                "multitask learning problem"
            ]
        },
        "id": 122,
        "cited_by": []
    },
    {
        "title": "A probabilistic framework for partial intrinsic symmetries in geometric data",
        "authors": [
            "Ruxandra Lasowski",
            "Art Tevs",
            "Hans-Peter Seidel",
            "Michael Wand"
        ],
        "abstract": "In this paper, we present a novel algorithm for partial intrinsic symmetry detection in 3D geometry. Unlike previous work, our algorithm is based on a conceptually simple and straightforward probabilistic formulation of partial shape matching: based on a Markov random field model, we obtain a probability distribution over all possible intrinsic matches of a shape to itself, which reveals the symmetry structure of the object. Rather than examining this exponentially sized distribution directly, which is infeasible, we approximate marginals of this distribution using sum-product loopy belief propagation and show how the symmetry information can subsequently be extracted from this condensed representation. Using a parallel implementation on graphics hardware, we are able to extract symmetries of deformable shapes in general poses efficiently. We apply our algorithm on several standard 3D models, demonstrating that a concise probabilistic model yields a practical and general symmetry detection algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459356",
        "reference_list": [
            {
                "year": "2007",
                "id": 373
            },
            {
                "year": "2007",
                "id": 268
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 15,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Data mining",
                "Information geometry",
                "Markov random fields",
                "Probability distribution",
                "Belief propagation",
                "Graphics",
                "Hardware",
                "Object detection",
                "Humans"
            ]
        },
        "id": 123,
        "cited_by": []
    },
    {
        "title": "Bayesian selection of scaling laws for motion modeling in images",
        "authors": [
            "Patrick H\u00e9as",
            "Etienne M\u00e9min",
            "Dominique Heitz",
            "Pablo D. Mininni"
        ],
        "abstract": "Based on scaling laws describing the statistical structure of turbulent motion across scales, we propose a multiscale and non-parametric regularizer for optic-flow estimation. Regularization is achieved by constraining motion increments to behave through scales as the most likely self-similar process given some image data. In a first level of inference, the hard constrained minimization problem is optimally solved by taking advantage of lagrangian duality. It results in a collection of first-order regularizers acting at different scales. This estimation is non-parametric since the optimal regularization parameters at the different scales are obtained by solving the dual problem. In a second level of inference, the most likely self-similar model given the data is optimally selected by maximization of Bayesian evidence. The motion estimator accuracy is first evaluated on a synthetic image sequence of simulated bi-dimensional turbulence and then on a real meteorological image sequence. Results obtained with the proposed physical based approach exceeds the best state of the art results. Furthermore, selecting from images the most evident multiscale motion model enables the recovery of physical quantities, which are of major interest for turbulence characterization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459353",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Optical sensors",
                "Motion estimation",
                "Image sequences",
                "Meteorology",
                "Inverse problems",
                "Apertures",
                "Image motion analysis",
                "Lagrangian functions",
                "Navier-Stokes equations"
            ],
            "INSPEC: Controlled Indexing": [
                "atmospheric turbulence",
                "Bayes methods",
                "flow simulation",
                "image sequences",
                "meteorology",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scaling laws",
                "turbulent motion",
                "nonparametric regularizer",
                "optic-flow estimation",
                "constrained minimization",
                "lagrangian duality",
                "optimal regularization parameter",
                "simulated bidimensional turbulence",
                "multiscale motion model",
                "turbulence characterization"
            ]
        },
        "id": 124,
        "cited_by": []
    },
    {
        "title": "Top-down color attention for object recognition",
        "authors": [
            "Fahad Shahbaz Khan",
            "Joost van de Weijer",
            "Maria Vanrell"
        ],
        "abstract": "Generally the bag-of-words based image representation follows a bottom-up paradigm. The subsequent stages of the process: feature detection, feature description, vocabulary construction and image representation are performed independent of the intentioned object classes to be detected. In such a framework, combining multiple cues such as shape and color often provides below-expected results. This paper presents a novel method for recognizing object categories when using multiple cues by separating the shape and color cue. Color is used to guide attention by means of a top-down category-specific attention map. The color attention map is then further deployed to modulate the shape features by taking more features from regions within an image that are likely to contain an object instance. This procedure leads to a category-specific image histogram representation for each category. Furthermore, we argue that the method combines the advantages of both early and late fusion. We compare our approach with existing methods that combine color and shape cues on three data sets containing varied importance of both cues, namely, Soccer ( color predominance), Flower (color and shape parity), and PASCAL VOC Challenge 2007 (shape predominance). The experiments clearly demonstrate that in all three data sets our proposed framework significantly outperforms the state-of-the-art methods for combining color and shape information.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459362",
        "reference_list": [
            {
                "year": "2003",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 10,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Shape",
                "Image representation",
                "Vocabulary",
                "Computer vision",
                "Histograms",
                "Object detection",
                "Computer science",
                "Information analysis",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image colour analysis",
                "image representation",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "top-down color attention",
                "object recognition",
                "image representation",
                "bottom-up paradigm",
                "bag-of-words",
                "feature detection",
                "feature description",
                "vocabulary construction",
                "multiple cues",
                "shape cue",
                "color cue",
                "shape features",
                "category-specific image histogram"
            ]
        },
        "id": 125,
        "cited_by": [
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2011",
                "id": 39
            },
            {
                "year": "2009",
                "id": 55
            }
        ]
    },
    {
        "title": "Detecting objects in large image collections and videos by efficient subimage retrieval",
        "authors": [
            "Christoph H. Lampert"
        ],
        "abstract": "We study the task of detecting the occurrence of objects in large image collections or in videos, a problem that combines aspects of content based image retrieval and object localization. While most previous approaches are either limited to special kinds of queries, or do not scale to large image sets, we propose a new method, efficient subimage retrieval (ESR), that is at the same time very flexible and very efficient. Relying on a two-layered branch-and-bound setup, ESR performs object-based image retrieval in sets of 100,000 or more images within seconds. An extensive evaluation on several datasets shows that ESR is not only very fast, but it also achieves excellent detection accuracies thereby improving over previous systems for object-based image retrieval.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459359",
        "reference_list": [],
        "citation": {
            "ieee": 29,
            "other": 13,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Videos",
                "Image retrieval",
                "Paramagnetic resonance",
                "Electronic switching systems",
                "Content based retrieval",
                "Cybernetics",
                "Information retrieval",
                "Image databases",
                "Image representation"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "object detection",
                "video retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object detection",
                "efficient subimage retrieval",
                "content based image retrieval",
                "object localization",
                "video retrieval",
                "two-layered branch-and-bound setup",
                "object-based image retrieval"
            ]
        },
        "id": 126,
        "cited_by": []
    },
    {
        "title": "Learning actions from the Web",
        "authors": [
            "Nazli Ikizler-Cinbis",
            "R. Gokberk Cinbis",
            "Stan Sclaroff"
        ],
        "abstract": "This paper proposes a generic method for action recognition in uncontrolled videos. The idea is to use images collected from the Web to learn representations of actions and use this knowledge to automatically annotate actions in videos. Our approach is unsupervised in the sense that it requires no human intervention other than the text querying. Its benefits are two-fold: (1) we can improve retrieval of action images, and (2) we can collect a large generic database of action poses, which can then be used in tagging videos. We present experimental evidence that using action images collected from the Web, annotating actions is possible.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459368",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 259
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 24,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Image retrieval",
                "Legged locomotion",
                "YouTube",
                "Humans",
                "Search engines",
                "Image recognition",
                "Information retrieval",
                "Vocabulary",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image retrieval",
                "Internet",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Web",
                "generic method",
                "human action recognition",
                "uncontrolled videos",
                "action representations",
                "unsupervised learning",
                "human intervention",
                "text querying",
                "action images retrieval",
                "generic database",
                "video tagging"
            ]
        },
        "id": 127,
        "cited_by": [
            {
                "year": "2015",
                "id": 113
            },
            {
                "year": "2011",
                "id": 168
            },
            {
                "year": "2011",
                "id": 261
            }
        ]
    },
    {
        "title": "Active Appearance Models with Rotation Invariant Kernels",
        "authors": [
            "Onur C. Hamsici",
            "Aleix M. Martinez"
        ],
        "abstract": "2D Active Appearance Models (AAM) and 3D Morphable Models (3DMM) are widely used techniques. AAM provide a fast fitting process, but may represent unwanted 3D transformations unless strictly constrained not to do so. The reverse is true for 3DMM. The two approaches also require of a pre-alignment of their 2D or 3D shapes before the modeling can be carried out which may lead to errors. Furthermore, current models are insufficient to represent nonlinear shape and texture variations. In this paper, we derive a new approach that can model nonlinear changes in examples without the need of a pre-alignment step. In addition, we show how the proposed approach carries the above mentioned advantages of AAM and 3DMM. To achieve this goal, we take advantage of the inherent properties of complex spherical distributions, which provide invariance to translation, scale and rotation. To reduce the complexity of parameter estimation we take advantage of a recent result that shows how to estimate spherical distributions using their Euclidean counterpart, e.g., the Gaussians. This leads to the definition of Rotation Invariant Kernels (RIK) for modeling nonlinear shape changes. We show the superiority of our algorithm to AAM in several face datasets. We also show how the derived algorithm can be used to model complex 3D facial expression changes observed in American Sign Language (ASL).",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459365",
        "reference_list": [
            {
                "year": "2007",
                "id": 183
            },
            {
                "year": "2007",
                "id": 266
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 7,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Kernel",
                "Gaussian distribution",
                "Principal component analysis",
                "Parameter estimation",
                "Handicapped aids",
                "Active shape model",
                "Statistics",
                "Robustness",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active appearance models",
                "rotation invariant kernels",
                "3D morphable models",
                "parameter estimation",
                "spherical distributions",
                "Euclidean counterpart",
                "Gaussians",
                "nonlinear shape modeling",
                "3D facial expression",
                "American sign language"
            ]
        },
        "id": 128,
        "cited_by": []
    },
    {
        "title": "Incremental action recognition using feature-tree",
        "authors": [
            "Kishore K Reddy",
            "Jingen Liu",
            "Mubarak Shah"
        ],
        "abstract": "Action recognition methods suffer from many drawbacks in practice, which include (1)the inability to cope with incremental recognition problems; (2)the requirement of an intensive training stage to obtain good performance; (3) the inability to recognize simultaneous multiple actions; and (4) difficulty in performing recognition frame by frame. In order to overcome all these drawbacks using a single method, we propose a novel framework involving the feature-tree to index large scale motion features using Sphere/Rectangle-tree (SR-tree). The recognition consists of the following two steps: 1) recognizing the local features by non-parametric nearest neighbor (NN), 2) using a simple voting strategy to label the action. The proposed method can provide the localization of the action. Since our method does not require feature quantization, the feature- tree can be efficiently grown by adding features from new training examples of actions or categories. Our method provides an effective way for practical incremental action recognition. Furthermore, it can handle large scale datasets due to the fact that the SR-tree is a disk-based data structure. We have tested our approach on two publicly available datasets, the KTH and the IXMAS multi-view datasets, and obtained promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459374",
        "reference_list": [
            {
                "year": "2007",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 29,
            "total": 58
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Computer vision",
                "Large-scale systems",
                "Neural networks",
                "Voting",
                "Data structures",
                "Humans",
                "Vocabulary",
                "Nearest neighbor searches",
                "Quantization"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "gesture recognition",
                "image motion analysis",
                "tree data structures"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "incremental action recognition",
                "feature-tree",
                "large scale motion feature",
                "sphere-rectangle-tree",
                "SR-tree",
                "local feature recognition",
                "nonparametric nearest neighbor",
                "voting strategy",
                "disk-based data structure"
            ]
        },
        "id": 129,
        "cited_by": [
            {
                "year": "2017",
                "id": 151
            },
            {
                "year": "2015",
                "id": 507
            }
        ]
    },
    {
        "title": "RankBoost with l1 regularization for facial expression recognition and intensity estimation",
        "authors": [
            "Peng Yang",
            "Qingshan Liu",
            "Dimitris N. Metaxas"
        ],
        "abstract": "Most previous facial expression analysis works only focused on expression recognition. In this paper, we propose a novel framework of facial expression analysis based on the ranking model. Different from previous works, it not only can do facial expression recognition, but also can estimate the intensity of facial expression, which is very important to further understand human emotion. Although it is hard to label expression intensity quantitatively, the ordinal relationship in temporal domain is actually a good relative measurement. Based on this observation, we convert the problem of intensity estimation to a ranking problem, which is modeled by the RankBoost. The output ranking score can be directly used for intensity estimation, and we also extend the ranking function for expression recognition. To further improve the performance, we propose to introduce l1 based regularization into the Rankboost. Experiments on the Cohn-Kanade database show that the proposed method has a promising performance compared to the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459371",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 15,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Pattern recognition",
                "Pattern analysis",
                "Humans",
                "Psychology",
                "Gold",
                "Computer science",
                "Laboratories",
                "Emotion recognition",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "emotion recognition",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "RankBoost",
                "facial expression recognition",
                "facial expression intensity estimation",
                "ranking model",
                "human emotion",
                "Cohn-Kanade database"
            ]
        },
        "id": 130,
        "cited_by": []
    },
    {
        "title": "Optimal multiple surfaces searching for video/image resizing - a graph-theoretic approach",
        "authors": [
            "Dongfeng Han",
            "Xiaodong Wu",
            "Milan Sonka"
        ],
        "abstract": "Content-aware video/image resizing is of increasing relevance to allow high-quality image and video resizing to be displayed on devices with different resolution. In this paper, we present a novel algorithm to find multiple 3-D surfaces simultaneously with globally optimal solution for video/image resizing. Our algorithm is based on graph theory and it first analyzes the video/image data to define the energy value for each voxel. Then, a 4-D graph is constructed and the costs are assigned according to the energy values. Finally, multiple 3-D surfaces are detected by a global optimization process which can be solved via s-t graph cuts. By removing or inserting these multiple 3-D surfaces, content-aware video/image resizing is achieved. We also have proved that our algorithm can find the globally optimal solution for crossing surfaces problem, in which several surfaces can cross each other. The proposed method is demonstrated on a variety of video/image data and compared to the state of the art in video/image resizing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459380",
        "reference_list": [
            {
                "year": "2007",
                "id": 170
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Energy resolution",
                "Dynamic programming",
                "Optimization methods",
                "Cost function",
                "Oncology",
                "Cities and towns",
                "Graph theory",
                "Image analysis",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "content-aware video-image resizing",
                "graph-theoretic approach",
                "image resolution",
                "multiple 3D surfaces",
                "4D graph",
                "global optimization process"
            ]
        },
        "id": 131,
        "cited_by": []
    },
    {
        "title": "Face alignment through subspace constrained mean-shifts",
        "authors": [
            "Jason M. Saragih",
            "Simon Lucey",
            "Jeffrey F. Cohn"
        ],
        "abstract": "Deformable model fitting has been actively pursued in the computer vision community for over a decade. As a result, numerous approaches have been proposed with varying degrees of success. A class of approaches that has shown substantial promise is one that makes independent predictions regarding locations of the model's landmarks, which are combined by enforcing a prior over their joint motion. A common theme in innovations to this approach is the replacement of the distribution of probable landmark locations, obtained from each local detector, with simpler parametric forms. This simplification substitutes the true objective with a smoothed version of itself, reducing sensitivity to local minima and outlying detections. In this work, a principled optimization strategy is proposed where a nonparametric representation of the landmark distributions is maximized within a hierarchy of smoothed estimates. The resulting update equations are reminiscent of mean-shift but with a subspace constraint placed on the shape's variability. This approach is shown to outperform other existing methods on the task of generic face fitting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459377",
        "reference_list": [
            {
                "year": "2007",
                "id": 266
            }
        ],
        "citation": {
            "ieee": 111,
            "other": 101,
            "total": 212
        },
        "keywords": {
            "IEEE Keywords": [
                "Subspace constraints",
                "Deformable models",
                "Shape",
                "Detectors",
                "Face detection",
                "Computer vision",
                "Robot vision systems",
                "Predictive models",
                "Technological innovation",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face alignment",
                "subspace constrained mean shifts",
                "deformable model fitting",
                "computer vision community",
                "model landmark distribution",
                "distribution replacement",
                "local detector",
                "principled optimization strategy",
                "smoothed estimate hierarchy"
            ]
        },
        "id": 132,
        "cited_by": [
            {
                "year": "2017",
                "id": 337
            },
            {
                "year": "2015",
                "id": 412
            },
            {
                "year": "2015",
                "id": 436
            },
            {
                "year": "2013",
                "id": 241
            },
            {
                "year": "2013",
                "id": 314
            }
        ]
    },
    {},
    {
        "title": "Face recognition with contiguous occlusion using markov random fields",
        "authors": [
            "Zihan Zhou",
            "Andrew Wagner",
            "Hossein Mobahi",
            "John Wright",
            "Yi Ma"
        ],
        "abstract": "Partially occluded faces are common in many applications of face recognition. While algorithms based on sparse representation have demonstrated promising results, they achieve their best performance on occlusions that are not spatially correlated (i.e. random pixel corruption). We show that such sparsity-based algorithms can be significantly improved by harnessing prior knowledge about the pixel error distribution. We show how a Markov Random Field model for spatial continuity of the occlusion can be integrated into the computation of a sparse representation of the test image with respect to the training images. Our algorithm efficiently and reliably identifies the corrupted regions and excludes them from the sparse representation. Extensive experiments on both laboratory and real-world datasets show that our algorithm tolerates much larger fractions and varieties of occlusion than current state-of-the-art algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459383",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 5,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Markov random fields",
                "Testing",
                "Lighting",
                "Robustness",
                "Feature extraction",
                "Independent component analysis",
                "Sparse matrices",
                "Laboratories",
                "Cellular phones"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "contiguous occlusion",
                "Markov random fields",
                "sparse representation",
                "pixel error distribution"
            ]
        },
        "id": 134,
        "cited_by": [
            {
                "year": "2015",
                "id": 253
            }
        ]
    },
    {
        "title": "Which faces to tag: Adding prior constraints into active learning",
        "authors": [
            "Ashish Kapoor",
            "Gang Hua",
            "Amir Akbarzadeh",
            "Simon Baker"
        ],
        "abstract": "We introduce an algorithm that guides the user to tag faces in the best possible order during a face recognition assisted tagging scenario. In particular, we extend the active learning paradigm to take advantage of constraints known a priori. For example, in the context of personal photo collections, if two faces come from the same source photograph, we know that they must be of different people. Similarly, in the context of video, we know that the faces from a single track must be of the same person. Given a set of unlabeled images and constraints, we use a probabilistic discriminative model that models the posterior distributions by propagating label information using a message passing scheme. The uncertainty estimate provided by the model naturally allows for active learning paradigms where the user is consulted after each iteration to tag additional faces. Our experiments show that performing active learning while incorporating a priori constraints provides a significant boost in many real-world face recognition tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459392",
        "reference_list": [
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2003",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 11,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Tagging",
                "Probes",
                "Support vector machines",
                "Support vector machine classification",
                "Message passing",
                "Uncertainty",
                "User interfaces",
                "Face detection",
                "Computer vision"
            ]
        },
        "id": 135,
        "cited_by": [
            {
                "year": "2013",
                "id": 374
            },
            {
                "year": "2011",
                "id": 197
            }
        ]
    },
    {
        "title": "Is a detector only good for detection?",
        "authors": [
            "Quan Yuan",
            "Stan Sclaroff"
        ],
        "abstract": "A common design of an object recognition system has two steps, a detection step followed by a foreground within-class classification step. For example, consider face detection by a boosted cascade of detectors followed by face ID recognition via one-vs-all (OVA) classifiers. Another example is human detection followed by pose recognition. Although the detection step can be quite fast, the foreground within-class classification process can be slow and becomes a bottleneck. In this work, we formulate a filter-and-refine scheme, where the binary outputs of the weak classifiers in a boosted detector are used to identify a small number of candidate foreground state hypotheses quickly via Hamming distance or weighted Hamming distance. The approach is evaluated in three applications: face recognition on the FRGC V2 data set, hand shape detection and parameter estimation on a hand data set and vehicle detection and view angle estimation on a multi-view vehicle data set. On all data sets, our approach has comparable accuracy and is at least five times faster than the brute force approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459389",
        "reference_list": [
            {
                "year": "2001",
                "id": 198
            },
            {
                "year": "2005",
                "id": 57
            },
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Face detection",
                "Face recognition",
                "Hamming distance",
                "Vehicle detection",
                "Object recognition",
                "Object detection",
                "Humans",
                "Shape",
                "Parameter estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image sensors",
                "object recognition",
                "vehicles"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "detectors",
                "face ID recognition",
                "one-vs-all classifiers",
                "human detection",
                "pose recognition",
                "within-class classification process",
                "filter-and-refine scheme",
                "candidate foreground state hypotheses",
                "weighted Hamming distance",
                "FRGC V2 data set",
                "hand shape detection",
                "parameter estimation",
                "vehicle detection",
                "angle estimation",
                "multiview vehicle data set",
                "brute force approach"
            ]
        },
        "id": 136,
        "cited_by": []
    },
    {
        "title": "Consensus set maximization with guaranteed global optimality for robust geometry estimation",
        "authors": [
            "Hongdong Li"
        ],
        "abstract": "Finding the largest consensus set is one of the key ideas used by the original RANSAC for removing outliers in robust-estimation. However, because of its random and non-deterministic nature, RANSAC does not fulfill the goal of consensus set maximization exactly and optimally. Based on global optimization, this paper presents a new algorithm that solves the problem exactly. We reformulate the problem as a mixed integer programming (MIP), and solve it via a tailored branch-and-bound method, where the bounds are computed from the MIP's convex under-estimators. By exploiting the special structure of linear robust-estimation, the new algorithm is also made efficient from a computational point of view.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459398",
        "reference_list": [
            {
                "year": "2007",
                "id": 237
            },
            {
                "year": "2003",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 2,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Geometry",
                "Iterative algorithms",
                "Sampling methods",
                "Histograms",
                "Australia",
                "Linear programming",
                "Parameter estimation",
                "Solid modeling",
                "Heuristic algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "estimation theory",
                "integer programming",
                "set theory",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "consensus set maximization",
                "robust geometry estimation",
                "global optimization",
                "mixed integer programming",
                "tailored branch-and-bound method",
                "MIP",
                "linear robust estimation"
            ]
        },
        "id": 137,
        "cited_by": [
            {
                "year": "2017",
                "id": 0
            },
            {
                "year": "2015",
                "id": 228
            },
            {
                "year": "2013",
                "id": 5
            },
            {
                "year": "2013",
                "id": 181
            },
            {
                "year": "2011",
                "id": 50
            }
        ]
    },
    {
        "title": "Efficient subset selection via the kernelized R\u00e9nyi distance",
        "authors": [
            "Balaji Vasan Srinivasan",
            "Ramani Duraiswami"
        ],
        "abstract": "With improved sensors, the amount of data available in many vision problems has increased dramatically and allows the use of sophisticated learning algorithms to perform inference on the data. However, since these algorithms scale with data size, pruning the data is sometimes necessary. The pruning procedure must be statistically valid and a representative subset of the data must be selected without introducing selection bias. Information theoretic measures have been used for sampling the data, retaining its original information content. We propose an efficient R\u00e9nyi entropy based subset selection algorithm. The algorithm is first validated and then applied to two sample applications where machine learning and data pruning are used. In the first application, Gaussian process regression is used to learn object pose. Here it is shown that the algorithm combined with the subset selection is significantly more efficient. In the second application, our subset selection approach is used to replace vector quantization in a standard object recognition algorithm, and improvements are shown.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459395",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Machine learning algorithms",
                "Inference algorithms",
                "Vector quantization",
                "Random variables",
                "Probability distribution",
                "Computer vision",
                "Support vector machines",
                "Histograms",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "learning (artificial intelligence)",
                "object recognition",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subset selection",
                "kernelized R\u00e9nyi distance",
                "learning algorithms",
                "R\u00e9nyi entropy",
                "data pruning",
                "Gaussian process regression",
                "object pose learning",
                "object recognition"
            ]
        },
        "id": 138,
        "cited_by": []
    },
    {
        "title": "A near optimal acceptance-rejection algorithm for exact cross-correlation search",
        "authors": [
            "Haim Schweitzer",
            "Robert Finis Anderson",
            "Rui Deng"
        ],
        "abstract": "We describe a fast algorithm that searches for the k most likely locations of a template in an image according to the standard normalized correlations criterion. The algorithm is exact; it always finds the best matches. Its speed is achieved by utilizing an acceptance-rejection pruning scheme, applied to easily computed bounds on the normalized correlation values. Previously proposed rejection schemes require a rejection threshold that has to be provided or estimated from the data. Our algorithm does not use such thresholds explicitly, but performs as well as if the perfect rejection threshold is known.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459404",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Costs",
                "Computer vision",
                "Upper bound",
                "Monitoring",
                "Measurement standards"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "near optimal acceptance-rejection algorithm",
                "cross-correlation search",
                "acceptance-rejection pruning scheme",
                "standard normalized correlations criterion"
            ]
        },
        "id": 139,
        "cited_by": []
    },
    {
        "title": "Heterogeneous feature machines for visual recognition",
        "authors": [
            "Liangliang Cao",
            "Jiebo Luo",
            "Feng Liang",
            "Thomas S. Huang"
        ],
        "abstract": "With the recent efforts made by computer vision researchers, more and more types of features have been designed to describe various aspects of visual characteristics. Modeling such heterogeneous features has become an increasingly critical issue. In this paper, we propose a machinery called the Heterogeneous Feature Machine (HFM) to effectively solve visual recognition tasks in need of multiple types of features. Our HFM builds a kernel logistic regression model based on similarities that combine different features and distance metrics. Different from existing approaches that use a linear weighting scheme to combine different features, HFM does not require the weights to remain the same across different samples, and therefore can effectively handle features of different types with different metrics. To prevent the model from overfitting, we employ the so-called group LASSO constraints to reduce model complexity. In addition, we propose a fast algorithm based on co-ordinate gradient descent to efficiently train a HFM. The power of the proposed scheme is demonstrated across a wide variety of visual recognition tasks including scene, event and action recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459401",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 0,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Computer vision",
                "Shape measurement",
                "Support vector machines",
                "Character recognition",
                "Laboratories",
                "Statistics",
                "Machinery",
                "Logistics",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "heterogeneous feature machines",
                "visual recognition",
                "computer vision researchers",
                "LASSO",
                "linear weighting scheme",
                "coordinate gradient descent"
            ]
        },
        "id": 140,
        "cited_by": []
    },
    {
        "title": "Efficient indexing for large scale visual search",
        "authors": [
            "Xiao Zhang",
            "Zhiwei Li",
            "Lei Zhang",
            "Wei-Ying Ma",
            "Heung-Yeung Shum"
        ],
        "abstract": "With the popularity of \u201cbag of visual terms\u201d representations of images, many text indexing techniques have been applied in large-scale image retrieval systems. However, due to a fundamental difference between an image query (e.g. 1500 visual terms) and a text query (e.g. 3-5 terms), the usages of some text indexing techniques, e.g. inverted list, are misleading. In this work, we develop a novel indexing technique for this problem. The basic idea is to decompose a document-like representation of an image into two components, one for dimension reduction and the other for residual information preservation. The computing of similarity of two images can be transferred to measuring similarities of their components. The decomposition has two major merits: (1) these components have good properties which enable them to be efficiently indexed and retrieved; (2) The decomposition has better generalization ability than other dimension reduction algorithms. The decomposition can be achieved by either a graphical model or a matrix factorization approach. Theoretic analysis and extensive experiments over a 2.3 million image database show that this framework is scalable to index large scale image database to support fast and accurate visual search.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459354",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 3,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Indexing",
                "Large-scale systems",
                "Image retrieval",
                "Image databases",
                "Quantization",
                "Indexes",
                "Feature extraction",
                "Image converters",
                "Vocabulary",
                "Visual databases"
            ],
            "INSPEC: Controlled Indexing": [
                "database indexing",
                "image retrieval",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large scale visual search",
                "text indexing techniques",
                "image indexing techniques",
                "residual information preservation",
                "feature extraction",
                "probabilistic decomposition model",
                "parameter estimation",
                "ranking scheme",
                "document-like image representation",
                "dimension reduction algorithms",
                "graphical model",
                "matrix factorization approach"
            ]
        },
        "id": 141,
        "cited_by": [
            {
                "year": "2013",
                "id": 156
            }
        ]
    },
    {
        "title": "Spectral error correcting output codes for efficient multiclass recognition",
        "authors": [
            "Xiao Zhang",
            "Lin Liang",
            "Heung-Yeung Shum"
        ],
        "abstract": "The error correcting output codes (ECOC) is a general framework to extend any binary classifier to the multiclass case. Finding the optimal ECOC is known as a NP hard problem. In this paper, we present a spectral analysis approach for the design of ECOC. We construct a similarity graph of the classes and generate ECOC with a subset of thresholded eigenvectors of the graph Laplacian. Using the spectral analysis, the coding efficiency, classifier's diversity, Hamming distance among codewords, and binary classifiers' accuracy can be simultaneously considered. The resulting ECOC is efficient, thus only a small set of binary classifiers are to be evaluated when making a decision. In experiments with large multiclass problems, our method is between 3 and 12 times faster comparing to one-against-all, with comparable classification accuracy. Our method also shows a better performance than the most of leading methods, e.g., ClassMap, random dense ECOC, random sparse ECOC, and discriminant ECOC.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459355",
        "reference_list": [
            {
                "year": "2007",
                "id": 214
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Error correction codes",
                "Face recognition",
                "Matrix decomposition",
                "Spectral analysis",
                "Laplace equations",
                "Computational complexity",
                "Asia",
                "NP-hard problem",
                "Hamming distance",
                "Large-scale systems"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "error correction codes",
                "graph theory",
                "Hamming codes",
                "image coding",
                "image recognition",
                "spectral analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spectral error correcting output code",
                "multiclass recognition",
                "binary classifier",
                "NP hard problem",
                "spectral analysis",
                "ECOC design",
                "similarity graph",
                "eigenvector",
                "graph Laplacian",
                "coding efficiency",
                "classifier diversity",
                "Hamming distance",
                "codeword"
            ]
        },
        "id": 142,
        "cited_by": []
    },
    {
        "title": "Kernel map compression using generalized radial basis functions",
        "authors": [
            "Omar Arif",
            "Patricio Antonio Vela"
        ],
        "abstract": "The use of Mercer kernel methods in statistical learning theory provides for strong learning capabilities, as seen in kernel principal component analysis and support vector machines. Unfortunately the computational complexity of the resulting method is of the order of the training set, which is quite large for many applications. This paper proposes a two step procedure for arriving at a compact and computationally efficient learning procedure. After learning, the second step takes advantage of the universal approximation capabilities of generalized radial basis function neural networks to efficiently approximate the empirical kernel maps. Sample applications demonstrate significant compression of the kernel representation with graceful performance loss.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459351",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Neural networks",
                "Image coding",
                "Computational complexity",
                "Computer vision",
                "Principal component analysis",
                "Performance loss",
                "Vectors",
                "Application software",
                "Statistical learning"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "computational complexity",
                "data compression",
                "image coding",
                "image representation",
                "principal component analysis",
                "radial basis function networks",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "kernel map compression",
                "Mercer kernel method",
                "statistical learning theory",
                "kernel principal component analysis",
                "support vector machines",
                "computational complexity",
                "universal approximation capability",
                "generalized radial basis function neural network",
                "empirical kernel map approximation",
                "kernel representation"
            ]
        },
        "id": 143,
        "cited_by": [
            {
                "year": "2009",
                "id": 102
            }
        ]
    },
    {
        "title": "Patch-based within-object classification",
        "authors": [
            "Jania Aghajanian",
            "Jonathan Warrell",
            "Simon J.D. Prince",
            "Peng Li",
            "Jennifer L. Rohn",
            "Buzz Baum"
        ],
        "abstract": "Advances in object detection have made it possible to collect large databases of certain objects. In this paper we exploit these datasets for within-object classification. For example, we classify gender in face images, pose in pedestrian images and phenotype in cell images. Previous work has mainly targeted the above tasks individually using object specific representations. Here, we propose a general Bayesian framework for within-object classification. Images are represented as a regular grid of non-overlapping patches. In training, these patches are approximated by a predefined library. In inference, the choice of approximating patch determines the classification decision. We propose a Bayesian framework in which we marginalize over the patch frequency parameters to provide a posterior probability for the class. We test our algorithm on several challenging \u201creal world\u201d databases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459352",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 11,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Image databases",
                "Face detection",
                "Testing",
                "Libraries",
                "Bayesian methods",
                "Detectors",
                "Educational institutions",
                "Computer vision",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image classification",
                "image representation",
                "maximum likelihood estimation",
                "object detection",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object classification",
                "object detection",
                "large databases",
                "face images",
                "pedestrian images",
                "cell images",
                "object specific representations",
                "general Bayesian framework",
                "non-overlapping patches",
                "a posterior probability"
            ]
        },
        "id": 144,
        "cited_by": [
            {
                "year": "2011",
                "id": 331
            }
        ]
    },
    {
        "title": "An algorithm for minimizing the Mumford-Shah functional",
        "authors": [
            "Thomas Pock",
            "Daniel Cremers",
            "Horst Bischof",
            "Antonin Chambolle"
        ],
        "abstract": "In this work we revisit the Mumford-Shah functional, one of the most studied variational approaches to image segmentation. The contribution of this paper is to propose an algorithm which allows to minimize a convex relaxation of the Mumford-Shah functional obtained by functional lifting. The algorithm is an efficient primal-dual projection algorithm for which we prove convergence. In contrast to existing algorithms for minimizing the full Mumford-Shah this is the first one which is based on a convex relaxation. As a consequence the computed solutions are independent of the initialization. Experimental results confirm that the proposed algorithm determines smooth approximations while preserving discontinuities of the underlying signal.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459348",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 63,
            "total": 93
        },
        "keywords": {
            "IEEE Keywords": [
                "Level set",
                "Image segmentation",
                "Approximation algorithms",
                "Projection algorithms",
                "Simulated annealing",
                "Area measurement",
                "Mathematics",
                "Computer graphics",
                "Computer vision",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "convergence",
                "image segmentation",
                "minimisation",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Mumford-Shah functional minimisation algorithm",
                "image segmentation",
                "primal-dual projection algorithm",
                "convergence",
                "convex relaxation"
            ]
        },
        "id": 145,
        "cited_by": [
            {
                "year": "2017",
                "id": 270
            }
        ]
    },
    {
        "title": "A biased sampling strategy for object categorization",
        "authors": [
            "Lei Yang",
            "Nanning Zheng",
            "Jie Yang",
            "Mei Chen",
            "Hong Chen"
        ],
        "abstract": "In this paper, we present a biased sampling strategy for object class modeling, which can effectively circumvent the scene matching problem commonly encountered in statistical image-based object categorization. The method optimally combines the bottom-up, biologically inspired saliency information with loose, top-down class prior information to form a probabilistic distribution for feature sampling. When sampling over different positions and scales of patches, the weak spatial coherency is preserved by a segment-based analysis. We evaluate the proposed sampling strategy within the bag-of-features (BoF) object categorization framework on three public data sets. Our technique outperforms other state-of-the-art sampling technologies, and leads to a better performance in object categorization on VOC2008 dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459349",
        "reference_list": [
            {
                "year": "2003",
                "id": 38
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Sampling methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image sampling",
                "object recognition",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "biased sampling strategy",
                "object class modeling",
                "image-based object categorization",
                "biologically inspired saliency information",
                "top-down class prior information",
                "probabilistic distribution",
                "segment-based analysis",
                "bag-of-features object categorization framework",
                "public data sets",
                "VOC2008 dataset"
            ]
        },
        "id": 146,
        "cited_by": []
    },
    {
        "title": "On optimizing subspaces for face recognition",
        "authors": [
            "Jilin Tu",
            "Xiaoming Liu",
            "Peter Tu"
        ],
        "abstract": "We propose a subspace learning algorithm for face recognition by directly optimizing recognition performance scores. Our approach is motivated by the following observations: 1) Different face recognition tasks (i.e., face identification and verification) have different performance metrics, which implies that there exist distinguished subspaces that optimize these scores, respectively. Most prior work focused on optimizing various discriminative or locality criteria and neglect such distinctions. 2) As the gallery (target) and the probe (query) data are collected in different settings in many real-world applications, there could exist consistent appearance incoherences between the gallery and the probe data for the same subject. Knowledge regarding these incoherences could be used to guide the algorithm design, resulting in performance gain. Prior efforts have not focused on these facts. In this paper, we rigorously formulate performance scores for both the face identification and the face verification tasks, provide a theoretical analysis on how the optimal subspaces for the two tasks are related, and derive gradient descent algorithms for optimizing these subspaces. Our extensive experiments on a number of public databases and a real-world face database demonstrate that our algorithm can improve the performance of given subspace based face recognition algorithms targeted at a specific face recognition task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459345",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Probes",
                "Algorithm design and analysis",
                "Performance analysis",
                "Image databases",
                "Bayesian methods",
                "Measurement",
                "Performance gain",
                "Principal component analysis",
                "Independent component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "gradient methods",
                "learning (artificial intelligence)",
                "optimisation",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subspaces optimization",
                "face recognition",
                "subspace learning algorithm",
                "performance scores",
                "face identification",
                "face verification",
                "gradient descent algorithms",
                "public databases"
            ]
        },
        "id": 147,
        "cited_by": []
    },
    {
        "title": "Structural SVM for visual localization and continuous state estimation",
        "authors": [
            "Catalin Ionescu",
            "Liefeng Bo",
            "Cristian Sminchisescu"
        ],
        "abstract": "We present an integrated model for visual object localization and continuous state estimation in a discriminative structured prediction framework. While existing discriminative `prediction through time' methods have showed remarkable versatility for visual reconstruction and tracking problems, they tend to assume that the input is known (or the object is segmented) a condition that can rarely be accommodated in images of real scenes. Our structural Support Vector Machine (structSVM) framework offers an end-to-end training and inference framework that overcomes these limitations by consistently searching both in the space of possible inputs (effectively an efficient form of object localization) and in the space of possible structured outputs, given those inputs. We demonstrate the potential of this methodology for 3d human pose reconstruction in monocular images both in the HumanEva benchmark, where 3d ground truth is available, and qualitatively, in un-instrumented images of real scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459346",
        "reference_list": [
            {
                "year": "2001",
                "id": 148
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 8,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "State estimation",
                "Image reconstruction",
                "Layout",
                "Object detection",
                "Image segmentation",
                "Humans",
                "Face detection",
                "Runtime",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "state estimation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structural SVM",
                "continuous state estimation",
                "visual object localization",
                "discriminative structured prediction framework",
                "support vector machine",
                "structSVM framework",
                "inference framework",
                "3H human pose reconstruction",
                "monocular image",
                "HumanEva benchmark"
            ]
        },
        "id": 148,
        "cited_by": [
            {
                "year": "2015",
                "id": 317
            },
            {
                "year": "2011",
                "id": 282
            }
        ]
    },
    {
        "title": "A Markov Clustering Topic Model for mining behaviour in video",
        "authors": [
            "Timothy Hospedales",
            "Shaogang Gong",
            "Tao Xiang"
        ],
        "abstract": "This paper addresses the problem of fully automated mining of public space video data. A novel Markov Clustering Topic Model (MCTM) is introduced which builds on existing Dynamic Bayesian Network models (e.g. HMMs) and Bayesian topic models (e.g. Latent Dirichlet Allocation), and overcomes their drawbacks on accuracy, robustness and computational efficiency. Specifically, our model profiles complex dynamic scenes by robustly clustering visual events into activities and these activities into global behaviours, and correlates behaviours over time. A collapsed Gibbs sampler is derived for offline learning with unlabeled training data, and significantly, a new approximation to online Bayesian inference is formulated to enable dynamic scene understanding and behaviour mining in new video data online in real-time. The strength of this model is demonstrated by unsupervised learning of dynamic scene models, mining behaviours and detecting salient events in three complex and crowded public scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459342",
        "reference_list": [
            {
                "year": "2007",
                "id": 95
            }
        ],
        "citation": {
            "ieee": 83,
            "other": 39,
            "total": 122
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Hidden Markov models",
                "Bayesian methods",
                "Robustness",
                "Event detection",
                "Humans",
                "Traffic control",
                "Data engineering",
                "Computer science",
                "Computational efficiency"
            ]
        },
        "id": 149,
        "cited_by": [
            {
                "year": "2013",
                "id": 397
            },
            {
                "year": "2011",
                "id": 279
            },
            {
                "year": "2011",
                "id": 307
            }
        ]
    },
    {
        "title": "A branch-and-bound algorithm for globally optimal calibration of a camera-and-rotation-sensor system",
        "authors": [
            "Yongduek Seo",
            "Young-Ju Choi",
            "Sang Wook Lee"
        ],
        "abstract": "We propose a branch-and-bound algorithm to obtain the globally optimal relative rotation between a camera and the rotation sensor attached to it. Compared to previous methods, our approach directly minimizes the image space error related to the measurements which is very natural for camera-based systems. Our algorithm is based on the observation that we may evaluate the residual when the rotation matrix is known. We propose a feasibility test algorithm for the branch-and-bound to efficiently reduce the search volume of the rotation domain. Experimental results are provided using synthetic and real data sets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459343",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Space technology",
                "Robot vision systems",
                "Testing",
                "Robot sensing systems",
                "Robot kinematics",
                "Computer vision",
                "Sensor systems",
                "Minimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "optical rotation",
                "optical sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "branch-and-bound algorithm",
                "globally optimal calibration",
                "camera-and-rotation-sensor system",
                "relative rotation",
                "image space error",
                "rotation matrix",
                "feasibility test algorithm",
                "rotation domain"
            ]
        },
        "id": 150,
        "cited_by": [
            {
                "year": "2015",
                "id": 241
            }
        ]
    },
    {
        "title": "Simultaneous camera pose and correspondence estimation in cornerless images",
        "authors": [
            "Wen-Yan Lin",
            "Guo Dong",
            "Ping Tan",
            "Loong-Fah Cheong",
            "Chye-Hwang Yan"
        ],
        "abstract": "We propose an algorithm which can jointly estimate camera pose and point set registration. Given point sets from two views of a stationary scene, our algorithm registers the point sets while retaining internal scene structure. It simultaneously ensures that the resultant registration is consistent with that of a moving camera viewing a static scene (adheres to some epipolar constraint). Our statistical formulation can incorporate but does not necessarily require additional constraints such as brightness constancy and high dimensional point descriptors such as SIFT. We show that our algorithm is stable over a variety of scenes and offers a pose from edge system which handles currently difficult structure from motion scenes more robustly.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459339",
        "reference_list": [
            {
                "year": "2001",
                "id": 132
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Apertures",
                "Iterative algorithms",
                "Image resolution",
                "Geometry",
                "Brightness",
                "Robustness",
                "Indoor environments",
                "Testing"
            ]
        },
        "id": 151,
        "cited_by": []
    },
    {
        "title": "Jointly estimating demographics and height with a calibrated camera",
        "authors": [
            "Andrew C. Gallagher",
            "Andrew C. Blose",
            "Tsuhan Chen"
        ],
        "abstract": "One important problem in computer vision is to provide a demographic description a person from an image. In practice, many of the state-of-the-art methods use only an analysis of the face to estimate the age and gender of a person of interest. We present a model that combines two problems, height estimation and demographic classification, which allows each to serve as context for the other. Our idea is to use a calibrated camera for measuring the height of people in the scene. Height is measured by jointly inferring across anthropometric dimensions, age, and gender using publicly available statistics. The height estimate provides context for recognizing the age and gender of the subject, and likewise age and gender conditions the distribution of the anthropometric features for estimating height. The performance of our method is explored on a new database of 127 people captured with a calibrated camera with recorded height, age, and gender. We show that estimating height leads to improvements in age and gender classification, and vice versa. To the best of our knowledge, our model produces the most accurate automatic height estimates reported, with the error having a standard deviation of 26.7 mm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459340",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Demography",
                "Cameras",
                "Face detection",
                "Humans",
                "Computer vision",
                "Face recognition",
                "Layout",
                "Anthropometry",
                "Calibration",
                "Legged locomotion"
            ]
        },
        "id": 152,
        "cited_by": []
    },
    {
        "title": "Plane-based calibration of central catadioptric cameras",
        "authors": [
            "Simone Gasparini",
            "Peter Sturm",
            "Jo\u00e3o P. Barreto"
        ],
        "abstract": "We present a novel calibration technique for all central catadioptric cameras using images of planar grids. We adopted the well-known sphere camera model to describe the catadioptric projection. We show that, using the so-called lifted coordinates, a linear relation mapping the grid points to the corresponding points on the image plane can be written as a 6 \u00d7 6 matrix H cata , which acts like the classical 3 \u00d7 3 homography for perspective cameras. We show how to compute the image of the absolute conic (IAC) from at least 3 homographies and how to recover from it the intrinsic parameters of the catadioptric camera. In the case of paracatadioptric cameras one such homography is enough to estimate the IAC, thus allowing the calibration from a single image.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459336",
        "reference_list": [
            {
                "year": "2003",
                "id": 177
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 9,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Mirrors",
                "Robot vision systems",
                "Transmission line matrix methods",
                "Robot kinematics",
                "Shape",
                "Taylor series",
                "Lenses",
                "Parameter estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "plane-based calibration",
                "central catadioptric cameras",
                "planar grids",
                "lifted coordinates",
                "linear relation mapping",
                "image of the absolute conic"
            ]
        },
        "id": 153,
        "cited_by": []
    },
    {
        "title": "Static multi-camera factorization using rigid motion",
        "authors": [
            "Roland Angst",
            "Marc Pollefeys"
        ],
        "abstract": "Camera networks have gained increased importance in recent years. Previous approaches mostly used point correspondences between different camera views to calibrate such systems. However, it is often difficult or even impossible to establish such correspondences. In this paper, we therefore present an approach to calibrate a static camera network where no correspondences between different camera views are required. Each camera tracks its own set of feature points on a commonly observed moving rigid object and these 2D feature trajectories are then fed into our algorithm. By assuming the cameras can be well approximated with an affine camera model, we show that the projection of any feature point trajectory onto any affine camera axis is restricted to a 13-dimensional subspace. This observation enables the computation of the camera calibration matrices, the coordinates of the tracked feature points, and the rigid motion of the object with a non-iterative trilinear factorization approach. This solution can then be used as an initial guess for iterative optimization schemes which make use of the strong algebraic structure contained in the data. Our new approach can handle extreme configurations, e.g. a camera in a camera network tracking only one single feature point. The applicability of our algorithm is evaluated with synthetic and real world data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459337",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Computer vision",
                "Matrix decomposition",
                "Trajectory",
                "Least squares approximation",
                "Iterative algorithms",
                "Tensile stress",
                "Computational geometry",
                "Computer science",
                "Calibration"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "feature extraction",
                "matrix decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "rigid motion",
                "point correspondences",
                "different camera views",
                "static camera network tracking",
                "moving rigid object",
                "2D feature trajectories",
                "affine camera model",
                "feature point trajectory",
                "affine camera axis",
                "13-dimensional subspace",
                "camera calibration matrices",
                "iterative optimization schemes",
                "algebraic structure",
                "static multicamera factorization",
                "noniterative trilinear factorization"
            ]
        },
        "id": 154,
        "cited_by": []
    },
    {
        "title": "Shadow cameras: Reciprocal views from illumination masks",
        "authors": [
            "Sanjeev J. Koppal",
            "Srinivasa G. Narasimhan"
        ],
        "abstract": "Scene appearance from the point of view of a light source is called a reciprocal or dual view. Since there exists a large diversity in illumination, these virtual views may be non-perspective and multi-viewpoint in nature. In this paper, we demonstrate the use of occluding masks to recover these dual views, which we term shadow cameras. We first show how to render a single reciprocal scene view by swapping the camera and light source positions. We extend this technique for multiple views by building a virtual shadow camera array with static masks and a moving source. We also capture non-perspective views such as orthographic, cross-slit and a pushbroom variant, while introducing novel applications such as converting between camera projections and removing refractive and catadioptric distortions. Finally, since a shadow camera is artificial, we can manipulate any of its intrinsic parameters, such as camera skew, to create perspective distortions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459333",
        "reference_list": [
            {
                "year": "2001",
                "id": 3
            },
            {
                "year": "2001",
                "id": 104
            },
            {
                "year": "2007",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Light sources",
                "Lighting control",
                "Geometry",
                "Switches",
                "Robot vision systems",
                "Optical refraction",
                "Light scattering",
                "Photometry"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "light sources",
                "lighting",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "illumination masks",
                "light source",
                "reciprocal view",
                "shadow cameras",
                "nonperspective views",
                "rendering",
                "dual view",
                "occluding masks",
                "virtual shadow camera array"
            ]
        },
        "id": 155,
        "cited_by": []
    },
    {
        "title": "Background Subtraction for Freely Moving Cameras",
        "authors": [
            "Yaser Sheikh",
            "Omar Javed",
            "Takeo Kanade"
        ],
        "abstract": "Background subtraction algorithms define the background as parts of a scene that are at rest. Traditionally, these algorithms assume a stationary camera, and identify moving objects by detecting areas in a video that change over time. In this paper, we extend the concept of `subtracting' areas at rest to apply to video captured from a freely moving camera. We do not assume that the background is well-approximated by a plane or that the camera center remains stationary during motion. The method operates entirely using 2D image measurements without requiring an explicit 3D reconstruction of the scene. A sparse model of background is built by robustly estimating a compact trajectory basis from trajectories of salient features across the video, and the background is `subtracted' by removing trajectories that lie within the space spanned by the basis. Foreground and background appearance models are then built, and an optimal pixel-wise foreground/background labeling is obtained by efficiently maximizing a posterior function.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459334",
        "reference_list": [
            {
                "year": "2003",
                "id": 8
            },
            {
                "year": "2001",
                "id": 184
            },
            {
                "year": "2003",
                "id": 170
            },
            {
                "year": "2003",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 114,
            "other": 66,
            "total": 180
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "freely moving cameras",
                "background subtraction algorithm",
                "2D image measurement",
                "sparse model",
                "trajectory removal",
                "foreground appearance model",
                "background appearance model",
                "optimal pixel-wise foreground labeling",
                "optimal pixel-wise background labeling",
                "posterior function"
            ]
        },
        "id": 156,
        "cited_by": [
            {
                "year": "2017",
                "id": 539
            },
            {
                "year": "2015",
                "id": 368
            },
            {
                "year": "2013",
                "id": 34
            },
            {
                "year": "2013",
                "id": 103
            },
            {
                "year": "2013",
                "id": 196
            },
            {
                "year": "2013",
                "id": 327
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2011",
                "id": 276
            }
        ]
    },
    {
        "title": "Display-camera calibration from eye reflections",
        "authors": [
            "Christian Nitschke",
            "Atsushi Nakazawa",
            "Haruo Takemura"
        ],
        "abstract": "We present a novel technique for calibrating display-camera systems from reflections in the user's eyes. Display-camera systems enable a range of vision applications that need controlled illumination, including 3D object reconstruction, facial modeling and human computer interaction. One important issue, though, is the geometric calibration of the display, which requires additional hardware and tedious user interaction. The proposed approach eliminates this requirement by analyzing patterns that are reflected in the cornea, a mirroring device that naturally exists in any display-camera system. We introduce an optimization strategy that is able to refine eye and spherical mirror calibration results. When applied to the eye, it even outperforms spherical mirror calibration unoptimized. Furthermore, we obtain a robust estimation of eye poses which can be used for eye tracking applications. Despite the difficult working conditions, the calibration results are good and should be sufficient for many applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459330",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Reflection",
                "Mirrors",
                "Eyes",
                "Application software",
                "Lighting",
                "Human computer interaction",
                "Computer displays",
                "Hardware",
                "Pattern analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "face recognition",
                "image reconstruction",
                "optimisation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "display-camera calibration",
                "eye reflections",
                "3D object reconstruction",
                "facial modeling",
                "human computer interaction",
                "optimization",
                "eye pose estimation",
                "eye tracking"
            ]
        },
        "id": 157,
        "cited_by": []
    },
    {
        "title": "A hand-held photometric stereo camera for 3-D modeling",
        "authors": [
            "Tomoaki Higo",
            "Yasuyuki Matsushita",
            "Neel Joshi",
            "Katsushi Ikeuchi"
        ],
        "abstract": "This paper presents a simple yet practical 3-D modeling method for recovering surface shape and reflectance from a set of images. We attach a point light source to a hand-held camera to add a photometric constraint to the multi-view stereo problem. Using the photometric constraint, we simultaneously solve for shape, surface normal, and reflectance. Unlike prior approaches, we formulate the problem using realistic assumptions of a near light source, non-Lambertian surfaces, perspective camera model, and the presence of ambient lighting. The effectiveness of the proposed method is verified using simulated and real-world scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459331",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 24,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Cameras",
                "Three dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "handheld photometric stereo camera",
                "3D modeling method",
                "surface shape recovery",
                "multiview stereo problem",
                "nonLambertian surfaces",
                "light source",
                "shape reconstruction"
            ]
        },
        "id": 158,
        "cited_by": [
            {
                "year": "2017",
                "id": 563
            },
            {
                "year": "2015",
                "id": 388
            }
        ]
    },
    {
        "title": "Body-relative navigation guidance using uncalibrated cameras",
        "authors": [
            "Olivier Koch",
            "Seth Teller"
        ],
        "abstract": "We present a vision-based method that assists human navigation within unfamiliar environments. Our main contribution is a novel algorithm that learns the correlation between user egomotion and feature matches on a wearable set of uncalibrated cameras. The primary advantage of this method is that it provides robust guidance cues in the user's body frame, and is tolerant to small changes in the camera configuration. We couple this method with a topological mapping algorithm that provides global localization within the traversed environment. We validate our approach with ground-truth experiments and demonstrate the method on several real-world datasets spanning two kilometers of indoor and outdoor walking excursions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459327",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 0,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Navigation",
                "Humans",
                "Simultaneous localization and mapping",
                "Robustness",
                "Legged locomotion",
                "Robot vision systems",
                "Smart cameras",
                "Computer science",
                "Artificial intelligence",
                "Global Positioning System"
            ]
        },
        "id": 159,
        "cited_by": []
    },
    {
        "title": "Non-iterative approach for fast and accurate vanishing point detection",
        "authors": [
            "Jean-Philippe Tardif"
        ],
        "abstract": "We present an algorithm that quickly and accurately estimates vanishing points in images of man-made environments. Contrary to previously proposed solutions, ours is neither iterative nor relies on voting in the space of vanishing points. Our formulation is based on a recently proposed algorithm for the simultaneous estimation of multiple models called J-Linkage. Our method avoids representing edges on the Gaussian sphere and the computations and error measures are done in the image. We show that a consistency measure between a vanishing point and an edge of the image can be computed in closed-form while being geometrically meaningful. Finally, given a set of estimated vanishing points, we show how this consistency measure can be used to identify the three vanishing points corresponding to the Manhattan directions. We compare our algorithm with other approaches on the York Urban Database and show significant performance improvements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459328",
        "reference_list": [],
        "citation": {
            "ieee": 80,
            "other": 42,
            "total": 122
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image edge detection",
                "Calibration",
                "Iterative algorithms",
                "Pixel",
                "Image reconstruction",
                "Buildings",
                "H infinity control",
                "Layout",
                "Voting"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "Gaussian processes",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "noniterative approach",
                "vanishing point detection",
                "J-Linkage",
                "Gaussian sphere",
                "consistency measure",
                "Manhattan direction"
            ]
        },
        "id": 160,
        "cited_by": [
            {
                "year": "2017",
                "id": 91
            },
            {
                "year": "2011",
                "id": 312
            }
        ]
    },
    {
        "title": "Robust matching of building facades under large viewpoint changes",
        "authors": [
            "Jimmy Addison Lee",
            "Kin-Choong Yow",
            "Alex Yong-Sang Chia"
        ],
        "abstract": "This paper presents a novel approach to finding point correspondences between images of building facades with wide viewpoint variations, and at the same time returning a large list of true matches between the images. Such images comprise repetitive and symmetric patterns, which render popular algorithms e.g., SIFT to be ineffective. Feature descriptors such as SIFT that are based on region patches are also unstable under large viewing angle variations. In this paper, we integrate both the appearance and geometric properties of an image to find unique matches. First we extract hypotheses of building facades based on a robust line fitting algorithm. Each hypothesis is defined by a planar convex quadrilateral in the image, which we call a \u201cq-region\u201d, and the four corners of each q-region provide the inputs from which a projective transformation model is derived. Next, a set of interest points are extracted from the images and are used to evaluate the correctness of the transformation model. The transformation model with the largest set of matched interest points is selected as the correct model, and this model also returns the best pair of corresponding q-regions and the most number of point correspondences in the two images. Extensive experimental results demonstrate the robustness of our approach in which we achieve a tenfold increase in true matches when compared to state of the art techniques such as SIFT and MSER.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459324",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            },
            {
                "year": "2001",
                "id": 69
            },
            {
                "year": "2003",
                "id": 80
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Computer vision",
                "Pattern matching",
                "Rendering (computer graphics)",
                "Entropy",
                "Feature extraction",
                "Buildings",
                "Object recognition",
                "Object detection",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "edge detection",
                "feature extraction",
                "image matching",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "building facade robust matching",
                "large viewpoint changes",
                "point correspondences",
                "wide viewpoint variations",
                "symmetric patterns",
                "feature descriptors",
                "SIFT",
                "large viewing angle variations",
                "geometric properties",
                "robust line fitting algorithm",
                "planar convex quadrilateral",
                "projective transformation model",
                "state of the art techniques",
                "MSER"
            ]
        },
        "id": 161,
        "cited_by": []
    },
    {
        "title": "Unsupervised face alignment by robust nonrigid mapping",
        "authors": [
            "Jianke Zhu",
            "Luc Van Gool",
            "Steven C.H. Hoi"
        ],
        "abstract": "We propose a novel approach to unsupervised facial image alignment. Differently from previous approaches, that are confined to affine transformations on either the entire face or separate patches, we extract a nonrigid mapping between facial images. Based on a regularized face model, we frame unsupervised face alignment into the Lucas-Kanade image registration approach. We propose a robust optimization scheme to handle appearance variations. The method is fully automatic and can cope with pose variations and expressions, all in an unsupervised manner. Experiments on a large set of images showed that the approach is effective.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459325",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 0,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Face detection",
                "Computer vision",
                "Face recognition",
                "Least squares methods",
                "Image registration",
                "Motion analysis",
                "Tracking",
                "Labeling",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image registration",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised facial image alignment",
                "robust nonrigid mapping",
                "affine transformations",
                "Lucas-Kanade image registration",
                "robust optimization scheme"
            ]
        },
        "id": 162,
        "cited_by": [
            {
                "year": "2017",
                "id": 338
            }
        ]
    },
    {
        "title": "Coarse registration of 3D surface triangulations based on moment invariants with applications to object alignment and identification",
        "authors": [
            "Michael Trummer",
            "Herbert Suesse",
            "Joachim Denzler"
        ],
        "abstract": "We present a new, direct way to register three-dimensional (3D) surfaces given the respective 3D points and surface triangulations. Our method is non-iterative and does not require any initial solution. The idea is to compute 3D invariants based on local surface moments. The resulting local surface descriptors are invariant with respect to Euclidean or to similarity transformations, by choice. In the final step we use the Hungarian method to find a minimum cost assignment of the computed descriptors. The method is robust against different point densities, noise and partial overlap. Our experiments with real data also show that the method can serve as automatic initialization of the iterative-closest-point (ICP) algorithm and, hence, extends the field of applications for this standard registration method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459321",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 4,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Iterative closest point algorithm",
                "Object recognition",
                "Surface fitting",
                "Application software",
                "Computer vision",
                "Noise robustness",
                "Iterative algorithms",
                "Databases",
                "Registers",
                "Costs"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "mesh generation",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D surface triangulation coarse registration",
                "3D moment invariants",
                "object alignment",
                "object identification",
                "three-dimensional surface registration",
                "local surface moments",
                "local surface descriptors",
                "Hungarian method",
                "minimum cost assignment",
                "iterative-closest-point algorithm"
            ]
        },
        "id": 163,
        "cited_by": []
    },
    {
        "title": "Feature correspondence and deformable object matching via agglomerative correspondence clustering",
        "authors": [
            "Minsu Cho",
            "Jungmin Lee",
            "Kyoung Mu Lee"
        ],
        "abstract": "We present an efficient method for feature correspondence and object-based image matching, which exploits both photometric similarity and pairwise geometric consistency from local invariant features. We formulate object-based image matching as an unsupervised multi-class clustering problem on a set of candidate feature matches, and propose a novel pairwise dissimilarity measure and a robust linkage model in the framework of hierarchical agglomerative clustering. The algorithm handles significant amount of outliers and deformation as well as multiple clusters, thus enabling simultaneous feature matching and clustering from real-world image pairs with significant clutter and multiple deformable objects. The experimental evaluation on feature correspondence, object recognition, and object-based image matching demonstrates that our method is robust to both outliers and deformation, and applicable to a wide range of image matching problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459322",
        "reference_list": [
            {
                "year": "2003",
                "id": 84
            },
            {
                "year": "2001",
                "id": 173
            },
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2003",
                "id": 42
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 1,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Image matching",
                "Robustness",
                "Object recognition",
                "Computer vision",
                "Photometry",
                "Clustering algorithms",
                "Couplings",
                "Application software",
                "Image reconstruction",
                "Motion segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "object recognition",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature correspondence",
                "deformable object matching",
                "agglomerative correspondence clustering",
                "object-based image matching",
                "photometric similarity",
                "pairwise geometric consistency",
                "unsupervised multi-class clustering problem",
                "pairwise dissimilarity measure",
                "hierarchical agglomerative clustering",
                "object recognition"
            ]
        },
        "id": 164,
        "cited_by": [
            {
                "year": "2015",
                "id": 12
            },
            {
                "year": "2013",
                "id": 427
            }
        ]
    },
    {
        "title": "Subspace matching: Unique solution to point matching with geometric constraints",
        "authors": [
            "Manuel Marques",
            "Marko Sto\u0161i\u0107",
            "Jo\u0101o Costeira"
        ],
        "abstract": "Finding correspondences between feature points is one of the most relevant problems in the whole set of visual tasks. In this paper we address the problem of matching a feature vector (or a matrix) to a given subspace. Given any vector base of such a subspace, we observe a linear combination of its elements with all entries swapped by an unknown permutation. We prove that such a computationally hard integer problem is uniquely solved in a convex set resulting from relaxing the original problem. Also, if noise is present, based on this result, we provide a robust estimate recurring to a linear programming-based algorithm. We use structure-from-motion and object recognition as motivating examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459318",
        "reference_list": [
            {
                "year": "2007",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Subspace constraints",
                "Vectors",
                "Object recognition",
                "Shape",
                "Acoustic noise",
                "Computer vision",
                "Image recognition",
                "Clouds",
                "Cameras",
                "Robots"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "geometry",
                "image matching",
                "image motion analysis",
                "integer programming",
                "linear programming",
                "object recognition",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subspace matching",
                "point matching",
                "geometric constraint",
                "feature point",
                "feature vector matching",
                "vector base",
                "computationally hard integer problem",
                "convex set",
                "noise",
                "linear programming",
                "structure-from-motion",
                "object recognition",
                "permutation"
            ]
        },
        "id": 165,
        "cited_by": []
    },
    {
        "title": "Optimal correspondences from pairwise constraints",
        "authors": [
            "Olof Enqvist",
            "Klas Josephson",
            "Fredrik Kahl"
        ],
        "abstract": "Correspondence problems are of great importance in computer vision. They appear as subtasks in many applications such as object recognition, merging partial 3D reconstructions and image alignment. Automatically matching features from appearance only is difficult and errors are frequent. Thus, it is necessary to use geometric consistency to remove incorrect correspondences. Typically heuristic methods like RANSAC or EM-like algorithms are used, but they risk getting trapped in local optima and are in no way guaranteed to find the best solution. This paper illustrates how pairwise constraints in combination with graph methods can be used to efficiently find optimal correspondences. These ideas are implemented on two basic geometric problems, 3D-3D registration and 2D-3D registration. The developed scheme can handle large rates of outliers and cope with multiple hypotheses. Despite the combinatorial explosion, the resulting algorithm which has been extensively evaluated on real data, yields competitive running times compared to state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459319",
        "reference_list": [
            {
                "year": "2007",
                "id": 185
            },
            {
                "year": "2007",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 18,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Image reconstruction",
                "Cameras",
                "Application software",
                "Object recognition",
                "Merging",
                "Computer errors",
                "Explosions",
                "Stereo image processing",
                "Stereo vision"
            ],
            "INSPEC: Controlled Indexing": [
                "combinatorial mathematics",
                "graph theory",
                "image reconstruction",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pairwise constraints",
                "computer vision",
                "object recognition",
                "partial 3D reconstructions",
                "image alignment",
                "geometric consistency",
                "RANSAC algorithms",
                "EM-like algorithms",
                "graph methods",
                "3D-3D registration",
                "2D-3D registration",
                "combinatorial methods"
            ]
        },
        "id": 166,
        "cited_by": [
            {
                "year": "2013",
                "id": 181
            },
            {
                "year": "2011",
                "id": 50
            }
        ]
    },
    {
        "title": "Deformation invariant image matching by spectrally controlled diffeomorphic alignment",
        "authors": [
            "Christopher M. Yang",
            "Sai Ravela"
        ],
        "abstract": "We present a new approach to deformation invariant image matching. Our matcher (a) aligns templates to targets over a broad range of nonlinear deformations, (b) factors the total deformation into spectral categories, where low wavenumber deformations are smooth and global and high wavenumbers are turbulent and local, and (c) weighs the reduction in template-target misfit within each category to differentiate between relevant and irrelevant deformations. It accomplishes this by aligning images in a scale-cascaded fashion, with more complex, local deformations following simpler, more global ones. Each step of the cascade involves finding an iterative solution to a nonlinear optimization problem using a Gabor deformation basis. Cascaded alignment makes deformation invariant matching feasible and efficient. Our approach is applied to recognize the flexible bodies of salamanders from a large database; results indicate that the method is very promising.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459315",
        "reference_list": [
            {
                "year": "2005",
                "id": 110
            },
            {
                "year": "2005",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 4,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image matching",
                "Image recognition",
                "Animals",
                "Geoscience",
                "Atmospheric waves",
                "Target recognition",
                "Databases",
                "Deformable models",
                "Object recognition",
                "Interpolation"
            ],
            "INSPEC: Controlled Indexing": [
                "Gabor filters",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformation invariant image matching",
                "spectrally controlled diffeomorphic alignment",
                "nonlinear deformations",
                "low wavenumber deformations",
                "nonlinear optimization problem",
                "Gabor deformation",
                "scale-cascaded image alignment"
            ]
        },
        "id": 167,
        "cited_by": []
    },
    {
        "title": "Wide-baseline image matching using Line Signatures",
        "authors": [
            "Lu Wang",
            "Ulrich Neumann",
            "Suya You"
        ],
        "abstract": "We present a wide-baseline image matching approach based on line segments. Line segments are clustered into local groups according to spatial proximity. Each group is treated as a feature called a Line Signature. Similar to local features, line signatures are robust to occlusion, image clutter, and viewpoint changes. The descriptor and similarity measure of line signatures are presented. Under our framework, the feature matching is not only robust against affine distortion but also a considerable range of 3D viewpoint changes for non-planar surfaces. When compared to matching approaches based on existing local features, our method shows improved results with low-texture scenes. Moreover, extensive experiments validate that our method has advantages in matching structured non-planar scenes under large viewpoint changes and illumination variations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459316",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 15,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Image matching",
                "Image segmentation",
                "Robustness",
                "Layout",
                "Lighting",
                "Object detection",
                "Geometry",
                "Object recognition",
                "Computer science",
                "Distortion measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "wide-baseline image matching",
                "line signatures",
                "line segments",
                "feature matching",
                "illumination variations"
            ]
        },
        "id": 168,
        "cited_by": [
            {
                "year": "2013",
                "id": 23
            }
        ]
    },
    {
        "title": "Matching as a non-cooperative game",
        "authors": [
            "Andrea Albarelli",
            "Samuel Rota Bul\u00f2",
            "Andrea Torsello",
            "Marcello Pelillo"
        ],
        "abstract": "With this paper we offer a game-theoretic perspective for the all-pervasive matching problem in computer vision. Specifically, we formulate the matching problem as a (population) non-cooperative game where the potential associations between the items to be matched correspond to (pure) strategies, while payoffs reflect the degree of compatibility between competing hypotheses. Within this formulation, the solutions of the matching problem correspond to evolutionary stable states (ESS's), a robust population-based generalization of the notion of a Nash equilibrium. In order to find ESS's of our matching game, we propose using a novel, fast evolutionary game dynamics motivated by Darwinian selection processes, which let the pure strategies play against each other until an equilibrium is reached. A distinguishing feature of the proposed framework is that it allows one to naturally deal with general many-to-many matching problems even in the presence of asymmetric compatibilities. The potential of the proposed approach is demonstrated via two sets of image matching experiments, both of which show that our results outperform those obtained using well-known domain-specific algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459312",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 17,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Electronic switching systems",
                "Robustness",
                "Parameter estimation",
                "Nash equilibrium",
                "Image matching",
                "Object recognition",
                "Stereo image processing",
                "Image reconstruction",
                "Voting"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "evolutionary computation",
                "game theory",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "noncooperative game",
                "game theoretic perspective",
                "all-pervasive matching problem",
                "computer vision",
                "evolutionary stable state",
                "Nash equilibrium",
                "evolutionary game dynamics",
                "Darwinian selection processes",
                "image matching",
                "domain specific algorithm"
            ]
        },
        "id": 169,
        "cited_by": [
            {
                "year": "2015",
                "id": 12
            }
        ]
    },
    {
        "title": "Simultaneous alignment and clustering for an image ensemble",
        "authors": [
            "Xiaoming Liu",
            "Yan Tong",
            "Frederick W. Wheeler"
        ],
        "abstract": "Joint alignment for an image ensemble can rectify images in the spatial domain such that the aligned images are as similar to each other as possible. This important technology has been applied to various object classes and medical applications. However, previous approaches to joint alignment work on an ensemble of a single object class. Given an ensemble with multiple object classes, we propose an approach to automatically and simultaneously solve two problems, image alignment and clustering. Both the alignment parameters and clustering parameters are formulated into a unified objective function, whose optimization leads to an unsupervised joint estimation approach. It is further extended to semi-supervised simultaneous estimation where a few labeled images are provided. Extensive experiments on diverse real-world databases demonstrate the capabilities of our work on this challenging problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459313",
        "reference_list": [
            {
                "year": "2007",
                "id": 19
            },
            {
                "year": "2007",
                "id": 25
            },
            {
                "year": "2007",
                "id": 73
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Medical services",
                "Biomedical equipment",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image matching",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image alignment",
                "image clustering",
                "image ensemble",
                "unsupervised joint estimation",
                "semisupervised simultaneous estimation"
            ]
        },
        "id": 170,
        "cited_by": []
    },
    {
        "title": "An algebraic approach to affine registration of point sets",
        "authors": [
            "Jeffrey Ho",
            "Adrian Peter",
            "Anand Rangarajan",
            "Ming-Hsuan Yang"
        ],
        "abstract": "This paper proposes a new affine registration algorithm for matching two point sets in IR2 or IR3. The input point sets are represented as probability density functions, using either Gaussian mixture models or discrete density models, and the problem of registering the point sets is treated as aligning the two distributions. Since polynomials transform as symmetric tensors under an affine transformation, the distributions' moments, which are the expected values of polynomials, also transform accordingly. Therefore, instead of solving the harder problem of aligning the two distributions directly, we solve the softer problem of matching the distributions' moments. By formulating a least-squares problem for matching moments of the two distributions up to degree three, the resulting cost function is a polynomial that can be efficiently optimized using techniques originated from algebraic geometry: the global minimum of this polynomial can be determined by solving a system of polynomial equations. The algorithm is robust in the presence of noises and outliers, and we validate the proposed algorithm on a variety of point sets with varying degrees of deformation and noise.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459309",
        "reference_list": [
            {
                "year": "2003",
                "id": 64
            },
            {
                "year": "2005",
                "id": 163
            },
            {
                "year": "2005",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 9,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Equations",
                "Density functional theory",
                "Probability density function",
                "Discrete transforms",
                "Cost function",
                "Gaussian distribution",
                "Tensile stress",
                "Geometry",
                "Noise robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "least squares approximations",
                "polynomials",
                "probability",
                "set theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "point sets",
                "affine registration algorithm",
                "probability density functions",
                "Gaussian mixture models",
                "discrete density models",
                "polynomials transform",
                "symmetric tensors",
                "affine transformation",
                "least-squares problem",
                "cost function",
                "algebraic geometry",
                "polynomial equations"
            ]
        },
        "id": 171,
        "cited_by": [
            {
                "year": "2011",
                "id": 273
            }
        ]
    },
    {
        "title": "Constructing implicit 3D shape models for pose estimation",
        "authors": [
            "Mica Arie-Nachimson",
            "Ronen Basri"
        ],
        "abstract": "We present a system that constructs \u201cimplicit shape models\u201d for classes of rigid 3D objects and utilizes these models to estimating the pose of class instances in single 2D images. We use the framework of implicit shape models to construct a voting procedure that allows for 3D transformations and projection and accounts for self occlusion. The model is comprised of a collection of learned features, their 3D locations, their appearances in different views, and the set of views in which they are visible. We further learn the parameters of a model from training images by applying a method that relies on factorization. We demonstrate the utility of the constructed models by applying them in pose estimation experiments to recover the viewpoint of class instances.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459310",
        "reference_list": [
            {
                "year": "2007",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 18,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Solid modeling",
                "Voting",
                "Deformable models",
                "Computer vision",
                "Computer science",
                "Image reconstruction",
                "Machine vision",
                "Buildings",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden feature removal",
                "image reconstruction",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "implicit 3D shape model construction",
                "pose estimation",
                "occlusion",
                "computer vision",
                "rigid 3D objects"
            ]
        },
        "id": 172,
        "cited_by": [
            {
                "year": "2015",
                "id": 278
            },
            {
                "year": "2011",
                "id": 124
            },
            {
                "year": "2011",
                "id": 161
            }
        ]
    },
    {
        "title": "Efficient human pose estimation via parsing a tree structure based human model",
        "authors": [
            "Xiaoqin Zhang",
            "Changcheng Li",
            "Xiaofeng Tong",
            "Weiming Hu",
            "Steve Maybank",
            "Yimin Zhang"
        ],
        "abstract": "Human pose estimation is the task of determining the states (location, orientation and scale) of each body part. It is important for many vision understanding applications, e.g. visual interactive gaming, immersive virtual reality, content-based image retrieval, etc. However, it remains a challenging task because of unknown image background, presence of clutter, partial occlusion and especially the high dimensional state space (usually 30+ dimensions). In this paper, we contribute to human pose estimation in two aspects. First, we design two efficient Markov Chain dynamics under the data-driven Markov Chain Monte Carlo (DDMCMC) framework to effectively explore the complex solution space. Second, we parse the tree structure state space into a lexicographic order according to the image observations and body topology, and the optimization process is conducted in this order. This realizes a much more efficient exploration than the sampling based search and exhaustive search, and thus achieves a tremendous speed-up. Experimental results demonstrate the efficiency and effectiveness of the proposed method in estimating various kinds of human poses, even with cluttered background , poor illumination or partial self-occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459306",
        "reference_list": [
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2005",
                "id": 60
            },
            {
                "year": "2005",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Tree data structures",
                "State-space methods",
                "Biological system modeling",
                "State estimation",
                "Virtual reality",
                "Image retrieval",
                "Content based retrieval",
                "Monte Carlo methods",
                "Topology"
            ],
            "INSPEC: Controlled Indexing": [
                "Markov processes",
                "Monte Carlo methods",
                "optimisation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human pose estimation",
                "parsing",
                "tree structure based human model",
                "image background",
                "partial occlusion",
                "Markov Chain dynamics",
                "data-driven Markov Chain Monte Carlo framework",
                "lexicographic order",
                "image observations",
                "body topology",
                "optimization process"
            ]
        },
        "id": 173,
        "cited_by": [
            {
                "year": "2017",
                "id": 246
            }
        ]
    },
    {
        "title": "Human pose estimation using consistent max-covering",
        "authors": [
            "Hao Jiang"
        ],
        "abstract": "We propose a novel consistent max-covering scheme for human pose estimation. Consistent max-covering formulates pose estimation as the covering of body part polygons on an object silhouette so that the body part tiles maximally cover the foreground, match local image features, and satisfy body linkage plan and color constraints. It uses high order constraints to anchor multiple body parts simultaneously; the hyper-edges in the part relation graph are essential for detecting complex poses. Because of using multiple clues in pose estimation, this method is resistant to cluttered foregrounds. We propose an efficient linear relaxation method to solve the consistent max-covering problem. Experiments on a variety of images and videos show that the proposed method is more robust than locally constrained methods for human pose estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459307",
        "reference_list": [
            {
                "year": "2003",
                "id": 85
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2005",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 16,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Immune system",
                "Assembly",
                "Tiles",
                "Couplings",
                "Relaxation methods",
                "Videos",
                "Robustness",
                "Application software",
                "Learning systems"
            ]
        },
        "id": 174,
        "cited_by": []
    },
    {
        "title": "Poselets: Body part detectors trained using 3D human pose annotations",
        "authors": [
            "Lubomir Bourdev",
            "Jitendra Malik"
        ],
        "abstract": "We address the classic problems of detection, segmentation and pose estimation of people in images with a novel definition of a part, a poselet. We postulate two criteria (1) It should be easy to find a poselet given an input image (2) it should be easy to localize the 3D configuration of the person conditioned on the detection of a poselet. To permit this we have built a new dataset, H3D, of annotations of humans in 2D photographs with 3D joint information, inferred using anthropometric constraints. This enables us to implement a data-driven search procedure for finding poselets that are tightly clustered in both 3D joint configuration space as well as 2D image appearance. The algorithm discovers poselets that correspond to frontal and profile faces, pedestrians, head and shoulder views, among others. Each poselet provides examples for training a linear SVM classifier which can then be run over the image in a multiscale scanning mode. The outputs of these poselet detectors can be thought of as an intermediate layer of nodes, on top of which one can run a second layer of classification or regression. We show how this permits detection and localization of torsos or keypoints such as left shoulder, nose, etc. Experimental results show that we obtain state of the art performance on people detection in the PASCAL VOC 2007 challenge, among other datasets. We are making publicly available both the H3D dataset as well as the poselet parameters for use by other researchers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459303",
        "reference_list": [
            {
                "year": "2005",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 365,
            "other": 175,
            "total": 540
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Humans",
                "Shoulder",
                "Image segmentation",
                "Clustering algorithms",
                "Face detection",
                "Head",
                "Support vector machines",
                "Support vector machine classification",
                "Torso"
            ]
        },
        "id": 175,
        "cited_by": [
            {
                "year": "2017",
                "id": 365
            },
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2015",
                "id": 117
            },
            {
                "year": "2015",
                "id": 127
            },
            {
                "year": "2015",
                "id": 175
            },
            {
                "year": "2015",
                "id": 219
            },
            {
                "year": "2015",
                "id": 220
            },
            {
                "year": "2015",
                "id": 264
            },
            {
                "year": "2015",
                "id": 267
            },
            {
                "year": "2015",
                "id": 275
            },
            {
                "year": "2015",
                "id": 356
            },
            {
                "year": "2015",
                "id": 362
            },
            {
                "year": "2015",
                "id": 431
            },
            {
                "year": "2015",
                "id": 434
            },
            {
                "year": "2013",
                "id": 4
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2013",
                "id": 89
            },
            {
                "year": "2013",
                "id": 90
            },
            {
                "year": "2013",
                "id": 188
            },
            {
                "year": "2013",
                "id": 217
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2013",
                "id": 314
            },
            {
                "year": "2013",
                "id": 330
            },
            {
                "year": "2013",
                "id": 376
            },
            {
                "year": "2013",
                "id": 398
            },
            {
                "year": "2013",
                "id": 423
            },
            {
                "year": "2013",
                "id": 424
            },
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 52
            },
            {
                "year": "2011",
                "id": 91
            },
            {
                "year": "2011",
                "id": 137
            },
            {
                "year": "2011",
                "id": 168
            },
            {
                "year": "2011",
                "id": 176
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2011",
                "id": 272
            },
            {
                "year": "2011",
                "id": 282
            },
            {
                "year": "2011",
                "id": 321
            }
        ]
    },
    {
        "title": "Joint pose estimator and feature learning for object detection",
        "authors": [
            "Karim Ali",
            "Fran\u00e7ois Fleuret",
            "David Hasler",
            "Pascal Fua"
        ],
        "abstract": "A new learning strategy for object detection is presented. The proposed scheme forgoes the need to train a collection of detectors dedicated to homogeneous families of poses, and instead learns a single classifier that has the inherent ability to deform based on the signal of interest. Specifically, we train a detector with a standard AdaBoost procedure by using combinations of pose-indexed features and pose estimators instead of the usual image features. This allows the learning process to select and combine various estimates of the pose with features able to implicitly compensate for variations in pose. We demonstrate that a detector built in such a manner provides noticeable gains on two hand video sequences and analyze the performance of our detector as these data sets are synthetically enriched in pose while not increased in size.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459304",
        "reference_list": [
            {
                "year": "2003",
                "id": 140
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 3,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Detectors",
                "Face detection",
                "Performance gain",
                "Kernel",
                "Video sequences",
                "Image analysis",
                "Image sequence analysis",
                "Performance analysis",
                "Labeling"
            ]
        },
        "id": 176,
        "cited_by": []
    },
    {
        "title": "Estimating human shape and pose from a single image",
        "authors": [
            "Peng Guan",
            "Alexander Weiss",
            "Alexandru O. B\u00e3lan",
            "Michael J. Black"
        ],
        "abstract": "We describe a solution to the challenging problem of estimating human body shape from a single photograph or painting. Our approach computes shape and pose parameters of a 3D human body model directly from monocular image cues and advances the state of the art in several directions. First, given a user-supplied estimate of the subject's height and a few clicked points on the body we estimate an initial 3D articulated body pose and shape. Second, using this initial guess we generate a tri-map of regions inside, outside and on the boundary of the human, which is used to segment the image using graph cuts. Third, we learn a low-dimensional linear model of human shape in which variations due to height are concentrated along a single dimension, enabling height-constrained estimation of body shape. Fourth, we formulate the problem of parametric human shape from shading. We estimate the body pose, shape and reflectance as well as the scene lighting that produces a synthesized body that robustly matches the image evidence. Quantitative experiments demonstrate how smooth shading provides powerful constraints on human shape. We further demonstrate a novel application in which we extract 3D human models from archival photographs and paintings.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459300",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 12,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Shape"
            ]
        },
        "id": 177,
        "cited_by": [
            {
                "year": "2017",
                "id": 415
            },
            {
                "year": "2011",
                "id": 3
            },
            {
                "year": "2011",
                "id": 138
            },
            {
                "year": "2011",
                "id": 247
            }
        ]
    },
    {
        "title": "Tracking in unstructured crowded scenes",
        "authors": [
            "Mikel Rodriguez",
            "Saad Ali",
            "Takeo Kanade"
        ],
        "abstract": "This paper presents a target tracking framework for unstructured crowded scenes. Unstructured crowded scenes are defined as those scenes where the motion of a crowd appears to be random with different participants moving in different directions over time. This means each spatial location in such scenes supports more than one, or multi-modal, crowd behavior. The case of tracking in structured crowded scenes, where the crowd moves coherently in a common direction, and the direction of motion does not vary over time, was previously handled in. In this work, we propose to model various crowd behavior (or motion) modalities at different locations of the scene by employing Correlated Topic Model (CTM) of. In our construction, words correspond to low level quantized motion features and topics correspond to crowd behaviors. It is then assumed that motion at each location in an unstructured crowd scene is generated by a set of behavior proportions, where behaviors represent distributions over low-level motion features. This way any one location in the scene may support multiple crowd behavior modalities and can be used as prior information for tracking. Our approach enables us to model a diverse set of unstructured crowd domains, which range from cluttered time-lapse microscopy videos of cell populations in vitro, to footage of crowded sporting events.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459301",
        "reference_list": [],
        "citation": {
            "ieee": 54,
            "other": 48,
            "total": 102
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Videos",
                "Legged locomotion",
                "Target tracking",
                "Robot vision systems",
                "Computer vision",
                "Microscopy",
                "In vitro",
                "Rail transportation",
                "Airports"
            ],
            "INSPEC: Controlled Indexing": [
                "target tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unstructured crowded scenes",
                "target tracking framework",
                "spatial location",
                "crowd behavior proportions",
                "correlated topic model",
                "CTM",
                "motion features",
                "time-lapse microscopy videos",
                "cell populations"
            ]
        },
        "id": 178,
        "cited_by": [
            {
                "year": "2017",
                "id": 31
            },
            {
                "year": "2011",
                "id": 94
            },
            {
                "year": "2011",
                "id": 156
            },
            {
                "year": "2011",
                "id": 308
            }
        ]
    },
    {
        "title": "Video stabilization using robust feature trajectories",
        "authors": [
            "Ken-Yi Lee",
            "Yung-Yu Chuang",
            "Bing-Yu Chen",
            "Ming Ouhyoung"
        ],
        "abstract": "This paper proposes a new approach for video stabilization. Most existing video stabilization methods adopt a framework of three steps, motion estimation, motion compensation and image composition. Camera motion is often estimated based on pairwise registration between frames. Thus, these methods often assume static scenes or distant backgrounds. Furthermore, for scenes with moving objects, robust methods are required for finding the dominant motion. Such assumptions and judgements could lead to errors in motion parameters. Errors are compounded by motion compensation which smoothes motion parameters. This paper proposes a method to directly stabilize a video without explicitly estimating camera motion, thus assuming neither motion models nor dominant motion. The method first extracts robust feature trajectories from the input video. Optimization is then performed to find a set of transformations to smooth out these trajectories and stabilize the video. In addition, the optimization also considers quality of the stabilized video and selects a video with not only smooth camera motion but also less unfilled area after stabilization. Experiments show that our method can deal with complicated videos containing near, large and multiple moving objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459297",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 7,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Cameras",
                "Motion estimation",
                "Videoconference",
                "Motion compensation",
                "Layout",
                "Smoothing methods",
                "Video recording",
                "Feature extraction",
                "Sensor systems"
            ]
        },
        "id": 179,
        "cited_by": [
            {
                "year": "2013",
                "id": 9
            }
        ]
    },
    {
        "title": "Complex volume and pose tracking with probabilistic dynamical models and visual hull constraints",
        "authors": [
            "Norimichi Ukita",
            "Michiro Hirai",
            "Masatsugu Kidode"
        ],
        "abstract": "We propose a method for estimating the pose of a human body using its approximate 3D volume (visual hull) obtained in real time from synchronized videos. Our method can cope with loose-fitting clothing, which hides the human body and produces non-rigid motions and critical reconstruction errors, as well as tight-fitting clothing. To follow the shape variations robustly against erratic motions and the ambiguity between a reconstructed body shape and its pose, the probabilistic dynamical model of human volumes is learned from training temporal volumes refined by error correction. The dynamical model of a body pose (joint angles) is also learned with its corresponding volume. By comparing the volume model with an input visual hull and regressing its pose from the pose model, pose estimation can be realized. In our method, this is improved by double volume comparison: 1) comparison in a low-dimensional latent space with probabilistic volume models and 2) comparison in an observation volume space using geometric constrains between a real volume and a visual hull. Comparative experiments demonstrate the effectiveness of our method faster than existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459298",
        "reference_list": [
            {
                "year": "2007",
                "id": 107
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 6,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Biological system modeling",
                "Humans",
                "Image reconstruction",
                "Clothing",
                "Joints",
                "Motion estimation",
                "Robustness",
                "Hidden Markov models",
                "Tracking"
            ]
        },
        "id": 180,
        "cited_by": []
    },
    {
        "title": "Absolute scale in structure from motion from a single vehicle mounted camera by exploiting nonholonomic constraints",
        "authors": [
            "Davide Scaramuzza",
            "Friedrich Fraundorfer",
            "Marc Pollefeys",
            "Roland Siegwart"
        ],
        "abstract": "In structure-from-motion with a single camera it is well known that the scene can be only recovered up to a scale. In order to compute the absolute scale, one needs to know the baseline of the camera motion or the dimension of at least one element in the scene. In this paper, we show that there exists a class of structure-from-motion problems where it is possible to compute the absolute scale completely automatically without using this knowledge, that is, when the camera is mounted on wheeled vehicles (e.g. cars, bikes, or mobile robots). The construction of these vehicles puts interesting constraints on the camera motion, which are known as \u201cnonholonomic constraints\u201d. The interesting case is when the camera has an offset to the vehicle's center of motion. We show that by just knowing this offset, the absolute scale can be computed with a good accuracy when the vehicle turns. We give a mathematical derivation and provide experimental results on both simulated and real data over a large image dataset collected during a 3 Km path. To our knowledge this is the first time nonholonomic constraints of wheeled vehicles are used to estimate the absolute scale. We believe that the proposed method can be useful in those research areas involving visual odometry and mapping with vehicle mounted cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459294",
        "reference_list": [
            {
                "year": "2003",
                "id": 183
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 23,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Robot vision systems",
                "Mobile robots",
                "Layout",
                "Computer vision",
                "Bicycles",
                "Axles",
                "Motion estimation",
                "Remotely operated vehicles",
                "Computational geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "distance measurement",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "absolute scale",
                "nonholonomic constraints",
                "camera motion",
                "structure-from-motion problem",
                "mathematical derivation",
                "wheeled vehicles",
                "visual odometry",
                "visual mapping",
                "vehicle mounted cameras"
            ]
        },
        "id": 181,
        "cited_by": []
    },
    {
        "title": "Domain adaptive semantic diffusion for large scale context-based video annotation",
        "authors": [
            "Yu-Gang Jiang",
            "Jun Wang",
            "Shih-Fu Chang",
            "Chong-Wah Ngo"
        ],
        "abstract": "Learning to cope with domain change has been known as a challenging problem in many real-world applications. This paper proposes a novel and efficient approach, named domain adaptive semantic diffusion (DASD), to exploit semantic context while considering the domain-shift-of-context for large scale video concept annotation. Starting with a large set of concept detectors, the proposed DASD refines the initial annotation results using graph diffusion technique, which preserves the consistency and smoothness of the annotation over a semantic graph. Different from the existing graph learning methods which capture relations among data samples, the semantic graph treats concepts as nodes and the concept affinities as the weights of edges. Particularly, the DASD approach is capable of simultaneously improving the annotation results and adapting the concept affinities to new test data. The adaptation provides a means to handle domain change between training and test data, which occurs very often in video annotation task. We conduct extensive experiments to improve annotation results of 374 concepts over 340 hours of videos from TRECVID 2005-2007 data sets. Results show consistent and significant performance gain over various baselines. In addition, the proposed approach is very efficient, completing DASD over 374 concepts within just 2 milliseconds for each video shot on a regular PC.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459295",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2007",
                "id": 145
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 2,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Large-scale systems",
                "Testing",
                "Detectors",
                "Airplanes",
                "Gunshot detection systems",
                "Computer vision",
                "Videoconference",
                "Training data",
                "Computer science",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "object detection",
                "task analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "domain adaptive semantic diffusion",
                "large scale context-based video concept annotation",
                "graph diffusion technique",
                "semantic graph",
                "graph learning methods"
            ]
        },
        "id": 182,
        "cited_by": []
    },
    {
        "title": "Stabilizing motion tracking using retrieved motion priors",
        "authors": [
            "Andreas Baak",
            "Bodo Rosenhahn",
            "Meinard M\u00fcller",
            "Hans-Peter Seidel"
        ],
        "abstract": "In this paper, we introduce a novel iterative motion tracking framework that combines 3D tracking techniques with motion retrieval for stabilizing markerless human motion capturing. The basic idea is to start human tracking without prior knowledge about the performed actions. The resulting 3D motion sequences, which may be corrupted due to tracking errors, are locally classified according to available motion categories. Depending on the classification result, a retrieval system supplies suitable motion priors, which are then used to regularize and stabilize the tracking in the next iteration step. Experiments with the HumanEVA-II benchmark show that tracking and classification are remarkably improved after few iterations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459291",
        "reference_list": [
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 8,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Humans",
                "Computer vision",
                "Information retrieval",
                "Joints",
                "Computer graphics",
                "Application software",
                "Animation",
                "Avatars",
                "Biomedical imaging"
            ]
        },
        "id": 183,
        "cited_by": [
            {
                "year": "2011",
                "id": 138
            },
            {
                "year": "2011",
                "id": 157
            }
        ]
    },
    {
        "title": "Robust visual tracking using \u21131minimization",
        "authors": [
            "Xue Mei",
            "Haibin Ling"
        ],
        "abstract": "In this paper we propose a robust visual tracking method by casting tracking as a sparse approximation problem in a particle filter framework. In this framework, occlusion, corruption and other challenging issues are addressed seamlessly through a set of trivial templates. Specifically, to find the tracking target at a new frame, each target candidate is sparsely represented in the space spanned by target templates and trivial templates. The sparsity is achieved by solving an \u2113 1 -regularized least squares problem. Then the candidate with the smallest projection error is taken as the tracking target. After that, tracking is continued using a Bayesian state inference framework in which a particle filter is used for propagating sample distributions over time. Two additional components further improve the robustness of our approach: 1) the nonnegativity constraints that help filter out clutter that is similar to tracked targets in reversed intensity patterns, and 2) a dynamic template update scheme that keeps track of the most representative templates throughout the tracking procedure. We test the proposed approach on five challenging sequences involving heavy occlusions, drastic illumination changes, and large pose variations. The proposed approach shows excellent performance in comparison with three previously proposed trackers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459292",
        "reference_list": [
            {
                "year": "2003",
                "id": 46
            },
            {
                "year": "2005",
                "id": 27
            },
            {
                "year": "2003",
                "id": 142
            }
        ],
        "citation": {
            "ieee": 52,
            "other": 60,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Target tracking",
                "Particle tracking",
                "Particle filters",
                "Casting",
                "Least squares approximation",
                "Least squares methods",
                "Bayesian methods",
                "Testing",
                "Lighting"
            ]
        },
        "id": 184,
        "cited_by": []
    },
    {
        "title": "The Normalized Subspace Inclusion: Robust clustering of motion subspaces",
        "authors": [
            "Nuno Pinho da Silva",
            "Jo\u00e3o Paulo Costeira"
        ],
        "abstract": "Perceiving dynamic scenes of rigid bodies, through affine projections of moving 3D point clouds, boils down to clustering the rigid motion subspaces supported by the points' image trajectories. For a physically meaningful interpretation, clusters must be consistent with the geometry of the underlying subspaces. Most of the existing measures for subspace clustering are ambiguous, or geometrically inconsistent. A practical consequence is that methods based on such (dis)similarities are unstable when the number of rigid bodies increase. This paper introduces the Normalized Subspace Inclusion (NSI) criterion to resolve these issues. Relying on this similarity, we propose a robust methodology for rigid motion segmentation, and test it, extensively, on the Hopkins155 database. The geometric consistency of the NSI assures the method's accuracy when the number of rigid bodies increases, while robustness proves to be suitable for dealing with challenging imaging conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459288",
        "reference_list": [
            {
                "year": "2007",
                "id": 133
            },
            {
                "year": "2007",
                "id": 124
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Layout",
                "Computer vision",
                "Image segmentation",
                "Clouds",
                "Geometry",
                "Motion segmentation",
                "Image databases",
                "Testing",
                "Spatial databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "normalized subspace inclusion",
                "robust clustering",
                "3D point clouds",
                "rigid motion subspaces",
                "affine projections",
                "point image trajectories",
                "rigid motion segmentation",
                "Hopkins 155 database"
            ]
        },
        "id": 185,
        "cited_by": []
    },
    {
        "title": "LabelMe video: Building a video database with human annotations",
        "authors": [
            "Jenny Yuen",
            "Bryan Russell",
            "Ce Liu",
            "Antonio Torralba"
        ],
        "abstract": "Currently, video analysis algorithms suffer from lack of information regarding the objects present, their interactions, as well as from missing comprehensive annotated video databases for benchmarking. We designed an online and openly accessible video annotation system that allows anyone with a browser and internet access to efficiently annotate object category, shape, motion, and activity information in real-world videos. The annotations are also complemented with knowledge from static image databases to infer occlusion and depth information. Using this system, we have built a scalable video database composed of diverse video samples and paired with human-guided annotations. We complement this paper demonstrating potential uses of this database by studying motion statistics as well as cause-effect motion relationships between objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459289",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2007",
                "id": 265
            }
        ],
        "citation": {
            "ieee": 55,
            "other": 24,
            "total": 79
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Image databases",
                "Algorithm design and analysis",
                "Layout",
                "Internet",
                "Statistics",
                "Computer vision",
                "Tracking",
                "Videoconference",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "Internet",
                "object recognition",
                "video databases",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "labelme video",
                "video database",
                "human-guided annotations",
                "video analysis algorithms",
                "video annotation system",
                "Internet",
                "real-world videos",
                "image databases",
                "diverse video samples",
                "motion statistics",
                "cause-effect motion relationships"
            ]
        },
        "id": 186,
        "cited_by": [
            {
                "year": "2017",
                "id": 30
            },
            {
                "year": "2015",
                "id": 361
            },
            {
                "year": "2015",
                "id": 489
            },
            {
                "year": "2013",
                "id": 202
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2011",
                "id": 253
            }
        ]
    },
    {
        "title": "A robust boosting tracker with minimum error bound in a co-training framework",
        "authors": [
            "Rong Liu",
            "Jian Cheng",
            "Hanqing Lu"
        ],
        "abstract": "The varying object appearance and unlabeled data from new frames are always the challenging problem in object tracking. Recently machine learning methods are widely applied to tracking, and some online and semi-supervised algorithms are developed to handle these difficulties. In this paper, we consider tracking as a classification problem and present a novel tracking method based on boosting in a co-training framework. The proposed tracker can be online updated and boosted with multi-view weak hypothesis. The most important contribution of this paper is that we find a boosting error upper bound in a co-training framework to guide the novel tracker construction. In theory, the proposed tracking method is proved to minimize this error bound. In experiments, the accuracy rate of foreground/ background classification and the tracking results are both served as evaluation metrics. Experimental results show good performance of proposed novel tracker on challenging sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459285",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Boosting",
                "Computer errors",
                "Learning systems",
                "Computer vision",
                "Linear discriminant analysis",
                "Support vector machines",
                "Support vector machine classification",
                "Laboratories",
                "Pattern recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object detection",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust boosting tracker",
                "minimum error bound",
                "co-training framework",
                "object appearance",
                "object tracking",
                "machine learning methods",
                "online algorithms",
                "semi-supervised algorithms",
                "foreground classification",
                "background classification",
                "computer vision"
            ]
        },
        "id": 187,
        "cited_by": []
    },
    {
        "title": "Using individuality to track individuals: Clustering individual trajectories in crowds using local appearance and frequency trait",
        "authors": [
            "Daisuke Sugimura",
            "Kris M. Kitani",
            "Takahiro Okabe",
            "Yoichi Sato",
            "Akihiro Sugimoto"
        ],
        "abstract": "In this work, we propose a method for tracking individuals in crowds. Our method is based on a trajectory-based clustering approach that groups trajectories of image features that belong to the same person. The key novelty of our method is to make use of a person's individuality, that is, the gait features and the temporal consistency of local appearance to track each individual in a crowd. Gait features in the frequency domain have been shown to be an effective biometric cue in discriminating between individuals, and our method uses such features for tracking people in crowds for the first time. Unlike existing trajectory-based tracking methods, our method evaluates the dissimilarity of trajectories with respect to a group of three adjacent trajectories. In this way, we incorporate the temporal consistency of local patch appearance to differentiate trajectories of multiple people moving in close proximity. Our experiments show that the use of gait features and the temporal consistency of local appearance contributes to significant performance improvement in tracking people in crowded scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459286",
        "reference_list": [
            {
                "year": "2007",
                "id": 235
            },
            {
                "year": "2001",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 12,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Frequency domain analysis",
                "Layout",
                "Biometrics",
                "Robustness",
                "Tracking",
                "Legged locomotion",
                "Frequency measurement",
                "Time measurement",
                "Informatics"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image motion analysis",
                "image recognition",
                "pattern clustering",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "individuality",
                "local appearance",
                "frequency trait",
                "crowds",
                "trajectory-based clustering",
                "image features",
                "gait features"
            ]
        },
        "id": 188,
        "cited_by": [
            {
                "year": "2011",
                "id": 220
            }
        ]
    },
    {
        "title": "Tracking a hand manipulating an object",
        "authors": [
            "Henning Hamer",
            "Konrad Schindler",
            "Esther Koller-Meier",
            "Luc Van Gool"
        ],
        "abstract": "We present a method for tracking a hand while it is interacting with an object. This setting is arguably the one where hand-tracking has most practical relevance, but poses significant additional challenges: strong occlusions by the object as well as self-occlusions are the norm, and classical anatomical constraints need to be softened due to the external forces between hand and object. To achieve robustness to partial occlusions, we use an individual local tracker for each segment of the articulated structure. The segments are connected in a pairwise Markov random field, which enforces the anatomical hand structure through soft constraints on the joints between adjacent segments. The most likely hand configuration is found with belief propagation. Both range and color data are used as input. Experiments are presented for synthetic data with ground truth and for real data of people manipulating objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459282",
        "reference_list": [
            {
                "year": "2001",
                "id": 161
            }
        ],
        "citation": {
            "ieee": 49,
            "other": 27,
            "total": 76
        },
        "keywords": {
            "IEEE Keywords": [
                "Fingers",
                "Computer vision",
                "Robustness",
                "Markov random fields",
                "Joints",
                "Belief propagation",
                "Application software",
                "Laboratories",
                "Computer science",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "belief maintenance",
                "computer vision",
                "image colour analysis",
                "image motion analysis",
                "Markov processes",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hand tracking",
                "self-occlusion",
                "anatomical constraint",
                "local tracker",
                "pairwise Markov random field",
                "anatomical hand structure",
                "soft constraint",
                "belief propagation"
            ]
        },
        "id": 189,
        "cited_by": [
            {
                "year": "2015",
                "id": 81
            },
            {
                "year": "2015",
                "id": 91
            },
            {
                "year": "2013",
                "id": 306
            },
            {
                "year": "2013",
                "id": 402
            },
            {
                "year": "2011",
                "id": 265
            }
        ]
    },
    {
        "title": "Robust facial feature tracking using selected multi-resolution linear predictors",
        "authors": [
            "Eng-Jon Ong",
            "Yuxuan Lan",
            "Barry Theobald",
            "Richard Harvey",
            "Richard Bowden"
        ],
        "abstract": "This paper proposes a learnt data-driven approach for accurate, real-time tracking of facial features using only intensity information. Constraints such as a-priori shape models or temporal models for dynamics are not required or used. Tracking facial features simply becomes the independent tracking of a set of points on the face. This allows us to cope with facial configurations not present in the training data. Tracking is achieved via linear predictors which provide a fast and effective method for mapping pixel-level information to tracked feature position displacements. To improve on this, a novel and robust biased linear predictor is proposed in this paper. Multiple linear predictors are grouped into a rigid flock to increase robustness. To further improve tracking accuracy, a novel probabilistic selection method is used to identify relevant visual areas for tracking a feature point. These selected flocks are then combined into a hierarchical multi-resolution LP model. Experimental results also show that this method performs more robustly and accurately than AAMs, without any a priori shape information and with minimal training examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459283",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 4,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Facial features",
                "Training data",
                "Active appearance model",
                "Active contours",
                "Active shape model",
                "Vectors",
                "Lips",
                "Markov random fields",
                "Tracking"
            ]
        },
        "id": 190,
        "cited_by": []
    },
    {
        "title": "Automatic annotation of human actions in video",
        "authors": [
            "Olivier Duchenne",
            "Ivan Laptev",
            "Josef Sivic",
            "Francis Bach",
            "Jean Ponce"
        ],
        "abstract": "This paper addresses the problem of automatic temporal annotation of realistic human actions in video using minimal manual supervision. To this end we consider two associated problems: (a) weakly-supervised learning of action models from readily available annotations, and (b) temporal localization of human actions in test videos. To avoid the prohibitive cost of manual annotation for training, we use movie scripts as a means of weak supervision. Scripts, however, provide only implicit, noisy, and imprecise information about the type and location of actions in video. We address this problem with a kernel-based discriminative clustering algorithm that locates actions in the weakly-labeled training data. Using the obtained action samples, we train temporal action detectors and apply them to locate actions in the raw video data. Our experiments demonstrate that the proposed method for weakly-supervised learning of action models leads to significant improvement in action detection. We present detection results for three action classes in four feature length movies with challenging and realistic video data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459279",
        "reference_list": [
            {
                "year": "2007",
                "id": 265
            }
        ],
        "citation": {
            "ieee": 78,
            "other": 50,
            "total": 128
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Motion pictures",
                "Costs",
                "Training data",
                "Computer vision",
                "Application software",
                "Clustering methods",
                "Testing",
                "Clustering algorithms",
                "Detectors"
            ]
        },
        "id": 191,
        "cited_by": [
            {
                "year": "2017",
                "id": 76
            },
            {
                "year": "2017",
                "id": 304
            },
            {
                "year": "2017",
                "id": 371
            },
            {
                "year": "2017",
                "id": 552
            },
            {
                "year": "2015",
                "id": 351
            },
            {
                "year": "2015",
                "id": 500
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2013",
                "id": 284
            },
            {
                "year": "2013",
                "id": 343
            }
        ]
    },
    {
        "title": "Detection and removal of chromatic moving shadows in surveillance scenarios",
        "authors": [
            "Ivan Huerta",
            "Michael Holte",
            "Thomas Moeslund",
            "Jordi Gonz\u00e0lez"
        ],
        "abstract": "Segmentation in the surveillance domain has to deal with shadows to avoid distortions when detecting moving objects. Most segmentation approaches dealing with shadow detection are typically restricted to penumbra shadows. Therefore, such techniques cannot cope well with umbra shadows. Consequently, umbra shadows are usually detected as part of moving objects. In this paper we present a novel technique based on gradient and colour models for separating chromatic moving cast shadows from detected moving objects. Firstly, both a chromatic invariant colour cone model and an invariant gradient model are built to perform automatic segmentation while detecting potential shadows. In a second step, regions corresponding to potential shadows are grouped by considering \u201ca bluish effect\u201d and an edge partitioning. Lastly, (i) temporal similarities between textures and (ii) spatial similarities between chrominance angle and brightness distortions are analysed for all potential shadow regions in order to finally identify umbra shadows. Unlike other approaches, our method does not make any a-priori assumptions about camera location, surface geometries, surface textures, shapes and types of shadows, objects, and background. Experimental results show the performance and accuracy of our approach in different shadowed materials and illumination conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459280",
        "reference_list": [
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 11,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Surveillance",
                "Object detection",
                "Vehicle dynamics",
                "Computer vision",
                "Shape",
                "Lighting",
                "Layout",
                "Brightness",
                "Surface texture",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image segmentation",
                "object detection",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "chromatic moving shadow detection",
                "surveillance domain segmentation",
                "moving object detection",
                "penumbra shadows",
                "chromatic invariant colour cone model",
                "invariant gradient model",
                "edge partitioning",
                "bluish effect",
                "brightness distortions",
                "chrominance angle distortion",
                "camera location",
                "surface textures"
            ]
        },
        "id": 192,
        "cited_by": [
            {
                "year": "2011",
                "id": 113
            }
        ]
    },
    {
        "title": "Learning deformable action templates from cluttered videos",
        "authors": [
            "Benjamin Yao",
            "Song-Chun Zhu"
        ],
        "abstract": "In this paper, we present a Deformable Action Template (DAT) model that is learnable from cluttered real-world videos with weak supervisions. In our generative model, an action template is a sequence of image templates each of which consists of a set of shape and motion primitives (Gabor wavelets and optical-flow patches) at selected orientations and locations. These primitives are allowed to slightly perturb their locations and orientations to account for spatial deformations. We use a shared pursuit algorithm to automatically discover a best set of primitives and weights by maximizing the likelihood over one or more aligned training examples. Since it is extremely hard to accurately label human actions from real-world videos, we use a three-step semi-supervised learning procedure. 1) For each human action class, a template is initialized from a labeled (one bounding-box per frame) training video. 2) The template is used to detect actions from other training videos of the same class by a dynamic space-time warping algorithm, which searches a best match between the template and target video in 5D space (x, y, scale, ttemplate and ttarget) using dynamic programming. 3) The template is updated by the shared pursuit algorithm over all aligned videos. The 2nd and 3rd steps iterate several times to arrive at an optimal action template. We tested our algorithm on a cluttered action dataset (the CMU dataset) and achieved favorable performance than. Our classification performance on the KTH dataset is also comparable to state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459277",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 139
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 15,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "image classification",
                "image segmentation",
                "image sequences",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable action learning",
                "cluttered videos",
                "deformable action template model",
                "shared pursuit algorithm",
                "likelihood maximization",
                "semisupervised learning",
                "dynamic space time warping algorithm",
                "dynamic programming",
                "aligned videos",
                "cluttered action dataset",
                "image sequences"
            ]
        },
        "id": 193,
        "cited_by": [
            {
                "year": "2015",
                "id": 368
            },
            {
                "year": "2013",
                "id": 335
            },
            {
                "year": "2011",
                "id": 162
            }
        ]
    },
    {
        "title": "Robust tracking-by-detection using a detector confidence particle filter",
        "authors": [
            "Michael D. Breitenstein",
            "Fabian Reichlin",
            "Bastian Leibe",
            "Esther Koller-Meier",
            "Luc Van Gool"
        ],
        "abstract": "We propose a novel approach for multi-person tracking-by-detection in a particle filtering framework. In addition to final high-confidence detections, our algorithm uses the continuous confidence of pedestrian detectors and online trained, instance-specific classifiers as a graded observation model. Thus, generic object category knowledge is complemented by instance-specific information. A main contribution of this paper is the exploration of how these unreliable information sources can be used for multi-person tracking. The resulting algorithm robustly tracks a large number of dynamically moving persons in complex scenes with occlusions, does not rely on background modeling, and operates entirely in 2D (requiring no camera or ground plane calibration). Our Markovian approach relies only on information from the past and is suitable for online applications. We evaluate the performance on a variety of datasets and show that it improves upon state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459278",
        "reference_list": [
            {
                "year": "2007",
                "id": 97
            },
            {
                "year": "2003",
                "id": 146
            }
        ],
        "citation": {
            "ieee": 185,
            "other": 116,
            "total": 301
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Particle tracking",
                "Detectors",
                "Particle filters",
                "Target tracking",
                "Object detection",
                "Filtering",
                "Layout",
                "Computer vision",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "Markov processes",
                "object detection",
                "particle filtering (numerical methods)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "particle filtering framework",
                "multiperson tracking-by-detection",
                "high-confidence detection",
                "pedestrian detectors",
                "online trained classifier",
                "instance-specific classifier",
                "graded observation model",
                "object category knowledge",
                "Markovian approach"
            ]
        },
        "id": 194,
        "cited_by": [
            {
                "year": "2015",
                "id": 278
            },
            {
                "year": "2015",
                "id": 338
            },
            {
                "year": "2015",
                "id": 340
            },
            {
                "year": "2011",
                "id": 308
            },
            {
                "year": "2009",
                "id": 33
            }
        ]
    },
    {
        "title": "An information theoretic approach for tracker performance evaluation",
        "authors": [
            "K. Kao Edward",
            "P. Daggett Matthew",
            "B. Hurley Michael"
        ],
        "abstract": "Automated tracking of vehicles and people is essential for the effective utilization of imagery in wide area surveillance applications. In order to determine the best tracking algorithm and parameters for a given application, a comprehensive evaluation procedure is required. However, despite half a century of research in multi-target tracking, there is no consensus on how to score the overall performance of these trackers. Existing evaluation approaches assess tracker performance through measures of correspondence between ground truth tracks and system tracks using metrics such as track detection rate, track completeness, track fragmentation rate, and track ID change rate. However, each of these only provides a partial measure of performance and no good method exists to combine them into a holistic metric. Towards this end, this paper presents a pair of information theoretic metrics with similar behavior to the Receiver Operating Characteristic (ROC) curves of signal detection theory. Overall performance is evaluated with the percentage of truth information that a tracker captured and the total amount of false information that it reported. Information content is quantified through conditional entropy and mutual information computations using numerical estimates of the probability of association between the truth and the system tracks. This paper demonstrates how these information quality metrics provide a comprehensive evaluation of overall tracker performance and how they can be used to perform tracker comparisons and parameter tuning on wide-area surveillance imagery and other applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459275",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Pathology",
                "Computer vision",
                "Laboratories",
                "Vehicles",
                "Signal detection",
                "Entropy",
                "Mutual information",
                "Performance evaluation",
                "Video surveillance",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "information theory",
                "performance evaluation",
                "probability",
                "signal detection",
                "target tracking",
                "vehicles",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "information theoretic approach",
                "tracker performance evaluation",
                "automated vehicle tracking",
                "automated people tracking",
                "wide area surveillance",
                "multi-target tracking",
                "holistic metric",
                "receiver operating characteristic",
                "ROC curves",
                "signal detection theory",
                "probability"
            ]
        },
        "id": 195,
        "cited_by": []
    },
    {
        "title": "Adaptive fragments-based tracking of non-rigid objects using level sets",
        "authors": [
            "Prakash Chockalingam",
            "Nalin Pradeep",
            "Stan Birchfield"
        ],
        "abstract": "We present an approach to visual tracking based on dividing a target into multiple regions, or fragments. The target is represented by a Gaussian mixture model in a joint feature-spatial space, with each ellipsoid corresponding to a different fragment. The fragments are automatically adapted to the image data, being selected by an efficient region-growing procedure and updated according to a weighted average of the past and present image statistics. Modeling of target and background are performed in a Chan-Vese manner, using the framework of level sets to preserve accurate boundaries of the target. The extracted target boundaries are used to learn the dynamic shape of the target over time, enabling tracking to continue under total occlusion. Experimental results on a number of challenging sequences demonstrate the effectiveness of the technique.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459276",
        "reference_list": [
            {
                "year": "2003",
                "id": 54
            },
            {
                "year": "2003",
                "id": 139
            }
        ],
        "citation": {
            "ieee": 57,
            "other": 28,
            "total": 85
        },
        "keywords": {
            "IEEE Keywords": [
                "Level set",
                "Target tracking",
                "Shape",
                "Pixel",
                "Time measurement",
                "Ellipsoids",
                "Statistics",
                "Data mining",
                "Bayesian methods",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image processing",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive fragments-based tracking",
                "nonrigid objects",
                "level sets",
                "visual tracking",
                "Gaussian mixture model",
                "feature-spatial space",
                "efficient region-growing procedure",
                "image statistics"
            ]
        },
        "id": 196,
        "cited_by": [
            {
                "year": "2017",
                "id": 576
            },
            {
                "year": "2015",
                "id": 361
            },
            {
                "year": "2013",
                "id": 196
            },
            {
                "year": "2013",
                "id": 309
            },
            {
                "year": "2011",
                "id": 196
            },
            {
                "year": "2011",
                "id": 253
            }
        ]
    },
    {
        "title": "Keyframe-based real-time camera tracking",
        "authors": [
            "Zilong Dong",
            "Guofeng Zhang",
            "Jiaya Jia",
            "Hujun Bao"
        ],
        "abstract": "We present a novel keyframe selection and recognition method for robust markerless real-time camera tracking. Our system contains an offline module to select features from a group of reference images and an online module to match them to the input live video in order to quickly estimate the camera pose. The main contribution lies in constructing an optimal set of keyframes from the input reference images, which are required to approximately cover the entire space and at the same time minimize the content redundancy amongst the selected frames. This strategy not only greatly saves the computation, but also helps significantly reduce the number of repeated features so as to improve the camera tracking quality. Our system also employs a parallel-computing scheme with multi-CPU hardware architecture. Experimental results show that our method dramatically enhances the computation efficiency and eliminates the jittering artifacts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459273",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2003",
                "id": 183
            },
            {
                "year": "2007",
                "id": 275
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Robustness",
                "Robot vision systems",
                "Feature extraction",
                "Layout",
                "Computer vision",
                "Hardware",
                "Computer architecture",
                "Parameter estimation",
                "Image reconstruction"
            ]
        },
        "id": 197,
        "cited_by": []
    },
    {
        "title": "Tracking a large number of objects from multiple views",
        "authors": [
            "Zheng Wu",
            "Nickolay I. Hristov",
            "Tyson L. Hedrick",
            "Thomas H. Kunz",
            "Margrit Betke"
        ],
        "abstract": "We propose a multi-object multi-camera framework for tracking large numbers of tightly-spaced objects that rapidly move in three dimensions. We formulate the problem of finding correspondences across multiple views as a multidimensional assignment problem and use a greedy randomized adaptive search procedure to solve this NP-hard problem efficiently. To account for occlusions, we relax the one-to-one constraint that one measurement corresponds to one object and iteratively solve the relaxed assignment problem. After correspondences are established, object trajectories are estimated by stereoscopic reconstruction using an epipolar-neighborhood search. We embedded our method into a tracker-to-tracker multi-view fusion system that not only obtains the three-dimensional trajectories of closely-moving objects but also accurately settles track uncertainties that could not be resolved from single views due to occlusion. We conducted experiments to validate our greedy assignment procedure and our technique to recover from occlusions. We successfully track hundreds of flying bats and provide an analysis of their group behavior based on 150 reconstructed 3D trajectories.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459274",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Image reconstruction",
                "Cameras",
                "Biology",
                "Multidimensional systems",
                "Layout",
                "Computer vision",
                "State estimation",
                "Stress",
                "Surveillance"
            ]
        },
        "id": 198,
        "cited_by": [
            {
                "year": "2011",
                "id": 313
            }
        ]
    },
    {
        "title": "Detection driven adaptive multi-cue integration for multiple human tracking",
        "authors": [
            "Ming Yang",
            "Fengjun Lv",
            "Wei Xu",
            "Yihong Gong"
        ],
        "abstract": "In video surveillance scenarios, appearances of both human and their nearby scenes may experience large variations due to scale and view angle changes, partial occlusions, or interactions of a crowd. These challenges may weaken the effectiveness of a dedicated target observation model even based on multiple cues, which demands for an agile framework to adjust target observation models dynamically to maintain their discriminative power. Towards this end, we propose a new adaptive way to integrate multi-cue in tracking multiple human driven by human detections. Given a human detection can be reliably associated with an existing trajectory, we adapt the way how to combine specifically devised models based on different cues in this tracker so as to enhance the discriminative power of the integrated observation model in its local neighborhood. This is achieved by solving a regression problem efficiently. Specifically, we employ 3 observation models for a single person tracker based on color models of part of torso regions, an elliptical head model, and bags of local features, respectively. Extensive experiments on 3 challenging surveillance datasets demonstrate long-term reliable tracking performance of this method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459252",
        "reference_list": [
            {
                "year": "2003",
                "id": 46
            },
            {
                "year": "2001",
                "id": 108
            },
            {
                "year": "2007",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 17,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Target tracking",
                "Detectors",
                "Robustness",
                "Layout",
                "Surveillance",
                "Object detection",
                "Torso",
                "National electric code",
                "Laboratories"
            ]
        },
        "id": 199,
        "cited_by": [
            {
                "year": "2011",
                "id": 314
            }
        ]
    },
    {
        "title": "Optical flow estimation on coarse-to-fine region-trees using discrete optimization",
        "authors": [
            "Cheng Lei",
            "Yee-Hong Yang"
        ],
        "abstract": "In this paper, we propose a new region-based method for accurate motion estimation using discrete optimization. In particular, the input image is represented as a tree of over-segmented regions and the optical flow is estimated by optimizing an energy function defined on such a region-tree using dynamic programming. To accommodate the sampling-inefficiency problem intrinsic to discrete optimization compared to the continuous optimization based methods, both spatial and solution domain coarse-to-fine (C2F) strategies are used. That is, multiple region-trees are built using different over-segmentation granularities. Starting from a global displacement label discretization, optical flow estimation on the coarser level region-tree is used for defining region-wise finer displacement samplings for finer level region-trees. Furthermore, cross-checking based occlusion detection and correction and continuous optimization are also used to improve accuracy. Extensive experiments using the Middlebury benchmark datasets have shown that our proposed method can produce top-ranking results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459253",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            },
            {
                "year": "2005",
                "id": 171
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 0,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Optimization methods",
                "Optical computing",
                "Motion estimation",
                "Sampling methods",
                "Minimization methods",
                "Dynamic programming",
                "Stereo vision",
                "Image segmentation",
                "Image representation"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "image segmentation",
                "image sequences",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "otical flow estimation",
                "coarse-to-fine region-trees",
                "discrete optimization",
                "region-based method",
                "motion estimation",
                "over-segmented regions",
                "energy function",
                "dynamic programming",
                "sampling-inefficiency problem",
                "continuous optimization",
                "over-segmentation granularities",
                "global displacement label discretization",
                "optical flow estimation",
                "region-wise finer displacement samplings",
                "cross-checking based occlusion detection",
                "Middlebury benchmark datasets"
            ]
        },
        "id": 200,
        "cited_by": [
            {
                "year": "2009",
                "id": 201
            }
        ]
    },
    {
        "title": "A new multiview spacetime-consistent depth recovery framework for free viewpoint video rendering",
        "authors": [
            "Cheng Lei",
            "Xi Da Chen",
            "Yee Hong Yang"
        ],
        "abstract": "In this paper, we present a new approach for recovering spacetime-consistent depth maps from multiple video sequences captured by stationary, synchronized and calibrated cameras for depth based free viewpoint video rendering. Our two-pass approach is generalized from the recently proposed region-tree based binocular stereo matching method. In each pass, to enforce temporal consistency between successive depth maps, the traditional region-tree is extended into a temporal one by including connections to \u201ctemporal neighbor regions\u201d in previous video frames, which are identified using estimated optical flow information. For enforcing spatial consistency, multi-view geometric constraints are used to identify inconsistencies between depth maps among different views which are captured in an inconsistency map for each view. Iterative optimizations are performed to progressively correct inconsistencies through inconsistency maps based depth hypotheses pruning and visibility reasoning. Furthermore, the background depth and color information is generated from the results of the first pass and is used in the second pass to enforce sequence-wise temporal consistency and to aid in identifying and correcting spatial inconsistencies. The extensive experimental evaluations have shown that our proposed approach is very effective in producing spatially and temporally consistent depth maps.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459357",
        "reference_list": [
            {
                "year": "2007",
                "id": 173
            },
            {
                "year": "2009",
                "id": 200
            },
            {
                "year": "2007",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image motion analysis",
                "Color",
                "Motion estimation",
                "Video sequences",
                "Geometry",
                "Degradation",
                "Constraint optimization",
                "Cameras",
                "Geometrical optics"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image sequences",
                "iterative methods",
                "optimisation",
                "rendering (computer graphics)",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "free viewpoint video rendering",
                "multiview spacetime consistent depth recovery",
                "video sequences",
                "binocular stereo matching method",
                "temporal neighbor regions",
                "Iterative optimizations",
                "inconsistency maps",
                "temporal consistency",
                "multiview geometric constraints"
            ]
        },
        "id": 201,
        "cited_by": [
            {
                "year": "2015",
                "id": 100
            },
            {
                "year": "2011",
                "id": 311
            }
        ]
    },
    {
        "title": "Reconstructing 3D motion trajectories of particle swarms by global correspondence selection",
        "authors": [
            "Danping Zou",
            "Qi Zhao",
            "Hai Shan Wu",
            "Yan Qiu Chen"
        ],
        "abstract": "This paper addresses the problem of reconstructing the 3D motion trajectories of particle swarms using two temporally synchronized and geometrically calibrated cameras. The 3D trajectory reconstruction problem involves two challenging tasks - stereo matching and temporal tracking. Existing methods separate the two and process them one at a time sequentially, and suffer from frequent irresolvable ambiguities in stereo matching and in tracking. We unify the two tasks, and propose a Global Correspondence Selection scheme to solve stereo matching and temporal tracking simultaneously. It treats 3D trajectory acquisition problem as selecting appropriate stereo correspondences among all possible ones for each target by minimizing a cost function. Experiment results show that the proposed method has significant performance advantage over existing approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459358",
        "reference_list": [
            {
                "year": "2007",
                "id": 161
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 10,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Particle swarm optimization",
                "Target tracking",
                "Trajectory",
                "Stereo vision",
                "Cameras",
                "Cost function",
                "Particle tracking",
                "Stereo image processing",
                "Image reconstruction",
                "Image sequences"
            ]
        },
        "id": 202,
        "cited_by": []
    },
    {
        "title": "SURF Tracking",
        "authors": [
            "Wei He",
            "Takayoshi Yamashita",
            "Hongtao Lu",
            "Shihong Lao"
        ],
        "abstract": "Most motion-based tracking algorithms assume that objects undergo rigid motion, which is most likely disobeyed in real world. In this paper, we present a novel motion-based tracking framework which makes no such assumptions. Object is represented by a set of local invariant features, whose motions are observed by a feature correspondence process. A generative model is proposed to depict the relationship between local feature motions and object global motion, whose parameters are learned efficiently by an on-line EM algorithm. And the object global motion is estimated in term of maximum likelihood of observations. Then an updating mechanism is employed to adapt object representation. Experiments show that our framework is flexible and robust in dealing with appearance changes, background clutter, illumination changes and occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459360",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 1,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Motion estimation",
                "Maximum likelihood estimation",
                "Robustness",
                "Lighting",
                "Computer science",
                "Pattern recognition",
                "Shape",
                "Search methods",
                "Monitoring"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image motion analysis",
                "maximum likelihood estimation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SURF tracking",
                "motion-based tracking algorithms",
                "rigid motion",
                "local invariant features",
                "generative model",
                "object global motion",
                "maximum likelihood estimation",
                "online EM algorithm"
            ]
        },
        "id": 203,
        "cited_by": [
            {
                "year": "2013",
                "id": 138
            }
        ]
    },
    {
        "title": "Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities",
        "authors": [
            "M. S. Ryoo",
            "J. K. Aggarwal"
        ],
        "abstract": "Human activity recognition is a challenging task, especially when its background is unknown or changing, and when scale or illumination differs in each video. Approaches utilizing spatio-temporal local features have proved that they are able to cope with such difficulties, but they mainly focused on classifying short videos of simple periodic actions. In this paper, we present a new activity recognition methodology that overcomes the limitations of the previous approaches using local features. We introduce a novel matching, spatio-temporal relationship match, which is designed to measure structural similarity between sets of features extracted from two videos. Our match hierarchically considers spatio-temporal relationships among feature points, thereby enabling detection and localization of complex non-periodic activities. In contrast to previous approaches to `classify' videos, our approach is designed to `detect and localize' all occurring activities from continuous videos where multiple actors and pedestrians are present. We implement and test our methodology on a newly-introduced dataset containing videos of multiple interacting persons and individual pedestrians. The results confirm that our system is able to recognize complex non-periodic activities (e.g. `push' and `hug') from sets of spatio-temporal features even when multiple activities are present in the scene.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459361",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2007",
                "id": 147
            }
        ],
        "citation": {
            "ieee": 169,
            "other": 138,
            "total": 307
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Layout",
                "Feature extraction",
                "System testing",
                "Computer vision",
                "Lighting",
                "Intelligent robots",
                "Surveillance",
                "Cameras",
                "Robot vision systems"
            ],
            "INSPEC: Controlled Indexing": [
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatio-temporal relationship",
                "video structure comparison",
                "complex human activities recognition",
                "human activity recognition",
                "spatio-temporal local features",
                "structural similarity",
                "multiple interacting persons",
                "individual pedestrians",
                "complex nonperiodic activities",
                "spatio-temporal features"
            ]
        },
        "id": 204,
        "cited_by": [
            {
                "year": "2017",
                "id": 308
            },
            {
                "year": "2015",
                "id": 500
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2013",
                "id": 437
            },
            {
                "year": "2013",
                "id": 447
            },
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2011",
                "id": 99
            },
            {
                "year": "2011",
                "id": 131
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2011",
                "id": 330
            }
        ]
    },
    {
        "title": "Robust motion estimation using trajectory spectrum learning: Application to aortic and mitral valve modeling from 4D TEE",
        "authors": [
            "Razvan Ioan Ionasec",
            "Yang Wang",
            "Bogdan Georgescu",
            "Ingmar Voigt",
            "Nassir Navab",
            "Dorin Comaniciu"
        ],
        "abstract": "In this paper we propose a robust and efficient approach to localizing and estimating the motion of non-rigid and articulated objects using marginal trajectory spectrum learning. Detecting the motion directly in the Euclidean space is often found difficult to guarantee a smooth and accurate result and might be affected by drifting. These issues, however, can be addressed effectively by formulating the motion estimation problem as spectrum detection in the trajectory space. The full trajectory space can be decomposed into orthogonal subspaces defined by generic bases, such as the Discrete Fourier Transform (DFT). The obtained representation is shown to be compact, facilitating efficient learning and optimization in its marginal spaces. In the training stage, local features are extended in the temporal domain to integrate the time coherence constraint and selected via boosting to form strong classifiers. An incremental optimization is performed in sparse marginal spaces learned from the training data. To maximize efficiency and robustness we constrain the search based on clusters of hypotheses defined in each subspace. Experiments demonstrate the performance of the proposed method on articulated motion estimation of aortic and mitral valves from ultrasound data. Our method is evaluated on 65 4D TEE sequences (1516 volumes) with the accuracy in the range of the inter-user variability of expert users. It provides in less than 60 seconds with an precision of 1.36 \u00b1 0.32mm a personalized 4D model of aortic and mitral valves crucial for the clinical workflow.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459363",
        "reference_list": [
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2003",
                "id": 97
            },
            {
                "year": "2003",
                "id": 144
            },
            {
                "year": "2007",
                "id": 85
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 5,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Motion estimation",
                "Valves",
                "Motion detection",
                "Discrete Fourier transforms",
                "Time factors",
                "Boosting",
                "Training data",
                "Subspace constraints",
                "Ultrasonic imaging"
            ]
        },
        "id": 205,
        "cited_by": []
    },
    {
        "title": "Large displacement optical flow computation withoutwarping",
        "authors": [
            "Frank Steinbr\u00fccker",
            "Thomas Pock",
            "Daniel Cremers"
        ],
        "abstract": "We propose an algorithm for large displacement optical flow estimation which does not require the commonly used coarse-to-fine warping strategy. It is based on a quadratic relaxation of the optical flow functional which decouples data term and regularizer in such a way that the non-linearized variational problem can be solved by an alternation of two globally optimal steps, one imposing optimal data consistency, the other imposing discontinuity-preserving regularity of the flow field. Experimental results confirm that the proposed algorithmic implementation outperforms the traditional warping strategy, in particular for the case of large displacements of small scale structures.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459364",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 25,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical computing",
                "Image motion analysis",
                "Computer vision",
                "Motion estimation",
                "Biomedical optical imaging",
                "Computer science",
                "Computer applications",
                "Robustness",
                "Computer graphics",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large displacement optical flow computation",
                "coarse-to-fine warping strategy",
                "quadratic relaxation",
                "nonlinearized variational problem",
                "optimal steps",
                "optimal data consistency"
            ]
        },
        "id": 206,
        "cited_by": [
            {
                "year": "2015",
                "id": 98
            },
            {
                "year": "2013",
                "id": 172
            },
            {
                "year": "2013",
                "id": 184
            }
        ]
    },
    {
        "title": "A direct approach for efficiently tracking with 3D morphable models",
        "authors": [
            "Enrique Mu\u00f1oz",
            "Jos\u00e9 M. Buenaposada",
            "Luis Baumela"
        ],
        "abstract": "We present an efficient algorithm for fitting a morphable model to an image sequence. It is built on a projective geometry formulation of perspective projection, which results in a linear mapping from 3D shape to the projective plane, and a factorisation of this mapping into matrices that can be partially computed off-line. This algorithm can cope with full 360 degrees object rotation and linear deformations. We validate our approach using synthetically generated and real sequences. Compared to a plain Lucas-Kanade implementation, we achieve a six fold increase in performance for a rigid object and two fold for a non-rigid face.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459366",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Face detection",
                "Jacobian matrices",
                "Facial animation",
                "Target tracking",
                "Active appearance model",
                "Image sequences",
                "Shape",
                "Humans",
                "Computer vision",
                "Gaussian processes"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "matrix decomposition",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D morphable model tracking",
                "image sequence",
                "projective geometry formulation",
                "linear mapping",
                "3D shape mapping",
                "matrix factorisation",
                "perspective projection"
            ]
        },
        "id": 207,
        "cited_by": []
    },
    {
        "title": "Illumination aware MCMC Particle Filter for long-term outdoor multi-object simultaneous tracking and classification",
        "authors": [
            "Fran\u00e7ois Bardet",
            "Thierry Chateau",
            "Datta Ramadasan"
        ],
        "abstract": "This paper addresses real-time automatic visual tracking, labeling and classification of a variable number of objects such as pedestrians or/and vehicles, under time-varying illumination conditions. The illumination and multi-object configuration are jointly tracked through a Markov Chain Monte-Carlo Particle Filter (MCMC PF). The measurement is provided by a static camera, associated to a basic foreground / background segmentation. As a first contribution, we propose in this paper to jointly track the light source within the Particle Filter, considering it as an additionnal object. Illumination-dependant shadows cast by objects are modeled and treated as foreground, thus avoiding the difficult task of shadow segmentation. As a second contribution, we estimate object category as a random variable also tracked within the Particle Filter, thus unifying object tracking and classification into a single process. Real time tracking results are shown and discussed on sequences involving various categories of users such as pedestrians, cars, light trucks and heavy trucks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459367",
        "reference_list": [
            {
                "year": "2001",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Particle filters",
                "Particle tracking",
                "Light sources",
                "Surveillance",
                "Target tracking",
                "Vehicles",
                "Image segmentation",
                "Computer vision",
                "Labeling"
            ]
        },
        "id": 208,
        "cited_by": []
    },
    {
        "title": "Real-time visual tracking via Incremental Covariance Tensor Learning",
        "authors": [
            "Yi Wu",
            "Jian Cheng",
            "Jinqiao Wang",
            "Hanqing Lu"
        ],
        "abstract": "Visual tracking is a challenging problem, as an object may change its appearance due to pose variations, illumination changes, and occlusions. Many algorithms have been proposed to update the target model using the large volume of available information during tracking, but at the cost of high computational complexity. To address this problem, we present a tracking approach that incrementally learns a low-dimensional covariance tensor representation, efficiently adapting online to appearance changes for each mode of the target with only \u0303(1) computational complexity. Moreover, a weighting scheme is adopted to ensure less modeling power is expended fitting older observations. Both of these features contribute measurably to improving overall tracking performance. Tracking is then led by the Bayesian inference framework in which a particle filter is used to propagate sample distributions over time. With the help of integral images, our tracker achieves real-time performance. Extensive experiments demonstrate the effectiveness of the proposed tracking algorithm for the targets undergoing appearance variations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459369",
        "reference_list": [
            {
                "year": "2005",
                "id": 164
            },
            {
                "year": "2003",
                "id": 195
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 15,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Target tracking",
                "Lighting",
                "Computational complexity",
                "Particle tracking",
                "Particle filters",
                "Robustness",
                "Optical filters",
                "Educational institutions",
                "Software"
            ]
        },
        "id": 209,
        "cited_by": [
            {
                "year": "2013",
                "id": 195
            }
        ]
    },
    {
        "title": "Efficient privacy preserving video surveillance",
        "authors": [
            "Maneesh Upmanyu",
            "Anoop M. Namboodiri",
            "Kannan Srinathan",
            "C.V. Jawahar"
        ],
        "abstract": "Widespread use of surveillance cameras in offices and other business establishments, pose a significant threat to the privacy of the employees and visitors. The challenge of introducing privacy and security in such a practical surveillance system has been stifled by the enormous computational and communication overhead required by the solutions. In this paper, we propose an efficient framework to carry out privacy preserving surveillance. We split each frame into a set of random images. Each image by itself does not convey any meaningful information about the original frame, while collectively, they retain all the information. Our solution is derived from a secret sharing scheme based on the Chinese Remainder Theorem, suitably adapted to image data. Our method enables distributed secure processing and storage, while retaining the ability to reconstruct the original data in case of a legal requirement. The system installed in an office like environment can effectively detect and track people, or solve similar surveillance tasks. Our proposed paradigm is highly efficient compared to Secure Multiparty Computation, making privacy preserving surveillance, practical.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459370",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 34,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Privacy",
                "Video surveillance",
                "Sliding mode control",
                "Cryptography",
                "Face detection",
                "Computer vision",
                "Business",
                "Secure storage",
                "Smart cameras",
                "Videoconference"
            ],
            "INSPEC: Controlled Indexing": [
                "data privacy",
                "distributed processing",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "privacy preserving video surveillance",
                "surveillance cameras",
                "random images",
                "secret sharing scheme",
                "Chinese remainder theorem",
                "distributed secure processing",
                "secure multiparty computation"
            ]
        },
        "id": 210,
        "cited_by": [
            {
                "year": "2015",
                "id": 395
            },
            {
                "year": "2011",
                "id": 27
            }
        ]
    },
    {
        "title": "Correlated probabilistic trajectories for pedestrian motion detection",
        "authors": [
            "Frank Perbet",
            "Atsuto Maki",
            "Bj\u00f6rn Stenger"
        ],
        "abstract": "This paper introduces an algorithm for detecting walking motion using point trajectories in video sequences. Given a number of point trajectories, we identify those which are spatio-temporally correlated as arising from feet in walking motion. Unlike existing techniques we do not assume clean point tracks but instead propose \u201cprobabilistic trajectories\u201d as new features to classify. These are extracted from directed acyclic graphs whose edges represent temporal point correspondences and are weighted with their matching probability in terms of appearance and location. This representation tolerates the inherent trajectory ambiguity, for example due to occlusions. We then learn the correlation between the movement of two feet using a random forest classifier. The effectiveness of the algorithm is demonstrated in experiments on image sequences captured with a static camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459372",
        "reference_list": [
            {
                "year": "2005",
                "id": 105
            },
            {
                "year": "2003",
                "id": 57
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion detection",
                "Legged locomotion",
                "Trajectory",
                "Image sequences",
                "Tracking",
                "Feature extraction",
                "Computer vision",
                "Foot",
                "Video sequences",
                "Cameras"
            ]
        },
        "id": 211,
        "cited_by": [
            {
                "year": "2011",
                "id": 220
            }
        ]
    },
    {
        "title": "Video scene categorization by 3D hierarchical histogram matching",
        "authors": [
            "Paritosh Gupta",
            "Sai Sankalp Arrabolu",
            "Mathew Brown",
            "Silvio Savarese"
        ],
        "abstract": "In this paper we present a new method for categorizing video sequences capturing different scene classes. This can be seen as a generalization of previous work on scene classification from single images. A scene is represented by a collection of 3D points with an appearance based codeword attached to each point. The cloud of points is recovered by using a robust SFM algorithm applied on the video sequence. A hierarchical structure of histograms located at different locations and at different scales is used to capture the typical spatial distribution of 3D points and codewords in the working volume. The scene is classified by SVM equipped with a histogram matching kernel, similar to. Results on a challenging dataset of 5 scene categories show competitive classification accuracy and superior performance with respect to a state-of-the-art 2D pyramid matching methods applied to individual image frames.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459373",
        "reference_list": [
            {
                "year": "2007",
                "id": 251
            },
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2005",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Histograms",
                "Video sequences",
                "Cameras",
                "Robustness",
                "Data mining",
                "Image reconstruction",
                "Support vector machines",
                "Support vector machine classification",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image recognition",
                "image reconstruction",
                "image sequences",
                "object recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video scene categorization",
                "3D hierarchical histogram matching",
                "video sequences",
                "appearance based codeword",
                "robust SFM algorithm",
                "SVM",
                "histogram matching kernel",
                "structure from motion",
                "3D point spatial distribution",
                "codeword spatial distribution",
                "scene classification"
            ]
        },
        "id": 212,
        "cited_by": []
    },
    {
        "title": "Structure- and motion-adaptive regularization for high accuracy optic flow",
        "authors": [
            "Andreas Wedel",
            "Daniel Cremers",
            "Thomas Pock",
            "Horst Bischof"
        ],
        "abstract": "The accurate estimation of motion in image sequences is of central importance to numerous computer vision applications. Most competitive algorithms compute flow fields by minimizing an energy made of a data and a regularity term. To date, the best performing methods rely on rather simple purely geometric regularizes favoring smooth motion. In this paper, we revisit regularization and show that appropriate adaptive regularization substantially improves the accuracy of estimated motion fields. In particular, we systematically evaluate regularizes which adoptively favor rigid body motion (if supported by the image data) and motion field discontinuities that coincide with discontinuities of the image structure. The proposed algorithm relies on sequential convex optimization, is real-time capable and outperforms all previously published algorithms by more than one average rank on the Middlebury optic flow benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459375",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 55,
            "other": 34,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "motion estimation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure-adaptive regularization",
                "motion-adaptive regularization",
                "high accuracy optic flow",
                "motion estimation",
                "image sequences",
                "computer vision",
                "sequential convex optimization"
            ]
        },
        "id": 213,
        "cited_by": [
            {
                "year": "2015",
                "id": 307
            },
            {
                "year": "2015",
                "id": 448
            },
            {
                "year": "2011",
                "id": 163
            },
            {
                "year": "2011",
                "id": 304
            }
        ]
    },
    {
        "title": "Video Scene Understanding Using Multi-scale Analysis",
        "authors": [
            "Yang Yang",
            "Jingen Liu",
            "Mubarak Shah"
        ],
        "abstract": "We propose a novel method for automatically discovering key motion patterns happening in a scene by observing the scene for an extended period. Our method does not rely on object detection and tracking, and uses low level features, the direction of pixel wise optical flow. We first divide the video into clips and estimate a sequence of flow-fields. Each moving pixel is quantized based on its location and motion direction. This is essentially a bag of words representation of clips. Once a bag of words representation is obtained, we proceed to the screening stage, using a measure called the `conditional entropy'. After obtaining useful words we apply Diffusion maps. Diffusion maps framework embeds the manifold points into a lower dimensional space while preserving the intrinsic local geometric structure. Finally, these useful words in lower dimensional space are clustered to discover key motion patterns. Diffusion map embedding involves diffusion time parameter which gives us ability to detect key motion patterns at different scales using multi-scale analysis. In addition, clips which are represented in terms of frequency of motion patterns can also be clustered to determine multiple dominant motion patterns which occur simultaneously, providing us further understanding of the scene. We have tested our approach on two challenging datasets and obtained interesting and promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459376",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 14,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Object detection",
                "Traffic control",
                "Computer vision",
                "Motion detection",
                "Pattern analysis",
                "Motion analysis",
                "Cameras",
                "Unmanned aerial vehicles",
                "Roads"
            ],
            "INSPEC: Controlled Indexing": [
                "entropy",
                "image motion analysis",
                "image sequences",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video scene understanding",
                "multiscale analysis",
                "object detection",
                "object tracking",
                "pixelwise optical flow",
                "bag of words representation",
                "conditional entropy",
                "diffusion maps",
                "geometric structure",
                "diffusion time parameter",
                "multiple dominant motion patterns"
            ]
        },
        "id": 214,
        "cited_by": [
            {
                "year": "2015",
                "id": 350
            },
            {
                "year": "2011",
                "id": 241
            }
        ]
    },
    {
        "title": "Superresolution texture maps for multiview reconstruction",
        "authors": [
            "Bastian Goldluecke",
            "Daniel Cremers"
        ],
        "abstract": "We study the scenario of a multiview setting, where several calibrated views of a textured object with known surface geometry are available. The objective is to estimate a diffuse texture map as precisely as possible. A superresolution image formation model based on the camera properties leads to a total variation energy for the desired texture map, which can be recovered as the minimizer of the functional by solving the Euler-Lagrange equation on the surface. The PDE is transformed to planar texture space via an automatically created conformal atlas, where it can be solved using total variation deblurring. The proposed approach allows to recover a high-resolution, high-quality texture map even from lower-resolution photographs, which is of interest for a variety of image-based modeling applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459378",
        "reference_list": [
            {
                "year": "2005",
                "id": 138
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 16,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Surface texture",
                "Surface reconstruction",
                "Image resolution",
                "Surface fitting",
                "Solid modeling",
                "Rendering (computer graphics)",
                "Geometry",
                "Spatial resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image resolution",
                "image restoration",
                "image texture",
                "partial differential equations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "superresolution texture maps",
                "multiview reconstruction",
                "surface geometry",
                "diffuse texture map",
                "superresolution image formation model",
                "camera property",
                "Euler-Lagrange equation",
                "PDE",
                "planar texture space",
                "conformal atlas",
                "total variation deblurring",
                "image-based modeling application"
            ]
        },
        "id": 215,
        "cited_by": [
            {
                "year": "2011",
                "id": 178
            },
            {
                "year": "2011",
                "id": 249
            }
        ]
    },
    {
        "title": "BLOGS: Balanced local and global search for non-degenerate two view epipolar geometry",
        "authors": [
            "Aveek Shankar Brahmachari",
            "Sudeep Sarkar"
        ],
        "abstract": "This work considers the problem of estimating the epipolar geometry between two cameras without needing a prespecified set of correspondences. It is capable of resolving the epipolar geometry for cases when the views differ significantly in terms of baseline and rotation, resulting in a large number features in one image that have no correspondence in the other image. We do conditional characterization of the probability space of correspondences based on Joint Feature Distributions (JFD). We seek to maximize the probabilistic support of the putative correspondence set over a number of MCMC iterations, guided by proposal distributions based on similarity or JFD. Similarity based guidance provides large movements (global) through correspondence space and JFD based guidance provides small movements (local) around the best known epipolar geometry the algorithm has found so far. We also propose a simple and novel method to rule out, at each iteration, correspondences that lead to degenerate configurations, thus speeding up convergence. We compare our algorithm with LO-RANSAC, NAPSAC, MAPSAC and BEEM, which are the current state of the art competing methods, on a dataset that has significantly more change in baseline, rotation, and scale than those used in the current literature. We quantitatively benchmark the performance using manually specified ground truth corresponding point pairs. We find that our approach can achieve results of similar quality as the current state of art in 10 times lesser number of iterations. We are also able to tolerate upto 90% outlier correspondences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459379",
        "reference_list": [
            {
                "year": "2001",
                "id": 130
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Blogs",
                "Motion estimation",
                "Iterative algorithms",
                "Cameras",
                "Layout",
                "Robustness",
                "Computational geometry",
                "Calibration",
                "Computer science",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "balanced local and global search",
                "probability space",
                "joint feature distributions",
                "putative correspondence set",
                "nondegenerate two view epipolar geometry",
                "MCMC iterations",
                "Monte Carlo Markov chain"
            ]
        },
        "id": 216,
        "cited_by": [
            {
                "year": "2013",
                "id": 308
            }
        ]
    },
    {
        "title": "Attached shadow coding: Estimating surface normals from shadows under unknown reflectance and lighting conditions",
        "authors": [
            "Takahiro Okabe",
            "Imari Sato",
            "Yoichi Sato"
        ],
        "abstract": "We present a novel technique, termed attached shadow coding, for estimating surface normals from shadows when the reflectance and lighting conditions are unknown. Our key idea is encoding surface points via attached shadows observed under different light source directions and then estimating surface normals on the basis of the similarity of the attached shadow codes. Because shadows do not rely on reflectance properties, our method is applicable to surfaces with various complex reflectances such as anisotropic and composite materials. Moreover, our method is robust against noise because it takes advantage of the combination of weak constraints imposed by a number of light sources. We theoretically show that the distance between the codes at two surface points is equal to the angle between the corresponding surface normals under the assumption of uniform lighting and a convex object. Our method embeds high-dimensional codes into a 3D surface normal space so that the inter-code distances are preserved. Furthermore, we extend the method in order to alleviate the effects of nonuniform lighting and cast shadows. Experimental results demonstrate the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459381",
        "reference_list": [
            {
                "year": "2007",
                "id": 42
            },
            {
                "year": "2003",
                "id": 107
            },
            {
                "year": "2005",
                "id": 129
            },
            {
                "year": "2007",
                "id": 180
            },
            {
                "year": "2005",
                "id": 83
            },
            {
                "year": "2007",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 11,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflectivity",
                "Light sources",
                "Stereo vision",
                "Photometry",
                "Computer vision",
                "Shape",
                "Informatics",
                "Encoding",
                "Anisotropic magnetoresistance",
                "Composite materials"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "light sources",
                "lighting",
                "reflectivity"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "attached shadow coding",
                "reflectance",
                "lighting",
                "light sources",
                "3D surface normal space",
                "surface normal estimation",
                "anisotropic materials",
                "composite materials",
                "inter-code distances"
            ]
        },
        "id": 217,
        "cited_by": []
    },
    {
        "title": "Factorizing Scene Albedo and Depth from a Single Foggy Image",
        "authors": [
            "Louis Kratz",
            "Ko Nishino"
        ],
        "abstract": "Atmospheric conditions induced by suspended particles, such as fog and haze, severely degrade image quality. Restoring the true scene colors (clear day image) from a single image of a weather-degraded scene remains a challenging task due to the inherent ambiguity between scene albedo and depth. In this paper, we introduce a novel probabilistic method that fully leverages natural statistics of both the albedo and depth of the scene to resolve this ambiguity. Our key idea is to model the image with a factorial Markov random field in which the. scene albedo and depth are. two statistically independent latent layers. We. show that we may exploit natural image and depth statistics as priors on these hidden layers and factorize a single foggy image via a canonical Expectation Maximization algorithm with alternating minimization. Experimental results show that the proposed method achieves more accurate restoration compared to state-of-the-art methods that focus on only recovering scene albedo or depth individually.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459382",
        "reference_list": [],
        "citation": {
            "ieee": 85,
            "other": 31,
            "total": 116
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "image colour analysis",
                "image restoration",
                "Markov processes",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "foggy image",
                "atmospheric conditions",
                "image quality",
                "scene color restoration",
                "weather-degraded scene",
                "scene albedo",
                "probabilistic method",
                "factorial Markov random field",
                "depth statistics",
                "expectation maximization algorithm",
                "alternating minimization"
            ]
        },
        "id": 218,
        "cited_by": [
            {
                "year": "2013",
                "id": 76
            }
        ]
    },
    {
        "title": "Complete multi-view reconstruction of dynamic scenes from probabilistic fusion of narrow and wide baseline stereo",
        "authors": [
            "Tony Tung",
            "Shohei Nobuhara",
            "Takashi Matsuyama"
        ],
        "abstract": "This paper presents a novel approach to achieve accurate and complete multi-view reconstruction of dynamic scenes (or 3D videos). 3D videos consist in sequences of 3D models in motion captured by a surrounding set of video cameras. To date 3D videos are reconstructed using multiview wide baseline stereo (MVS) reconstruction techniques. However it is still tedious to solve stereo correspondence problems: reconstruction accuracy falls when stereo photo-consistency is weak, and completeness is limited by self-occlusions. Most MVS techniques were indeed designed to deal with static objects in a controlled environment and therefore cannot solve these issues. Hence we propose to take advantage of the image content stability provided by each single-view video to recover any surface regions visible by at least one camera. In particular we present an original probabilistic framework to derive and predict the true surface of models. We propose to fuse multi-view structure-from-motion with robust 3D features obtained by MVS in order to significantly improve reconstruction completeness and accuracy. A min-cut problem where all exact features serve as priors is solved in a final step to reconstruct the 3D models. In addition, experimental results were conducted on synthetic and challenging real world datasets to illustrate the robustness and accuracy of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459384",
        "reference_list": [
            {
                "year": "2001",
                "id": 52
            },
            {
                "year": "2007",
                "id": 100
            },
            {
                "year": "2007",
                "id": 143
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 9,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image reconstruction",
                "Videos",
                "Stereo image processing",
                "Cameras",
                "Surface reconstruction",
                "Robustness",
                "Stability",
                "Predictive models",
                "Fuses"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "optimisation",
                "probability",
                "stereo image processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic scenes complete multiview reconstruction",
                "probabilistic fusion",
                "narrow baseline stereo",
                "3D videos",
                "multiview wide baseline stereo reconstruction technique",
                "stereo correspondence problems",
                "stereo photo consistency",
                "image content stability",
                "original probabilistic framework",
                "multiview structure from motion",
                "mincut problem"
            ]
        },
        "id": 219,
        "cited_by": [
            {
                "year": "2017",
                "id": 326
            },
            {
                "year": "2015",
                "id": 85
            },
            {
                "year": "2015",
                "id": 100
            }
        ]
    },
    {
        "title": "Hierarchical 3D diffusion wavelet shape priors",
        "authors": [
            "Salma Essafi",
            "Georg Langs",
            "Nikos Paragios"
        ],
        "abstract": "In this paper, we propose a novel representation of prior knowledge for image segmentation, using diffusion wavelets that can reflect arbitrary continuous interdependencies in shape data. The application of diffusion wavelets has, so far, largely been confined to signal processing. In our approach, and in contrast to state-of-the-art methods, we optimize the coefficients, the number and the position of landmarks, and the object topology - the domain on which the wavelets are defined - during the model learning phase, in a coarse-to-fine manner. The resulting paradigm supports hierarchies both in the model and the search space, can encode complex geometric and photometric dependencies of the structure of interest, and can deal with arbitrary topologies. We report results on two challenging medical data sets, that illustrate the impact of the soft parameterization and the potential of the diffusion operator.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459385",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 8,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Topology",
                "Image segmentation",
                "Continuous wavelet transforms",
                "Signal processing",
                "Optimization methods",
                "Wavelet domain",
                "Solid modeling",
                "Photometry",
                "Biomedical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image segmentation",
                "object recognition",
                "wavelet transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical 3D diffusion wavelet shape",
                "image segmentation",
                "signal processing",
                "object topology",
                "coarse-to-fine manner",
                "complex geometric dependencies",
                "photometric dependencies"
            ]
        },
        "id": 220,
        "cited_by": []
    },
    {
        "title": "A new minimal solution to the relative pose of a calibrated stereo camera with small field of view overlap",
        "authors": [
            "Brian Clipp",
            "Christopher Zach",
            "Jan-Michael Frahm",
            "Marc Pollefeys"
        ],
        "abstract": "In this paper we present a new minimal solver for the relative pose of a calibrated stereo camera (i.e. a pair of rigidly mounted cameras). Our method is based on the fact that a feature visible in all four images (two image pairs acquired at two points in time) constrains the relative pose of the second stereo camera to lie on a sphere around this feature, which has a known, triangulated position in the first stereo camera coordinate frame. This constraint leaves three degrees of freedom; two for the location of the second camera on the sphere, and the third for the rotation in the respective tangent plane. We use three 2D correspondences, in particular two correspondences from the left (or right) camera and one correspondence from the other camera, to solve for these three remaining degrees of freedom. This approach is amenable to stereo cameras having a small overlap in their views. We present an efficient solution for this novel relative pose problem, describe the incorporation of our proposed solver into the RANSAC framework, evaluate its performance given noise and outliers, and demonstrate its use in a real-time structure from motion system.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459387",
        "reference_list": [
            {
                "year": "2007",
                "id": 46
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 6,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Eyes",
                "Computer science",
                "Humans",
                "Time factors",
                "Real time systems",
                "Robot vision systems",
                "Simultaneous localization and mapping",
                "Humanoid robots",
                "Rotation measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relative pose",
                "calibrated stereo camera",
                "small field of view overlap",
                "minimal solver",
                "stereo camera coordinate frame",
                "tangent plane",
                "RANSAC framework",
                "real-time structure",
                "motion system"
            ]
        },
        "id": 221,
        "cited_by": [
            {
                "year": "2011",
                "id": 150
            }
        ]
    },
    {
        "title": "Improving accuracy of geometric parameter estimation using projected score method",
        "authors": [
            "Takayuki Okatani",
            "Koichiro Deguchi"
        ],
        "abstract": "A fundamental problem in computer vision (CV) is the estimation of geometric parameters from multiple observations obtained from images; examples of such problems range from ellipse fitting to multi-view structure from motion (SFM). The maximum likelihood (ML) method is widely used to estimate the parameters in such problems, assuming Gaussian noises to be present in the observations, for example, bundle adjustment for SFM. According to the theory of statistics, the ML estimates are nearly optimal for these problems, provided that the variance of the observation noises is sufficiently small. This implies that when noises are not small, more accurate estimates can be derived as compared to the ML estimates. In this study, we propose the application of a method called the projected score method, developed in statistics for computing higher-accuracy estimates, to the CV problems. We describe how it can be customized to solve the CV problems and propose a numerical algorithm to implement the method. We show that the method works effectively for such problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459388",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 5,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Parameter estimation",
                "Maximum likelihood estimation",
                "Computer vision",
                "Statistics",
                "Motion estimation",
                "Gaussian noise",
                "H infinity control",
                "Layout",
                "Estimation theory"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Gaussian noise",
                "maximum likelihood estimation",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric parameter estimation",
                "projected score method",
                "computer vision",
                "multiple image observations",
                "maximum likelihood method",
                "Gaussian noises",
                "statistics theory",
                "observation noises",
                "CV problems",
                "ellipse fitting",
                "structure from motion"
            ]
        },
        "id": 222,
        "cited_by": []
    },
    {
        "title": "Moving in stereo: Efficient structure and motion using lines",
        "authors": [
            "Manmohan Chandraker",
            "Jongwoo Lim",
            "David Kriegman"
        ],
        "abstract": "We present a fast and robust system for estimating structure and motion using a stereo pair, with straight lines as features. Our first set of contributions are efficient algorithms to perform this estimation using a few (two or three) lines, which are well-suited for use in a hypothesize-and-test framework. Our second contribution is the design of an efficient structure from motion system that performs robustly in complex indoor environments. By using infinite lines rather than line segments, our approach avoids the issues arising due to uncertain determination of end-points. Our cost function stems from a rank condition on planes backprojected from corresponding image lines. We propose a framework that imposes orthonormality constraints on the rigid body motion and can perform estimation using only two or three lines, through efficient solution of an overdetermined system of polynomials. This is in contrast to simple approaches which first reconstruct 3D lines and then align them, but perform poorly in real-world scenes with narrow baseline stereo. Experiments using synthetic as well as real data demonstrate the speed, accuracy and reliability of our system.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459390",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 12,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Layout",
                "Stereo vision",
                "Robustness",
                "Image reconstruction",
                "Cameras",
                "Tracking",
                "Motion detection",
                "Stereo image processing",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "motion estimation",
                "polynomials",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure estimation",
                "motion estimation",
                "stereo pair",
                "hypothesize-and-test framework",
                "orthonormality constraints",
                "rigid body motion",
                "polynomials",
                "3D line reconstruction"
            ]
        },
        "id": 223,
        "cited_by": []
    },
    {
        "title": "Color constancy using 3D scene geometry",
        "authors": [
            "Rui Lu",
            "Arjan Gijsenij",
            "Theo Gevers",
            "Vladimir Nedovi\u0107",
            "De Xu",
            "Jan-Mark Geusebroek"
        ],
        "abstract": "The aim of color constancy is to remove the effect of the color of the light source. As color constancy is inherently an ill-posed problem, most of the existing color constancy algorithms are based on specific imaging assumptions such as the grey-world and white patch assumptions. In this paper, 3D geometry models are used to determine which color constancy method to use for the different geometrical regions found in images. To this end, images are first classified into stages (rough 3D geometry models). According to the stage models, images are divided into different regions using hard and soft segmentation. After that, the best color constancy algorithm is selected for each geometry segment. As a result, light source estimation is tuned to the global scene geometry. Our algorithm opens the possibility to estimate the remote scene illumination color, by distinguishing nearby light source from distant illuminants. Experiments on large scale image datasets show that the proposed algorithm outperforms state-of-the-art single color constancy algorithms with an improvement of almost 14% of median angular error. When using an ideal classifier (i.e, all of the test images are correctly classified into stages), the performance of the proposed method achieves an improvement of 31% of median angular error compared to the best-performing single color constancy algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459391",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2007",
                "id": 216
            },
            {
                "year": "2007",
                "id": 269
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Geometry",
                "Light sources",
                "Solid modeling",
                "Image segmentation",
                "Lighting",
                "Large-scale systems",
                "Testing",
                "Error correction",
                "Color"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image colour analysis",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color constancy",
                "3D scene geometry",
                "hard segmentation",
                "soft segmentation",
                "light source estimation",
                "remote scene illumination color",
                "median angular error"
            ]
        },
        "id": 224,
        "cited_by": []
    },
    {
        "title": "Simultaneous photometric invariance and shape recovery",
        "authors": [
            "Cong Phuoc Huynh",
            "Antonio Robles-Kelly"
        ],
        "abstract": "In this paper, we identify the constraints under which the generally ill-posed problem of simultaneous recovery of surface shape and its photometric invariants can be rendered tractable. We examine the cases where a single or more images are acquired using different lighting directions with known illuminant power. Given these conditions, we state the constraints upon which the recovery of the surface geometry and its photometric parameters can be estimated. With these constraints, we then show how the recovery process may be formulated as an optimisation algorithm which aims to fit the reflectance models under study to the image reflectance. The approach presented here is general and can be applied to a family of reflectance models that are based on the Fresnel reflection theory. Thus, we provide a theoretical and computational background for recovering shape, material index of refraction and microscopic roughness from multi-spectral images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459393",
        "reference_list": [
            {
                "year": "2003",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Shape",
                "Reflectivity",
                "Rendering (computer graphics)",
                "Surface fitting",
                "Geometry",
                "Parameter estimation",
                "State estimation",
                "Constraint optimization",
                "Fresnel reflection"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "photometry",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "simultaneous photometric invariance",
                "surface shape recovery",
                "ill-posed problem",
                "simultaneous recovery",
                "photometric invariants",
                "lighting directions",
                "illuminant power",
                "surface geometry",
                "recovery process",
                "optimisation algorithm",
                "image reflectance models",
                "Fresnel reflection theory",
                "recovering shape",
                "material index",
                "multi-spectral images"
            ]
        },
        "id": 225,
        "cited_by": []
    },
    {
        "title": "Component analysis approach to estimation of tissue intensity distributions of 3D images",
        "authors": [
            "Arridhana Ciptadi",
            "Cheng Chen",
            "Vitali Zagorodnov"
        ],
        "abstract": "Many segmentation problems in medical imaging rely on accurate modeling and estimation of tissue intensity probability density functions. Gaussian mixture modeling, currently the most common approach, has several drawbacks, such as reliance on a specific model and iterative optimization. It also does not take advantage of substantially larger amount of data provided by 3D acquisitions, which are becoming standard in clinical environment. We propose a novel completely non-parametric algorithm to estimate the tissue intensity probabilities in 3D images. Instead of relying on traditional framework of iterating between classification and estimation, we pose the problem as an instance of a blind source separation problem, where the unknown distributions are treated as sources and histograms of image subvolumes as mixtures. The new approach performed well on synthetic data and real magnetic resonance (MR) scans, robustly capturing intensity distributions of even small image structures and partial volume voxels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459394",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Image analysis",
                "Image segmentation",
                "Biomedical imaging",
                "Probability density function",
                "Iterative methods",
                "Iterative algorithms",
                "Blind source separation",
                "Histograms",
                "Magnetic resonance",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "blind source separation",
                "Gaussian processes",
                "image segmentation",
                "medical image processing",
                "principal component analysis",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "component analysis approach",
                "tissue intensity distribution estimation",
                "3D image segmentation",
                "medical imaging",
                "tissue intensity probability density function estimation",
                "Gaussian mixture modeling",
                "iterative optimization",
                "3D acquisitions",
                "clinical environment",
                "nonparametric algorithm",
                "blind source separation problem",
                "image subvolume histograms",
                "synthetic data",
                "magnetic resonance scans",
                "image structures",
                "partial volume voxels"
            ]
        },
        "id": 226,
        "cited_by": []
    },
    {
        "title": "Simultaneous color consistency and depth map estimation for radiometrically varying stereo images",
        "authors": [
            "Yong Seok Heo",
            "Kyoung Mu Lee",
            "Sang Uk Lee"
        ],
        "abstract": "In this paper, we propose a new method that infers accurate depth maps and color-consistent images between radiometrically varying stereo images, simultaneously. In general, stereo matching and performing color consistency between stereo images are a chicken-and-egg problem. Color consistency enhances the performance of stereo matching, while accurate correspondences from stereo disparities improve color consistency between stereo images. We devise a new iterative framework in which these two processes can boost each other. For robust stereo matching, we utilize the mutual information-based method combined with the SIFT descriptor from which we can estimate the joint pdf in log-chromaticity color space. From this joint pdf, we can estimate a linear relationship between the corresponding pixels in stereo images. Using this linear relationship and the estimated depth maps, we devise a stereo color histogram equalization method to make color-consistent stereo images which conversely boost the disparity map estimation. Experimental results show that our method produces both accurate depth maps and color-consistent stereo images even for stereo images with severe radiometric differences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459396",
        "reference_list": [
            {
                "year": "2003",
                "id": 136
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Radiometry",
                "Stereo vision",
                "Cameras",
                "Lighting",
                "Calibration",
                "Robustness",
                "Histograms",
                "Internet",
                "Photometry",
                "Image sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth map estimation",
                "radiometrically varying stereo images",
                "color-consistent images",
                "stereo matching",
                "mutual information-based method",
                "stereo color histogram equalization method"
            ]
        },
        "id": 227,
        "cited_by": []
    },
    {
        "title": "Dense 3D reconstruction method using a single pattern for fast moving object",
        "authors": [
            "Ryusuke Sagawa",
            "Yuichi Ota",
            "Yasushi Yagi",
            "Ryo Furukawa",
            "Naoki Asada",
            "Hiroshi Kawasaki"
        ],
        "abstract": "Dense 3D reconstruction of extremely fast moving objects could contribute to various applications such as body structure analysis and accident avoidance and so on. The actual cases for scanning we assume are, for example, acquiring sequential shape at the moment when an object explodes, or observing fast rotating turbine's blades. In this paper, we propose such a technique based on a one-shot scanning method that reconstructs 3D shape from a single image where dense and simple pattern are projected onto an object. To realize dense 3D reconstruction from a single image, there are several issues to be solved; e.g. instability derived from using multiple colors, and difficulty on detecting dense pattern because of influence of object color and texture compression. This paper describes the solutions of the issues by combining two methods, that is (1) an efficient line detection technique based on de Bruijn sequence and belief propagation, and (2) an extension of shape from intersections of lines method. As a result, a scanning system that can capture an object in fast motion has been actually developed by using a high-speed camera. In the experiments, the proposed method successfully captured the sequence of dense shapes of an exploding balloon, and a breaking ceramic dish at 300\u20131000 fps.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459397",
        "reference_list": [
            {
                "year": "2001",
                "id": 153
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 22,
            "total": 57
        },
        "keywords": {
            "IEEE Keywords": [
                "Reconstruction algorithms",
                "Shape",
                "Image reconstruction",
                "Accidents",
                "Blades",
                "Object detection",
                "Image coding",
                "Belief propagation",
                "Cameras",
                "Ceramics"
            ]
        },
        "id": 228,
        "cited_by": [
            {
                "year": "2017",
                "id": 487
            },
            {
                "year": "2011",
                "id": 242
            }
        ]
    },
    {
        "title": "High-resolution shape reconstruction from multiple range images based on simultaneous estimation of surface and motion",
        "authors": [
            "Yoshihiro Watanabe",
            "Takashi Komuro",
            "Masatoshi Ishikawa"
        ],
        "abstract": "Recognition of dynamic scenes based on shape information could be useful for various applications. In this study, we aimed at improving the resolution of three-dimensional (3D) data obtained from moving targets. We present a simple clean and robust method that jointly estimates motion parameters and a high-resolution 3D shape. Experimental results are provided to illustrate the performance of the proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459399",
        "reference_list": [
            {
                "year": "2001",
                "id": 187
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image reconstruction",
                "Motion estimation",
                "Surface reconstruction",
                "Layout",
                "Light sources",
                "Image resolution",
                "Information science",
                "Robustness",
                "Parameter estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image reconstruction",
                "image resolution",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high-resolution shape reconstruction",
                "multiple range images",
                "image recognition",
                "shape information",
                "motion parameters",
                "motion estimation",
                "surface estimation"
            ]
        },
        "id": 229,
        "cited_by": []
    },
    {
        "title": "Single view reconstruction using shape grammars for urban environments",
        "authors": [
            "Panagiotis Koutsourakis",
            "Lo\u00efc Simon",
            "Olivier Teboul",
            "Georgios Tziritas",
            "Nikos Paragios"
        ],
        "abstract": "In this paper we introduce a novel approach to single view reconstruction using shape grammars. Our approach consists in modeling architectural styles using a set of basic shapes and a set of parametric rules, corresponding to increasing levels of detail. This approach is able to model elaborate and varying architectural styles, using a tree representation of variable depth and complexity. Towards reconstruction, the parameters of the rules are optimized using image-based and architectural costs. This is done through an efficient MRF formulation based on the shape grammar itself. The resulting framework can produce precise 3D models from single views, can deal with lack of texture and the presence of occlusions and specular reflections, while maintaining the ability to cope with very complex architectural styles. Promising results demonstrate the potential of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459400",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 17,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image reconstruction",
                "Buildings",
                "Computer science",
                "Computer vision",
                "Layout",
                "Scalability",
                "Cities and towns",
                "Surface fitting",
                "Navigation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "grammars",
                "image reconstruction",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single view reconstruction",
                "shape grammars",
                "urban environments",
                "parametric rules",
                "image-based-architectural costs"
            ]
        },
        "id": 230,
        "cited_by": []
    },
    {
        "title": "3D reconstruction from image collections with a single known focal length",
        "authors": [
            "Martin Bujnak",
            "Zuzana Kukelova",
            "Tomas Pajdla"
        ],
        "abstract": "In this paper we aim at reconstructing 3D scenes from images with unknown focal lengths downloaded from photosharing websites such as Flickr. First we provide a minimal solution to finding the relative pose between a completely calibrated camera and a camera with an unknown focal length given six point correspondences. We show that this problem has up to nine solutions in general and present two efficient solvers to the problem. They are based on Gr\u00f6bner basis, resp. on generalized eigenvalues, computation. We demonstrate by experiments with synthetic and real data that both solvers are correct, fast, numerically stable and work well even in some situations when the classical 6-point algorithm fails, e.g. when optical axes of the cameras are parallel or intersecting. Based on this solution we present a new efficient method for large-scale structure from motion from unordered data sets downloaded from the Internet. We show that this method can be effectively used to reconstruct 3D scenes from collection of images with very few (in principle single) images with known focal lengths.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459402",
        "reference_list": [
            {
                "year": "2007",
                "id": 46
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 8,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Cameras",
                "Layout",
                "Internet",
                "Optical distortion",
                "Large-scale systems",
                "Computer vision",
                "Pipelines",
                "Robustness",
                "Eigenvalues and eigenfunctions"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "eigenvalues and eigenfunctions",
                "image reconstruction",
                "Internet",
                "Web sites"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D image reconstruction",
                "single known focal length",
                "photo-sharing Web sites",
                "Flickr",
                "camera",
                "Grobner basis",
                "generalized eigenvalues",
                "Internet",
                "3D scenes reconstruction"
            ]
        },
        "id": 231,
        "cited_by": [
            {
                "year": "2015",
                "id": 89
            },
            {
                "year": "2015",
                "id": 231
            }
        ]
    },
    {
        "title": "Template-free monocular reconstruction of deformable surfaces",
        "authors": [
            "Aydin Varol",
            "Mathieu Salzmann",
            "Engin Tola",
            "Pascal Fua"
        ],
        "abstract": "It has recently been shown that deformable 3D surfaces could be recovered from single video streams. However, existing techniques either require a reference view in which the shape of the surface is known a priori, which often may not be available, or require tracking points over long sequences, which is hard to do. In this paper, we overcome these limitations. To this end, we establish correspondences between pairs of frames in which the shape is different and unknown. We then estimate homographies between corresponding local planar patches in both images. These yield approximate 3D reconstructions of points within each patch up to a scale factor. Since we consider overlapping patches, we can enforce them to be consistent over the whole surface. Finally, a local deformation model is used to fit a triangulated mesh to the 3D point cloud, which makes the reconstruction robust to both noise and outliers in the image data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459403",
        "reference_list": [
            {
                "year": "2007",
                "id": 191
            },
            {
                "year": "2005",
                "id": 140
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 21,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Shape",
                "Deformable models",
                "Image reconstruction",
                "Surface fitting",
                "Video sequences",
                "Streaming media",
                "Clouds",
                "Noise robustness",
                "Noise shaping"
            ]
        },
        "id": 232,
        "cited_by": [
            {
                "year": "2017",
                "id": 96
            },
            {
                "year": "2017",
                "id": 409
            },
            {
                "year": "2017",
                "id": 488
            },
            {
                "year": "2011",
                "id": 54
            }
        ]
    },
    {
        "title": "Seeing 3D objects in a single 2D image",
        "authors": [
            "Diego Rother",
            "Guillermo Sapiro"
        ],
        "abstract": "A general framework simultaneously addressing pose estimation, 2D segmentation, object recognition, and 3D reconstruction from a single image is introduced in this paper. The proposed approach partitions 3D space into voxels and estimates the voxel states that maximize a likelihood integrating two components: the object fidelity, that is, the probability that an object occupies the given voxels, here encoded as a 3D shape prior learned from 3D samples of objects in a class; and the image fidelity, meaning the probability that the given voxels would produce the input image when properly projected to the image plane. We derive a loop-less graphical model for this likelihood and propose a computationally efficient optimization algorithm that is guaranteed to produce the global likelihood maximum. Furthermore, we derive a multi-resolution implementation of this algorithm that permits to trade reconstruction and estimation accuracy for computation. The presentation of the proposed framework is complemented with experiments on real data demonstrating the accuracy of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459405",
        "reference_list": [
            {
                "year": "2001",
                "id": 108
            },
            {
                "year": "2005",
                "id": 228
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 6,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Image segmentation",
                "Graphical models",
                "Object recognition",
                "Shape",
                "Layout",
                "State estimation",
                "Pixel",
                "Inference algorithms",
                "Videos"
            ]
        },
        "id": 233,
        "cited_by": [
            {
                "year": "2011",
                "id": 3
            }
        ]
    },
    {
        "title": "Multiperspective stereo matching and volumetric reconstruction",
        "authors": [
            "Yuanyuan Ding",
            "Jingyi Yu",
            "Peter Sturm"
        ],
        "abstract": "Stereo matching and volumetric reconstruction are the most explored 3D scene recovery techniques in computer vision. Many existing approaches assume perspective input images and use the epipolar constraint to reduce the search space and improve the accuracy. In this paper we present a novel framework that uses multi-perspective cameras for stereo matching and volumetric reconstruction. Our approach first decomposes a multi-perspective camera into piecewise primitive General Linear Cameras or GLCs. A pair of GLCs in general do not satisfy the epipolar constraint. However, they still form a nearly stereo pair. We develop a new Graph-Cut-based algorithm to account for the slight vertical parallax using the GLC ray geometry. We show that the recovered pseudo disparity map conveys important depth cues analogous to perspective stereo matching. To more accurately reconstruct a 3D scene, we develop a new multi-perspective volumetric reconstruction method. We discretize the scene into voxels and apply the GLC back-projections to map the voxel onto each input multi-perspective camera. Finally, we apply the graph-cut algorithm to optimize the 3D embedded voxel graph. We demonstrate our algorithms on both synthetic and real multi-perspective cameras. Experimental results show that our methods are robust and reliable.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459406",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2001",
                "id": 52
            },
            {
                "year": "2003",
                "id": 130
            },
            {
                "year": "2001",
                "id": 104
            },
            {
                "year": "2005",
                "id": 74
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Stereo vision",
                "Image reconstruction",
                "Layout",
                "Stereo image processing",
                "Geometry",
                "Computer vision",
                "Reconstruction algorithms",
                "Laboratories",
                "Noise robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "image matching",
                "image reconstruction",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiperspective stereo matching",
                "volumetric reconstruction",
                "3D scene recovery techniques",
                "computer vision",
                "multiperspective cameras",
                "piecewise primitive general linear cameras",
                "graph-cut-based algorithm",
                "slight vertical parallax",
                "GLC ray geometry",
                "pseudo disparity map",
                "3D embedded voxel graph"
            ]
        },
        "id": 234,
        "cited_by": [
            {
                "year": "2017",
                "id": 102
            },
            {
                "year": "2013",
                "id": 295
            },
            {
                "year": "2011",
                "id": 44
            }
        ]
    },
    {
        "title": "Structure and kinematics triangulation with a rolling shutter stereo rig",
        "authors": [
            "Omar Ait-Aider",
            "Fran\u00e7ois Berry"
        ],
        "abstract": "We describe a spatio-temporal triangulation method to be used with rolling shutter cameras. We show how a single pair of rolling shutter images enables the computation of both structure and motion of rigid moving objects. Starting from a set of point correspondences in the left and right images, we introduce the velocity and shutter characteristics in the triangulation equations. This results in a non-linear error criterion whose minimization in the least square sense provides the shape and velocity parameters. Unlike previous work on rolling shutter cameras, the constraining assumption of a-priori knowledge about the object geometry is removed and a full 3D motion model is considered. The aim of this work is thus to make the use of rolling shutter cameras of a broader interest. Experimental evaluation results confirm the feasibility of the approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459408",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 8,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Kinematics",
                "Cameras",
                "CMOS image sensors",
                "Geometry",
                "Solid modeling",
                "Biosensors",
                "Nonlinear distortion",
                "Computer vision",
                "Nonlinear equations",
                "Least squares methods"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "rolling shutter stereo rig",
                "spatio-temporal triangulation method",
                "rolling shutter cameras",
                "rigid moving objects",
                "shape parameters",
                "velocity parameters",
                "nonlinear error criterion"
            ]
        },
        "id": 235,
        "cited_by": [
            {
                "year": "2013",
                "id": 118
            },
            {
                "year": "2013",
                "id": 251
            }
        ]
    },
    {
        "title": "The Self-Aware Matching Measure for stereo",
        "authors": [
            "Philippos Mordohai"
        ],
        "abstract": "We revisit stereo matching functions, a topic that is considered well understood, from a different angle. Our goal is to discover a transformation that operates on the cost or similarity measures between pixels in binocular stereo. This transformation should produce a new matching curve that results in higher matching accuracy. The desired transformation must have no additional parameters over those of the original matching function and must result in a new matching function that can be used by existing local, global and semi-local stereo algorithms without having to modify the algorithms. We propose a transformation that meets these requirements, taking advantage of information derived from matching the input images against themselves. We analyze the behavior of this transformation, which we call Self-Aware Matching Measure (SAMM), on a diverse set of experiments on data with ground truth. Our results show that the SAMM improves the performance of dense and semi-dense stereo. Moreover, as opposed to the current state of the art, it does not require distinctiveness to match pixels reliably.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459409",
        "reference_list": [
            {
                "year": "2007",
                "id": 143
            },
            {
                "year": "2007",
                "id": 162
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 3,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Cost function",
                "Stereo vision",
                "Pixel",
                "Impedance matching",
                "Area measurement",
                "Data mining",
                "Image converters",
                "Focusing",
                "Feature extraction",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo self-aware matching measure",
                "stereo matching functions",
                "binocular stereo",
                "semilocal stereo algorithms"
            ]
        },
        "id": 236,
        "cited_by": [
            {
                "year": "2017",
                "id": 549
            }
        ]
    },
    {
        "title": "Recovering the spatial layout of cluttered rooms",
        "authors": [
            "Varsha Hedau",
            "Derek Hoiem",
            "David Forsyth"
        ],
        "abstract": "In this paper, we consider the problem of recovering the spatial layout of indoor scenes from monocular images. The presence of clutter is a major problem for existing single-view 3D reconstruction algorithms, most of which rely on finding the ground-wall boundary. In most rooms, this boundary is partially or entirely occluded. We gain robustness to clutter by modeling the global room space with a parameteric 3D \u201cbox\u201d and by iteratively localizing clutter and refitting the box. To fit the box, we introduce a structured learning algorithm that chooses the set of parameters to minimize error, based on global perspective cues. On a dataset of 308 images, we demonstrate the ability of our algorithm to recover spatial layout in cluttered rooms and show several examples of estimated free space.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459411",
        "reference_list": [
            {
                "year": "2007",
                "id": 216
            }
        ],
        "citation": {
            "ieee": 133,
            "other": 72,
            "total": 205
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Robustness",
                "Parameter estimation",
                "Image segmentation",
                "Iterative algorithms",
                "Labeling",
                "Floors",
                "Computer science",
                "Reconstruction algorithms",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cluttered rooms",
                "monocular images",
                "single-view 3D reconstruction algorithms",
                "ground-wall boundary",
                "structured learning algorithm",
                "error minimization",
                "image dataset",
                "spatial layout recovery",
                "global room space model",
                "iterative clutter localization"
            ]
        },
        "id": 237,
        "cited_by": [
            {
                "year": "2017",
                "id": 511
            },
            {
                "year": "2015",
                "id": 87
            },
            {
                "year": "2015",
                "id": 117
            },
            {
                "year": "2015",
                "id": 147
            },
            {
                "year": "2015",
                "id": 188
            },
            {
                "year": "2015",
                "id": 300
            },
            {
                "year": "2013",
                "id": 44
            },
            {
                "year": "2013",
                "id": 61
            },
            {
                "year": "2013",
                "id": 158
            },
            {
                "year": "2013",
                "id": 233
            },
            {
                "year": "2013",
                "id": 267
            },
            {
                "year": "2013",
                "id": 381
            },
            {
                "year": "2013",
                "id": 423
            },
            {
                "year": "2011",
                "id": 15
            }
        ]
    },
    {
        "title": "3D open-surface shape correspondence for statistical shape modeling: Identifying topologically consistent landmarks",
        "authors": [
            "Pahal Dalal",
            "Lili Ju",
            "Michael McLaughlin",
            "Xiangrong Zhou",
            "Hiroshi Fujita",
            "Song Wang"
        ],
        "abstract": "Shape correspondence, which aims at accurately identifying corresponding landmarks from a given population of shape instances, is a very challenging step in constructing a statistical shape model such as the Point Distribution Model. The state-of-the-art methods such as MDL and SPHARM are primarily focused on closed-surface shape correspondence. In this paper we develop a novel method aimed at identifying accurately corresponding landmarks on 3D open-surfaces with a closed boundary. In particular, we enforce explicit topology consistency on the identified landmarks to ensure that they form a simple, consistent triangle mesh to more accurately model the correspondence of the underlying continuous shape instances. The proposed method also ensures the correspondence of the boundary of the open surfaces. For our experiments, we test the proposed method by constructing a statistical shape model of the human diaphragm from 26 shape instances.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459412",
        "reference_list": [
            {
                "year": "2003",
                "id": 121
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 4,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Topology",
                "Mathematical model",
                "Humans",
                "Biomedical imaging",
                "Medical diagnostic imaging",
                "Conformal mapping",
                "Computer science",
                "Mathematics",
                "Testing"
            ]
        },
        "id": 238,
        "cited_by": []
    },
    {
        "title": "Diagram techniques for multiple view geometry",
        "authors": [
            "Alberto Ruiz",
            "Pedro E. Lopez-de-Teruel"
        ],
        "abstract": "Multilinear algebra is a powerful theoretical tool for visual geometry, but widespread usage of traditional typographical notation often hides its conceptual elegance and simplicity. As demonstrated in other scientific fields, we can take full advantage of multilinear methods using graphical notation. In this paper we adapt standard tensor diagrammatic techniques to the specific requirements of visual geometry, so that geometric relations are represented by circuits which can be manipulated using simple rules. The advantages of this approach are illustrated in several constructions, including straightforward derivations of the standard multiview relations (Fundamental Matrix, Trifocal and Quadrifocal Tensors), and nearly mechanical procedures for camera extraction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459414",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Tensile stress",
                "Vectors",
                "Cameras",
                "Shape",
                "Algebra",
                "Visualization",
                "Electronic circuits",
                "Logic gates",
                "Logic circuits"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "feature extraction",
                "graph theory",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multilinear algebra",
                "visual geometry",
                "conceptual elegance",
                "graphical notation",
                "tensor diagrammatic techniques",
                "camera extraction"
            ]
        },
        "id": 239,
        "cited_by": []
    },
    {
        "title": "Modeling 3D human poses from uncalibrated monocular images",
        "authors": [
            "Xiaolin K. Wei",
            "Jinxiang Chai"
        ],
        "abstract": "This paper introduces an efficient algorithm that reconstructs 3D human poses as well as camera parameters from a small number of 2D point correspondences obtained from uncalibrated monocular images. This problem is challenging because 2D image constraints (e.g. 2D point correspondences) are often not sufficient to determine 3D poses of an articulated object. The key idea of this paper is to identify a set of new constraints and use them to eliminate the ambiguity of 3D pose reconstruction. We also develop an optimization process to simultaneously reconstruct both human poses and camera parameters from various forms of reconstruction constraints. We demonstrate the power and effectiveness of our system by evaluating the performance of the algorithm on both real and synthetic data. We show the algorithm can accurately reconstruct 3D poses and camera parameters from a wide variety of real images, including internet photos and key frames extracted from monocular video sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459415",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 15,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Image reconstruction",
                "Cameras",
                "Joints",
                "Constraint optimization",
                "Bones",
                "Image segmentation",
                "Internet",
                "Computer vision",
                "Data mining"
            ]
        },
        "id": 240,
        "cited_by": [
            {
                "year": "2017",
                "id": 278
            },
            {
                "year": "2013",
                "id": 235
            },
            {
                "year": "2011",
                "id": 14
            },
            {
                "year": "2011",
                "id": 25
            }
        ]
    },
    {
        "title": "Piecewise planar stereo for image-based rendering",
        "authors": [
            "Sudipta N. Sinha",
            "Drew Steedly",
            "Richard Szeliski"
        ],
        "abstract": "We present a novel multi-view stereo method designed for image-based rendering that generates piecewise planar depth maps from an unordered collection of photographs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459417",
        "reference_list": [
            {
                "year": "2007",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 66,
            "other": 48,
            "total": 114
        },
        "keywords": {
            "IEEE Keywords": [
                "Rendering (computer graphics)",
                "Layout",
                "Surface reconstruction",
                "Image reconstruction",
                "Interpolation",
                "Cameras",
                "Robustness",
                "Markov random fields",
                "Surface texture",
                "Stereo vision"
            ]
        },
        "id": 241,
        "cited_by": [
            {
                "year": "2015",
                "id": 147
            },
            {
                "year": "2015",
                "id": 229
            },
            {
                "year": "2011",
                "id": 111
            },
            {
                "year": "2011",
                "id": 235
            }
        ]
    },
    {
        "title": "Radiometric compensation using stratified inverses",
        "authors": [
            "Tian-Tsong Ng",
            "Ramanpreet S. Pahwa",
            "Jiamin Bai",
            "Tony Q.S. Quek",
            "Kar-Han Tan"
        ],
        "abstract": "Through radiometric compensation, a projector-camera system can project a desired image onto a non-flat and non-white surface. This can be achieved by computing the inverse light transport of a scene. A light transport matrix is in general large and on the order of 10 6 \u00d7 10 6 elements. Therefore, computing the inverse light transport matrix is computationally and memory intensive. Two prior methods were proposed to simplify matrix inversion by ignoring scene inter-reflection between individual or clusters of camera pixels. However, compromising scene inter-reflection in spatial domain introduces spatial artifacts and how to systematically adjust the compensation quality is not obvious. In this work, we show how scene inter-reflection can be systematically approximated by stratifying the light transport of a scene. The stratified light transport enables a similar stratification in the inverse light transport. We can show that the stratified inverse light transport converges to the true inverse. For radiometric compensation, the set of stratified inverse light transport provides a systematic way of quantifying the tradeoff between computational efficiency and accuracy. The framework of stratified matrix inversion is general and can have other applications, especially for applications that involve large-size sparse matrices.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459418",
        "reference_list": [
            {
                "year": "2005",
                "id": 188
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Radiometry",
                "Layout",
                "Sparse matrices",
                "Cameras",
                "Computational efficiency",
                "Light sources",
                "Lighting control",
                "Reflectivity",
                "Cities and towns",
                "Image restoration"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "display instrumentation",
                "matrix inversion",
                "optical design techniques",
                "optical projectors",
                "radiometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "radiometric compensation",
                "stratified inverse",
                "projector-camera system",
                "inverse light transport",
                "light transport matrix",
                "matrix inversion",
                "camera pixel",
                "scene inter-reflection"
            ]
        },
        "id": 242,
        "cited_by": []
    },
    {
        "title": "Computation complexity of branch-and-bound model selection",
        "authors": [
            "Ninad Thakoor",
            "Venkat Devarajan",
            "Jean Gao"
        ],
        "abstract": "Segmentation problems are one of the most important areas of research in computer vision. While segmentation problems are generally solved with clustering paradigms, they formulate the problem as recursive. Additionally, most approaches need the number of clusters to be known beforehand. This requirement is unreasonable for majority of the computer vision problems. This paper analyzes the model selection perspective which can overcome these limitations. Under this framework multiple hypotheses for cluster centers are generated using spatially coherent sampling. An optimal subset of these hypotheses is selected according to a model selection criterion. The selection can be carried out with a branch-and-bound procedure. The worst case complexity of any branch-and-bound algorithm is exponential. However, the average complexity of the algorithm is significantly lower. In this paper, we develop a framework for analysis of average complexity of the algorithm from the statistics of model selection costs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459420",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Spatial coherence",
                "Motion segmentation",
                "Image segmentation",
                "Clustering algorithms",
                "Computational complexity",
                "Sampling methods",
                "Cost function",
                "Computer science",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "image segmentation",
                "pattern clustering",
                "statistics",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computation complexity",
                "branch-and-bound model selection",
                "image segmentation",
                "computer vision",
                "clustering paradigms",
                "statistics"
            ]
        },
        "id": 243,
        "cited_by": []
    },
    {
        "title": "A novel approach to expression recognition from non-frontal face images",
        "authors": [
            "Wenming Zheng",
            "Hao Tang",
            "Zhouchen Lin",
            "Thomas S. Huang"
        ],
        "abstract": "Non-frontal view facial expression recognition is important in many scenarios where the frontal view face images may not be available. However, few work on this issue has been done in the past several years because of its technical challenges and the lack of appropriate databases. Recently, a 3D facial expression database (BU-3DFE database) is collected by Yin et al. [10] and has attracted some researchers to study this issue. Based on the BU-3DFE database, in this paper we propose a novel approach to expression recognition from non-frontal view facial images. The novelty of the proposed method lies in recognizing the multi-view expressions under the unified Bayes theoretical framework, where the recognition problem can be formulated as an optimization problem of minimizing an upper bound of Bayes error. We also propose a close-form solution method based on the power iteration approach and rank-one update (ROU) technique to find the optimal solutions of the proposed method. Extensive experiments on BU-3DFE database with 100 subjects and 5 yaw rotation view angles demonstrate the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459421",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Image recognition",
                "Image databases",
                "Face detection",
                "Computer vision",
                "Testing",
                "Upper bound",
                "Pattern recognition",
                "Performance evaluation",
                "Asia"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "face recognition",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonfrontal face images",
                "nonfrontal view facial expression recognition",
                "3D facial expression database",
                "BU-3DFE database",
                "multiview expressions",
                "unified Bayes theoretical framework",
                "recognition problem",
                "optimization problem",
                "upper bound",
                "Bayes error",
                "close-form solution",
                "power iteration approach",
                "rank-one update technique"
            ]
        },
        "id": 244,
        "cited_by": []
    },
    {
        "title": "Mode-detection via median-shift",
        "authors": [
            "Lior Shapira",
            "Shai Avidan",
            "Ariel Shamir"
        ],
        "abstract": "Median-shift is a mode seeking algorithm that relies on computing the median of local neighborhoods, instead of the mean. We further combine median-shift with Locality Sensitive Hashing (LSH) and show that the combined algorithm is suitable for clustering large scale, high dimensional data sets. In particular, we propose a new mode detection step that greatly accelerates performance. In the past, LSH was used in conjunction with mean shift only to accelerate nearest neighbor queries. Here we show that we can analyze the density of the LSH bins to quickly detect potential mode candidates and use only them to initialize the median-shift procedure. We use the median, instead of the mean (or its discrete counterpart - the medoid) because the median is more robust and because the median of a set is a point in the set. A median is well defined for scalars but there is no single agreed upon extension of the median to high dimensional data. We adopt a particular extension, known as the Tukey median, and show that it can be computed efficiently using random projections of the high dimensional data onto 1D lines, just like LSH, leading to a tightly integrated and efficient algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459423",
        "reference_list": [
            {
                "year": "2007",
                "id": 137
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 7,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Clustering algorithms",
                "Acceleration",
                "Nearest neighbor searches",
                "Large-scale systems",
                "Robustness",
                "Computer vision",
                "Application software",
                "Shape",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "object detection",
                "pattern clustering",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mode-detection",
                "median-shift procedure",
                "mode seeking algorithm",
                "locality sensitive hashing",
                "Tukey median",
                "random projection"
            ]
        },
        "id": 245,
        "cited_by": []
    },
    {
        "title": "The dimensionality of scene appearance",
        "authors": [
            "Rahul Garg",
            "Hao Du",
            "Steven M. Seitz",
            "Noah Snavely"
        ],
        "abstract": "Low-rank approximation of image collections (e.g., via PCA) is a popular tool in many areas of computer vision. Yet, surprisingly little is known justifying the observation that images of an object or scene tend to be low dimensional, beyond the special case of Lambertian scenes. This paper considers the question of how many basis images are needed to span the space of images of a scene under real-world lighting and viewing conditions, allowing for general BRDFs. We establish new theoretical upper bounds on the number of basis images necessary to represent a wide variety of scenes under very general conditions, and perform empirical studies to justify the assumptions. We then demonstrate a number of novel applications of linear models for scene appearance for Internet photo collections. These applications include, image reconstruction, occluder-removal, and expanding field of view.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459424",
        "reference_list": [
            {
                "year": "2007",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 6,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Lighting",
                "Upper bound",
                "Principal component analysis",
                "Computer vision",
                "Internet",
                "Matrix decomposition",
                "Rendering (computer graphics)",
                "Image reconstruction",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene appearance",
                "low-rank approximation",
                "image collections",
                "PCA",
                "computer vision",
                "Lambertian scenes",
                "principle component analysis",
                "Internet photo collections",
                "image reconstruction",
                "occluder removal"
            ]
        },
        "id": 246,
        "cited_by": []
    },
    {
        "title": "Weakly supervised discriminative localization and classification: a joint learning process",
        "authors": [
            "Minh Hoai Nguyen",
            "Lorenzo Torresani",
            "Fernando de la Torre",
            "Carsten Rother"
        ],
        "abstract": "Visual categorization problems, such as object classification or action recognition, are increasingly often approached using a detection strategy: a classifier function is first applied to candidate subwindows of the image or the video, and then the maximum classifier score is used for class decision. Traditionally, the subwindow classifiers are trained on a large collection of examples manually annotated with masks or bounding boxes. The reliance on time-consuming human labeling effectively limits the application of these methods to problems involving very few categories. Furthermore, the human selection of the masks introduces arbitrary biases (e.g. in terms of window size and location) which may be suboptimal for classification. In this paper we propose a novel method for learning a discriminative subwindow classifier from examples annotated with binary labels indicating the presence of an object or action of interest, but not its location. During training, our approach simultaneously localizes the instances of the positive class and learns a subwindow SVM to recognize them. We extend our method to classification of time series by presenting an algorithm that localizes the most discriminative set of temporal segments in the signal. We evaluate our approach on several datasets for object and action recognition and show that it achieves results similar and in many cases superior to those obtained with full supervision.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459426",
        "reference_list": [
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 52,
            "other": 11,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Object detection",
                "Humans",
                "Image recognition",
                "Support vector machines",
                "Support vector machine classification",
                "Robustness",
                "Computer vision",
                "Computational complexity",
                "Animals"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weakly supervised discriminative localization",
                "joint learning process",
                "visual categorization problems",
                "action recognition",
                "object classification",
                "image classification",
                "video classifier function",
                "human labeling",
                "discriminative subwindow classifier",
                "subwindow SVM",
                "support vector machine",
                "time series classification",
                "object recognition"
            ]
        },
        "id": 247,
        "cited_by": [
            {
                "year": "2015",
                "id": 165
            },
            {
                "year": "2015",
                "id": 403
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2011",
                "id": 43
            }
        ]
    },
    {
        "title": "Selection and context for action recognition",
        "authors": [
            "Dong Han",
            "Liefeng Bo",
            "Cristian Sminchisescu"
        ],
        "abstract": "Recognizing human action in non-instrumented video is a challenging task not only because of the variability produced by general scene factors like illumination, background, occlusion or intra-class variability, but also because of subtle behavioral patterns among interacting people or between people and objects in images. To improve recognition, a system may need to use not only low-level spatio-temporal video correlations but also relational descriptors between people and objects in the scene. In this paper we present contextual scene descriptors and Bayesian multiple kernel learning methods for recognizing human action in complex non-instrumented video. Our contribution is threefold: (1) we introduce bag-of-detector scene descriptors that encode presence/absence and structural relations between object parts; (2) we derive a novel Bayesian classification method based on Gaussian processes with multiple kernel covariance functions (MKGPC), in order to automatically select and weight multiple features, both low-level and high-level, out of a large collection, in a principled way, and (3) perform large scale evaluation using a variety of features on the KTH and a recently introduced, challenging, Hollywood movie dataset. On the KTH dataset, we obtain 94.1% accuracy, the best result reported to date. On the Hollywood dataset we obtain promising results in several action classes using fewer descriptors and about 9.1% improvement in a previous benchmark test.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459427",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 225
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2007",
                "id": 84
            },
            {
                "year": "2007",
                "id": 26
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 1,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Humans",
                "Bayesian methods",
                "Kernel",
                "Pattern recognition",
                "Image recognition",
                "Lighting",
                "Learning systems",
                "Gaussian processes",
                "Performance evaluation"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "covariance analysis",
                "Gaussian processes",
                "image classification",
                "image coding",
                "image motion analysis",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human action recognition",
                "noninstrumented video",
                "low-level spatio-temporal video correlations",
                "relational descriptors",
                "contextual scene descriptors",
                "Bayesian multiple kernel learning method",
                "bag-of-detector scene descriptors",
                "presence encoding",
                "absence encoding",
                "structural relation encoding",
                "Bayesian classification method",
                "Gaussian process",
                "multiple kernel covariance function",
                "Hollywood movie dataset",
                "KTH dataset"
            ]
        },
        "id": 248,
        "cited_by": [
            {
                "year": "2017",
                "id": 466
            },
            {
                "year": "2015",
                "id": 366
            },
            {
                "year": "2011",
                "id": 225
            }
        ]
    },
    {
        "title": "Time series prediction by chaotic modeling of nonlinear dynamical systems",
        "authors": [
            "Arslan Basharat",
            "Mubarak Shah"
        ],
        "abstract": "We use concepts from chaos theory in order to model nonlinear dynamical systems that exhibit deterministic behavior. Observed time series from such a system can be embedded into a higher dimensional phase space without the knowledge of an exact model of the underlying dynamics. Such an embedding warps the observed data to a strange attractor, in the phase space, which provides precise information about the dynamics involved. We extract this information from the strange attractor and utilize it to predict future observations. Given an initial condition, the predictions in the phase space are computed through kernel regression. This approach has the advantage of modeling dynamics without making any assumptions about the exact form (linear, polynomial, radial basis, etc.) of the mapping function. The predicted points are then warped back to the observed time series. We demonstrate the utility of these predictions for human action synthesis, and dynamic texture synthesis. Our main contributions are: multivariate phase space reconstruction for human actions and dynamic textures, a deterministic approach to model dynamics in contrast to the popular noise-driven approaches for dynamic textures, and video synthesis from kernel regression in the phase space. Experimental results provide qualitative and quantitative analysis of our approach on standard data sets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459429",
        "reference_list": [
            {
                "year": "2007",
                "id": 206
            },
            {
                "year": "2007",
                "id": 254
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Chaos",
                "Predictive models",
                "Nonlinear dynamical systems",
                "Humans",
                "Kernel",
                "Polynomials",
                "Application software",
                "Computer vision",
                "Biological system modeling",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "chaos",
                "nonlinear dynamical systems",
                "prediction theory",
                "regression analysis",
                "time series",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "time series prediction",
                "chaotic modeling",
                "nonlinear dynamical systems",
                "chaos theory",
                "future observation prediction",
                "kernel regression",
                "human action synthesis",
                "dynamic texture synthesis",
                "multivariate phase space reconstruction",
                "dynamic textures",
                "video synthesis"
            ]
        },
        "id": 249,
        "cited_by": []
    },
    {
        "title": "Least-squares congealing for large numbers of images",
        "authors": [
            "Mark Cox",
            "Sridha Sridharan",
            "Simon Lucey",
            "Jeffrey Cohn"
        ],
        "abstract": "In this paper we pursue the task of aligning an ensemble of images in an unsupervised manner. This task has been commonly referred to as \u201ccongealing\u201d in literature. A form of congealing, using a least-squares criteria, has been recently demonstrated to have desirable properties over conventional congealing. Least-squares congealing can be viewed as an extension of the Lucas & Kanade (LK) image alignment algorithm. It is well understood that the alignment performance for the LK algorithm, when aligning a single image with another, is theoretically and empirically equivalent for additive and compositional warps. In this paper we: (i) demonstrate that this equivalence does not hold for the extended case of congealing, (ii) characterize the inherent drawbacks associated with least-squares congealing when dealing with large numbers of images, and (iii) propose a novel method for circumventing these limitations through the application of an inverse-compositional strategy that maintains the attractive properties of the original method while being able to handle very large numbers of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459430",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Cost function",
                "Entropy",
                "Convergence",
                "Iterative algorithms",
                "Australia",
                "Robots",
                "Additives",
                "Object recognition",
                "Object detection",
                "Employment"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "inverse problems",
                "least squares approximations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "least squares congealing",
                "Lucas & Kanade image alignment algorithm",
                "inverse compositional strategy"
            ]
        },
        "id": 250,
        "cited_by": [
            {
                "year": "2015",
                "id": 449
            }
        ]
    },
    {
        "title": "Landmark classification in large-scale image collections",
        "authors": [
            "Yunpeng Li",
            "David J. Crandall",
            "Daniel P. Huttenlocher"
        ],
        "abstract": "With the rise of photo-sharing websites such as Facebook and Flickr has come dramatic growth in the number of photographs online. Recent research in object recognition has used such sites as a source of image data, but the test images have been selected and labeled by hand, yielding relatively small validation sets. In this paper we study image classification on a much larger dataset of 30 million images, including nearly 2 million of which have been labeled into one of 500 categories. The dataset and categories are formed automatically from geotagged photos from Flickr, by looking for peaks in the spatial geotag distribution corresponding to frequently-photographed landmarks. We learn models for these landmarks with a multiclass support vector machine, using vector-quantized interest point descriptors as features. We also explore the non-visual information available on modern photo-sharing sites, showing that using textual tags and temporal constraints leads to significant improvements in classification rate. We find that in some cases image features alone yield comparable classification accuracy to using text tags as well as to the performance of human observers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459432",
        "reference_list": [
            {
                "year": "2007",
                "id": 259
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 6,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Large-scale systems",
                "Object recognition",
                "Image classification",
                "Internet",
                "Testing",
                "Support vector machines",
                "Support vector machine classification",
                "Computer vision",
                "Computer science",
                "Facebook"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object recognition",
                "support vector machines",
                "Web sites"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "landmark classification",
                "large-scale image collections",
                "photo-sharing Web sites",
                "object recognition",
                "image classification",
                "multiclass support vector machine",
                "vector-quantized interest point descriptors"
            ]
        },
        "id": 251,
        "cited_by": [
            {
                "year": "2015",
                "id": 167
            },
            {
                "year": "2015",
                "id": 516
            }
        ]
    },
    {
        "title": "Detection of human actions from a single example",
        "authors": [
            "Hae Jong Seo",
            "Peyman Milanfar"
        ],
        "abstract": "We present an algorithm for detecting human actions based upon a single given video example of such actions. The proposed method is unsupervised, does not require learning, segmentation, or motion estimation. The novel features employed in our method are based on space-time locally adaptive regression kernels. Our method is based on the dense computation of so-called space-time local regression kernels (i.e. local descriptors) from a query video, which measure the likeness of a voxel to its spatio-temporal surroundings. Salient features are then extracted from these descriptors using principal components analysis (PCA). These are efficiently compared against analogous features from the target video using a matrix generalization of the cosine similarity measure. The algorithm yields a scalar resemblance volume; each voxel indicating the like-lihood of similarity between the query video and all cubes in the target video. By employing non-parametric significance tests and non-maxima suppression, we accurately detect the presence and location of actions similar to the given query video. High performance is demonstrated on a challenging set of action data indicating successful detection of multiple complex actions even in the presence of fast motions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459433",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 1,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Kernel",
                "Motion estimation",
                "Principal component analysis",
                "Motion detection",
                "Computer vision",
                "Videoconference",
                "Spatiotemporal phenomena",
                "Feature extraction",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "object detection",
                "principal component analysis",
                "query processing",
                "regression analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human action detection",
                "motion estimation",
                "space-time locally adaptive regression kernels",
                "video query",
                "salient feature extraction",
                "principal component analysis",
                "PCA",
                "matrix generalization",
                "cosine similarity measure",
                "scalar resemblance volume",
                "nonmaxima suppression"
            ]
        },
        "id": 252,
        "cited_by": [
            {
                "year": "2011",
                "id": 330
            }
        ]
    },
    {
        "title": "Hierarchical Gaussianization for image classification",
        "authors": [
            "Xi Zhou",
            "Na Cui",
            "Zhen Li",
            "Feng Liang",
            "Thomas S. Huang"
        ],
        "abstract": "In this paper, we propose a new image representation to capture both the appearance and spatial information for image classification applications. First, we model the feature vectors, from the whole corpus, from each image and at each individual patch, in a Bayesian hierarchical framework using mixtures of Gaussians. After such a hierarchical Gaussianization, each image is represented by a Gaussian mixture model (GMM) for its appearance, and several Gaussian maps for its spatial layout. Then we extract the appearance information from the GMM parameters, and the spatial information from global and local statistics over Gaussian maps. Finally, we employ a supervised dimension reduction technique called DAP (discriminant attribute projection) to remove noise directions and to further enhance the discriminating power of our representation. We justify that the traditional histogram representation and the spatial pyramid matching are special cases of our hierarchical Gaussianization. We compare our new representation with other approaches in scene classification, object recognition and face recognition, and our performance ranks among the top in all three tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459435",
        "reference_list": [
            {
                "year": "2005",
                "id": 235
            },
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2007",
                "id": 198
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 26,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Gaussian processes",
                "Image classification",
                "Image representation",
                "Bayesian methods",
                "Data mining",
                "Statistics",
                "Digital audio players",
                "Noise reduction",
                "Histograms",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "Gaussian processes",
                "image classification",
                "image representation",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical Gaussianization",
                "image representation",
                "spatial information",
                "image classification applications",
                "feature vectors",
                "Bayesian hierarchical framework",
                "Gaussian mixture model",
                "Gaussian maps",
                "supervised dimension reduction technique",
                "discriminant attribute projection",
                "traditional histogram representation",
                "spatial pyramid matching",
                "object recognition",
                "face recognition"
            ]
        },
        "id": 253,
        "cited_by": [
            {
                "year": "2011",
                "id": 188
            },
            {
                "year": "2011",
                "id": 231
            }
        ]
    },
    {
        "title": "Context by region ancestry",
        "authors": [
            "Joseph J. Lim",
            "Pablo Arbel\u00e1ez",
            "Pablo Arbel\u00e1ez",
            "Chunhui Gu",
            "Jitendra Malik"
        ],
        "abstract": "In this paper, we introduce a new approach for modeling visual context. For this purpose, we consider the leaves of a hierarchical segmentation tree as elementary units. Each leaf is described by features of its ancestral set, the regions on the path linking the leaf to the root. We construct region trees by using a high-performance segmentation method. We then learn the importance of different descriptors (e.g. color, texture, shape) of the ancestors for classification. We report competitive results on the MSRC segmentation dataset and the MIT scene dataset, showing that region ancestry efficiently encodes information about discriminative parts, objects and scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459436",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2005",
                "id": 168
            },
            {
                "year": "2007",
                "id": 145
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 8,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Statistical distributions",
                "Pixel",
                "Image denoising",
                "Markov random fields",
                "Application software",
                "Computer science",
                "Educational institutions",
                "Probability distribution",
                "Random number generation"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image segmentation",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "region ancestry",
                "visual context",
                "hierarchical segmentation tree",
                "ancestral set",
                "region trees",
                "high-performance segmentation method",
                "MSRC segmentation dataset",
                "MIT scene dataset"
            ]
        },
        "id": 254,
        "cited_by": [
            {
                "year": "2011",
                "id": 64
            },
            {
                "year": "2011",
                "id": 272
            }
        ]
    },
    {
        "title": "A study on automatic age estimation using a large database",
        "authors": [
            "Guodong Guo",
            "Guowang Mu",
            "Yun Fu",
            "Charles Dyer",
            "Thomas Huang"
        ],
        "abstract": "In this paper we study some problems related to human age estimation using a large database. First, we study the influence of gender on age estimation based on face representations that combine biologically-inspired features with manifold learning techniques. Second, we study age estimation using smaller gender and age groups rather than on all ages. Significant error reductions are observed in both cases. Based on these results, we designed three frameworks for automatic age estimation that exhibit high performance. Unlike previous methods that require manual separation of males and females prior to age estimation, our work is the first to estimate age automatically on a large database. Furthermore, a data fusion approach is proposed using one of the frameworks, which gives an age estimation error more than 40% smaller than previous methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459438",
        "reference_list": [
            {
                "year": "2007",
                "id": 210
            },
            {
                "year": "2005",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 5,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Image databases",
                "Humans",
                "Aging",
                "Biological system modeling",
                "Brain modeling",
                "Computer vision",
                "Image representation",
                "Face recognition",
                "Spatial databases",
                "Estimation error"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image fusion",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic age estimation",
                "large database",
                "face representations",
                "biologically-inspired features",
                "manifold learning techniques",
                "data fusion approach",
                "gender"
            ]
        },
        "id": 255,
        "cited_by": [
            {
                "year": "2011",
                "id": 30
            }
        ]
    },
    {
        "title": "A multi-sample, multi-tree approach to bag-of-words image representation for image retrieval",
        "authors": [
            "Zhong Wu",
            "Qifa Ke",
            "Jian Sun",
            "Heung-Yeung Shum"
        ],
        "abstract": "The state-of-the-art content based image retrieval systems has been significantly advanced by the introduction of SIFT features and the bag-of-words image representation. Converting an image into a bag-of-words, however, involves three non-trivial steps: feature detection, feature description, and feature quantization. At each of these steps, there is a significant amount of information lost, and the resulted visual words are often not discriminative enough for large scale image retrieval applications. In this paper, we propose a novel multi-sample multi-tree approach to computing the visual word codebook. By encoding more information of the original image feature, our approach generates a much more discriminative visual word codebook that is also efficient in terms of both computation and space consumption, without losing the original repeatability of the visual features. We evaluate our approach using both a ground-truth data set and a real-world large scale image database. Our results show that a significant improvement in both precision and recall can be achieved by using the codebook derived from our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459439",
        "reference_list": [
            {
                "year": "2007",
                "id": 185
            },
            {
                "year": "2007",
                "id": 67
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2007",
                "id": 198
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image representation",
                "Image retrieval",
                "Computer vision",
                "Large-scale systems",
                "Content based retrieval",
                "Image converters",
                "Quantization",
                "Information retrieval",
                "Image coding",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "feature extraction",
                "image representation",
                "image retrieval",
                "quantisation (signal)",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bag-of-words image representation",
                "content based image retrieval systems",
                "SIFT features",
                "feature detection",
                "feature description",
                "feature quantization",
                "discriminative visual word codebook",
                "real-world large scale image database",
                "multisample multitree approach"
            ]
        },
        "id": 256,
        "cited_by": []
    },
    {
        "title": "Discriminative generalized hough transform for object dectection",
        "authors": [
            "Ryuzo Okada"
        ],
        "abstract": "This paper present a part-based approach for detecting objects with large variation of appearance. We extract local image patches as local features both from the object and from the background in training images to learn an object part model discriminatively. Our object part model discriminates the local features whether they are an object part or not. Based on the discrimination results, each local feature casts probabilistic votes for the object location and size which are learned from the training images. Our object part model also requires regression performance for predicting the object location and size through the voting procedure. We build such an object part model with an ensemble of randomized trees trained by splitting each tree node so as to reduce the entropy of class label distribution and the variance of object location and size. Experimental results on hand detection with large pose variation show that our approach outperforms conventional generalized Hough transform. We verified the performance on a public dataset of side-view cars.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459441",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 19,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Voting",
                "Object detection",
                "Predictive models",
                "Entropy"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Hough transforms",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative generalized Hough transform",
                "object detection",
                "local image patches",
                "feature extraction",
                "regression performance",
                "tree node",
                "randomized trees",
                "label distribution",
                "hand detection",
                "pose variation"
            ]
        },
        "id": 257,
        "cited_by": [
            {
                "year": "2015",
                "id": 363
            },
            {
                "year": "2011",
                "id": 10
            }
        ]
    },
    {
        "title": "Untangling fibers by quotient appearance manifold mapping for grayscale shape classification",
        "authors": [
            "Yoshihisa Shinagawa",
            "Yuping Lin"
        ],
        "abstract": "Appearance manifolds have been one of the most powerful methods for object recognition. However, they could not be used for grayscale shape classification, particularly in three dimensions, such as classifying medical lesion volumes or galaxy images. The main cause of the difficulty is that the appearance manifolds of shape classes have entangled fibers in their embedded Euclidean space. This paper proposes a novel appearance-based method called the quotient appearance manifold mapping to untangle the fibers of the appearance manifolds. First, the quotient manifold is constructed to untangle the fiber bundles of appearance manifolds. The mapping from each point of the manifold to the quotient submanifold is then proposed to classify grayscale shapes. We show the effectiveness in grayscale 3D shape recognition using medical images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459442",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Gray-scale",
                "Shape",
                "Biomedical imaging",
                "Medical diagnostic imaging",
                "Object recognition",
                "Lesions",
                "Wavelet analysis",
                "Manifolds",
                "Video sequences",
                "Medical services"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "untangling fibers",
                "quotient appearance manifold mapping",
                "grayscale 3D shape classification",
                "object recognition",
                "medical lesion volumes classification",
                "galaxy images",
                "appearance manifolds",
                "entangled fibers",
                "embedded Euclidean space",
                "medical images"
            ]
        },
        "id": 258,
        "cited_by": []
    },
    {
        "title": "Building recognition using sketch-based representations and spectral graph matching",
        "authors": [
            "Yu-Chia Chung",
            "Tony X. Han",
            "Zhihai He"
        ],
        "abstract": "In this work, we address the problem of building recognition across two camera views with large changes in scales and viewpoints. The main idea is to construct a semantically rich sketch-based representation for buildings which is invariant under large scale and perspective changes. After multi-scale maximally stable extremal regions (MSER) detection, the proposed approach finds repeated structural components of buildings, such as window, doors, and facades, and extracts semantically rich features, which are organized into a sketch-based representation of buildings. These descriptors are then clustered in association with different planes of the building and matched across video frames using spectral graph analysis. Our experiments demonstrate that the proposed approach outperforms SIFT-based matching schemes, especially for images with large viewpoint changes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459444",
        "reference_list": [
            {
                "year": "2007",
                "id": 185
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 1,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Buildings",
                "Cameras",
                "Robot vision systems",
                "Layout",
                "Surveillance",
                "Windows",
                "Image edge detection",
                "Large-scale systems",
                "Navigation",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "feature extraction",
                "image matching",
                "object recognition",
                "spectral analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "building recognition",
                "spectral graph matching",
                "sketch-based representation",
                "spectral graph analysis",
                "multiscale maximally stable extremal regions detection",
                "video frame matching",
                "feature extraction"
            ]
        },
        "id": 259,
        "cited_by": []
    },
    {
        "title": "Hierarchical learning for tubular structure parsing in medical imaging: A study on coronary arteries using 3D CT Angiography",
        "authors": [
            "Le Lu",
            "Jinbo Bi",
            "Shipeng Yu",
            "Zhigang Peng",
            "Arun Krishnan",
            "Xiang Sean Zhou"
        ],
        "abstract": "Automatic coronary artery centerline extraction from 3D CT Angiography (CTA) has significant clinical importance for diagnosis of atherosclerotic heart disease. The focus of past literature is dominated by segmenting the complete coronary artery system as trees by computer. Though the labeling of different vessel branches (defined by their medical semantics) is much needed clinically, this task has been performed manually. In this paper, we propose a hierarchical machine learning approach to tackle the problem of tubular structure parsing in medical imaging. It has a progressive three-tiered classification process at volumetric voxel level, vessel segment level, and inter-segment level. Generative models are employed to project from low-level, ambiguous data to class-conditional probabilities; and discriminative classifiers are trained on the upper-level structural patterns of probabilities to label and parse the vessel segments. Our method is validated by experiments of detecting and segmenting clinically defined coronary arteries, from the initial noisy vessel segment networks generated by low-level heuristics-based tracing algorithms. The proposed framework is also generically applicable to other tubular structure parsing tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459445",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical imaging",
                "Arteries",
                "Computed tomography",
                "Angiography",
                "Image segmentation",
                "Cardiac disease",
                "Focusing",
                "Labeling",
                "Medical diagnostic imaging",
                "Machine learning"
            ],
            "INSPEC: Controlled Indexing": [
                "angiocardiography",
                "blood vessels",
                "computerised tomography",
                "diseases",
                "image classification",
                "image segmentation",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tubular structure parsing",
                "medical imaging",
                "coronary arteries",
                "3-D CT angiography",
                "hierarchical machine learning",
                "volumetric voxel level",
                "vessel segment level",
                "disease"
            ]
        },
        "id": 260,
        "cited_by": []
    },
    {
        "title": "Image annotation using multi-label correlated Green's function",
        "authors": [
            "Hua Wang",
            "Heng Huang",
            "Chris Ding"
        ],
        "abstract": "Image annotation has been an active research topic in the recent years due to its potentially large impact on both image understanding and web/database image search. In this paper, we target at solving the automatic image annotation problem in a novel semi-supervised learning framework. A novel multi-label correlated Green's function approach is proposed to annotate images over a graph. The correlations among labels are integrated into the objective function which improves the performance significantly. We also propose a new adaptive decision boundary method for multi-label assignment to deal with the difficulty of label assignment in most of the existing rank-based multi-label classification algorithms. Instead of setting the threshold heuristically or by experience, our method principally compute it upon the prior knowledge in the training data. We perform our methods on three commonly used image annotation testing data sets. Experimental results show significant improvements on classification performance over four other state-of-the-art methods. As a general semi-supervised learning framework, other local feature based image annotation methods could be easily incorporated into our framework to improve the performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459447",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 13,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Green's function methods",
                "Semisupervised learning",
                "Image retrieval",
                "Training data",
                "Information retrieval",
                "Content based retrieval",
                "Computer vision",
                "Pairwise error probability",
                "Computer science",
                "Data engineering"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "Green's function methods",
                "image classification",
                "image retrieval",
                "learning (artificial intelligence)",
                "query formulation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image annotation",
                "multi-label correlated Green function",
                "image understanding",
                "Web/database image search",
                "semi-supervised learning",
                "graph",
                "adaptive decision boundary method",
                "multi-label classification algorithms"
            ]
        },
        "id": 261,
        "cited_by": [
            {
                "year": "2011",
                "id": 69
            }
        ]
    },
    {
        "title": "Incremental discriminative-analysis of canonical correlations for action recognition",
        "authors": [
            "Xinxiao Wu",
            "Wei Liang",
            "Yunde Jia"
        ],
        "abstract": "Human action recognition is a challenging problem due to the large changes of human appearance in the cases of partial occlusions, non-rigid deformations and high irregularities. It is difficult to collect a large set of training samples with the hope of covering all possible variations of an action. In this paper, we propose an online recognition method, namely Incremental Discriminant-Analysis of Canonical Correlations (IDCC), whose discriminative model is incrementally updated to capture the changes of human appearance and thereby facilitates the recognition task in changing environments. As the training sets are acquired sequentially instead of being given completely in advance, our method is able to compute a new discriminant matrix by updating the existing one using the eigenspace merging algorithm. Experimental results on both Weizmann and KTH action data sets show that our method performs better than state-of-the-art methods on both accuracy and efficiency. Moreover, the robustness of our method is demonstrated on the irregular action recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459448",
        "reference_list": [
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 206
            },
            {
                "year": "2005",
                "id": 182
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Image recognition",
                "Biological system modeling",
                "Brain modeling",
                "Merging",
                "Training data",
                "Computer vision",
                "Laboratories",
                "Information technology",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "correlation theory",
                "eigenvalues and eigenfunctions",
                "gesture recognition",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "incremental discriminative analysis",
                "canonical correlations",
                "human action recognition",
                "occlusions",
                "nonrigid deformations",
                "online recognition method",
                "discriminative model",
                "human appearance",
                "discriminant matrix",
                "eigenspace merging algorithm",
                "Weizmann action data sets",
                "KTH action data sets"
            ]
        },
        "id": 262,
        "cited_by": [
            {
                "year": "2011",
                "id": 225
            }
        ]
    },
    {
        "title": "Minimizing energy functions on 4-connected lattices using elimination",
        "authors": [
            "Peter Carr",
            "Richard Hartley"
        ],
        "abstract": "We describe an energy minimization algorithm for functions defined on 4-connected lattices, of the type usually encountered in problems involving images. Such functions are often minimized using graph-cuts/max-flow, but this method is only applicable to submodular problems. In this paper, we describe an algorithm that will solve any binary problem, irrespective of whether it is submodular or not, and for multilabel problems we use alpha-expansion. The method is based on the elimination algorithm, which eliminates nodes from the graph until the remaining function is submodular. It can then be solved using max-flow. Values of eliminated variables are recovered using back-substitution. We compare the algorithm's performance against alternative methods for solving non-submodular problems, with favourable results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459450",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Lattices",
                "Minimization methods",
                "Least squares approximation",
                "Cost function",
                "Partitioning algorithms",
                "Approximation algorithms",
                "Iterative algorithms",
                "Message passing",
                "Springs",
                "Australia"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image processing",
                "lattice theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "4-connected lattices",
                "graph-cuts-max-flow",
                "submodular problems",
                "binary sequence problem",
                "alpha-expansion",
                "elimination algorithm",
                "back-substitution",
                "energy function minimization algorithm"
            ]
        },
        "id": 263,
        "cited_by": [
            {
                "year": "2011",
                "id": 175
            }
        ]
    },
    {
        "title": "A Riemannian analysis of 3D nose shapes for partial human biometrics",
        "authors": [
            "Hassen Drira",
            "Boulbaba Ben Amor",
            "Anuj Srivastava",
            "Mohamed Daoudi"
        ],
        "abstract": "In this paper we explore the use of shapes of noses for performing partial human biometrics. The basic idea is to represent nasal surfaces using indexed collections of iso-curves, and to analyze shapes of noses by comparing their corresponding curves. We extend past work in Riemannian analysis of shapes of closed curves in R 3 to obtain a similar Riemannian analysis for nasal surfaces. In particular, we obtain algorithms for computing geodesics, computing statistical means, and stochastic clustering. We demonstrate these ideas in two application contexts : authentication and identification. We evaluate performances on a large database involving 2000 scans from FRGC v2 database, and present a hierarchical organization of nose databases to allow for efficient searches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459451",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 5,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Nose",
                "Shape",
                "Humans",
                "Biometrics",
                "Databases",
                "Geophysics computing",
                "Clustering algorithms",
                "Stochastic processes",
                "Authentication",
                "Performance evaluation"
            ],
            "INSPEC: Controlled Indexing": [
                "biometrics (access control)",
                "shape recognition",
                "statistical analysis",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Riemannian analysis",
                "3D nose shapes",
                "partial human biometrics",
                "nasal surfaces",
                "isocurves",
                "geodesics computation",
                "statistical means compution",
                "stochastic clustering",
                "authentication",
                "identification",
                "FRGC v2 database"
            ]
        },
        "id": 264,
        "cited_by": []
    },
    {
        "title": "A hybrid generative/discriminative classification framework based on free-energy terms",
        "authors": [
            "A. Perina",
            "M. Cristani",
            "U. Castellani",
            "V. Murino",
            "N. Jojic"
        ],
        "abstract": "Hybrid, generative-discriminative, techniques have proven to be valuable approaches in tackling difficult object or scene recognition problems. In general, a generative model over the available data for each image class is first learned providing a relatively comprehensive statistical multi-level representation. In this way, new meaningful image features become available, which encode the degree of fitness of the data with respect to the model at different representation levels. Such features are then fed into a discriminative classifier which can exploit the intrinsic data separability. In this paper, we propose the use of variational free energy terms as feature vectors, so that the degree of fitness of the data and the uncertainty over the generative process are explicitly included in the data description. The proposed method is automatically superior to a pure generative classification, and we also experimentally validate it on a wide selection of generative models applied to challenging benchmarks in hard computer vision tasks such as scene, object, and shape recognition. In several instances, the proposed approach outperforms the current state-of-the-art techniques as for classification results, while also showing to be computationally inexpensive.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459453",
        "reference_list": [
            {
                "year": "2005",
                "id": 17
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 12,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Hybrid power systems",
                "Computer vision",
                "Conference management",
                "Character generation",
                "Layout",
                "Uncertainty",
                "Testing",
                "Shape",
                "Taxonomy",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "image representation",
                "object recognition",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hybrid generative-discriminative classification framework",
                "free-energy terms",
                "scene recognition problems",
                "object recognition problems",
                "image classification",
                "statistical multilevel representation",
                "data description",
                "computer vision",
                "shape recognition"
            ]
        },
        "id": 265,
        "cited_by": []
    },
    {
        "title": "Realtime background subtraction from dynamic scenes",
        "authors": [
            "Li Cheng",
            "Minglun Gong"
        ],
        "abstract": "This paper examines the problem of moving object detection. More precisely, it addresses the difficult scenarios where background scene textures in the video might change over time. In this paper, we formulate the problem mathematically as minimizing a constrained risk functional motivated from the large margin principle. It is a generalization of the one class support vector machines (1-SVMs) to accommodate spatial interactions, which is further incorporated into an online learning framework to track temporal changes. As a result it yields a closed-form update formula, a central component of the proposed algorithm to enable prompt adaptation to spatio-temporal changes. We also analyze the mistake bound and discuss issues such as dealing with non-stationary distributions, making use of kernels and efficient inference by a variant of dynamic programming. By exploiting the inherently concurrent structure, the proposed approach is designed to work with the highly parallel graphics processors (GPUs) to facilitate realtime analysis. Our empirical study demonstrates that the proposed approach works in realtime (over 80 frames per second) and at the same time performs competitively against state-of-the-art offline and quasi-realtime methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459454",
        "reference_list": [
            {
                "year": "2003",
                "id": 170
            },
            {
                "year": "2003",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Machine learning",
                "Kernel",
                "Inference algorithms",
                "Biological system modeling",
                "Object detection",
                "Support vector machines",
                "Dynamic programming",
                "Graphics",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "realtime background subtraction",
                "dynamic scenes",
                "object detection",
                "background scene textures",
                "constrained risk functional",
                "support vector machines",
                "spatial interactions",
                "parallel graphics processors",
                "GPU",
                "quasirealtime methods"
            ]
        },
        "id": 266,
        "cited_by": []
    },
    {
        "title": "Exploiting uncertainty in random sample consensus",
        "authors": [
            "Rahul Raguram",
            "Jan-Michael Frahm",
            "Marc Pollefeys"
        ],
        "abstract": "In this work, we present a technique for robust estimation, which by explicitly incorporating the inherent uncertainty of the estimation procedure, results in a more efficient robust estimation algorithm. In addition, we build on recent work in randomized model verification, and use this to characterize the `non-randomness' of a solution. The combination of these two strategies results in a robust estimation procedure that provides a significant speed-up over existing RANSAC techniques, while requiring no prior information to guide the sampling process. In particular, our algorithm requires, on average, 3-10 times fewer samples than standard RANSAC, which is in close agreement with theoretical predictions. The efficiency of the algorithm is demonstrated on a selection of geometric estimation problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459456",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 14,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Uncertainty",
                "Robustness",
                "Sampling methods",
                "Computer science",
                "Computer vision",
                "Solid modeling",
                "Runtime",
                "Noise generators",
                "Resumes",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "estimation theory",
                "sampling methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "random sample consensus",
                "robust estimation algorithm",
                "inherent uncertainty",
                "randomized model verification",
                "solution nonrandomness",
                "RANSAC techniques",
                "sampling process",
                "geometric estimation problems"
            ]
        },
        "id": 267,
        "cited_by": [
            {
                "year": "2011",
                "id": 164
            }
        ]
    },
    {
        "title": "A robust elastic and partial matching metric for face recognition",
        "authors": [
            "Gang Hua",
            "Amir Akbarzadeh"
        ],
        "abstract": "We present a robust elastic and partial matching metric for face recognition. To handle challenges such as pose, facial expression and partial occlusion, we enable both elastic and partial matching by computing a part based face representation. In which N local image descriptors are extracted from densely sampled overlapping image patches. We then define a distance metric where each descriptor in one face is matched against its spatial neighborhood in the other face and the minimal distance is recorded. For implicit partial matching, the list of all minimal distances are sorted in ascending order and the distance at the \u03b1N-th position is picked up as the final distance. The parameter 0 \u2264 \u03b1 \u2264 1 controls how much occlusion, facial expression changes, or pixel degradations we would allow. The optimal parameter values of this new distance metric are extensively studied and identified with real-life photo collections. We also reveal that filtering the face image by a simple difference of Gaussian brings significant robustness to lighting variations and beats the more utilized self-quotient image. Extensive evaluations on face recognition benchmarks show that our method is leading or is competitive in performance when compared to state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459457",
        "reference_list": [
            {
                "year": "2007",
                "id": 16
            },
            {
                "year": "2007",
                "id": 18
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 3,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Face recognition",
                "Degradation",
                "Filtering",
                "Photometry",
                "Filters",
                "Face detection",
                "Computer vision",
                "Bayesian methods",
                "Supervised learning"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "filtering theory",
                "image matching",
                "image sampling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "elastic matching",
                "partial matching metric",
                "face recognition",
                "facial expression",
                "pose expression",
                "local image descriptors",
                "face representation",
                "overlapping image patches",
                "distance metric",
                "minimal distance",
                "real-life photo collections",
                "Gaussian filtering",
                "self-quotient image",
                "spatial neighborhood"
            ]
        },
        "id": 268,
        "cited_by": [
            {
                "year": "2013",
                "id": 410
            },
            {
                "year": "2011",
                "id": 114
            }
        ]
    },
    {
        "title": "SCRAMSAC: Improving RANSAC's efficiency with a spatial consistency filter",
        "authors": [
            "Torsten Sattler",
            "Bastian Leibe",
            "Leif Kobbelt"
        ],
        "abstract": "Geometric verification with RANSAC has become a crucial step for many local feature based matching applications. Therefore, the details of its implementation are directly relevant for an application's run-time and the quality of the estimated results. In this paper, we propose a RANSAC extension that is several orders of magnitude faster than standard RANSAC and as fast as and more robust to degenerate configurations than PROSAC, the currently fastest RANSAC extension from the literature. In addition, our proposed method is simple to implement and does not require parameter tuning. Its main component is a spatial consistency check that results in a reduced correspondence set with a significantly increased inlier ratio, leading to faster convergence of the remaining estimation steps. In addition, we experimentally demonstrate that RANSAC can operate entirely on the reduced set not only for sampling, but also for its consensus step, leading to additional speed-ups. The resulting approach is widely applicable and can be readily combined with other extensions from the literature. We quantitatively evaluate our approach's robustness on a variety of challenging datasets and compare its performance to the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459459",
        "reference_list": [
            {
                "year": "2001",
                "id": 177
            },
            {
                "year": "2003",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 22,
            "total": 61
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image matching",
                "iterative methods",
                "spatial filters"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SCRAMSAC",
                "spatial consistency filter",
                "geometric verification",
                "local feature based matching applications",
                "PROSAC",
                "RANSAC extension",
                "inlier ratio",
                "spatial consistency check"
            ]
        },
        "id": 269,
        "cited_by": [
            {
                "year": "2013",
                "id": 5
            },
            {
                "year": "2013",
                "id": 434
            },
            {
                "year": "2011",
                "id": 143
            },
            {
                "year": "2011",
                "id": 164
            }
        ]
    },
    {
        "title": "Efficient multi-label ranking for multi-class learning: Application to object recognition",
        "authors": [
            "Serhat S. Bucak",
            "Pavan Kumar Mallapragada",
            "Rong Jin",
            "Anil K. Jain"
        ],
        "abstract": "Multi-label learning is useful in visual object recognition when several objects are present in an image. Conventional approaches implement multi-label learning as a set of binary classification problems, but they suffer from imbalanced data distributions when the number of classes is large. In this paper, we address multi-label learning with many classes via a ranking approach, termed multi-label ranking. Given a test image, the proposed scheme aims to order all the object classes such that the relevant classes are ranked higher than the irrelevant ones. We present an efficient algorithm for multi-label ranking based on the idea of block coordinate descent. The proposed algorithm is applied to visual object recognition. Empirical results on the PASCAL VOC 2006 and 2007 data sets show promising results in comparison to the state-of-the-art algorithms for multi-label learning.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459460",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 8,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Computer vision",
                "Testing",
                "Labeling",
                "Error correction codes",
                "Robustness",
                "Computational efficiency",
                "Boosting",
                "Fasteners",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multilabel ranking",
                "multiclass learning",
                "visual object recognition",
                "binary classification problems",
                "object classes",
                "block coordinate descent"
            ]
        },
        "id": 270,
        "cited_by": [
            {
                "year": "2017",
                "id": 49
            }
        ]
    },
    {
        "title": "Learning to predict where humans look",
        "authors": [
            "Tilke Judd",
            "Krista Ehinger",
            "Fr\u00e9do Durand",
            "Antonio Torralba"
        ],
        "abstract": "For many applications in graphics, design, and human computer interaction, it is essential to understand where humans look in a scene. Where eye tracking devices are not a viable option, models of saliency can be used to predict fixation locations. Most saliency approaches are based on bottom-up computation that does not consider top-down image semantics and often does not match actual eye movements. To address this problem, we collected eye tracking data of 15 viewers on 1003 images and use this database as training and testing examples to learn a model of saliency based on low, middle and high-level image features. This large database of eye tracking data is publicly available with this paper.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459462",
        "reference_list": [],
        "citation": {
            "ieee": 463,
            "other": 382,
            "total": 845
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Application software",
                "Predictive models",
                "Spatial databases",
                "Biological system modeling",
                "Context modeling",
                "Computer graphics",
                "Human computer interaction",
                "Image databases",
                "Biology computing"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "human computer interaction",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human computer interaction",
                "saliency approaches",
                "top-down image semantics",
                "eye tracking data",
                "high-level image features"
            ]
        },
        "id": 271,
        "cited_by": [
            {
                "year": "2017",
                "id": 178
            },
            {
                "year": "2017",
                "id": 206
            },
            {
                "year": "2017",
                "id": 230
            },
            {
                "year": "2017",
                "id": 262
            },
            {
                "year": "2017",
                "id": 594
            },
            {
                "year": "2015",
                "id": 21
            },
            {
                "year": "2015",
                "id": 24
            },
            {
                "year": "2015",
                "id": 29
            },
            {
                "year": "2015",
                "id": 121
            },
            {
                "year": "2015",
                "id": 436
            },
            {
                "year": "2013",
                "id": 77
            },
            {
                "year": "2013",
                "id": 114
            },
            {
                "year": "2013",
                "id": 152
            },
            {
                "year": "2013",
                "id": 219
            },
            {
                "year": "2013",
                "id": 246
            },
            {
                "year": "2013",
                "id": 401
            },
            {
                "year": "2013",
                "id": 403
            },
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2011",
                "id": 130
            }
        ]
    },
    {
        "title": "Tensor completion for estimating missing values in visual data",
        "authors": [
            "Ji Liu",
            "Przemyslaw Musialski",
            "Peter Wonka",
            "Jieping Ye"
        ],
        "abstract": "In this paper we propose an algorithm to estimate missing values in tensors of visual data. The values can be missing due to problems in the acquisition process, or because the user manually identified unwanted outliers. Our algorithm works even with a small amount of samples and it can propagate structure to fill larger missing regions. Our methodology is built on recent studies about matrix completion using the matrix trace norm. The contribution of our paper is to extend the matrix case to the tensor case by laying out the theoretical foundations and then by building a working algorithm. First, we propose a definition for the tensor trace norm, that generalizes the established definition of the matrix trace norm. Second, similar to matrix completion, the tensor completion is formulated as a convex optimization problem. Unfortunately, the straightforward problem extension is significantly harder to solve than the matrix case because of the dependency among multiple constraints. To tackle this problem, we employ a relaxation technique to separate the dependant relationships and use the block coordinate descent (BCD) method to achieve a globally optimal solution. Our experiments show potential applications of our algorithm and the quantitative evaluation indicates that our method is more accurate and robust than heuristic approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459463",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 20,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "State estimation",
                "Robustness",
                "Image reconstruction",
                "Heuristic algorithms",
                "Computer vision",
                "Buildings",
                "Iterative algorithms",
                "Computer graphics",
                "Iterative decoding"
            ]
        },
        "id": 272,
        "cited_by": [
            {
                "year": "2017",
                "id": 348
            },
            {
                "year": "2015",
                "id": 30
            },
            {
                "year": "2013",
                "id": 245
            }
        ]
    },
    {
        "title": "Unsupervised learning of high-order structural semantics from images",
        "authors": [
            "Jizhou Gao",
            "Yin Hu",
            "Jinze Liu",
            "Ruigang Yang"
        ],
        "abstract": "Structural semantics are fundamental to understanding both natural and man-made objects from languages to buildings. They are manifested as repeated structures or patterns and are often captured in images. Finding repeated patterns in images, therefore, has important applications in scene understanding, 3D reconstruction, and image retrieval as well as image compression. Previous approaches in visual-pattern mining limited themselves by looking for frequently co-occurring features within a small neighborhood in an image. However, semantics of a visual pattern are typically defined by specific spatial relationships between features regardless of the spatial proximity. In this paper, semantics are represented as visual elements and geometric relationships between them. A novel unsupervised learning algorithm finds pair-wise associations of visual elements that have consistent geometric relationships sufficiently often. The algorithms are efficient - maximal matchings are determined without combinatorial search. High-order structural semantics are extracted by mining patterns that are composed of pairwise spatially consistent associations of visual elements. We demonstrate the effectiveness of our approach for discovering repeated visual patterns on a variety of image collections.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459465",
        "reference_list": [
            {
                "year": "2007",
                "id": 87
            },
            {
                "year": "2007",
                "id": 38
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2007",
                "id": 30
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 1,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Unsupervised learning",
                "Windows",
                "Eyes",
                "Buildings",
                "Image retrieval",
                "Costs",
                "Polynomials",
                "Visualization",
                "Virtual environment",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image reconstruction",
                "image retrieval",
                "semantic networks",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised learning",
                "high-order structural semantics",
                "man-made objects",
                "scene understanding",
                "3D image reconstruction",
                "image retrieval",
                "image compression",
                "visual-pattern mining",
                "spatial proximity",
                "efficient maximal matchings",
                "image collections"
            ]
        },
        "id": 273,
        "cited_by": []
    },
    {
        "title": "Kernelized locality-sensitive hashing for scalable image search",
        "authors": [
            "Brian Kulis",
            "Kristen Grauman"
        ],
        "abstract": "Fast retrieval methods are critical for large-scale and data-driven vision applications. Recent work has explored ways to embed high-dimensional features or complex distance functions into a low-dimensional Hamming space where items can be efficiently searched. However, existing methods do not apply for high-dimensional kernelized data when the underlying feature embedding for the kernel is unknown. We show how to generalize locality-sensitive hashing to accommodate arbitrary kernel functions, making it possible to preserve the algorithm's sub-linear time similarity search guarantees for a wide class of useful similarity functions. Since a number of successful image-based kernels have unknown or incomputable embeddings, this is especially valuable for image retrieval tasks. We validate our technique on several large-scale datasets, and show that it enables accurate and fast performance for example-based object classification, feature matching, and content-based retrieval.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459466",
        "reference_list": [
            {
                "year": "2007",
                "id": 18
            },
            {
                "year": "2007",
                "id": 20
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 230,
            "other": 154,
            "total": 384
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image databases",
                "Spatial databases",
                "Information retrieval",
                "Large-scale systems",
                "Image retrieval",
                "Content based retrieval",
                "Visual databases",
                "Indexing",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "file organisation",
                "image classification",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "kernelized locality sensitive hashing",
                "scalable image search",
                "fast retrieval method",
                "data driven vision application",
                "low dimensional Hamming space",
                "high dimensional kernelized data",
                "generalize locality sensitive hashing",
                "sublinear time similarity search",
                "image retrieval",
                "example based object classification"
            ]
        },
        "id": 274,
        "cited_by": [
            {
                "year": "2017",
                "id": 431
            },
            {
                "year": "2015",
                "id": 116
            },
            {
                "year": "2015",
                "id": 122
            },
            {
                "year": "2015",
                "id": 426
            },
            {
                "year": "2015",
                "id": 463
            },
            {
                "year": "2015",
                "id": 467
            },
            {
                "year": "2013",
                "id": 41
            },
            {
                "year": "2013",
                "id": 265
            },
            {
                "year": "2011",
                "id": 206
            },
            {
                "year": "2011",
                "id": 338
            }
        ]
    },
    {
        "title": "Actionable information in vision",
        "authors": [
            "Stefano Soatto"
        ],
        "abstract": "I propose a notion of visual information as the complexity not of the raw images, but of the images after the effects of nuisance factors such as viewpoint and illumination are discounted. It is rooted in ideas of J. J. Gibson, and stands in contrast to traditional information as entropy or coding length of the data regardless of its use, and regardless of the nuisance factors affecting it. The non-invertibility of nuisances such as occlusion and quantization induces an \u201cinformation gap\u201d that can only be bridged by controlling the data acquisition process. Measuring visual information entails early vision operations, tailored to the structure of the nuisances so as to be \u201clossless\u201d with respect to visual decision and control tasks (as opposed to data transmission and storage tasks implicit in traditional Information Theory). I illustrate these ideas on visual exploration, whereby a \u201cShannonian Explorer\u201d guided by the entropy of the data navigates unaware of the structure of the physical space surrounding it, while a \u201cGibsonian Explorer\u201d is guided by the topology of the environment, despite measuring only images of it, without performing 3D reconstruction. The operational definition of visual information suggests desirable properties that a visual representation should possess to best accomplish vision-based decision and control tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459468",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 10,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Lighting",
                "Quantization",
                "Data acquisition",
                "Data communication",
                "Information theory",
                "Navigation",
                "Topology",
                "Performance evaluation",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "data acquisition",
                "entropy",
                "quantisation (signal)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual information",
                "nuisance factor effect",
                "information entropy",
                "occlusion",
                "quantization",
                "data acquisition process",
                "Shannonian explorer",
                "3D reconstruction",
                "vision-based decision"
            ]
        },
        "id": 275,
        "cited_by": [
            {
                "year": "2009",
                "id": 85
            }
        ]
    },
    {
        "title": "What is the best multi-stage architecture for object recognition?",
        "authors": [
            "Kevin Jarrett",
            "Koray Kavukcuoglu",
            "Marc'Aurelio Ranzato",
            "Yann LeCun"
        ],
        "abstract": "In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (> 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459469",
        "reference_list": [
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 382,
            "other": 288,
            "total": 670
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Filter bank",
                "Feature extraction",
                "Refining",
                "Brain modeling",
                "Gabor filters",
                "Learning systems",
                "Image edge detection",
                "Error analysis",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "object recognition",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multistage architecture",
                "object recognition",
                "feature extraction",
                "filter bank",
                "nonlinear transformation",
                "feature pooling layer",
                "unsupervised learning",
                "supervised learning",
                "feature rectification",
                "local contrast normalization",
                "Caltech-101",
                "NORB dataset",
                "unprocessed MNIST dataset"
            ]
        },
        "id": 276,
        "cited_by": [
            {
                "year": "2017",
                "id": 142
            },
            {
                "year": "2017",
                "id": 323
            },
            {
                "year": "2017",
                "id": 591
            },
            {
                "year": "2015",
                "id": 13
            },
            {
                "year": "2013",
                "id": 383
            },
            {
                "year": "2011",
                "id": 256
            },
            {
                "year": "2011",
                "id": 337
            }
        ]
    },
    {
        "title": "Shape-based recognition of 3D point clouds in urban environments",
        "authors": [
            "Aleksey Golovinskiy",
            "Vladimir G. Kim",
            "Thomas Funkhouser"
        ],
        "abstract": "This paper investigates the design of a system for recognizing objects in 3D point clouds of urban environments. The system is decomposed into four steps: locating, segmenting, characterizing, and classifying clusters of 3D points. Specifically, we first cluster nearby points to form a set of potential object locations (with hierarchical clustering). Then, we segment points near those locations into foreground and background sets (with a graph-cut algorithm). Next, we build a feature vector for each point cluster (based on both its shape and its context). Finally, we label the feature vectors using a classifier trained on a set of manually labeled objects. The paper presents several alternative methods for each step. We quantitatively evaluate the system and tradeoffs of different alternatives in a truthed part of a scan of Ottawa that contains approximately 100 million points and 1000 objects of interest. Then, we use this truth data as a training set to recognize objects amidst approximately 1 billion points of the remainder of the Ottawa scan.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459471",
        "reference_list": [],
        "citation": {
            "ieee": 98,
            "other": 58,
            "total": 156
        },
        "keywords": {
            "IEEE Keywords": [
                "Clouds"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image segmentation",
                "object detection",
                "pattern clustering",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape based recognition",
                "3D point clouds",
                "urban environments",
                "objects recognition",
                "hierarchical clustering",
                "points segmentation",
                "graph cut algorithm",
                "feature vectors",
                "quantitative evaluation"
            ]
        },
        "id": 277,
        "cited_by": [
            {
                "year": "2015",
                "id": 239
            },
            {
                "year": "2011",
                "id": 135
            }
        ]
    },
    {
        "title": "Multiscale symmetric part detection and grouping",
        "authors": [
            "Alex Levinshtein",
            "Sven Dickinson",
            "Cristian Sminchisescu"
        ],
        "abstract": "Skeletonization algorithms typically decompose an object's silhouette into a set of symmetric parts, offering a powerful representation for shape categorization. However, having access to an object's silhouette assumes correct figure-ground segmentation, leading to a disconnect with the mainstream categorization community, which attempts to recognize objects from cluttered images. In this paper, we present a novel approach to recovering and grouping the symmetric parts of an object from a cluttered scene. We begin by using a multiresolution superpixel segmentation to generate medial point hypotheses, and use a learned affinity function to perceptually group nearby medial points likely to belong to the same medial branch. In the next stage, we learn higher granularity affinity functions to group the resulting medial branches likely to belong to the same object. The resulting framework yields a skeletal approximation that's free of many of the instabilities plaguing traditional skeletons. More importantly, it doesn't require a closed contour, enabling the application of skeleton-based categorization systems to more realistic imagery",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459472",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 15,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image segmentation",
                "Skeleton",
                "Layout",
                "Clustering algorithms",
                "Image recognition",
                "Image resolution",
                "Accidents",
                "Training data",
                "Object recognition"
            ]
        },
        "id": 278,
        "cited_by": [
            {
                "year": "2015",
                "id": 183
            },
            {
                "year": "2013",
                "id": 218
            },
            {
                "year": "2011",
                "id": 56
            }
        ]
    },
    {
        "title": "Recovering planar homographies between 2D shapes",
        "authors": [
            "Jozsef Nemeth",
            "Csaba Domokos",
            "Zoltan Kato"
        ],
        "abstract": "Images taken from different views of a planar object are related by planar homography. Recovering the parameters of such transformations is a fundamental problem in computer vision with various applications. This paper proposes a novel method to estimate the parameters of a homography that aligns two binary images. It is obtained by solving a system of nonlinear equations generated by integrating linearly independent functions over the domains determined by the shapes. The advantage of the proposed solution is that it is easy to implement, less sensitive to the strength of the deformation, works without established correspondences and robust against segmentation errors. The method has been tested on synthetic as well as on real images and its efficiency has been demonstrated in the context of two different applications: alignment of hip prosthesis X-ray images and matching of traffic signs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459474",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Computer vision",
                "Application software",
                "Parameter estimation",
                "Nonlinear equations",
                "Robustness",
                "Image segmentation",
                "Testing",
                "Hip",
                "Prosthetics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "nonlinear equations",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "planar homography recovery",
                "2D shapes",
                "computer vision",
                "parameter estimation",
                "binary images",
                "nonlinear equation system",
                "linearly independent functions",
                "segmentation errors",
                "traffic sign matching",
                "hip prosthesis X-ray image alignment"
            ]
        },
        "id": 279,
        "cited_by": []
    },
    {
        "title": "Multimodal partial estimates fusion",
        "authors": [
            "Jiang Xu",
            "Junsong Yuan",
            "Ying Wu"
        ],
        "abstract": "Fusing partial estimates is a critical and common problem in many computer vision tasks such as part-based detection and tracking. It generally becomes complicated and intractable when there are a large number of multimodal partial estimates, and thus it is desirable to find an effective and scalable fusion method to integrate these partial estimates. This paper presents a novel and effective approach to fusing multimodal partial estimates in a principled way. In this new approach, fusion is related to a computational geometry problem of finding the minimum-volume orthotope, and an effective and scalable branch and bound search algorithm is designed to obtain the global optimal solution. Experiments on tracking articulated objects and occluded objects show the effectiveness of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459475",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Fuses",
                "Computer vision",
                "Computational geometry",
                "Algorithm design and analysis",
                "Target tracking",
                "Concrete",
                "Object detection",
                "Motion detection",
                "Detectors",
                "Multimodal sensors"
            ]
        },
        "id": 280,
        "cited_by": []
    },
    {
        "title": "Image saliency by isocentric curvedness and color",
        "authors": [
            "Roberto Valenti",
            "Nicu Sebe",
            "Theo Gevers"
        ],
        "abstract": "In this paper we propose a novel computational method to infer visual saliency in images. The method is based on the idea that salient objects should have local characteristics that are different than the rest of the scene, being edges, color or shape. By using a novel operator, these characteristics are combined to infer global information. The obtained information is used as a weighting for the output of a segmentation algorithm so that the salient object in the scene can easily be distinguished from the background. The proposed approach is fast and it does not require any learning. The experimentation shows that the system can enhance interesting objects in images and it is able to correctly locate the same object annotated by humans with an F-measure of 85.61% when the object size is known, and 79.19% when the object size is unknown, improving the state of the art performance on a public dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459240",
        "reference_list": [],
        "citation": {
            "ieee": 51,
            "other": 32,
            "total": 83
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Layout",
                "Computer vision",
                "Image segmentation",
                "Humans",
                "Object detection",
                "Intelligent systems",
                "Computational intelligence",
                "Shape",
                "Detection algorithms"
            ]
        },
        "id": 281,
        "cited_by": [
            {
                "year": "2013",
                "id": 219
            },
            {
                "year": "2011",
                "id": 130
            }
        ]
    },
    {
        "title": "GroupSAC: Efficient consensus in the presence of groupings",
        "authors": [
            "Kai Ni",
            "Hailin Jin",
            "Frank Dellaert"
        ],
        "abstract": "We present a novel variant of the RANSAC algorithm that is much more efficient, in particular when dealing with problems with low inlier ratios. Our algorithm assumes that there exists some grouping in the data, based on which we introduce a new binomial mixture model rather than the simple binomial model as used in RANSAC. We prove that in the new model it is more efficient to sample data from a smaller numbers of groups and groups with more tentative correspondences, which leads to a new sampling procedure that uses progressive numbers of groups. We demonstrate our algorithm on two classical geometric vision problems: wide-baseline matching and camera resectioning. The experiments show that the algorithm serves as a general framework that works well with three possible grouping strategies investigated in this paper, including a novel optical flow based clustering approach. The results show that our algorithm is able to achieve a significant performance gain compared to the standard RANSAC and PROSAC.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459241",
        "reference_list": [
            {
                "year": "2005",
                "id": 225
            },
            {
                "year": "2003",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 3,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Sampling methods",
                "Clustering algorithms",
                "Geometrical optics",
                "Cameras",
                "Computer vision",
                "Testing",
                "Image segmentation",
                "Performance gain",
                "Internet"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "image sampling",
                "image sequences",
                "iterative methods",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "GroupSAC",
                "RANSAC algorithm",
                "binomial mixture model",
                "sampling procedure",
                "geometric vision problem",
                "wide-baseline matching",
                "camera resectioning",
                "optical flow based clustering"
            ]
        },
        "id": 282,
        "cited_by": [
            {
                "year": "2013",
                "id": 5
            },
            {
                "year": "2011",
                "id": 164
            }
        ]
    },
    {
        "title": "Fast visibility restoration from a single color or gray level image",
        "authors": [
            "Jean-Philippe Tarel",
            "Nicolas Hauti\u00e8re"
        ],
        "abstract": "One source of difficulties when processing outdoor images is the presence of haze, fog or smoke which fades the colors and reduces the contrast of the observed objects. We introduce a novel algorithm and variants for visibility restoration from a single image. The main advantage of the proposed algorithm compared with other is its speed: its complexity is a linear function of the number of image pixels only. This speed allows visibility restoration to be applied for the first time within real-time processing applications such as sign, lane-marking and obstacle detection from an in-vehicle camera. Another advantage is the possibility to handle both color images or gray level images since the ambiguity between the presence of fog and the objects with low color saturation is solved by assuming only small objects can have colors with low saturation. The algorithm is controlled only by a few parameters and consists in: atmospheric veil inference, image restoration and smoothing, tone mapping. A comparative study and quantitative evaluation is proposed with a few other state of the art algorithms which demonstrates that similar or better quality results are obtained. Finally, an application is presented to lane-marking extraction in gray level images, illustrating the interest of the approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459251",
        "reference_list": [],
        "citation": {
            "ieee": 314,
            "other": 162,
            "total": 476
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Color",
                "Layout",
                "Pixel",
                "Cameras",
                "Inference algorithms",
                "Atmospheric modeling",
                "Smoothing methods",
                "Surveillance",
                "Intelligent vehicles"
            ]
        },
        "id": 283,
        "cited_by": [
            {
                "year": "2017",
                "id": 502
            },
            {
                "year": "2013",
                "id": 76
            }
        ]
    },
    {
        "title": "Scale invariance and noise in natural images",
        "authors": [
            "Daniel Zoran",
            "Yair Weiss"
        ],
        "abstract": "Natural images are known to have scale invariant statistics. While some eariler studies have reported the kurtosis of marginal bandpass filter response distributions to be constant throughout scales, other studies have reported that the kurtosis values are lower for high frequency filters than for lower frequency ones. In this work we propose a resolution for this discrepancy and suggest that this change in kurtosis values is due to noise present in the image. We suggest that this effect is consistent with a clean, natural image corrupted by white noise. We propose a model for this effect, and use it to estimate noise standard deviation in corrupted natural images. In particular, our results suggest that classical benchmark images used in low-level vision are actually noisy and can be cleaned up. Our results on noise estimation on two sets of 50 and a 100 natural images are significantly better than the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459476",
        "reference_list": [],
        "citation": {
            "ieee": 86,
            "other": 28,
            "total": 114
        },
        "keywords": {
            "IEEE Keywords": [
                "Frequency",
                "Shape",
                "Statistical distributions",
                "Band pass filters",
                "Noise shaping",
                "Layout",
                "Gaussian distribution",
                "Computer science",
                "Image resolution",
                "White noise"
            ],
            "INSPEC: Controlled Indexing": [
                "band-pass filters",
                "image denoising",
                "realistic images",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "natural images",
                "scale invariant statistics",
                "bandpass filter response distributions",
                "kurtosis values",
                "high frequency filters",
                "lower frequency filters",
                "white noise",
                "noise standard deviation",
                "benchmark images",
                "low-level vision",
                "noise estimation",
                "sets"
            ]
        },
        "id": 284,
        "cited_by": [
            {
                "year": "2013",
                "id": 361
            }
        ]
    },
    {
        "title": "Image restoration using online photo collections",
        "authors": [
            "Kevin Dale",
            "Micah K. Johnson",
            "Kalyan Sunkavalli",
            "Wojciech Matusik",
            "Hanspeter Pfister"
        ],
        "abstract": "We present an image restoration method that leverages a large database of images gathered from the web. Given an input image, we execute an efficient visual search to find the closest images in the database; these images define the input's visual context. We use the visual context as an image-specific prior and show its value in a variety of image restoration operations, including white balance correction, exposure correction, and contrast enhancement. We evaluate our approach using a database of 1 million images downloaded from Flickr and demonstrate the effect of database size on performance. Our results show that priors based on the visual context consistently out-perform generic or even domain-specific priors for these operations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459473",
        "reference_list": [
            {
                "year": "2003",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 11,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Image databases",
                "Visual databases",
                "Image segmentation",
                "Cameras",
                "Gray-scale",
                "Digital photography",
                "Nearest neighbor searches",
                "Graphics",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image restoration",
                "online photo collection",
                "visual context",
                "Flickr",
                "white balance correction",
                "exposure correction",
                "contrast enhancement"
            ]
        },
        "id": 285,
        "cited_by": [
            {
                "year": "2017",
                "id": 617
            }
        ]
    },
    {
        "title": "Finding good composition in panoramic scenes",
        "authors": [
            "Yuan-Yang Chang",
            "Hwann-Tzong Chen"
        ],
        "abstract": "We introduce a new problem of automatic photo composition, and present an effective technique for finding good views within a panoramic scene. Instead of applying heuristic rules of photo composition, we propose to imitate good composition presented in the artworks of professional photographers. Our approach tries to model the composition styles of professional photographs by analyzing the structural features and the layout of visual saliency. The task of finding good photo composition through a viewfinder is formulated as a search problem, and we present a stochastic search algorithm to look for good viewing configurations and to choose suitable reference images from the collection of masterpiece photographs. Given any initial location in the panoramic scene, our algorithm is able to suggest a better view that would often yield professional-like photo composition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459470",
        "reference_list": [
            {
                "year": "2007",
                "id": 170
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Photography",
                "Computer vision",
                "Cameras",
                "Computer science",
                "Search problems",
                "Stochastic processes",
                "Painting",
                "Automatic control",
                "Guidelines"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "panoramic scenes",
                "automatic photo composition",
                "photo composition",
                "professional photographers",
                "search problem",
                "stochastic search algorithm"
            ]
        },
        "id": 286,
        "cited_by": []
    },
    {
        "title": "A framework for visual saliency detection with applications to image thumbnailing",
        "authors": [
            "Luca Marchesotti",
            "Claudio Cifarelli",
            "Gabriela Csurka"
        ],
        "abstract": "We propose a novel framework for visual saliency detection based on a simple principle: images sharing their global visual appearances are likely to share similar salience. Assuming that an annotated image database is available, we first retrieve the most similar images to the target image; secondly, we build a simple classifier and we use it to generate saliency maps. Finally, we refine the maps and we extract thumbnails. We show that in spite of its simplicity, our framework outperforms state-of-the-art approaches. Another advantage is its ability to deal with visual pop-up and application/task-driven saliency, if appropriately annotated images are available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459467",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 95,
            "other": 68,
            "total": 163
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Object detection",
                "Face detection",
                "Layout",
                "Image retrieval",
                "Detectors",
                "Pattern analysis",
                "Europe",
                "Image databases",
                "Information retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image thumbnailing",
                "visual saliency detection",
                "annotated image database",
                "thumbnail extraction",
                "visual pop-up",
                "application-task-driven saliency"
            ]
        },
        "id": 287,
        "cited_by": [
            {
                "year": "2017",
                "id": 22
            },
            {
                "year": "2017",
                "id": 230
            },
            {
                "year": "2013",
                "id": 215
            },
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2011",
                "id": 130
            },
            {
                "year": "2011",
                "id": 281
            }
        ]
    },
    {
        "title": "Optimizing parametric total variation models",
        "authors": [
            "Petter Strandmark",
            "Fredrik Kahl",
            "Niels Chr. Overgaard"
        ],
        "abstract": "One of the key factors for the success of recent energy minimization methods is that they seek to compute global solutions. Even for non-convex energy functionals, optimization methods such as graph cuts have proven to produce high-quality solutions by iterative minimization based on large neighborhoods, making them less vulnerable to local minima. Our approach takes this a step further by enlarging the search neighborhood with one dimension. In this paper we consider binary total variation problems that depend on an additional set of parameters. Examples include: (i) the Chan-Vese model that we solve globally (ii) ratio and constrained minimization which can be formulated as parametric problems, and (iii) variants of the Mumford-Shah functional. Our approach is based on a recent theorem of Chambolle which states that solving a one-parameter family of binary problems amounts to solving a single convex variational problem. We prove a generalization of this result and show how it can be applied to parametric optimization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459464",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Minimization methods",
                "Optimization methods",
                "Iterative methods",
                "Weight control",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image segmentation",
                "iterative methods",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "energy minimization methods",
                "iterative minimization",
                "Chan-Vese model",
                "binary problems",
                "convex variational problem",
                "parametric optimization",
                "nonconvex energy functionals",
                "binary total variation problems",
                "ratio minimization",
                "constrained minimization",
                "Mumford-Shah functional",
                "parametric total variation models"
            ]
        },
        "id": 288,
        "cited_by": []
    },
    {
        "title": "Deformable model fitting with a mixture of local experts",
        "authors": [
            "Jason M. Saragih",
            "Simon Lucey",
            "Jeffrey F. Cohn"
        ],
        "abstract": "Local experts have been used to great effect for fitting deformable models to images. Typically, the best location in an image for the deformable model's landmarks are found through a locally exhaustive search using these experts. In order to achieve efficient fitting, these experts should afford an efficient evaluation, which often leads to forms with restricted discriminative capacity. In this work, a framework is proposed in which multiple simple experts can be utilized to increase the capacity of the detections overall. In particular, the use of a mixture of linear classifiers is proposed, the computational complexity of which scales linearly with the number of mixture components. The fitting objective is maximized using the expectation maximization (EM) algorithm, where approximations to the true objective are made in order to facilitate efficient and numerically stable fitting. The efficacy of the proposed approach is evaluated on the task of generic face fitting where performance improvement is observed over two existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459461",
        "reference_list": [
            {
                "year": "2007",
                "id": 266
            },
            {
                "year": "2003",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Shape",
                "Humans",
                "Face",
                "Robots",
                "Fitting",
                "Computational complexity",
                "Training data",
                "Robustness",
                "Biomedical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "expectation-maximisation algorithm",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable model fitting",
                "linear classifiers",
                "computational complexity",
                "expectation maximization algorithm",
                "generic face fitting",
                "local expert mixture"
            ]
        },
        "id": 289,
        "cited_by": [
            {
                "year": "2011",
                "id": 243
            }
        ]
    },
    {
        "title": "Detecting interpretable and accurate scale-invariant keypoints",
        "authors": [
            "Wolfgang F\u00f6rstner",
            "Timo Dickscheid",
            "Falko Schindler"
        ],
        "abstract": "This paper presents a novel method for detecting scale invariant keypoints. It fills a gap in the set of available methods, as it proposes a scale-selection mechanism for junction-type features. The method is a scale-space extension of the detector proposed by F\u00f6rstner (1994) and uses the general spiral feature model of Big\u00fcn (1990) to unify different types of features within the same framework. By locally optimising the consistency of image regions with respect to the spiral model, we are able to detect and classify image structures with complementary properties over scale-space, especially star and circular shapes as interpretable and identifiable subclasses. Our motivation comes from calibrating images of structured scenes with poor texture, where blob detectors alone cannot find sufficiently many keypoints, while existing corner detectors fail due to the lack of scale invariance. The procedure can be controlled by semantically clear parameters. One obtains a set of keypoints with position, scale, type and consistency measure. We characterise the detector and show results on common benchmarks. It competes in repeatability with the Lowe detector, but finds more stable keypoints in poorly textured areas, and shows comparable or higher accuracy than other recent detectors. This makes it useful for both object recognition and camera calibration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459458",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 34,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Cameras",
                "Calibration",
                "Computer vision",
                "Spirals",
                "Layout",
                "Geodesy",
                "Shape",
                "Position measurement",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale-selection mechanism",
                "detector",
                "spiral feature model",
                "corner detectors",
                "object recognition",
                "camera calibration",
                "scale invariant keypoint detection",
                "image structure detection",
                "image structure classification",
                "structured scene images"
            ]
        },
        "id": 290,
        "cited_by": []
    },
    {
        "title": "The Swap and Expansion moves revisited and fused",
        "authors": [
            "Ido Leichter"
        ],
        "abstract": "Many solutions to computer vision and image processing problems involve the minimization of multi-label energy functions with up to K variables in each term. In the minimization process, Swap and Expansion are two commonly used moves. This paper re-derives the optimal swap and expansion moves for K = 2 in a short manner by using the original pseudo-Boolean quadratic function minimization method as a \u201cblack box\u201d. It is then revealed that the found minima w.r.t. expansion moves are in fact also minima w.r.t. swap moves. This process is repeated for K = 3. The minima-related result is extended to all functions under the condition that they are reduced into submodular ones, which makes it applicable to all expansion algorithms. These may explain the prevalent impression that expansion algorithms are more effective than swap algorithms. To make the search space larger, Exwap - a generalization of the expansion and the swap moves - is introduced. Efficient algorithms for minimizing w.r.t. it for K = 2 (including a `truncation' procedure), K = 3, and the Pn Potts model are provided. The move is capable of reaching lower energies than those reached by the expansion algorithm, as demonstrated for several benchmark problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459455",
        "reference_list": [
            {
                "year": "2007",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Computer vision",
                "Image processing",
                "Computer science",
                "Minimization methods",
                "Maximum a posteriori estimation",
                "Markov random fields"
            ],
            "INSPEC: Controlled Indexing": [
                "Boolean functions",
                "computer vision",
                "functions",
                "image processing",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "image processing",
                "multilabel energy functions",
                "K variables",
                "Swap",
                "Expansion",
                "pseudo-Boolean quadratic function minimization method",
                "black box",
                "submodular functions",
                "expansion algorithms"
            ]
        },
        "id": 291,
        "cited_by": []
    },
    {
        "title": "Non-local sparse models for image restoration",
        "authors": [
            "Julien Mairal",
            "Francis Bach",
            "Jean Ponce",
            "Guillermo Sapiro",
            "Andrew Zisserman"
        ],
        "abstract": "We propose in this paper to unify two different approaches to image restoration: On the one hand, learning a basis set (dictionary) adapted to sparse signal descriptions has proven to be very effective in image reconstruction and classification tasks. On the other hand, explicitly exploiting the self-similarities of natural images has led to the successful non-local means approach to image restoration. We propose simultaneous sparse coding as a framework for combining these two approaches in a natural manner. This is achieved by jointly decomposing groups of similar signals on subsets of the learned dictionary. Experimental results in image denoising and demosaicking tasks with synthetic and real noise show that the proposed method outperforms the state of the art, making it possible to effectively restore raw images from digital cameras at a reasonable speed and memory cost.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459452",
        "reference_list": [],
        "citation": {
            "ieee": 450,
            "other": 223,
            "total": 673
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Dictionaries",
                "Image reconstruction",
                "Digital cameras",
                "Noise reduction",
                "Signal restoration",
                "Color",
                "Image sensors",
                "Matched filters",
                "Filtering"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image coding",
                "image denoising",
                "image reconstruction",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonlocal sparse models",
                "image restoration",
                "sparse signal descriptions",
                "image reconstruction",
                "image classification",
                "simultaneous sparse coding",
                "learned dictionary",
                "image denoising",
                "image demosaicking",
                "digital cameras"
            ]
        },
        "id": 292,
        "cited_by": [
            {
                "year": "2017",
                "id": 25
            },
            {
                "year": "2017",
                "id": 115
            },
            {
                "year": "2017",
                "id": 179
            },
            {
                "year": "2017",
                "id": 181
            },
            {
                "year": "2015",
                "id": 27
            },
            {
                "year": "2015",
                "id": 35
            },
            {
                "year": "2015",
                "id": 38
            },
            {
                "year": "2015",
                "id": 49
            },
            {
                "year": "2015",
                "id": 67
            },
            {
                "year": "2013",
                "id": 133
            },
            {
                "year": "2013",
                "id": 422
            },
            {
                "year": "2011",
                "id": 36
            },
            {
                "year": "2011",
                "id": 59
            },
            {
                "year": "2011",
                "id": 304
            }
        ]
    },
    {
        "title": "Weighted graph characteristics from oriented line graph polynomials",
        "authors": [
            "Peng Ren",
            "Richard C. Wilson",
            "Edwin R. Hancock"
        ],
        "abstract": "We develop a novel method for extracting graph characteristics from edge-weighted graphs, based on an extension of the Ihara zeta function from unweighted to edge-weighted graphs. This is effected by generalizing the determinant form of the Ihara zeta function. We use the set of the reciprocal polynomial coefficients of the resulting Ihara zeta function, i.e. the Ihara coefficients, to construct our characterization. We also present a spectral analysis of the edge-weighted graph Ihara coefficients and indicate their advantages over graph spectral methods. Experimental results reveal that the Ihara coefficients are effective for the purpose of clustering edge-weighted graphs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459449",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2001",
                "id": 102
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Graph theory",
                "Computer vision",
                "Laplace equations",
                "Shape",
                "Object recognition",
                "Pattern recognition",
                "Layout",
                "Computer science",
                "Spectral analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image processing",
                "pattern clustering",
                "polynomials"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weighted graph characteristics",
                "oriented line graph polynomials",
                "edge-weighted graphs",
                "Ihara zeta function",
                "reciprocal polynomial coefficients"
            ]
        },
        "id": 293,
        "cited_by": []
    },
    {
        "title": "Shape guided contour grouping with particle filters",
        "authors": [
            "ChengEn Lu",
            "Longin Jan Latecki",
            "Nagesh Adluru",
            "Xingwei Yang",
            "Haibin Ling"
        ],
        "abstract": "We propose a novel framework for contour based object detection and recognition, which we formulate as a joint contour fragment grouping and labeling problem. For a given set of contours of model shapes, we simultaneously perform selection of relevant contour fragments in edge images, grouping of the selected contour fragments, and their matching to the model contours. The inference in all these steps is performed using particle filters (PF) but with static observations. Our approach needs one example shape per class as training data. The PF framework combined with decomposition of model contour fragments to part bundles allows us to implement an intuitive search strategy for the target contour in a clutter of edge fragments. First a rough sketch of the model shape is identified, followed by fine tuning of shape details. We show that this framework yields not only accurate object detections but also localizations in real cluttered images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459446",
        "reference_list": [
            {
                "year": "2007",
                "id": 81
            },
            {
                "year": "2007",
                "id": 144
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2005",
                "id": 64
            },
            {
                "year": "2007",
                "id": 3
            },
            {
                "year": "2007",
                "id": 80
            },
            {
                "year": "2007",
                "id": 182
            },
            {
                "year": "2007",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 0,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Particle filters",
                "Image edge detection",
                "Object detection",
                "Image recognition",
                "Humans",
                "Computer vision",
                "Labeling",
                "Training data",
                "Visual perception"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "object recognition",
                "particle filtering (numerical methods)",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape guided contour grouping",
                "particle filters",
                "contour based object detection",
                "contour based object recognition",
                "edge images",
                "model contour fragments",
                "intuitive search strategy",
                "target contour",
                "edge fragments",
                "contour fragment grouping problem",
                "contour fragment labeling problem",
                "object localization",
                "cluttered images"
            ]
        },
        "id": 294,
        "cited_by": [
            {
                "year": "2015",
                "id": 179
            }
        ]
    },
    {
        "title": "An algebraic model for fast corner detection",
        "authors": [
            "Andrew Willis",
            "Yunfeng Sui"
        ],
        "abstract": "This paper revisits the classical problem of detecting interest points, popularly known as \"corners\", in 2D images by proposing a technique based on fitting algebraic shape models to contours in the edge image. Our method for corner detection is targeted for use on structural images, i.e., images that contain man-made structures for which corner detection algorithms are known to perform well. Further, our detector seeks to find image regions that contain two distinct linear contours that intersect. We define the intersection point as the corner, and, in contrast to previous approaches such as the Harris detector, we consider the spatial coherence of the edge points, i.e., the fact that the edge points must lie close to one of the two intersecting lines, an important aspect to stable corner detection. Comparisons between results for the proposed method and that for several popular feature detectors are provided using input images exhibiting a number of standard image variations, including blurring, affine transformation, scaling, rotation, and illumination variation. A modified version of the repeatability rate is proposed for evaluating the stability of the detector under these variations which requires a 1-to-1 mapping between matched features. Using this performance metric, our method is found to perform well in contrast to several current methods for corner detection. Discussion is provided that motivates our method of evaluation and provides an explanation for the observed performance of our algorithm in contrast to other algorithms. Our approach is distinct from other contour-based methods since we need only compute the edge image, from which we explicitly solve for the unknown linear contours and their intersections that provide image corner location estimates. The key benefits to this approach are: (1) performance (in space and time); since no image pyramid (space) and no edge-linking (time) is required and (2) compactness; the estimated model includes the co...",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459443",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 6,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Detectors",
                "Solid modeling",
                "Shape",
                "Detection algorithms",
                "Spatial coherence",
                "Computer vision",
                "Lighting",
                "Stability",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "algebra",
                "computational geometry",
                "edge detection",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "corner detection algorithm",
                "algebraic shape model",
                "edge image",
                "structural image",
                "linear contour",
                "intersection point",
                "spatial coherence",
                "edge point",
                "blurring",
                "image scaling",
                "image rotation",
                "affine transformation",
                "illumination variation",
                "repeatability rate",
                "feature detector"
            ]
        },
        "id": 295,
        "cited_by": []
    },
    {
        "title": "Seeing through water: Image restoration using model-based tracking",
        "authors": [
            "Yuandong Tian",
            "Srinivasa G. Narasimhan"
        ],
        "abstract": "A video sequence of an underwater scene taken from above the water surface suffers from severe distortions due to water fluctuations. In this paper, we simultaneously estimate the shape of the water surface and recover the planar underwater scene without using any calibration patterns, image priors, multiple viewpoints or active illumination. The key idea is to build a compact spatial distortion model of the water surface using the wave equation. Based on this model, we present a novel tracking technique that is designed specifically for water surfaces and addresses two unique challenges\u2014the absence of an object model or template and the presence of complex appearance changes in the scene due to water fluctuation. We show the effectiveness of our approach on both simulated and real scenes, with text and texture.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459440",
        "reference_list": [
            {
                "year": "2005",
                "id": 205
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 10,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Layout",
                "Underwater tracking",
                "Fluctuations",
                "Video sequences",
                "Shape",
                "Calibration",
                "Lighting",
                "Surface waves",
                "Partial differential equations"
            ]
        },
        "id": 296,
        "cited_by": [
            {
                "year": "2015",
                "id": 377
            },
            {
                "year": "2011",
                "id": 44
            },
            {
                "year": "2011",
                "id": 142
            }
        ]
    },
    {
        "title": "Piecewise-consistent color mappings of images acquired under various conditions",
        "authors": [
            "Sefy Kagarlitsky",
            "Yael Moses",
            "Yacov Hel-Or"
        ],
        "abstract": "Many applications in computer vision require comparisons between two images of the same scene. Comparison applications usually assume that corresponding regions in the two images have similar colors. However, this assumption is not always true. One way to deal with this problem is to apply a color mapping to one of the images. In this paper we address the challenge of computing color mappings between pairs of images acquired under different acquisition conditions, and possibly by different cameras. For images taken from different viewpoints, our proposed method overcomes the lack of pixel correspondence. For images taken under different illumination, we show that no single color mapping exists, and we address and solve a new problem of computing a minimal set of piecewise color mappings. When both viewpoint and illumination vary, our method can only handle planar regions of the scene. In this case, the scene planar regions are simultaneously co-segmented in the two images, and piecewise color mappings for these regions are calculated. We demonstrate applications of the proposed method for each of these cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459437",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 6,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Statistical distributions",
                "Pixel",
                "Image denoising",
                "Markov random fields",
                "Application software",
                "Computer science",
                "Educational institutions",
                "Probability distribution",
                "Random number generation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "piecewise-consistent color mapping",
                "computer vision",
                "illumination"
            ]
        },
        "id": 297,
        "cited_by": []
    },
    {
        "title": "A global perspective on MAP inference for low-level vision",
        "authors": [
            "Oliver J. Woodford",
            "Carsten Rother",
            "Vladimir Kolmogorov"
        ],
        "abstract": "In recent years the Markov Random Field (MRF) has become the de facto probabilistic model for low-level vision applications. However, in a maximum a posteriori (MAP) framework, MRFs inherently encourage delta function marginal statistics. By contrast, many low-level vision problems have heavy tailed marginal statistics, making the MRF model unsuitable. In this paper we introduce a more general Marginal Probability Field (MPF), of which the MRF is a special, linear case, and show that convex energy MPFs can be used to encourage arbitrary marginal statistics. We introduce a flexible, extensible framework for effectively optimizing the resulting NP-hard MAP problem, based around dual-decomposition and a modified min-cost flow algorithm, and which achieves global optimality in some instances. We use a range of applications, including image denoising and texture synthesis, to demonstrate the benefits of this class of MPF over MRFs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459434",
        "reference_list": [
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2007",
                "id": 51
            },
            {
                "year": "2009",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 9,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Statistical distributions",
                "Pixel",
                "Image denoising",
                "Markov random fields",
                "Application software",
                "Computer science",
                "Educational institutions",
                "Probability distribution",
                "Random number generation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "image denoising",
                "image texture",
                "Markov processes",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "low-level vision",
                "Markov random field",
                "probabilistic model",
                "maximum a posteriori framework",
                "delta function marginal statistics",
                "texture synthesis",
                "image denoising",
                "NP-hard MAP problem"
            ]
        },
        "id": 298,
        "cited_by": [
            {
                "year": "2013",
                "id": 38
            },
            {
                "year": "2011",
                "id": 24
            },
            {
                "year": "2009",
                "id": 96
            }
        ]
    },
    {
        "title": "Riemannian Bayesian estimation of diffusion tensor images",
        "authors": [
            "Kai Krajsek",
            "Marion I. Menzel",
            "Hanno Scharr"
        ],
        "abstract": "Diffusion tensor magnetic resonance imaging (DT-MRI) is a non-invasive imaging technique allowing to estimate the molecular self-diffusion tensors of water within surrounding tissue. Due to the low signal-to-noise ratio of magnetic resonance images, reconstructed tensor images usually require some sort of regularization in a post-processing step. Previous approaches are either suboptimal with respect to the reconstructing or regularization step. This paper presents a Bayesian approach for simultaneous reconstructing and regularization of DT-MR images that allows to resolve the disadvantages of previous approaches. To this end, estimation theoretical concepts are generalized to tensor valued images that are considered as Riemannian manifolds. Doing so allows us to derive a maximum a posterior estimator of the tensor image that considers both the statistical characteristics of the Rician noise occurring in MR images as well as the nonlinear structure of tensor valued images. Experiments on synthetic data as well as real DT-MRI data validate the advantage of considering both statistical as well as geometrical characteristics of DT-MRI.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459431",
        "reference_list": [
            {
                "year": "2007",
                "id": 333
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Tensile stress",
                "Diffusion tensor imaging",
                "Image reconstruction",
                "Magnetic resonance imaging",
                "Signal to noise ratio",
                "Magnetic resonance",
                "Image resolution",
                "Signal resolution",
                "Estimation theory"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "biological tissues",
                "biomedical MRI",
                "estimation theory",
                "image reconstruction",
                "manifolds"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Riemannian Bayesian estimation",
                "diffusion tensor magnetic resonance imaging",
                "non-invasive imaging",
                "tissue",
                "signal-to-noise ratio",
                "tensor images reconstruction",
                "Riemannian manifolds",
                "Rician noise occurring"
            ]
        },
        "id": 299,
        "cited_by": []
    },
    {
        "title": "Ground truth dataset and baseline evaluations for intrinsic image algorithms",
        "authors": [
            "Roger Grosse",
            "Micah K. Johnson",
            "Edward H. Adelson",
            "William T. Freeman"
        ],
        "abstract": "The intrinsic image decomposition aims to retrieve \u201cintrinsic\u201d properties of an image, such as shading and reflectance. To make it possible to quantitatively compare different approaches to this problem in realistic settings, we present a ground-truth dataset of intrinsic image decompositions for a variety of real-world objects. For each object, we separate an image of it into three components: Lambertian shading, reflectance, and specularities. We use our dataset to quantitatively compare several existing algorithms; we hope that this dataset will serve as a means for evaluating future work on intrinsic images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459428",
        "reference_list": [
            {
                "year": "2001",
                "id": 90
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 97,
            "other": 56,
            "total": 153
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Reflectivity",
                "Image decomposition",
                "Computer vision",
                "Shape",
                "Layout",
                "Geometry",
                "Information retrieval",
                "Image retrieval",
                "Light sources"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ground truth dataset",
                "intrinsic image algorithms",
                "ground-truth dataset",
                "intrinsic image decompositions",
                "Lambertian shading"
            ]
        },
        "id": 300,
        "cited_by": [
            {
                "year": "2017",
                "id": 413
            },
            {
                "year": "2015",
                "id": 19
            },
            {
                "year": "2015",
                "id": 43
            },
            {
                "year": "2015",
                "id": 48
            },
            {
                "year": "2015",
                "id": 63
            },
            {
                "year": "2015",
                "id": 69
            },
            {
                "year": "2015",
                "id": 90
            },
            {
                "year": "2015",
                "id": 333
            },
            {
                "year": "2015",
                "id": 387
            },
            {
                "year": "2013",
                "id": 30
            }
        ]
    },
    {
        "title": "Image compression with anisotropic triangulations",
        "authors": [
            "S\u00e9bastien Bougleux",
            "Gabriel Peyr\u00e9",
            "Laurent D. Cohen"
        ],
        "abstract": "We propose a new image compression method based on geodesic Delaunay triangulations. Triangulations are generated by a progressive geodesic meshing algorithm which exploits the anisotropy of images through a farthest point sampling strategy. This seeding is performed according to anisotropic geodesic distances which force the anisotropic Delaunay triangles to follow the geometry of the image. Geodesic computations are performed using a Riemannian Fast Marching, which recursively updates the geodesic distance to the seed points. A linear spline approximation on this triangulation allows to approximate faithfully sharp edges and directional features in images. The compression is achieved by coding both the coefficients of the spline approximation and the deviation of the geodesic triangulation from an Euclidean Delaunay triangulation. Numerical results show that taking into account the anisotropy improves the approximation by isotropic triangulations of complex images. The resulting encoder competes well with wavelet-based encoder such as JPEG-2000 on geometric images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459425",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Image coding",
                "Anisotropic magnetoresistance",
                "Image sampling",
                "Spline",
                "Linear approximation",
                "Feature extraction",
                "Geometry",
                "Geophysics computing",
                "Rate distortion theory",
                "Wavelet transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "data compression",
                "differential geometry",
                "image coding",
                "mesh generation",
                "splines (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image compression method",
                "progressive geodesic meshing algorithm",
                "sampling strategy",
                "anisotropic geodesic distances",
                "anisotropic geodesic Delaunay triangulations",
                "geometry",
                "Riemannian fast marching",
                "seeding points",
                "linear spline approximation",
                "Euclidean Delaunay triangulation",
                "image coding",
                "encoder",
                "wavelet based encoder",
                "JPEG-2000",
                "geometric images"
            ]
        },
        "id": 301,
        "cited_by": []
    },
    {
        "title": "Shape analysis with multivariate tensor-based morphometry and holomorphic differentials",
        "authors": [
            "Yalin Wang",
            "Tony F. Chan",
            "Arthur W. Toga",
            "Paul M. Thompson"
        ],
        "abstract": "In this paper, we propose multivariate tensor-based surface morphometry, a new method for surface analysis, using holomorphic differentials; we also apply it to study brain anatomy. Differential forms provide a natural way to parameterize 3D surfaces, but the multivariate statistics of the resulting surface metrics have not previously been investigated. We computed new statistics from the Riemannian metric tensors that retain the full information in the deformation tensor fields. We present the canonical holomorphic one-forms with improved numerical accuracy and computational efficiency. We applied this framework to 3D MRI data to analyze hippocampal surface morphometry in Alzheimer's Disease (AD; 12 subjects), lateral ventricular surface morphometry in HIV/AIDS (11 subjects) and biomarkers in lateral ventricles in HIV/AIDS (11 subjects). Experimental results demonstrated that our method powerfully detected brain surface abnormalities. Multivariate statistics on the local tensors outperformed other TBM methods including analysis of the Jacobian determinant, the largest eigenvalue, or the pair of eigenvalues, of the surface Jacobian matrix.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459422",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Tensile stress",
                "Brain",
                "Statistics",
                "Human immunodeficiency virus",
                "Acquired immune deficiency syndrome",
                "Jacobian matrices",
                "Eigenvalues and eigenfunctions",
                "Anatomy",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain",
                "computational geometry",
                "diseases",
                "medical image processing",
                "shape recognition",
                "statistics",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape analysis",
                "multivariate tensor-based surface morphometry",
                "holomorphic differentials",
                "brain anatomy",
                "Riemannian metric tensors",
                "deformation tensor fields",
                "3D MRI data",
                "hippocampal surface morphometry",
                "alzheimer disease",
                "lateral ventricular surface morphometry",
                "biomarkers",
                "lateral ventricles",
                "brain surface abnormalities",
                "Jacobian determinant",
                "surface Jacobian matrix",
                "HIV",
                "multivariate statistics",
                "surface analysis",
                "AIDS"
            ]
        },
        "id": 302,
        "cited_by": []
    },
    {
        "title": "Packing bag-of-features",
        "authors": [
            "Herv\u00e9 J\u00e9gou",
            "Matthijs Douze",
            "Cordelia Schmid"
        ],
        "abstract": "One of the main limitations of image search based on bag-of-features is the memory usage per image. Only a few million images can be handled on a single machine in reasonable response time. In this paper, we first evaluate how the memory usage is reduced by using lossless index compression. We then propose an approximate representation of bag-of-features obtained by projecting the corresponding histogram onto a set of pre-defined sparse projection functions, producing several image descriptors. Coupled with a proper indexing structure, an image is represented by a few hundred bytes. A distance expectation criterion is then used to rank the images. Our method is at least one order of magnitude faster than standard bag-of-features while providing excellent search quality.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459419",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 54,
            "other": 37,
            "total": 91
        },
        "keywords": {
            "IEEE Keywords": [
                "Image coding",
                "Indexing",
                "File systems",
                "Delay",
                "Histograms",
                "Vocabulary",
                "Image retrieval",
                "Large-scale systems",
                "Image databases",
                "Binary codes"
            ],
            "INSPEC: Controlled Indexing": [
                "data compression",
                "image coding",
                "indexing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image search",
                "lossless index compression",
                "histogram",
                "sparse projection function",
                "image descriptors",
                "indexing structure",
                "distance expectation criterion"
            ]
        },
        "id": 303,
        "cited_by": [
            {
                "year": "2013",
                "id": 156
            },
            {
                "year": "2013",
                "id": 441
            }
        ]
    },
    {
        "title": "Studying brain morphometry using conformal equivalence class",
        "authors": [
            "Yalin Wang",
            "Wei Dai",
            "Yi-Yu Chou",
            "Xianfeng Gu",
            "Tony F. Chan",
            "Arthur W. Toga",
            "Paul M. Thompson"
        ],
        "abstract": "Two surfaces are conformally equivalent if there exists a bijective angle-preserving map between them. The Teichm\u00fcller space for surfaces with the same topology is a finite-dimensional manifold, where each point represents a conformal equivalence class, and the conformal map is homotopic to the identity map. In this paper, we propose a novel method to apply conformal equivalence based shape index to study brain morphometry. The shape index is defined based on Teichm\u00fcller space coordinates. It is intrinsic, and invariant under conformal transformations, rigid motions and scaling. It is also simple to compute; no registration of surfaces is needed. Using the Yamabe flow method, we can conformally map a genus-zero open boundary surface to the Poincar\u00e9 disk. The shape indices that we compute are the lengths of a special set of geodesics under hyperbolic metric. By computing and studying this shape index and its statistical behavior, we can analyze differences in anatomical morphometry due to disease or development. Study on twin lateral ventricular surface data shows it may help detect generic influence on lateral ventricular shapes. In leave-one-out validation tests, we achieved 100% accurate classification (versus only 68% accuracy for volume measures) in distinguishing 11 HIV/AIDS individuals from 8 healthy control subjects, based on Teichm\u00fcller coordinates for lateral ventricular surfaces extracted from their 3D MRI scans.Our conformal invariants, the Teichm\u00fcller coordinates, successfully classified all lateral ventricular surfaces, showing their promise for analyzing anatomical surface morphometry.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459416",
        "reference_list": [
            {
                "year": "2007",
                "id": 188
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Geophysics computing",
                "Topology",
                "Diseases",
                "Testing",
                "Coordinate measuring machines",
                "Volume measurement",
                "Human immunodeficiency virus",
                "Acquired immune deficiency syndrome",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain models",
                "diseases",
                "medical computing",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "brain morphometry",
                "conformal equivalence class",
                "conformal equivalence based 3D shape index",
                "Teichmilller space coordinates",
                "conformal transformations",
                "rigid motions",
                "Yamabe flow method",
                "genus-zero open boundary surface",
                "Poincare disk",
                "hyperbolic metric",
                "statistical analysis",
                "anatomical morphometry",
                "disease",
                "lateral ventricular surface data",
                "lateral ventricular shapes",
                "leave-one-out validation tests",
                "HIV-AIDS individuals",
                "healthy control subjects",
                "Teichmilller coordinates",
                "3D MRI scans",
                "anatomical surface morphometry"
            ]
        },
        "id": 304,
        "cited_by": []
    },
    {
        "title": "Large-scale privacy protection in Google Street View",
        "authors": [
            "Andrea Frome",
            "German Cheung",
            "Ahmad Abdulkader",
            "Marco Zennaro",
            "Bo Wu",
            "Alessandro Bissacco",
            "Hartwig Adam",
            "Hartmut Neven",
            "Luc Vincent"
        ],
        "abstract": "The last two years have witnessed the introduction and rapid expansion of products based upon large, systematically-gathered, street-level image collections, such as Google Street View, EveryScape, and Mapjack. In the process of gathering images of public spaces, these projects also capture license plates, faces, and other information considered sensitive from a privacy standpoint. In this work, we present a system that addresses the challenge of automatically detecting and blurring faces and license plates for the purpose of privacy protection in Google Street View. Though some in the field would claim face detection is \u201csolved\u201d, we show that state-of-the-art face detectors alone are not sufficient to achieve the recall desired for large-scale privacy protection. In this paper we present a system that combines a standard sliding-window detector tuned for a high recall, low-precision operating point with a fast post-processing stage that is able to remove additional false positives by incorporating domain-specific information not available to the sliding-window detector. Using a completely automatic system, we are able to sufficiently blur more than 89% of faces and 94 - 96% of license plates in evaluation sets sampled from Google Street View imagery.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459413",
        "reference_list": [],
        "citation": {
            "ieee": 37,
            "other": 26,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Large-scale systems",
                "Privacy",
                "Protection",
                "Face detection",
                "Licenses",
                "Detectors",
                "Computer vision",
                "Continents",
                "Automatic control",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "data privacy",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Google Street View",
                "EveryScape",
                "Mapjack",
                "image gathering",
                "face detection",
                "face blurring",
                "privacy protection",
                "face detectors",
                "sliding-window detector"
            ]
        },
        "id": 305,
        "cited_by": [
            {
                "year": "2013",
                "id": 355
            }
        ]
    },
    {
        "title": "Efficient, high-quality image contour detection",
        "authors": [
            "Bryan Catanzaro",
            "Bor-Yiing Su",
            "Narayanan Sundaram",
            "Yunsup Lee",
            "Mark Murphy",
            "Kurt Keutzer"
        ],
        "abstract": "Image contour detection is fundamental to many image analysis applications, including image segmentation, object recognition and classification. However, highly accurate image contour detection algorithms are also very computationally intensive, which limits their applicability, even for offline batch processing. In this work, we examine efficient parallel algorithms for performing image contour detection, with particular attention paid to local image analysis as well as the generalized eigensolver used in Normalized Cuts. Combining these algorithms into a contour detector, along with careful implementation on highly parallel, commodity processors from Nvidia, our contour detector provides uncompromised contour accuracy, with an F-metric of 0.70 on the Berkeley Segmentation Dataset. Runtime is reduced from 4 minutes to 1.8 seconds. The efficiency gains we realize enable high-quality image contour detection on much larger images than previously practical, and the algorithms we propose are applicable to several image segmentation approaches. Efficient, scalable, yet highly accurate image contour detection will facilitate increased performance in many computer vision applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459410",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 29,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image analysis",
                "Detectors",
                "Object detection",
                "Object recognition",
                "Detection algorithms",
                "Parallel algorithms",
                "Runtime",
                "Computer vision",
                "Application software"
            ]
        },
        "id": 306,
        "cited_by": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2013",
                "id": 272
            }
        ]
    },
    {
        "title": "Estimating contact dynamics",
        "authors": [
            "Marcus A. Brubaker",
            "Leonid Sigal",
            "David J. Fleet"
        ],
        "abstract": "Motion and interaction with the environment are fundamentally intertwined. Few people-tracking algorithms exploit such interactions, and those that do assume that surface geometry and dynamics are given. This paper concerns the converse problem, i.e., the inference of contact and environment properties from motion. For 3D human motion, with a 12-segment articulated body model, we show how one can estimate the forces acting on the body in terms of internal forces (joint torques), gravity, and the parameters of a contact model (e.g., the geometry and dynamics of a spring-based model). This is tested on motion capture data and video-based tracking data, with walking, jogging, cartwheels, and jumping.",
        "ieee_link": "https://ieeexplore.ieee.org/document/5459407",
        "reference_list": [
            {
                "year": "2007",
                "id": 252
            }
        ],
        "citation": {
            "ieee": 30,
            "other": 9,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Biological system modeling",
                "Solid modeling",
                "Geometry",
                "Inference algorithms",
                "Humans",
                "Motion estimation",
                "Joints",
                "Gravity",
                "Testing",
                "Tracking"
            ]
        },
        "id": 307,
        "cited_by": [
            {
                "year": "2017",
                "id": 197
            },
            {
                "year": "2015",
                "id": 84
            },
            {
                "year": "2011",
                "id": 262
            }
        ]
    }
]