[
    {
        "title": "Segmentation of hybrid motions via hybrid quadratic surface analysis",
        "authors": [
            "S.R. Rao",
            "A.Y. Yang",
            "A.W. Wagner",
            "Yi Ma"
        ],
        "abstract": "In this paper, we investigate the mathematical problem underlying segmentation of hybrid motions: given a series of tracked feature correspondences between two (perspective) images, we seek to segment and estimate multiple motions, possibly of different types (e.g., affine, epipolar, and homography). In order to accomplish this task, we cast the problem into a more general mathematical framework of segmenting data samples drawn from a mixture of linear subspaces and quadratic surfaces. The result is a novel algorithm called hybrid quadratic surface analysis (HQSA). HQSA uses both the derivatives and Hessians of fitting polynomials for the data to separate linear data samples from quadratic data samples. These derivatives and Hessians also lead to important necessary conditions, based on the so-called mutual contraction subspace, to separate data samples on different quadratic surfaces. The algebraic solution we derive is non-iterative and numerically stable. It tolerates moderate noise and can be used in conjunction with outlier removal techniques. We show how to solve the hybrid motion segmentation problem using HQSA, and demonstrate its performance on simulated data with noise and on real perspective images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541232",
        "reference_list": [
            {
                "year": "2001",
                "id": 185
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 5,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion analysis",
                "Image segmentation",
                "Surface fitting",
                "Image motion analysis",
                "Tracking",
                "Motion estimation",
                "Algorithm design and analysis",
                "Polynomials",
                "Motion segmentation",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "motion estimation",
                "image sampling",
                "Hessian matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hybrid motion segmentation",
                "hybrid quadratic surface analysis",
                "linear subspace",
                "fitting polynomial",
                "linear data sample",
                "quadratic data sample",
                "mutual contraction subspace",
                "motion estimation",
                "data segmentation"
            ]
        },
        "id": 0,
        "cited_by": []
    },
    {
        "title": "Detection and tracking of moving objects from a moving platform in presence of strong parallax",
        "authors": [
            "Jinman Kang",
            "I. Cohen",
            "G. Medioni",
            "Chang Yuan"
        ],
        "abstract": "We present a novel approach to detect and track independently moving regions in a 3D scene observed by a moving camera in the presence of strong parallax. Detected moving pixels are classified into independently moving regions or parallax regions by analyzing two geometric constraints: the commonly used epipolar constraint, and the structure consistency constraint. The second constraint is implemented within a \"plane+parallax\" framework and represented by a bilinear relationship which relates the image points to their relative depths. This newly derived relationship is related to trilinear tensor, but can be enforced into more than three frames. It does not assume a constant reference plane in the scene and therefore eliminates the need for manual selection of reference plane. Then, a robust parallax filtering scheme is proposed to accumulate the geometric constraint errors within a sliding window and estimate a likelihood map for pixel classification. The likelihood map is integrated into our tracking framework based on the spatio-temporal joint probability data association filter (JPDAF). This tracking approach infers the trajectory and bounding box of the moving objects by searching the optimal path with maximum joint probability within a fixed size of buffer. We demonstrate the performance of the proposed approach on real video sequences where parallax effects are significant.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541233",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 3,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Layout",
                "Intelligent robots",
                "Robustness",
                "Motion detection",
                "Parallel robots",
                "Intelligent systems",
                "Robot vision systems",
                "Smart cameras",
                "Tensile stress"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "image classification",
                "object detection",
                "video signal processing",
                "tracking filters",
                "cameras",
                "geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "moving object detection",
                "moving object tracking",
                "parallax region filtering",
                "3D scene moving region",
                "moving pixel detection",
                "geometric constraint",
                "epipolar constraint",
                "structure consistency constraint",
                "plane+parallax framework",
                "trilinear tensor",
                "sliding window",
                "likelihood map",
                "pixel classification",
                "joint probability data association filter",
                "optimal path searching",
                "video sequence"
            ]
        },
        "id": 1,
        "cited_by": []
    },
    {
        "title": "Simultaneous multiple 3D motion estimation via mode finding on Lie groups",
        "authors": [
            "O. Tuzel",
            "R. Subbarao",
            "P. Meer"
        ],
        "abstract": "We propose a new method to estimate multiple rigid motions from noisy 3D point correspondences in the presence of outliers. The method does not require prior specification of number of motion groups and estimates all the motion parameters simultaneously. We start with generating samples from the rigid motion distribution. The motion parameters are then estimated via mode finding operations on the sampled distribution. Since rigid motions do not lie on a vector space, classical statistical methods can not be used for mode finding. We develop a mean shift algorithm which estimates modes of the sampled distribution using the Lie group structure of the rigid motions. We also show that proposed mean shift algorithm is general and can be applied to any distribution having a matrix Lie group structure. Experimental results on synthetic and real image data demonstrate the superior performance of the algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541234",
        "reference_list": [],
        "citation": {
            "ieee": 32,
            "other": 16,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Parameter estimation",
                "Motion detection",
                "Stereo vision",
                "Iterative methods",
                "Iterative algorithms",
                "Statistical analysis",
                "Computer vision",
                "Singular value decomposition",
                "Least squares approximation"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "Lie groups",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D motion estimation",
                "mode finding",
                "matrix Lie group structure",
                "multiple rigid motion estimation",
                "motion parameter",
                "rigid motion distribution",
                "vector space",
                "mean shift algorithm"
            ]
        },
        "id": 2,
        "cited_by": [
            {
                "year": "2009",
                "id": 6
            }
        ]
    },
    {
        "title": "Separating transparent layers of repetitive dynamic behaviors",
        "authors": [
            "B. Sarel",
            "M. Irani"
        ],
        "abstract": "In this paper, we present an approach for separating two transparent layers of complex nonrigid scene dynamics. The dynamics in one of the layers is assumed to be repetitive, while the other can have any arbitrary dynamics. Such repetitive dynamics includes, among other, human actions in video (e.g., a walking person), or a repetitive musical tune in audio signals. We use a global to local space time alignment approach to detect and align the repetitive behavior. Once aligned, a median operator applied to space time derivatives is used to recover the intrinsic repeating behavior, and separate it from the other transparent layer. We show results on synthetic and real video sequences. In addition, we show the applicability of our approach to separating mixed audio signals (from a single source).",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541235",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 3,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Legged locomotion",
                "Video sequences",
                "Layout",
                "Humans",
                "Motion estimation",
                "Computer science",
                "Face detection",
                "Face recognition",
                "Image segmentation",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "video signal processing",
                "audio signal processing",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "transparent layer separation",
                "repetitive dynamic behavior",
                "complex nonrigid scene dynamics",
                "space time alignment",
                "repetitive behavior detection",
                "repetitive behavior alignment",
                "median operator",
                "space time derivative",
                "intrinsic repeating behavior",
                "video sequence",
                "mixed audio signal"
            ]
        },
        "id": 3,
        "cited_by": [
            {
                "year": "2017",
                "id": 254
            },
            {
                "year": "2017",
                "id": 413
            },
            {
                "year": "2015",
                "id": 337
            },
            {
                "year": "2013",
                "id": 303
            }
        ]
    },
    {
        "title": "Learning layered motion segmentations of video",
        "authors": [
            "M.P. Kumar",
            "P.H.S. Torr",
            "A. Zisserman"
        ],
        "abstract": "We present an unsupervised approach for learning a generative layered representation of a scene from a video for motion segmentation. The learnt model is a composition of layers, which consist of one or more segments. Included in the model are the effects of image projection, lighting, and motion blur. The two main contributions of our method are: (i) a novel algorithm for obtaining the initial estimate of the model using efficient loopy belief propagation; and (ii) using /spl alpha//spl beta/-swap and /spl alpha/-expansion algorithms, which guarantee a strong local minima, for refining the initial estimate. Results are presented on several classes of objects with different types of camera motion. We compare our method with the state of the art and demonstrate significant improvements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541236",
        "reference_list": [
            {
                "year": "2003",
                "id": 116
            },
            {
                "year": "2003",
                "id": 45
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 17,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Computer vision",
                "Cameras",
                "Video sequences",
                "Legged locomotion",
                "Sprites (computer)",
                "Shape",
                "Layout",
                "Torso",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image motion analysis",
                "video signal processing",
                "unsupervised learning",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video motion segmentation",
                "unsupervised learning",
                "image projection",
                "image lighting",
                "motion blur",
                "loopy belief propagation",
                "/spl alpha//spl beta/-swap algorithm",
                "/spl alpha/-expansion algorithm",
                "camera motion"
            ]
        },
        "id": 4,
        "cited_by": [
            {
                "year": "2009",
                "id": 95
            },
            {
                "year": "2007",
                "id": 123
            }
        ]
    },
    {
        "title": "On the spatial statistics of optical flow",
        "authors": [
            "S. Roth",
            "M.J. Black"
        ],
        "abstract": "We develop a method for learning the spatial statistics of optical flow fields from a novel training database. Training flow fields are constructed using range images of natural scenes and 3D camera motions recovered from handheld and car-mounted video sequences. A detailed analysis of optical flow statistics in natural scenes is presented and machine learning methods are developed to learn a Markov random field model of optical flow. The prior probability of a flow field is formulated as a field-of-experts model that captures the higher order spatial statistics in overlapping patches and is trained using contrastive divergence. This new optical flow prior is compared with previous robust priors and is incorporated into a recent, accurate algorithm for dense optical flow computation. Experiments with natural and synthetic sequences illustrate how the learned optical flow prior quantitatively improves flow accuracy and how it captures the rich spatial structure found in natural scene motion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541237",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 21,
            "total": 61
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Layout",
                "Higher order statistics",
                "Optical computing",
                "Image databases",
                "Spatial databases",
                "Cameras",
                "Video sequences",
                "Statistical analysis",
                "Learning systems"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "video signal processing",
                "image motion analysis",
                "learning (artificial intelligence)",
                "Markov processes",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optical flow",
                "training database",
                "training flow field",
                "range image",
                "natural scene motion",
                "3D camera motion",
                "car-mounted video sequence",
                "machine learning",
                "Markov random field",
                "spatial statistics"
            ]
        },
        "id": 5,
        "cited_by": [
            {
                "year": "2015",
                "id": 449
            },
            {
                "year": "2011",
                "id": 304
            }
        ]
    },
    {
        "title": "Sparse image coding using a 3D non-negative tensor factorization",
        "authors": [
            "T. Hazan",
            "S. Polak",
            "A. Shashua"
        ],
        "abstract": "We introduce an algorithm for a non-negative 3D tensor factorization for the purpose of establishing a local parts feature decomposition from an object class of images. In the past, such a decomposition was obtained using non-negative matrix factorization (NMF) where images were vectorized before being factored by NMF. A tensor factorization (NTF) on the other hand preserves the 2D representations of images and provides a unique factorization (unlike NMF which is not unique). The resulting \"factors\" from the NTF factorization are both sparse (like with NMF) but also separable allowing efficient convolution with the test image. Results show a superior decomposition to what an NMF can provide on all fronts - degree of sparsity, lack of ghost residue due to invariant parts and efficiency of coding of around an order of magnitude better. Experiments on using the local parts decomposition for face detection using SVM and Adaboost classifiers demonstrate that the recovered features are discriminatory and highly effective for classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541238",
        "reference_list": [],
        "citation": {
            "ieee": 58,
            "other": 33,
            "total": 91
        },
        "keywords": {
            "IEEE Keywords": [
                "Image coding",
                "Tensile stress",
                "Filters",
                "Principal component analysis",
                "Convolution",
                "Face detection",
                "Face recognition",
                "Independent component analysis",
                "Computer science",
                "Matrix decomposition"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "tensors",
                "feature extraction",
                "image classification",
                "matrix decomposition",
                "sparse matrices",
                "face recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparse image coding",
                "3D nonnegative tensor factorization",
                "feature decomposition",
                "image object class",
                "nonnegative matrix factorization",
                "image vector",
                "image representation",
                "unique factorization",
                "image convolution",
                "face detection",
                "support vector machine",
                "Adaboost classifier"
            ]
        },
        "id": 6,
        "cited_by": [
            {
                "year": "2015",
                "id": 8
            }
        ]
    },
    {
        "title": "Perceptual scale space and its applications",
        "authors": [
            "Yizhou Wang",
            "S. Bahrami",
            "Song-Chun Zhu"
        ],
        "abstract": "In this paper, we study a perceptual scale space by constructing a so-called sketch pyramid which augments the Gaussian and Laplacian pyramid representations in traditional image scale space theory. Each level of this sketch pyramid is a generic attributed graph - called the primal sketch which is inferred from the corresponding image at the same level of the Gaussian pyramid. When images are viewed at increasing resolutions, more details are revealed. This corresponds to perceptual transitions which are represented by topological changes in the sketch graph in terms of a graph grammar. We compute the sketch or perceptual pyramid by Bayesian inference upwards-downwards the pyramid using Markov chain Monte Carlo reversible jumps. We show two example applications of this perceptual scale space: (1) motion tracking of objects over scales, and (2) adaptive image displays which can efficiently show a large high resolution image in a small screen (of a PDA for example) through a selective tour of its image pyramid. Other potential applications include super resolution and multiresolution object recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541239",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Laplace equations",
                "Image resolution",
                "Dictionaries",
                "Application software",
                "Computer science",
                "Statistics",
                "Bayesian methods",
                "Monte Carlo methods",
                "Tracking",
                "Large screen displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image resolution",
                "Bayes methods",
                "Markov processes",
                "Monte Carlo methods",
                "Gaussian processes",
                "graph grammars",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "perceptual scale space",
                "sketch pyramid",
                "Gaussian pyramid representation",
                "Laplacian pyramid representation",
                "image scale space theory",
                "generic attributed graph",
                "primal sketch graph",
                "image resolution",
                "graph grammar",
                "perceptual pyramid",
                "Bayesian inference",
                "Markov chain",
                "Monte Carlo",
                "object motion tracking",
                "adaptive image display",
                "image pyramid",
                "multiresolution object recognition"
            ]
        },
        "id": 7,
        "cited_by": []
    },
    {
        "title": "Active search for real-time vision",
        "authors": [
            "A.J. Davison"
        ],
        "abstract": "In most cases when information is to be extracted from an image, there are priors available on the state of the world and therefore on the detailed measurements which are obtained. While such priors are commonly combined with the actual measurements via Bayes' rule to calculate posterior probability distributions on model parameters, their additional value in guiding efficient image processing has almost always been overlooked. Priors tell us where to look for information in an image, how much computational effort we can expect to expend to extract it, and of how much utility to the task in hand it is likely to be. Such considerations are of importance in all practical real time vision systems, where the processing resources available at each frame in a sequence are strictly limited - and it is exactly in high frame rate real time systems such as trackers where strong priors are most likely to be available. In this paper, we use Shannon information theory to analyse the fundamental value of measurements using mutual information scores in absolute units of bits, specifically looking at the overwhelming case where uncertainty can be characterised by Gaussian probability distributions. We then compare these measurement values with the computational cost of the image processing required to obtain them. This theory puts on a firm footing for the first time principles of 'active search' for efficient guided image processing, in which candidate features of possibly different types can be compared and selected automatically for measurement.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541240",
        "reference_list": [],
        "citation": {
            "ieee": 36,
            "other": 20,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Image processing",
                "Data mining",
                "Probability distribution",
                "Real time systems",
                "Machine vision",
                "Information theory",
                "Information analysis",
                "Mutual information",
                "Computational efficiency",
                "Time measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "information theory",
                "Gaussian distribution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active search",
                "real-time vision",
                "Shannon information theory",
                "mutual information score",
                "Gaussian probability distribution",
                "image processing"
            ]
        },
        "id": 8,
        "cited_by": []
    },
    {
        "title": "Fast recognition of multi-view faces with feature selection",
        "authors": [
            "Zhi-Gang Fan",
            "Bao-Liang Lu"
        ],
        "abstract": "We propose a discriminative feature selection method utilizing support vector machines for the challenging task of multiview face recognition. According to the statistical relationship between the two tasks, feature selection and multiclass classification, we integrate the two tasks into a single consistent framework and effectively realize the goal of discriminative feature selection. The classification process can be made faster without degrading the generalization performance through this discriminative feature selection method. On the UMIST multiview face database, our experiments show that this discriminative feature selection method can speed up the multiview face recognition process without degrading the correct rate and outperform the traditional kernel subspace methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541241",
        "reference_list": [
            {
                "year": "2003",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Kernel",
                "Principal component analysis",
                "Support vector machines",
                "Support vector machine classification",
                "Linear discriminant analysis",
                "Degradation",
                "Independent component analysis",
                "Computer science",
                "Spatial databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "feature extraction",
                "face recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview face recognition",
                "discriminative feature selection",
                "support vector machine",
                "multiclass classification",
                "multiview face database"
            ]
        },
        "id": 9,
        "cited_by": [
            {
                "year": "2017",
                "id": 146
            }
        ]
    },
    {
        "title": "Combining image regions and human activity for indirect object recognition in indoor wide-angle views",
        "authors": [
            "P. Peursum",
            "G. West",
            "S. Venkatesh"
        ],
        "abstract": "Traditional methods of object recognition are reliant on shape and so are very difficult to apply in cluttered, wide angle and low detail views such as surveillance scenes. To address this, a method of indirect object recognition is proposed, where human activity is used to infer both the location and identity of objects. No shape analysis is necessary. The concept is dubbed 'interaction signatures', since the premise is that a human interacts with objects in ways characteristic of the function of that object - for example, a person sits in a chair and drinks from a cup. The human-centred approach means that recognition is possible in low detail views and is largely invariant to the shape of objects within the same functional class. This paper implements a Bayesian network for classifying region patches with object labels, building upon our previous work in automatically segmenting and recognising a human's interactions with the objects. Experiments show that interaction signatures can successfully find and label objects in low detail views and are equally effective at recognising test objects that differ markedly in appearance from the training objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541242",
        "reference_list": [],
        "citation": {
            "ieee": 19,
            "other": 14,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Object recognition",
                "Shape",
                "Layout",
                "Surveillance",
                "Computer vision",
                "Smart homes",
                "Australia",
                "Bayesian methods",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image classification",
                "belief networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image region",
                "human activity",
                "object recognition",
                "indoor wide-angle view",
                "interaction signature",
                "human-centred approach",
                "Bayesian network",
                "region patch classification",
                "human object interaction"
            ]
        },
        "id": 10,
        "cited_by": [
            {
                "year": "2007",
                "id": 26
            }
        ]
    },
    {
        "title": "Detection of multiple, partially occluded humans in a single image by Bayesian combination of edgelet part detectors",
        "authors": [
            "Bo Wu",
            "R. Nevatia"
        ],
        "abstract": "This paper proposes a method for human detection in crowded scene from static images. An individual human is modeled as an assembly of natural body parts. We introduce edgelet features, which are a new type of silhouette oriented features. Part detectors, based on these features, are learned by a boosting method. Responses of part detectors are combined to form a joint likelihood model that includes cases of multiple, possibly inter-occluded humans. The human detection problem is formulated as maximum a posteriori (MAP) estimation. We show results on a commonly used previous dataset as well as new data sets that could not be processed by earlier methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541243",
        "reference_list": [],
        "citation": {
            "ieee": 155,
            "other": 52,
            "total": 207
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Humans",
                "Bayesian methods",
                "Detectors",
                "Face detection",
                "Shape",
                "Biological system modeling",
                "Legged locomotion",
                "Cameras",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian combination",
                "edgelet part detector",
                "human image detection",
                "crowded scene",
                "static image",
                "human image modeling",
                "silhouette oriented feature",
                "joint likelihood model",
                "interoccluded human",
                "maximum a posteriori estimation"
            ]
        },
        "id": 11,
        "cited_by": [
            {
                "year": "2013",
                "id": 15
            },
            {
                "year": "2013",
                "id": 187
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2013",
                "id": 257
            },
            {
                "year": "2011",
                "id": 308
            }
        ]
    },
    {
        "title": "A semi-supervised framework for mapping data to the intrinsic manifold",
        "authors": [
            "Haifeng Gong",
            "Chunhong Pan",
            "Qing Yang",
            "Hanqing Lu",
            "Songde Ma"
        ],
        "abstract": "This paper presents a novel scheme for manifold learning. Different from the previous work reducing data to Euclidean space which cannot handle the looped manifold well, we map the scattered data to its intrinsic parameter manifold by semisupervised learning. Given a set of partially labeled points, the map to a specified parameter manifold is computed by an iterative neighborhood average method called anchor points diffusion procedure (APD). We explore this idea on the most frequently used close formed manifolds, Stiefel manifolds whose special cases include hyper sphere and orthogonal group. The experiments show that APD can recover the underlying intrinsic parameters of points on scattered data manifold successfully.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541244",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Independent component analysis",
                "Manifolds",
                "Laplace equations",
                "Pattern recognition",
                "Scattering parameters",
                "Principal component analysis",
                "Machine learning",
                "Computer vision",
                "Pattern analysis",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "iterative methods",
                "data handling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semisupervised learning",
                "data mapping",
                "intrinsic parameter manifold",
                "manifold learning",
                "iterative neighborhood average method",
                "anchor points diffusion",
                "close formed manifold",
                "Stiefel manifold",
                "scattered data manifold"
            ]
        },
        "id": 12,
        "cited_by": []
    },
    {
        "title": "Registration of multimodal fluorescein images sequence of the retina",
        "authors": [
            "Tae Eun Choe",
            "I. Cohen"
        ],
        "abstract": "In this study, we present a Y-feature extraction method for registering color and fluorescein angiograms of the retina. The registration of multimodal fluorescein imagery requires the identification of strong geometric features in the retinal images that are invariant across modalities and to temporal grey level variations due to the propagation of the dye in the vessels. The most informative features, invariant across the considered modalities, are the locations of vessels' ramification: the so-called Y-features. We propose a Y-feature extraction method based on the local classification of image gradient information and an articulated model. An appropriate cost function is proposed for fitting the model using a gradient based approach. The fitted Y-features are subsequently matched across the images for registering the color and fluorescein images. Experimental results obtained on a large database validate the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541245",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image sequences",
                "Retina",
                "Biomedical optical imaging",
                "Geometrical optics",
                "Intelligent robots",
                "Data mining",
                "Color",
                "Angiography",
                "Image registration",
                "Intelligent systems"
            ],
            "INSPEC: Controlled Indexing": [
                "medical image processing",
                "biomedical MRI",
                "image registration",
                "image sequences",
                "image colour analysis",
                "image matching",
                "image classification",
                "feature extraction",
                "gradient methods",
                "eye"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image registration",
                "multimodal fluorescein images sequence",
                "retina color registration",
                "fluorescein angiograms",
                "retinal image",
                "grey level variation",
                "Y-feature extraction",
                "image classification",
                "gradient based approach",
                "image matching"
            ]
        },
        "id": 13,
        "cited_by": [
            {
                "year": "2007",
                "id": 241
            }
        ]
    },
    {
        "title": "Kernel-based multifactor analysis for image synthesis and recognition",
        "authors": [
            "Yang Li",
            "Yangzhou Du",
            "Xueyin Lin"
        ],
        "abstract": "In many vision problems, the appearances of the observed images, e.g. the human facial images, are often influenced by multiple underlying factors. In this paper, a kernel-based factorization framework is proposed to analyze a multifactor dataset. Specifically, we perform N-mode singular value decomposition (N-mode SVD) in a higher dimensional feature space instead of the input space by using kernel approaches. Given an input sample, its specific underlying factors which may be all absent in the training set can be extracted and translated from one sample to another by using kernel-based 'translation'. Therefore our framework is suitable for tasks of new image synthesis and underlying factor recognition. We demonstrate the capabilities of our framework on ensembles of facial images subjected to different person identities, viewpoints and illuminations with high-quality synthetic faces and high face recognition accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541246",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 1,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Image analysis",
                "Image generation",
                "Image recognition",
                "Lighting",
                "Face detection",
                "Humans",
                "Face recognition",
                "Data analysis",
                "Singular value decomposition",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "singular value decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Kernel-based multifactor analysis",
                "image synthesis",
                "image recognition",
                "kernel-based factorization",
                "multifactor dataset",
                "N-mode singular value decomposition",
                "facial image",
                "high-quality synthetic face",
                "face recognition"
            ]
        },
        "id": 14,
        "cited_by": []
    },
    {
        "title": "Non-parametric self-calibration",
        "authors": [
            "D. Nister",
            "H. Stewenius",
            "E. Grossmann"
        ],
        "abstract": "In this paper, we develop a theory of non-parametric self-calibration. Recently, schemes have been devised for non-parametric laboratory calibration, but not for self-calibration. We allow an arbitrary warp to model the intrinsic mapping, with the only restriction that the camera is central and that the intrinsic mapping has a well-defined non-singular matrix derivative at a finite number of points under study. We give a number of theoretical results, both for infinitesimal motion and finite motion, for a finite number of observations and when observing motion over a dense image, for rotation and translation. Our main result is that through observing the flow induced by three instantaneous rotations at a finite number of points of the distorted image, we can perform projective reconstruction of those image points on the undistorted image. We present some results with synthetic and real data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541247",
        "reference_list": [
            {
                "year": "2001",
                "id": 117
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Image reconstruction",
                "Layout",
                "Computer science",
                "Visualization",
                "Virtual environment",
                "Laboratories",
                "Object recognition",
                "Parametric statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "matrix algebra",
                "image motion analysis",
                "calibration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric self-calibration",
                "intrinsic mapping",
                "nonsingular matrix derivative",
                "infinitesimal motion",
                "finite motion",
                "dense image",
                "distorted image",
                "projective reconstruction"
            ]
        },
        "id": 15,
        "cited_by": [
            {
                "year": "2009",
                "id": 24
            },
            {
                "year": "2007",
                "id": 273
            },
            {
                "year": "2007",
                "id": 296
            }
        ]
    },
    {
        "title": "Basic gray level aura matrices: theory and its application to texture synthesis",
        "authors": [
            "Xuejie Qin",
            "Yee-Hong Yang"
        ],
        "abstract": "In this paper, we present a new mathematical framework for modeling texture images using independent basic gray level aura matrices (BGLAMs). We prove that independent BGLAMs are the basis of gray level aura matrices (GLAMs), and that an image can be uniquely represented by its independent BGLAMs. We propose a new BGLAM distance measure for automatically evaluating synthesis results w.r.t. input textures to determine if the output is a successful synthesis of the input. For the application to texture synthesis, we present a new algorithm to synthesize textures by sampling only the independent BGLAMs of an input texture. With respect to synthesis of textures and evaluation of the results, the performance of our approach is extensively evaluated and compared with symmetric GLAMs that are used in existing techniques and with gray level cooccurrence matrices (GLCMs). Experimental results have shown that (1) our approach significantly outperforms both symmetric GLAMs and GLCMs; (2) the new BGLAM distance measure has the ability to evaluate synthesis results, which can be used to automate the conventional visual inspection process for determining whether or not the output texture is a successful synthesis of the input; and (3) a broad range of textures can be faithfully synthesized using independent BGLAMs and the synthesis results are comparable to existing techniques",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541248",
        "reference_list": [
            {
                "year": "2003",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Symmetric matrices",
                "Sampling methods",
                "Independent component analysis",
                "Mathematical model",
                "Area measurement",
                "Inspection",
                "Probability distribution",
                "Stochastic processes",
                "Character generation",
                "Image texture analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image texture",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "basic gray level aura matrix",
                "texture synthesis",
                "gray level cooccurrence matrix",
                "BGLAM distance measure",
                "visual inspection process",
                "texture image modeling"
            ]
        },
        "id": 16,
        "cited_by": []
    },
    {
        "title": "Combining generative models and Fisher kernels for object recognition",
        "authors": [
            "A.D. Holub",
            "M. Welling",
            "P. Perona"
        ],
        "abstract": "Learning models for detecting and classifying object categories is a challenging problem in machine vision. While discriminative approaches to learning and classification have, in principle, superior performance, generative approaches provide many useful features, one of which is the ability to naturally establish explicit correspondence between model components and scene features - this, in turn, allows for the handling of missing data and unsupervised learning in clutter. We explore a hybrid generative/discriminative approach using 'Fisher kernels' by Jaakkola and Haussler (1999) which retains most of the desirable properties of generative methods, while increasing the classification performance through a discriminative setting. Furthermore, we demonstrate how this kernel framework can be used to combine different types of features and models into a single classifier. Our experiments, conducted on a number of popular benchmarks, show strong performance improvements over the corresponding generative approach and are competitive with the best results reported in the literature.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541249",
        "reference_list": [
            {
                "year": "2003",
                "id": 35
            },
            {
                "year": "2001",
                "id": 196
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 16,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Object recognition",
                "Hybrid power systems",
                "Machine learning",
                "Machine vision",
                "Solid modeling",
                "Computer vision",
                "Object detection",
                "Computer science",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "object detection",
                "feature extraction",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generative model",
                "Fisher kernel",
                "object recognition",
                "object detection",
                "object category classification",
                "machine vision",
                "missing data handling",
                "unsupervised learning"
            ]
        },
        "id": 17,
        "cited_by": [
            {
                "year": "2009",
                "id": 265
            },
            {
                "year": "2007",
                "id": 1
            }
        ]
    },
    {
        "title": "Exploring the space of a human action",
        "authors": [
            "Y. Sheikh",
            "M. Sheikh",
            "M. Shah"
        ],
        "abstract": "One of the fundamental challenges of recognizing actions is accounting for the variability that arises when arbitrary cameras capture humans performing actions. In this paper, we explicitly identify three important sources of variability: (1) viewpoint, (2) execution rate, and (3) anthropometry of actors, and propose a model of human actions that allows us to investigate all three. Our hypothesis is that the variability associated with the execution of an action can be closely approximated by a linear combination of action bases in joint spatio-temporal space. We demonstrate that such a model bounds the rank of a matrix of image measurements and that this bound can be used to achieve recognition of actions based only on imaged data. A test employing principal angles between subspaces that is robust to statistical fluctuations in measurement data is presented to find the membership of an instance of an action. The algorithm is applied to recognize several actions, and promising results have been obtained.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541250",
        "reference_list": [
            {
                "year": "2003",
                "id": 123
            }
        ],
        "citation": {
            "ieee": 51,
            "other": 39,
            "total": 90
        },
        "keywords": {
            "IEEE Keywords": [
                "Space exploration",
                "Humans",
                "Cameras",
                "Computer vision",
                "Anthropometry",
                "Object recognition",
                "Laboratories",
                "Computer science",
                "Linear approximation",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image motion analysis",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human action recognition",
                "execution rate",
                "actor anthropometry",
                "spatio-temporal space",
                "image measurement",
                "principal angle",
                "statistical fluctuation",
                "image recognition"
            ]
        },
        "id": 18,
        "cited_by": [
            {
                "year": "2007",
                "id": 171
            }
        ]
    },
    {
        "title": "Recognizing human actions in videos acquired by uncalibrated moving cameras",
        "authors": [
            "A. Yilmaz",
            "M. Shah"
        ],
        "abstract": "Most work in action recognition deals with sequences acquired by stationary cameras with fixed viewpoints. Due to the camera motion, the trajectories of the body parts contain not only the motion of the performing actor but also the motion of the camera. In addition to the camera motion, different viewpoints of the same action in different environments result in different trajectories, which can not be matched using standard approaches. In order to handle these problems, we propose to use the multi-view geometry between two actions. However, well known epipolar geometry of the static scenes where the cameras are stationary is not suitable for our task. Thus, we propose to extend the standard epipolar geometry to the geometry of dynamic scenes where the cameras are moving. We demonstrate the versatility of the proposed geometric approach for recognition of actions in a number of challenging sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541251",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2003",
                "id": 123
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 43,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Videos",
                "Cameras",
                "Geometry",
                "Computer science",
                "Layout",
                "Image motion analysis",
                "Optical computing",
                "Surveillance",
                "Content based retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image motion analysis",
                "object recognition",
                "video cameras",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic scene geometry",
                "multiview geometry",
                "camera motion",
                "uncalibrated moving camera",
                "video image",
                "human action recognition"
            ]
        },
        "id": 19,
        "cited_by": [
            {
                "year": "2013",
                "id": 75
            },
            {
                "year": "2007",
                "id": 10
            },
            {
                "year": "2007",
                "id": 265
            }
        ]
    },
    {
        "title": "Hilbert functions and applications to the estimation of subspace arrangements",
        "authors": [
            "A.Y. Yang",
            "S. Rao",
            "A. Wagner",
            "Yi Ma",
            "R.M. Fossum"
        ],
        "abstract": "This paper develops a new mathematical framework for studying the subspace-segmentation problem. We examine some important algebraic properties of subspace arrangements that are closely related to the subspace-segmentation problem. More specifically, we introduce an important class of invariants given by the Hilbert functions. We show that there exist rich relations between subspace arrangements and their corresponding Hilbert functions. We propose a new subspace-segmentation algorithm, and showcase two applications to demonstrate how the new theoretical revelation may solve subspace segmentation and model selection problems under less restrictive conditions with improved results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541252",
        "reference_list": [
            {
                "year": "2005",
                "id": 98
            },
            {
                "year": "2001",
                "id": 185
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Mathematics",
                "Principal component analysis",
                "Polynomials",
                "Image segmentation",
                "Computer vision",
                "Motion estimation",
                "Motion segmentation",
                "Face recognition",
                "Image representation",
                "Engineering profession"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "Hilbert spaces"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Hilbert function",
                "subspace arrangement estimation",
                "subspace-segmentation problem",
                "algebraic property"
            ]
        },
        "id": 20,
        "cited_by": []
    },
    {
        "title": "Efficient visual event detection using volumetric features",
        "authors": [
            "Yan Ke",
            "R. Sukthankar",
            "M. Hebert"
        ],
        "abstract": "This paper studies the use of volumetric features as an alternative to popular local descriptor approaches for event detection in video sequences. Motivated by the recent success of similar ideas in object detection on static images, we generalize the notion of 2D box features to 3D spatio-temporal volumetric features. This general framework enables us to do real-time video analysis. We construct a realtime event detector for each action of interest by learning a cascade of filters based on volumetric features that efficiently scans video sequences in space and time. This event detector recognizes actions that are traditionally problematic for interest point methods - such as smooth motions where insufficient space-time interest points are available. Our experiments demonstrate that the technique accurately detects actions on real-world sequences and is robust to changes in viewpoint, scale and action speed. We also adapt our technique to the related task of human action classification and confirm that it achieves performance comparable to a current interest point based human activity recognizer on a standard database of human activities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541253",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2003",
                "id": 45
            },
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 95,
            "other": 12,
            "total": 107
        },
        "keywords": {
            "IEEE Keywords": [
                "Event detection",
                "Computer vision",
                "Humans",
                "Video sequences",
                "Detectors",
                "Object detection",
                "Filters",
                "Motion detection",
                "Robustness",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "feature extraction",
                "image sequences",
                "video signal processing",
                "image motion analysis",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual event detection",
                "local descriptor",
                "video sequence",
                "object detection",
                "static image",
                "2D box feature",
                "3D spatiotemporal volumetric feature",
                "real-time video analysis",
                "smooth motion",
                "real-world sequence",
                "human action classification",
                "interest point based human activity recognizer"
            ]
        },
        "id": 21,
        "cited_by": [
            {
                "year": "2017",
                "id": 609
            },
            {
                "year": "2011",
                "id": 179
            },
            {
                "year": "2009",
                "id": 118
            },
            {
                "year": "2009",
                "id": 205
            },
            {
                "year": "2007",
                "id": 26
            },
            {
                "year": "2007",
                "id": 84
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 209
            },
            {
                "year": "2007",
                "id": 265
            }
        ]
    },
    {
        "title": "An algebraic approach to surface reconstruction from gradient fields",
        "authors": [
            "A. Agrawal",
            "R. Chellappa",
            "R. Raskar"
        ],
        "abstract": "Several important problems in computer vision such as shape from shading (SFS) and photometric stereo (PS) require reconstructing a surface from an estimated gradient field, which is usually non-integrable, i.e. have non-zero curl. We propose a purely algebraic approach to enforce integrability in discrete domain. We first show that enforcing integrability can be formulated as solving a single linear system Ax =b over the image. In general, this system is under-determined. We show conditions under which the system can be solved and a method to get to those conditions based on graph theory. The proposed approach is non-iterative, has the important property of local error confinement and can be applied to several problems. Results on SFS and PS demonstrate the applicability of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541254",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 32,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Shape",
                "Photometry",
                "Stereo vision",
                "Computer vision",
                "Linear systems",
                "Stereo image processing",
                "Least squares methods",
                "Automation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "computer vision",
                "algebra",
                "gradient methods",
                "graph theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "algebraic approach",
                "surface reconstruction",
                "gradient field",
                "computer vision",
                "shape from shading",
                "photometric stereo",
                "linear system",
                "graph theory",
                "local error confinement",
                "discrete domain integrability"
            ]
        },
        "id": 22,
        "cited_by": [
            {
                "year": "2017",
                "id": 563
            }
        ]
    },
    {
        "title": "On optimizing template matching via performance characterization",
        "authors": [
            "T.X. Han",
            "V. Ramesh",
            "Ying Zhut",
            "T.S. Huang"
        ],
        "abstract": "Template matching is a fundamental operator in computer vision and is widely used in feature tracking, motion estimation, image alignment, and mosaicing. Under a certain parameterized warping model, the traditional template matching algorithm estimates the geometric warp parameters that minimize the SSD between the target and a warped template. The performance of the template matching can be characterized by deriving the distribution of warp parameter estimate as a function of the ideal template, the ideal warp parameters, and a given noise or perturbation model. In this paper, we assume a discretization of the warp parameter space and derive the theoretical expression for the probability mass function (PMF) of the parameter estimate. As the PMF is also a function of the template size, we can optimize the choice of the template or block size by determining the template/block size that gives the estimate with minimum entropy. Experimental results illustrate the correctness of the theory. An experiment involving feature point tracking in face video is shown to illustrate the robustness of the algorithm in a real-world problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541255",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Parameter estimation",
                "Target tracking",
                "Brightness",
                "Entropy",
                "Upper bound",
                "Stability",
                "Educational institutions",
                "Computer vision",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "feature extraction",
                "face recognition",
                "probability",
                "entropy",
                "geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "template matching optimization",
                "performance characterization",
                "computer vision",
                "parameterized warping model",
                "geometric warp parameter",
                "perturbation model",
                "warp parameter space discretization",
                "probability mass function",
                "minimum entropy",
                "feature point tracking",
                "face video"
            ]
        },
        "id": 23,
        "cited_by": []
    },
    {
        "title": "Mutual information regularized Bayesian framework for multiple image restoration",
        "authors": [
            "Yunqiang Chen",
            "Hongcheng Wang",
            "Tong Fang",
            "J. Tyan"
        ],
        "abstract": "Bayesian methods have been extensively used in various applications. However, there are two intrinsic issues rarely addressed, namely generalization and validity, in the context of multiple image restoration, we show that traditional Bayesian methods are sensitive to model errors and cannot guarantee valid results satisfying the underlying prior knowledge, e.g. independent noise property. To improve the Bayesian framework's generalization, we propose to explicitly enforce the validity of the result. Independent noise prior is very important but largely under-utilized in previous literature. In this paper, we use mutual information (MI) to explicitly enforce the independence. Efficient approximations based on Taylor expansion are proposed to adapt MI into standard energy forms to regularize the Bayesian methods. The new regularized Bayesian framework effectively utilizes the traditional generative signal/noise models but is much more robust to various model errors, as demonstrated in experiments on some demanding imaging applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541256",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Mutual information",
                "Bayesian methods",
                "Image restoration",
                "Ultrasonic imaging",
                "Image edge detection",
                "Noise generators",
                "Signal restoration",
                "Signal generators",
                "Hidden Markov models",
                "Wiener filter"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "image denoising",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mutual information",
                "Bayesian method",
                "multiple image restoration",
                "Bayesian generalization",
                "Taylor expansion",
                "regularized Bayesian framework"
            ]
        },
        "id": 24,
        "cited_by": []
    },
    {
        "title": "Is ICA significantly better than PCA for face recognition?",
        "authors": [
            "Jian Yang",
            "D. Zhang",
            "Jing-yu Yang"
        ],
        "abstract": "The standard PCA was always used as baseline algorithm to evaluate ICA-based face recognition systems in the previous research. In this paper, we examine the two architectures of ICA for image representation and find that ICA architecture I involves a PCA process by vertically centering (PCA I), while ICA architecture II involves a whitened PCA process by horizontally centering (PCA II). So, it is reasonable to use these two PCA versions as baseline algorithms to revaluate the ICA-based face recognition systems. The experiments were performed on the FERET face database. The experimental results show there is no significant performance differences between ICA architecture I (II) and PCA I (II), although ICA architecture II significantly outperforms the standard PCA. It can be concluded that the performance of ICA strongly depends on its involved PCA process. The pure ICA projection has little effect on the performance of face recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541257",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 1,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Independent component analysis",
                "Principal component analysis",
                "Face recognition",
                "Biometrics",
                "Image representation",
                "Computer science",
                "Computer architecture",
                "Databases",
                "Information security",
                "Law enforcement"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "visual databases",
                "independent component analysis",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "baseline algorithm",
                "image representation",
                "vertical centering",
                "horizontal centering",
                "FERET face database",
                "principal component analysis",
                "independent component analysis"
            ]
        },
        "id": 25,
        "cited_by": []
    },
    {
        "title": "Prior-based segmentation by projective registration and level sets",
        "authors": [
            "T. Riklin-Raviv",
            "N. Kiryati",
            "N. Sochen"
        ],
        "abstract": "Object detection and segmentation can be facilitated by the availability of a reference object. However, accounting for possible transformations between the different object views, as part of the segmentation process, remains a challenge. Recent works address this problem by using comprehensive training data. Other approaches are applicable only to limited object classes or can only accommodate similarity transformations. We suggest a novel variational approach to prior-based segmentation, which accounts for planar projective transformation, using a single reference object. The prior shape is registered concurrently with the segmentation process, without point correspondence. The algorithm detects the object of interest and correctly extracts its boundaries. The homography between the two object views is accurately recovered as well. Extending the Chan-Vese level set framework, we propose a region-based segmentation functional that includes explicit representation of the projective homography between the prior shape and the shape to segment. The formulation is derived from two-view geometry. Segmentation of a variety of objects is demonstrated and the recovered transformation is verified.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541258",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 6,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape measurement",
                "Object detection",
                "Training data",
                "Level set",
                "Geometry",
                "Data mining",
                "Mathematics",
                "Minimization methods",
                "Topology"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "image segmentation",
                "image registration",
                "variational techniques",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "prior-based segmentation",
                "projective registration",
                "object detection",
                "object segmentation",
                "planar projective transformation",
                "single reference object",
                "Chan-Vese level set framework",
                "region-based segmentation",
                "projective homography",
                "two-view geometry"
            ]
        },
        "id": 26,
        "cited_by": [
            {
                "year": "2007",
                "id": 127
            }
        ]
    },
    {
        "title": "Fast multiple object tracking via a hierarchical particle filter",
        "authors": [
            "Changjiang Yang",
            "R. Duraiswami",
            "L. Davis"
        ],
        "abstract": "A very efficient and robust visual object tracking algorithm based on the particle filter is presented. The method characterizes the tracked objects using color and edge orientation histogram features. While the use of more features and samples can improve the robustness, the computational load required by the particle filter increases. To accelerate the algorithm while retaining robustness we adopt several enhancements in the algorithm. The first is the use of integral images for efficiently computing the color features and edge orientation histograms, which allows a large amount of particles and a better description of the targets. Next, the observation likelihood based on multiple features is computed in a coarse-to-fine manner, which allows the computation to quickly focus on the more promising regions. Quasi-random sampling of the particles allows the filter to achieve a higher convergence rate. The resulting tracking algorithm maintains multiple hypotheses and offers robustness against clutter or short period occlusions. Experimental results demonstrate the efficiency and effectiveness of the algorithm for single and multiple object tracking.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541259",
        "reference_list": [],
        "citation": {
            "ieee": 66,
            "other": 19,
            "total": 85
        },
        "keywords": {
            "IEEE Keywords": [
                "Particle tracking",
                "Particle filters",
                "Robustness",
                "Histograms",
                "Colored noise",
                "Computer vision",
                "State-space methods",
                "Sampling methods",
                "Vehicle dynamics",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "edge detection",
                "image colour analysis",
                "convergence of numerical methods",
                "particle filtering (numerical methods)",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast multiple object tracking",
                "hierarchical particle filter",
                "visual object tracking algorithm",
                "color histogram",
                "edge orientation histogram",
                "integral images",
                "observation likelihood",
                "quasirandom sampling",
                "computer vision"
            ]
        },
        "id": 27,
        "cited_by": [
            {
                "year": "2017",
                "id": 268
            },
            {
                "year": "2011",
                "id": 17
            },
            {
                "year": "2009",
                "id": 184
            }
        ]
    },
    {
        "title": "Feature hierarchies for object classification",
        "authors": [
            "B. Epshtein",
            "S. Uliman"
        ],
        "abstract": "The paper describes a method for automatically extracting informative feature hierarchies for object classification, and shows the advantage of the features constructed hierarchically over previous methods. The extraction process proceeds in a top-down manner: informative top-level fragments are extracted first, and by a repeated application of the same feature extraction process the classification fragments are broken down successively into their own optimal components. The hierarchical decomposition terminates with atomic features that cannot be usefully decomposed into simpler features. The entire hierarchy, the different features and sub-features, and their optimal parameters, are learned during a training phase using training examples. Experimental comparisons show that these feature hierarchies are significantly more informative and better for classification compared with similar nonhierarchical features as well as previous methods for using feature hierarchies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541260",
        "reference_list": [],
        "citation": {
            "ieee": 29,
            "other": 22,
            "total": 51
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Training data",
                "Computer vision",
                "Object detection",
                "Computer science",
                "Mathematics",
                "Face detection",
                "Eyelids",
                "Mutual information",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature hierarchy",
                "object classification",
                "informative feature extraction",
                "hierarchical decomposition"
            ]
        },
        "id": 28,
        "cited_by": []
    },
    {
        "title": "Using frontier points to recover shape, reflectance and illumination",
        "authors": [
            "G. Vogiatzis",
            "P. Favaro",
            "R. Cipolla"
        ],
        "abstract": "We describe a method to recover the surface reflectance and the 3D shape of a non-Lambertian object as well as illumination, from a collection of images. It is based on the so-called frontier points, which are extracted from the outlines of an object. Frontier points provide 3D locations on the object surface where the surface normal is known. This information is exploited to infer the surface reflectance of the object and the light distribution of the scene both under varying illumination and fixed vantage point, and under varying vantage point and fixed illumination. We also show how to apply frontier points for shape recovery in photometric stereo. The effectiveness of frontier points for recovering reflectance, illumination and shape is confirmed by a number of experiments on both real and synthetic data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541261",
        "reference_list": [
            {
                "year": "2003",
                "id": 107
            },
            {
                "year": "2003",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 8,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Reflectivity",
                "Lighting",
                "Surface reconstruction",
                "Cameras",
                "Data engineering",
                "Data systems",
                "Data mining",
                "Layout",
                "Photometry"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "feature extraction",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "frontier points",
                "shape recovery",
                "surface reflectance",
                "illumination recovery",
                "3D shape",
                "nonLambertian object",
                "object outline",
                "light distribution",
                "photometric stereo"
            ]
        },
        "id": 29,
        "cited_by": []
    },
    {
        "title": "Multi-scale gesture recognition from time-varying contours",
        "authors": [
            "Hong Li",
            "M. Greenspan"
        ],
        "abstract": "A novel method is introduced to recognize and estimate the scale of time-varying human gestures. It exploits the changes in contours along spatiotemporal directions. Each contour is first parameterized as a 2D function of radius vs. cumulative contour length, and a 3D surface is composed from a sequence of such functions. In a two-phase recognition process, dynamic time warping is employed to rule out significantly different gesture models, and then mutual information (MI) is applied for matching the remaining models. The system has been tested on 8 gestures performed by 5 subjects with varied time scales. The two-phase process is compared against exhaustively testing three similarity measures based upon MI, correlation, and nonparametric kernel density estimation. Experimental results demonstrate that the exhaustive application of MI is the most robust with a recognition rate of 90.6%, however, the two-phase approach is much more computationally efficient with a comparable recognition rate of 90.0%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541262",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 0,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Hidden Markov models",
                "Shape",
                "Speech recognition",
                "Mutual information",
                "System testing",
                "Performance evaluation",
                "Density measurement",
                "Kernel",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale gesture recognition",
                "time-varying contour",
                "time-varying human gesture",
                "dynamic time warping",
                "mutual information",
                "similarity measure",
                "nonparametric kernel density estimation"
            ]
        },
        "id": 30,
        "cited_by": [
            {
                "year": "2007",
                "id": 77
            }
        ]
    },
    {
        "title": "Integrated spatial and frequency domain 2D motion segmentation and estimation",
        "authors": [
            "A. Briassouli",
            "N. Ahuja"
        ],
        "abstract": "A video containing multiple objects in rotational and translational motion is analyzed through a combination of spatial and frequency domain representations. It is argued that the combined analysis can take advantage of the strengths of both representations. Initial estimates of constant, as well as time-varying, translation and rotation velocities are obtained from frequency analysis. Improved motion estimates and motion segmentation for the case of translation are achieved by integrating spatial and Fourier domain information. For combined rotational and translational motions, the frequency representation is used for motion estimation, but only spatial information can be used to separate and extract the independently moving objects. The proposed algorithms are tested on synthetic and real videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541263",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Frequency domain analysis",
                "Motion segmentation",
                "Computer vision",
                "Frequency estimation",
                "Motion estimation",
                "Motion analysis",
                "Video sequences",
                "Data mining",
                "Testing",
                "Fourier transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "video signal processing",
                "motion estimation",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatial-frequency domain",
                "motion segmentation",
                "motion estimation",
                "real video",
                "rotational motion",
                "translational motion",
                "frequency analysis",
                "Fourier domain information",
                "spatial domain information"
            ]
        },
        "id": 31,
        "cited_by": []
    },
    {
        "title": "Shape parameter optimization for Adaboosted active shape model",
        "authors": [
            "Yuanzhong Li",
            "W. Ito"
        ],
        "abstract": "Active shape model (ASM) has been shown to be a powerful tool to aid the interpretation of images, especially in face alignment. ASM local appearance model parameter estimation is based on the assumption that residuals between model fit and data have a Gaussian distribution. Moreover, to generate an allowable face shape, ASM truncates coefficients of shape principal components into the bounds determined by eigenvalues. In this paper, an algorithm of modeling local appearances, called AdaBoosted ASM, and a shape parameter optimization method are proposed. In the algorithm of modeling the local appearances, we describe our novel modeling method by using AdaBoosted histogram classifiers, in which the assumption of the Gaussian distribution is not necessary. In the shape parameter optimization, we describe that there is an inadequacy on controlling shape parameters in ASM, and our novel method on how to solve it. Experimental results demonstrate that the AdaBoosted histogram classifiers improve robustness of landmark displacement greatly, and the shape parameter optimization solves the inadequacy problem of ASM on shape constraint effectively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541264",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 0,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Active shape model",
                "Gaussian distribution",
                "Shape control",
                "Robustness",
                "Histograms",
                "Parameter estimation",
                "Eigenvalues and eigenfunctions",
                "Optimization methods",
                "Lighting",
                "Glass"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape parameter optimization",
                "Adaboosted active shape model",
                "image interpretation",
                "face alignment",
                "local appearance model parameter estimation",
                "Gaussian distribution",
                "shape principal component",
                "eigenvalues",
                "local appearance modeling",
                "histogram classifier",
                "landmark displacement",
                "shape constraint"
            ]
        },
        "id": 32,
        "cited_by": []
    },
    {
        "title": "A multilevel banded graph cuts method for fast image segmentation",
        "authors": [
            "H. Lombaert",
            "Yiyong Sun",
            "L. Grady",
            "Chenyang Xu"
        ],
        "abstract": "In the short time since publication of Boykov and Jolly's seminal paper [2001], graph cuts have become well established as a leading method in 2D and 3D semi-automated image segmentation. Although this approach is computationally feasible for many tasks, the memory overhead and supralinear time complexity of leading algorithms results in an excessive computational burden for high-resolution data. In this paper, we introduce a multilevel banded heuristic for computation of graph cuts that is motivated by the well-known narrow band algorithm in level set computation. We perform a number of numerical experiments to show that this heuristic drastically reduces both the running time and the memory consumption of graph cuts while producing nearly the same segmentation result as the conventional graph cuts. Additionally, we are able to characterize the type of segmentation target for which our multilevel banded heuristic yields different results from the conventional graph cuts. The proposed method has been applied to both 2D and 3D images with promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541265",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 62,
            "other": 60,
            "total": 122
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Narrowband",
                "Level set",
                "Computed tomography",
                "Sun",
                "Visualization",
                "Educational institutions",
                "Explosions",
                "Image resolution",
                "Digital cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "graph theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multilevel banded graph cuts",
                "image segmentation",
                "multilevel banded heuristic",
                "narrow band algorithm",
                "level set computation"
            ]
        },
        "id": 33,
        "cited_by": [
            {
                "year": "2015",
                "id": 201
            },
            {
                "year": "2007",
                "id": 130
            }
        ]
    },
    {
        "title": "A practical single image based approach for estimating illumination distribution from shadows",
        "authors": [
            "Taeone Kim",
            "Ki-Sang Hong"
        ],
        "abstract": "This paper presents a practical method that estimates illumination distribution from shadows where the shadows are assumed to be cast on a textured, Lambertian surface. Previous methods usually require that the reflectance property of the surface be constant or uniform, or need an additional image to cancel out the effects of varying albedo of the textured surface. We deal with an estimation problem for which surface albedo information is not available. In this case, the estimation problem corresponds to an underdetermined one. We show that combination of regularization by correlation and some user-specified information can be a practical method for solving the problem. In addition, as an optimization tool for solving the problem, we develop a constrained nonnegative quadratic programming (NNQP) technique into which not only regularization but also user-specified information are easily incorporated. We test and validate our method on both synthetic and real images and present some experimental results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541266",
        "reference_list": [
            {
                "year": "2001",
                "id": 149
            },
            {
                "year": "2003",
                "id": 178
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Surface texture",
                "Layout",
                "Light sources",
                "Reflectivity",
                "Application software",
                "Rendering (computer graphics)",
                "Optical reflection",
                "Constraint optimization",
                "Quadratic programming"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "quadratic programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "illumination distribution estimation",
                "textured Lambertian surface",
                "optimization tool",
                "constrained nonnegative quadratic programming",
                "synthetic image",
                "real image"
            ]
        },
        "id": 34,
        "cited_by": [
            {
                "year": "2011",
                "id": 113
            },
            {
                "year": "2009",
                "id": 23
            }
        ]
    },
    {
        "title": "3D object reconstruction from a single 2D line drawing without hidden lines",
        "authors": [
            "Liangliang Cao",
            "Jianzhuang Liu",
            "Xiaoou Tang"
        ],
        "abstract": "The human vision system can interpret a single 2D line drawing as a 3D object without much difficulty even if the hidden lines of the object are invisible. Several reconstruction approaches have tried to emulate this ability, but they cannot recover the complete object if the hidden lines of the object are not shown. This paper proposes a novel approach for reconstructing complete 3D objects from line drawings without hidden lines. First, we develop some constraints and properties for the inference of the topology of the invisible edges and vertices of an object. Then we present a reconstruction method based on perceptual symmetry and planarity of the object. We give a number of examples to demonstrate the ability of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541267",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Engineering drawings",
                "Humans",
                "Machine vision",
                "Topology",
                "Computer vision",
                "Labeling",
                "Shape",
                "Asia",
                "Reconstruction algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "edge detection",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object reconstruction",
                "single 2D line drawing",
                "hidden lines",
                "human vision system",
                "topology inference",
                "invisible edges",
                "object vertices",
                "perceptual symmetry",
                "object planarity"
            ]
        },
        "id": 35,
        "cited_by": [
            {
                "year": "2007",
                "id": 219
            }
        ]
    },
    {
        "title": "Robust path-based spectral clustering with application to image segmentation",
        "authors": [
            "Hong Chang",
            "Dit-Yan Yeung"
        ],
        "abstract": "Spectral clustering and path-based clustering are two recently developed clustering approaches that have delivered impressive results in a number of challenging clustering tasks. However, they are not robust enough against noise and outliers in the data. In this paper, based on M-estimation from robust statistics, we develop a robust path-based spectral clustering method by defining a robust path-based similarity measure for spectral clustering. Our method is significantly more robust than spectral clustering and path-based clustering. We have performed experiments based on both synthetic and real-world data, comparing our method with some other methods. In particular, color images from the Berkeley segmentation dataset and benchmark are used in the image segmentation experiments. Experimental results show that our method consistently outperforms other methods due to its higher robustness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541268",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 4,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Noise robustness",
                "Clustering algorithms",
                "Statistics",
                "Clustering methods",
                "Color",
                "Machine learning algorithms",
                "Kernel",
                "Bagging",
                "Machine learning"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern clustering",
                "image segmentation",
                "image colour analysis",
                "estimation theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "path-based spectral clustering",
                "image segmentation",
                "M-estimation",
                "robust statistics",
                "similarity measure",
                "color images",
                "Berkeley segmentation dataset"
            ]
        },
        "id": 36,
        "cited_by": []
    },
    {
        "title": "Building a classification cascade for visual identification from one example",
        "authors": [
            "A. Ferencz",
            "E.G. Learned-Miller",
            "J. Malik"
        ],
        "abstract": "Object identification (OID) is specialized recognition where the category is known (e.g. cars) and the algorithm recognizes an object's exact identity (e.g. Bob's BMW). Two special challenges characterize OID. (1) Interclass variation is often small (many cars look alike) and may be dwarfed by illumination or pose changes. (2) There may be many classes but few or just one positive \"training\" examples per class. Due to (1), a solution must locate possibly subtle object-specific salient features (a door handle) while avoiding distracting ones (a specular highlight). However, (2) rules out direct techniques of feature selection. We describe an online algorithm that takes one model image from a known category and builds an efficient \"same\" vs. \"different\" classification cascade by predicting the most discriminative feature set for that object. Our method not only estimates the saliency and scoring function for each candidate feature, but also models the dependency between features, building an ordered feature sequence unique to a specific model image, maximizing cumulative information content. Learned stopping thresholds make the classifier very efficient. To make this possible, category-specific characteristics are learned automatically in an off-line training procedure from labeled image pairs of the category, without prior knowledge about the category. Our method, using the same algorithm for both cars and faces, outperforms a wide variety of other methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541269",
        "reference_list": [
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2003",
                "id": 38
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 9,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer science",
                "Lighting",
                "Object recognition",
                "Face detection",
                "Cameras",
                "Hair",
                "Face recognition",
                "Vehicles",
                "Eyebrows",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "feature extraction",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "classification cascade",
                "visual identification",
                "object identification",
                "positive training example",
                "feature selection",
                "online algorithm",
                "discriminative feature set",
                "ordered feature sequence",
                "stopping thresholds"
            ]
        },
        "id": 37,
        "cited_by": [
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2007",
                "id": 215
            },
            {
                "year": "2007",
                "id": 224
            }
        ]
    },
    {
        "title": "A unifying approach to hard and probabilistic clustering",
        "authors": [
            "R. Zass",
            "A. Shashua"
        ],
        "abstract": "We derive the clustering problem from first principles showing that the goal of achieving a probabilistic, or \"hard\", multi class clustering result is equivalent to the algebraic problem of a completely positive factorization under a doubly stochastic constraint. We show that spectral clustering, normalized cuts, kernel K-means and the various normalizations of the associated affinity matrix are particular instances and approximations of this general principle. We propose an efficient algorithm for achieving a completely positive factorization and extend the basic clustering scheme to situations where partial label information is available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541270",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 21,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Kernel",
                "Chromium",
                "Computer science",
                "Clustering algorithms",
                "Labeling",
                "Particle measurements",
                "Euclidean distance",
                "Matrices",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern clustering",
                "probability",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hard clustering",
                "probabilistic clustering",
                "multi class clustering",
                "algebraic problem",
                "doubly stochastic constraint",
                "spectral clustering",
                "normalized cuts",
                "kernel K-means",
                "affinity matrix",
                "positive factorization"
            ]
        },
        "id": 38,
        "cited_by": [
            {
                "year": "2007",
                "id": 128
            }
        ]
    },
    {
        "title": "Probabilistic contour extraction using hierarchical shape representation",
        "authors": [
            "Xin Fan",
            "Chun Qi",
            "Dequn Liang",
            "Hua Huang"
        ],
        "abstract": "In this paper, we address the issue of extracting contour of the object with a specific shape. A hierarchical graphical model is proposed to represent shape variations. A complex shape is decomposed into several components which are described as principal component analysis (PCA) based models in various levels. The hierarchical representation allows for chain-like conditional dependency within a single level and bidirectional communication between different levels. Additionally, a sequential Monte-Carlo (SMC) based inference algorithm that can explore the graphical structure is proposed to estimate the contour. The experiments performed on real-world hand and face images show that the proposed method is effective in combating occlusion and cluttered background. Moreover, it is possible to isolate the localization error to an individual component of a shape attributed to the hierarchical representation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541271",
        "reference_list": [
            {
                "year": "2001",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 3,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Sliding mode control",
                "Data mining",
                "Graphical models",
                "Principal component analysis",
                "Computer vision",
                "Target tracking",
                "Humans",
                "Bidirectional control",
                "Inference algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation",
                "image recognition",
                "principal component analysis",
                "Monte Carlo methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic contour extraction",
                "hierarchical shape representation",
                "hierarchical graphical model",
                "principal component analysis",
                "bidirectional communication",
                "sequential Monte-Carlo",
                "inference algorithm",
                "contour estimation",
                "real-world hand image",
                "face image"
            ]
        },
        "id": 39,
        "cited_by": []
    },
    {
        "title": "Multi-view surface reconstruction using polarization",
        "authors": [
            "G.A. Atkinson",
            "E.R. Hancock"
        ],
        "abstract": "A new technique for surface reconstruction is developed that uses polarization information from two views. One common problem arising from many multiple view techniques is that of finding correspondences between pixels on each image. In the new method, these correspondences are found by exploiting the spontaneous polarization of light caused by reflection to recover surface normals. These normals are then used to recover surface height. The similarity between reconstructed surface regions determines whether or not a pair of points correspond to each other. The technique is thus able to overcome the convex/concave ambiguity found in many single view techniques. Because the technique relies on smooth surface regions to detect correspondences, rather than feature detection, it is applicable to objects normally inaccessible to stereo vision. Also due to this fact, it is possible to remove noise without causing oversmoothing problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541272",
        "reference_list": [
            {
                "year": "2003",
                "id": 129
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 10,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Optical polarization",
                "Electromagnetic wave polarization",
                "Optical reflection",
                "Shape",
                "Photometry",
                "Light sources",
                "Cameras",
                "Image reconstruction",
                "Stereo vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image denoising",
                "light polarisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview surface reconstruction",
                "polarization information",
                "pixel correspondence",
                "light polarization",
                "surface normals",
                "surface height",
                "convex-concave ambiguity",
                "image noise removal"
            ]
        },
        "id": 40,
        "cited_by": [
            {
                "year": "2015",
                "id": 376
            }
        ]
    },
    {
        "title": "Degenerate cases and closed-form solutions for camera calibration with one-dimensional objects",
        "authors": [
            "P. Hammarstedt",
            "P. Sturm",
            "A. Heyden"
        ],
        "abstract": "Camera calibration with one-dimensional objects is based on an algebraic constraint on the image of the absolute conic. We give an alternative derivation to this constraint, allowing a geometrical interpretation. From this, we derive the degenerate cases, or critical motions, where the calibration algorithm fails. We also show that constraints on the intrinsic parameters lead to simplified closed-form solutions and a reduced set of critical motions. A simulation and a real data experiment is performed to evaluate the accuracy of the calibration result for motions close to being critical.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541273",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 15,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer aided software engineering",
                "Cameras",
                "Calibration",
                "Closed-form solution",
                "Computer vision",
                "Geometry",
                "Mathematics",
                "Computational modeling",
                "Performance evaluation",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "cameras",
                "calibration",
                "computational geometry",
                "algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "degenerate cases",
                "closed-form solution",
                "camera calibration",
                "1D objects",
                "algebraic constraint",
                "absolute conic",
                "geometrical interpretation",
                "critical motion"
            ]
        },
        "id": 41,
        "cited_by": [
            {
                "year": "2007",
                "id": 153
            }
        ]
    },
    {
        "title": "Using extended light sources for modeling object appearance under varying illumination",
        "authors": [
            "I. Sato",
            "T. Okabe",
            "Y. Sato",
            "K. Ikeuchi"
        ],
        "abstract": "In this study, we demonstrate the effectiveness of using extended light sources for modeling the appearance of an object for varying illumination. Extended light sources have a radiance distribution that is similar to that of the Gaussian function and have the potential of functioning as a low-pass filter when the appearance of an object is sampled under them. This enables us to obtain a set of basis images of an object for variable illumination from input images of the object taken under those light sources without suffering aliasing caused by insufficient sampling of its appearance. Furthermore, extended light sources are useful in terms of reducing high contrast in image intensities due to specular and diffuse reflection components. This helps us observe both specular and diffuse reflection components of an object in the same image taken with a single shutter speed. We have tested our proposed approach based on extended light sources with objects of complex appearance that are generally difficult to model using image-based modeling techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541274",
        "reference_list": [
            {
                "year": "2001",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Light sources",
                "Lighting",
                "Optical reflection",
                "Object recognition",
                "Informatics",
                "Filters",
                "Image sampling",
                "Testing",
                "Reflectivity",
                "Image generation"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "object recognition",
                "low-pass filters",
                "light sources"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "extended light sources",
                "object appearance modeling",
                "varying illumination",
                "Gaussian function",
                "low-pass filter",
                "high contrast reduction",
                "image intensity",
                "shutter speed",
                "image-based modeling",
                "surface reflectance"
            ]
        },
        "id": 42,
        "cited_by": []
    },
    {
        "title": "Detection of concentric circles for camera calibration",
        "authors": [
            "Guang Jiang",
            "Long Quan"
        ],
        "abstract": "The geometry of plane-based calibration methods is well understood, but some user interaction is often needed in practice for feature detection. This paper presents a fully automatic calibration system that uses patterns of pairs of concentric circles. The key observation is to introduce a geometric method that constructs a sequence of points strictly convergent to the image of the circle center from an arbitrary point. The method automatically detects the points of the pattern features by the construction method, and identify them by invariants. It then takes advantage of homological constraints to consistently and optimally estimate the features in the image. The experiments demonstrate the robustness and the accuracy of the new method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541275",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 10,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Computer vision",
                "Robustness",
                "Image edge detection",
                "Detection algorithms",
                "Computer science",
                "Computational geometry",
                "Image converters",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image sequences",
                "cameras",
                "calibration",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "concentric circle detection",
                "camera calibration",
                "plane-based calibration geometry",
                "automatic calibration system",
                "point sequence",
                "homological constraints",
                "feature estimation",
                "homological constraint"
            ],
            "Author Keywords": [
                "camera calibration",
                "concentric circles",
                "cross ratio",
                "homological constraint"
            ]
        },
        "id": 43,
        "cited_by": []
    },
    {
        "title": "Shape and spatially-varying BRDFs from photometric stereo",
        "authors": [
            "D.B. Goldman",
            "B. Curless",
            "A. Hertzmann",
            "S.M. Seitz"
        ],
        "abstract": "This paper describes a photometric stereo method designed for surfaces with spatially-varying BRDFs, including surfaces with both varying diffuse and specular properties. Our method builds on the observation that most objects are composed of a small number of fundamental materials. This approach recovers not only the shape but also material BRDFs and weight maps, yielding compelling results for a wide variety of objects. We also show examples of interactive lighting and editing operations made possible by our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541276",
        "reference_list": [],
        "citation": {
            "ieee": 56,
            "other": 35,
            "total": 91
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Image reconstruction",
                "Surface reconstruction",
                "Shape measurement",
                "Lighting",
                "Weight control",
                "Design methodology",
                "Layout",
                "Graphics",
                "Material properties"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image reconstruction",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape BRDF",
                "photometric stereo method",
                "spatially-varying BRDF",
                "interactive lighting",
                "interactive editing"
            ]
        },
        "id": 44,
        "cited_by": [
            {
                "year": "2017",
                "id": 563
            },
            {
                "year": "2013",
                "id": 312
            },
            {
                "year": "2007",
                "id": 42
            },
            {
                "year": "2007",
                "id": 100
            },
            {
                "year": "2007",
                "id": 263
            }
        ]
    },
    {
        "title": "Multi-view reconstruction using photo-consistency and exact silhouette constraints: a maximum-flow formulation",
        "authors": [
            "S.N. Sinha",
            "M. Pollefeys"
        ],
        "abstract": "This paper describes a novel approach for reconstructing a closed continuous surface of an object from multiple calibrated color images and silhouettes. Any accurate reconstruction must satisfy (1) photo-consistency and (2) silhouette consistency constraints. Most existing techniques treat these cues identically in optimization frameworks where silhouette constraints are traded off against photo-consistency and smoothness priors. Our approach strictly enforces silhouette constraints, while optimizing photo-consistency and smoothness in a global graph-cut framework. We transform the reconstruction problem into computing max-flow/min-cut in a geometric graph, where any cut corresponds to a surface satisfying exact silhouette constraints (its silhouettes should exactly coincide with those of the visual hull); a minimum cut is the most photo-consistent surface amongst them. Our graph-cut formulation is based on the rim mesh, (the combinatorial arrangement of rims or contour generators from many views) which can be computed directly from the silhouettes. Unlike other methods, our approach enforces silhouette constraints without introducing a bias near the visual hull boundary and also recovers the rim curves. Results are presented for synthetic and real datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541277",
        "reference_list": [
            {
                "year": "2003",
                "id": 174
            },
            {
                "year": "2003",
                "id": 76
            }
        ],
        "citation": {
            "ieee": 41,
            "other": 35,
            "total": 76
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Layout",
                "Shape",
                "Surface reconstruction",
                "Constraint optimization",
                "Color",
                "Optimization methods",
                "Computer vision",
                "Computer science",
                "Surface treatment"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image colour analysis",
                "computer graphics",
                "mesh generation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview reconstruction",
                "photo-consistency",
                "maximum-flow formulation",
                "closed continuous surface",
                "color image",
                "silhouette consistency constraint",
                "smoothness priors",
                "geometric graph",
                "graph-cut formulation",
                "contour generators",
                "visual hull boundary",
                "rim curves"
            ]
        },
        "id": 45,
        "cited_by": [
            {
                "year": "2007",
                "id": 53
            },
            {
                "year": "2007",
                "id": 157
            },
            {
                "year": "2007",
                "id": 163
            }
        ]
    },
    {
        "title": "Avoiding the \"streetlight effect\": tracking by exploring likelihood modes",
        "authors": [
            "D. Demirdjian",
            "L. Taycher",
            "G. Shakhnarovich",
            "K. Grauman",
            "T. Darrell"
        ],
        "abstract": "Classic methods for Bayesian inference effectively constrain search to lie within regions of significant probability of the temporal prior. This is efficient with an accurate dynamics model, but otherwise is prone to ignore significant peaks in the true posterior. A more accurate posterior estimate can be obtained by explicitly finding modes of the likelihood function and combining them with a weak temporal prior. In our approach, modes are found using efficient example-based matching followed by local refinement to find peaks and estimate peak bandwidth. By reweighting these peaks according to the temporal prior we obtain an estimate of the full posterior model. We show comparative results on real and synthetic images in a high degree of freedom articulated tracking task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541278",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 13,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Bayesian methods",
                "Bandwidth",
                "Search methods",
                "State-space methods",
                "Shape",
                "Computer science",
                "Artificial intelligence",
                "Laboratories",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "streetlight effect",
                "likelihood modes",
                "posterior estimate",
                "likelihood function",
                "example-based matching",
                "local refinement",
                "full posterior model",
                "real image",
                "synthetic image",
                "tracking task"
            ]
        },
        "id": 46,
        "cited_by": [
            {
                "year": "2011",
                "id": 138
            }
        ]
    },
    {
        "title": "A theoretical limit on the number of effective pixels that can be optically resolved on a non-planar subject",
        "authors": [
            "R. Boman"
        ],
        "abstract": "In normal imaging systems, the depth of field is inversely-proportional to the lens aperture. If we assume the system is diffraction limited, then the maximum resolution (i.e. pixels per mm) is proportional to the lens aperture. Thus there is a tradeoff between depth of field and resolution. This tradeoff creates an upper limit on the number of pixels that can be resolved on a nonplanar subject. This paper presents the theoretical limit on the number of pixels. The derivations of the limit show that the limit is only a function of the subject size and depth. The subject distance, focal length, and sensor size do not matter. For small subjects, the limit is well below the capabilities of modern imaging systems. For example, a subject 15 cm wide and 10cm deep can only be imaged with 300,000 pixels even though sensors with 10 times that many pixels are readily available. The resulting limit has obvious applications in machine vision, particularly when specifying optics and imaging sensors. Experimental results are provided to validate the main result of the paper.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541279",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Apertures",
                "Optical sensors",
                "Diffraction",
                "Image sensors",
                "Lenses",
                "Optical imaging",
                "Pixel",
                "Equations",
                "Sensor systems",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "imaging system",
                "maximum resolution",
                "nonplanar subject",
                "machine vision"
            ]
        },
        "id": 47,
        "cited_by": []
    },
    {
        "title": "Discovering objects and their location in images",
        "authors": [
            "J. Sivic",
            "B.C. Russell",
            "A.A. Efros",
            "A. Zisserman",
            "W.T. Freeman"
        ],
        "abstract": "We seek to discover the object categories depicted in a set of unlabelled images. We achieve this using a model developed in the statistical text literature: probabilistic latent semantic analysis (pLSA). In text analysis, this is used to discover topics in a corpus using the bag-of-words document representation. Here we treat object categories as topics, so that an image containing instances of several categories is modeled as a mixture of topics. The model is applied to images by using a visual analogue of a word, formed by vector quantizing SIFT-like region descriptors. The topic discovery approach successfully translates to the visual domain: for a small set of objects, we show that both the object categories and their approximate spatial layout are found without supervision. Performance of this unsupervised method is compared to the supervised approach of Fergus et al. (2003) on a set of unseen images containing only one object per image. We also extend the bag-of-words vocabulary to include 'doublets' which encode spatially local co-occurring regions. It is demonstrated that this extended vocabulary gives a cleaner image segmentation. Finally, the classification and segmentation methods are applied to a set of images containing multiple objects per image. These results demonstrate that we can successfully build object class models from an unsupervised analysis of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541280",
        "reference_list": [
            {
                "year": "2001",
                "id": 159
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 305,
            "other": 215,
            "total": 520
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Vocabulary",
                "Graphical models",
                "Histograms",
                "Computer vision",
                "Random variables",
                "Labeling",
                "Clustering algorithms",
                "Frequency",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image classification",
                "image representation",
                "vector quantisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object discovery",
                "object location",
                "object category",
                "probabilistic latent semantic analysis",
                "text analysis",
                "word visual analogue",
                "SIFT-like region descriptor",
                "topic discovery approach",
                "image segmentation",
                "image classification",
                "unsupervised analysis"
            ]
        },
        "id": 48,
        "cited_by": [
            {
                "year": "2009",
                "id": 61
            },
            {
                "year": "2007",
                "id": 288
            }
        ]
    },
    {
        "title": "Vehicle identification between non-overlapping cameras without direct feature matching",
        "authors": [
            "Ying Shan",
            "H.S. Sawhney",
            "R. Kumar"
        ],
        "abstract": "We propose a novel method for identifying road vehicles between two nonoverlapping cameras. The problem is formulated as a same-different classification problem: probability of two vehicle images from two distinct cameras being from the same vehicle or from different vehicles. The key idea is to compute the probability without matching the two vehicle images directly, which is a process vulnerable to drastic appearance and aspect changes. We represent each vehicle image as an embedding amongst representative exemplars of vehicles within the same camera. The embedding is computed as a vector each of whose components is a nonmetric distance for a vehicle to an exemplar. The nonmetric distances are computed using robust matching of oriented edge images. A set of truthed training examples of same-different vehicle pairings across the two cameras is used to learn a classifier that encodes the probability distributions. A pair of the embeddings representing two vehicles across two cameras is then used to compute the same-different probability. In order for the vehicle exemplars to be representative for both cameras, we also propose a method for jointly selection of corresponding exemplars using the training data. Experiments on observations of over 400 vehicles under drastically illumination and camera conditions demonstrate promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541281",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 2,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Lighting",
                "Road vehicles",
                "Embedded computing",
                "Robustness",
                "Laboratories",
                "Probability distribution",
                "Training data",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image classification",
                "feature extraction",
                "edge detection",
                "probability",
                "vehicles",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonoverlapping camera",
                "feature matching",
                "road vehicle identification",
                "image classification",
                "image matching",
                "nonmetric distance",
                "oriented edge image",
                "probability distribution"
            ]
        },
        "id": 49,
        "cited_by": []
    },
    {
        "title": "High resolution tracking of non-rigid 3D motion of densely sampled data using harmonic maps",
        "authors": [
            "Yang Wang",
            "M. Gupta",
            "Song Zhang",
            "Sen Wang",
            "Xianfeng Gu",
            "D. Samaras",
            "Peisen Huang"
        ],
        "abstract": "We present a novel fully automatic method for high resolution, nonrigid dense 3D point tracking. High quality dense point clouds of nonrigid geometry moving at video speeds are acquired using a phase-shifting structured light ranging technique. To use such data for the temporal study of subtle motions such as those seen in facial expressions, an efficient nonrigid 3D motion tracking algorithm is needed to establish inter-frame correspondences. The novelty of this paper is the development of an algorithmic framework for 3D tracking that unifies tracking of intensity and geometric features, using harmonic maps with added feature correspondence constraints. While the previous uses of harmonic maps provided only global alignment, the proposed introduction of interior feature constraints guarantees that nonrigid deformations are accurately tracked as well. The harmonic map between two topological disks is a diffeomorphism with minimal stretching energy and bounded angle distortion. The map is stable, insensitive to resolution changes and is robust to noise. Due to the strong implicit and explicit smoothness constraints imposed by the algorithm and the high-resolution data, the resulting registration/deformation field is smooth, continuous and gives dense one-to-one inter-frame correspondences. Our method is validated through a series of experiments demonstrating its accuracy and efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541282",
        "reference_list": [
            {
                "year": "2001",
                "id": 200
            },
            {
                "year": "2003",
                "id": 93
            },
            {
                "year": "2003",
                "id": 188
            },
            {
                "year": "2003",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Geometry",
                "Computer vision",
                "Face detection",
                "Shape",
                "Computer science",
                "Mechanical engineering",
                "Clouds",
                "Harmonic distortion",
                "Energy resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "feature extraction",
                "image registration",
                "geometry",
                "smoothing methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high resolution tracking",
                "nonrigid 3D motion tracking",
                "densely sampled data",
                "harmonic map",
                "nonrigid dense 3D point tracking",
                "high quality dense point clouds",
                "nonrigid geometry",
                "video speed",
                "phase-shifting structured light ranging technique",
                "intensity tracking",
                "geometric feature tracking",
                "minimal stretching energy",
                "bounded angle distortion",
                "smoothness constraint",
                "diffeomorphism"
            ]
        },
        "id": 50,
        "cited_by": [
            {
                "year": "2017",
                "id": 566
            },
            {
                "year": "2013",
                "id": 419
            },
            {
                "year": "2011",
                "id": 74
            },
            {
                "year": "2007",
                "id": 69
            }
        ]
    },
    {
        "title": "A stochastic filter for fluid motion tracking",
        "authors": [
            "A. Cuzol",
            "E. Memin"
        ],
        "abstract": "In this paper, we present a method for the tracking of fluid flows velocity fields. The technique we propose is formalized within sequential Bayesian filter framework. The filter we propose here combines an Ito diffusion process coming from a stochastic formulation of the vorticity-velocity form of Navier-Stokes equation and discrete measurements extracted from an image sequence. The resulting tracker provides robust and consistent estimations of instantaneous motion fields along the whole image sequence. In order to handle a state space of reasonable dimension for the stochastic filtering problem, we represent the motion field as a combination of adapted basis functions. The used basis functions ensue from a mollification of Biot-Savart integral and a discretization of the vorticity and divergence maps of the fluid vector field. The efficiency of the method is demonstrated on a long real world sequence showing a vortex launch at tip of airplane wing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541283",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 5,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Filters",
                "Tracking",
                "Image sequences",
                "Fluid flow",
                "Bayesian methods",
                "Indium tin oxide",
                "Diffusion processes",
                "Navier-Stokes equations",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "Bayes methods",
                "Navier-Stokes equations",
                "flow",
                "integral equations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stochastic filter",
                "fluid motion tracking",
                "fluid flow velocity field",
                "sequential Bayesian filter",
                "Ito diffusion process",
                "Navier-Stokes equation",
                "image sequence",
                "instantaneous motion estimation",
                "stochastic filtering",
                "Biot-Savart integral",
                "fluid vector field"
            ]
        },
        "id": 51,
        "cited_by": []
    },
    {
        "title": "Priors for people tracking from small training sets",
        "authors": [
            "R. Urtasun",
            "D.J. Fleet",
            "A. Hertzmann",
            "P. Fua"
        ],
        "abstract": "We advocate the use of scaled Gaussian process latent variable models (SGPLVM) to learn prior models of 3D human pose for 3D people tracking. The SGPLVM simultaneously optimizes a low-dimensional embedding of the high-dimensional pose data and a density function that both gives higher probability to points close to training data and provides a nonlinear probabilistic mapping from the low-dimensional latent space to the full-dimensional pose space. The SGPLVM is a natural choice when only small amounts of training data are available. We demonstrate our approach with two distinct motions, golfing and walking. We show that the SGPLVM sufficiently constrains the problem such that tracking can be accomplished with straightforward deterministic optimization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541284",
        "reference_list": [],
        "citation": {
            "ieee": 79,
            "other": 62,
            "total": 141
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Training data",
                "Biological system modeling",
                "Density functional theory",
                "Computer science",
                "Gaussian processes",
                "Legged locomotion",
                "Simultaneous localization and mapping",
                "Constraint optimization",
                "Clothing"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "Gaussian processes",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scaled Gaussian process latent variable model",
                "3D human pose",
                "people tracking",
                "nonlinear probabilistic mapping",
                "low-dimensional latent space",
                "full-dimensional pose space",
                "golfing",
                "walking"
            ]
        },
        "id": 52,
        "cited_by": [
            {
                "year": "2015",
                "id": 452
            },
            {
                "year": "2011",
                "id": 262
            },
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2007",
                "id": 104
            },
            {
                "year": "2007",
                "id": 107
            },
            {
                "year": "2007",
                "id": 112
            },
            {
                "year": "2007",
                "id": 190
            },
            {
                "year": "2007",
                "id": 191
            },
            {
                "year": "2007",
                "id": 204
            },
            {
                "year": "2007",
                "id": 265
            }
        ]
    },
    {
        "title": "Discontinuity preserving stereo with small baseline multi-flash illumination",
        "authors": [
            "R. Feris",
            "R. Raskar",
            "Longbin Chen",
            "Kar-Han Tan",
            "M. Turk"
        ],
        "abstract": "Currently, sharp discontinuities in depth and partial occlusions in multiview imaging systems pose serious challenges for many dense correspondence algorithms. However, it is important for 3D reconstruction methods to preserve depth edges as they correspond to important shape features like silhouettes which are critical for understanding the structure of a scene. In this paper, we show how active illumination algorithms can produce a rich set of feature maps that are useful in dense 3D reconstruction. We start by showing a method to compute a qualitative depth map from a single camera, which encodes object relative distances and can be used as a prior for stereo. In a multiview setup, we show that along with depth edges, binocular half-occluded pixels can also be explicitly and reliably labeled. To demonstrate the usefulness of these feature maps, we show how they can be used in two different algorithms for dense stereo correspondence. Our experimental results show that our enhanced stereo algorithms are able to extract high quality, discontinuity preserving correspondence maps from scenes that are extremely challenging for conventional stereo methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541285",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Layout",
                "Image reconstruction",
                "Image edge detection",
                "Cameras",
                "Shape",
                "Pixel",
                "Stereo vision",
                "Computer vision",
                "Reconstruction algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image reconstruction",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discontinuity preserving stereo",
                "multiflash illumination",
                "partial occlusion",
                "multiview imaging system",
                "dense correspondence algorithm",
                "3D reconstruction",
                "feature map",
                "binocular half-occluded pixel",
                "discontinuity preserving correspondence map"
            ]
        },
        "id": 53,
        "cited_by": []
    },
    {
        "title": "Structured light in scattering media",
        "authors": [
            "S.G. Narasimhan",
            "S.K. Nayar",
            "Bo Sun",
            "S.J. Koppal"
        ],
        "abstract": "Virtually all structured light methods assume that the scene and the sources are immersed in pure air and that light is neither scattered nor absorbed. Recently, however, structured lighting has found growing application in underwater and aerial imaging, where scattering effects cannot be ignored. In this paper, we present a comprehensive analysis of two representative methods - light stripe range scanning and photometric stereo - in the presence of scattering. For both methods, we derive physical models for the appearances of a surface immersed in a scattering medium. Based on these models, we present results on (a) the condition for object detectability in light striping and (b) the number of sources required for photometric stereo. In both cases, we demonstrate that while traditional methods fail when scattering is significant, our methods accurately recover the scene (depths, normals, albedos) as well as the properties of the medium. These results are in turn used to restore the appearances of scenes as if they were captured in clear air. Although we have focused on light striping and photometric stereo, our approach can also be extended to other methods such as grid coding, gated and active polarization imaging.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541286",
        "reference_list": [],
        "citation": {
            "ieee": 48,
            "other": 35,
            "total": 83
        },
        "keywords": {
            "IEEE Keywords": [
                "Light scattering",
                "Optical scattering",
                "Layout",
                "Photometry",
                "Optical imaging",
                "Surface reconstruction",
                "Optical attenuators",
                "Sun",
                "Robots",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image restoration",
                "light scattering",
                "photometry",
                "underwater optics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scattering media",
                "structured light",
                "underwater imaging",
                "aerial imaging",
                "light stripe range scanning",
                "photometric stereo",
                "light striping",
                "grid coding",
                "gated polarization imaging",
                "active polarization imaging"
            ]
        },
        "id": 54,
        "cited_by": [
            {
                "year": "2017",
                "id": 253
            },
            {
                "year": "2015",
                "id": 262
            },
            {
                "year": "2015",
                "id": 377
            },
            {
                "year": "2015",
                "id": 381
            }
        ]
    },
    {
        "title": "Globally optimal solutions for energy minimization in stereo vision using reweighted belief propagation",
        "authors": [
            "T. Meltzer",
            "C. Yanover",
            "Y. Weiss"
        ],
        "abstract": "A wide range of low level vision problems have been formulated in terms of finding the most probable assignment of a Markov random field (or equivalently the lowest energy configuration). Perhaps the most successful example is stereo vision. For the stereo problem, it has been shown that finding the global optimum is NP hard but good results have been obtained using a number of approximate optimization algorithms. In this paper, we show that for standard benchmark stereo pairs, the global optimum can be found in about 30 minutes using a variant of the belief propagation (BP) algorithm. We extend previous theoretical results on reweighted belief propagation to account for possible ties in the beliefs and using these results we obtain easily checkable conditions that guarantee that the BP disparities are the global optima. We verify experimentally that these conditions are typically met for the standard benchmark stereo pairs and discuss the implications of our results for further progress in stereo.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541287",
        "reference_list": [
            {
                "year": "2003",
                "id": 118
            }
        ],
        "citation": {
            "ieee": 30,
            "other": 21,
            "total": 51
        },
        "keywords": {
            "IEEE Keywords": [
                "Stereo vision",
                "Belief propagation",
                "Computer science",
                "Power engineering and energy",
                "Markov random fields",
                "Filters",
                "Energy resolution",
                "Energy measurement",
                "Cameras",
                "Polynomials"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "optimisation",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "energy minimization",
                "stereo vision",
                "reweighted belief propagation"
            ]
        },
        "id": 55,
        "cited_by": [
            {
                "year": "2017",
                "id": 577
            },
            {
                "year": "2015",
                "id": 386
            },
            {
                "year": "2009",
                "id": 72
            }
        ]
    },
    {
        "title": "BRDF invariant stereo using light transport constancy",
        "authors": [
            "J.E. Davis",
            "Ruigang Yang",
            "Liang Wang"
        ],
        "abstract": "Nearly all existing methods for stereo reconstruction assume that scene reflectance is Lambertian, and make use of color constancy as a matching invariant. We introduce a new invariant for stereo reconstruction called light transport constancy, which allows completely arbitrary scene reflectance (BRDFs). This invariant can be used to formulate a rank constraint on multiview stereo matching when the scene is observed in several lighting configurations. In addition, we show that this multiview constraint can be used with as few as two cameras and two lighting configurations. Unlikely previous methods for BRDF invariant stereo, light transport constancy does not require precisely configured or calibrated light sources, nor calibration objects in the scene. Importantly, the new constraint can be used to provide BRDF invariance to any existing stereo method, whenever appropriate lighting variation is available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541288",
        "reference_list": [
            {
                "year": "2001",
                "id": 157
            },
            {
                "year": "2003",
                "id": 76
            },
            {
                "year": "2003",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 5,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Stereo vision",
                "Reflectivity",
                "Light sources",
                "Cameras",
                "Calibration",
                "Lighting",
                "Computer vision",
                "Surface reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image reconstruction",
                "image colour analysis",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "BRDF invariant stereo",
                "light transport constancy",
                "stereo reconstruction",
                "Lambertian scene reflectance",
                "color constancy",
                "arbitrary scene reflectance",
                "multiview stereo matching"
            ]
        },
        "id": 56,
        "cited_by": [
            {
                "year": "2007",
                "id": 43
            }
        ]
    },
    {
        "title": "Vector boosting for rotation invariant multi-view face detection",
        "authors": [
            "Chang Huang",
            "Haizhou Ai",
            "Yuan Li",
            "Shihong Lao"
        ],
        "abstract": "In this paper, we propose a novel tree-structured multiview face detector (MVFD), which adopts the coarse-to-fine strategy to divide the entire face space into smaller and smaller subspaces. For this purpose, a newly extended boosting algorithm named vector boosting is developed to train the predictors for the branching nodes of the tree that have multicomponents outputs as vectors. Our MVFD covers a large range of the face space, say, +/-45/spl deg/ rotation in plane (RIP) and +/-90/spl deg/ rotation off plane (ROP), and achieves high accuracy and amazing speed (about 40 ms per frame on a 320 /spl times/ 240 video sequence) compared with previous published works. As a result, by simply rotating the detector 90/spl deg/, 180/spl deg/ and 270/spl deg/, a rotation invariant (360/spl deg/ RIP) MVFD is implemented that achieves real time performance (11 fps on a 320 /spl times/ 240 video sequence) with high accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541289",
        "reference_list": [
            {
                "year": "2003",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 71,
            "other": 28,
            "total": 99
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Face detection",
                "Detectors",
                "Space technology",
                "Tree data structures",
                "Artificial intelligence",
                "Computer science",
                "Laboratories",
                "Video sequences",
                "Bayesian methods"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "decision trees"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "vector boosting",
                "multiview face detection",
                "coarse-to-fine strategy",
                "branching nodes",
                "decision trees",
                "face space"
            ]
        },
        "id": 57,
        "cited_by": [
            {
                "year": "2015",
                "id": 31
            },
            {
                "year": "2009",
                "id": 136
            },
            {
                "year": "2007",
                "id": 11
            },
            {
                "year": "2007",
                "id": 166
            }
        ]
    },
    {},
    {
        "title": "Detecting irregularities in images and in video",
        "authors": [
            "O. Boiman",
            "M. Irani"
        ],
        "abstract": "We address the problem of detecting irregularities in visual data, e.g., detecting suspicious behaviors in video sequences, or identifying salient patterns in images. The term \"irregular\" depends on the context in which the \"regular\" or \"valid\" are defined. Yet, it is not realistic to expect explicit definition of all possible valid configurations for a given context. We pose the problem of determining the validity of visual data as a process of constructing a puzzle: We try to compose a new observed image region or a new video segment (\"the query\") using chunks of data (\"pieces of puzzle\") extracted from previous visual examples (\"the database \"). Regions in the observed data which can be composed using large contiguous chunks of data from the database are considered very likely, whereas regions in the observed data which cannot be composed from the database (or can be composed, but only using small fragmented pieces) are regarded as unlikely/suspicious. The problem is posed as an inference process in a probabilistic graphical model. We show applications of this approach to identifying saliency in images and video, and for suspicious behavior recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541291",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 104,
            "other": 61,
            "total": 165
        },
        "keywords": {
            "IEEE Keywords": [
                "Image databases",
                "Visual databases",
                "Video sequences",
                "Data mining",
                "Object detection",
                "Statistical analysis",
                "Legged locomotion",
                "Computer science",
                "Image segmentation",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "video signal processing",
                "image recognition",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image irregularity detection",
                "visual data",
                "video sequence",
                "image pattern",
                "image region",
                "video segmentation",
                "probabilistic graphical model",
                "inference process"
            ]
        },
        "id": 59,
        "cited_by": [
            {
                "year": "2013",
                "id": 203
            },
            {
                "year": "2011",
                "id": 307
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2007",
                "id": 30
            },
            {
                "year": "2007",
                "id": 95
            },
            {
                "year": "2007",
                "id": 265
            }
        ]
    },
    {
        "title": "Beyond trees: common-factor models for 2D human pose recovery",
        "authors": [
            "Xiangyang Lan",
            "D.P. Huttenlocher"
        ],
        "abstract": "Tree structured models have been widely used for determining the pose of a human body, from either 2D or 3D data. While such models can effectively represent the kinematic constraints of the skeletal structure, they do not capture additional constraints such as coordination of the limbs. Tree structured models thus miss an important source of information about human body pose, as limb coordination is necessary for balance while standing, walking, or running, as well as being evident in other activities such as dancing and throwing. In this paper, we consider the use of undirected graphical models that augment a tree structure with latent variables in order to account for coordination between limbs. We refer to these as common-factor models, since they are constructed by using factor analysis to identify additional correlations in limb position that are not accounted for by the kinematic tree structure. These common-factor models have an underlying tree structure and thus a variant of the standard Viterbi algorithm for a tree can be applied for efficient estimation. We present some experimental results contrasting common-factor models with tree models, and quantify the improvement in pose estimation for 2D image data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541292",
        "reference_list": [
            {
                "year": "2001",
                "id": 110
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 1,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Biological system modeling",
                "Kinematics",
                "Tree graphs",
                "Tree data structures",
                "Joints",
                "Graphical models",
                "Viterbi algorithm",
                "Torso",
                "Joining processes"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "image restoration",
                "computer vision",
                "maximum likelihood estimation",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "common-factor model",
                "2D human pose recovery",
                "tree structured model",
                "undirected graphical model",
                "tree structure",
                "limb coordination",
                "factor analysis",
                "Viterbi algorithm",
                "pose estimation"
            ]
        },
        "id": 60,
        "cited_by": [
            {
                "year": "2017",
                "id": 458
            },
            {
                "year": "2015",
                "id": 224
            },
            {
                "year": "2013",
                "id": 159
            },
            {
                "year": "2011",
                "id": 14
            },
            {
                "year": "2011",
                "id": 91
            },
            {
                "year": "2009",
                "id": 173
            }
        ]
    },
    {
        "title": "A Bayesian approach for shadow extraction from a single image",
        "authors": [
            "Tai-Pang Wu",
            "Chi-Keung Tang"
        ],
        "abstract": "This paper addresses the problem of shadow extraction from a single image of a complex natural scene. No simplifying assumption on the camera and the light source other than the Lambertian assumption is used. Our method is unique because it is capable of translating very rough user-supplied hints into the effective likelihood and prior functions for our Bayesian optimization. The likelihood function requires a decent estimation of the shadowless image, which is obtained by solving the associated Poisson equation. Our Bayesian framework allows for the optimal extraction of smooth shadows while preserving texture appearance under the extracted shadow. Thus our technique can be applied to shadow removal, producing some best results to date compared with the current state-of-the-art techniques using a single input image. We propose related applications in shadow compositing and image repair using our Bayesian technique.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541293",
        "reference_list": [
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 3,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Cameras",
                "Lighting",
                "Light sources",
                "Computer vision",
                "Layout",
                "Optical computing",
                "Reflectivity",
                "Graphics",
                "Optimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image texture",
                "Bayes methods",
                "Poisson equation",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shadow extraction",
                "natural scene",
                "prior functions",
                "Bayesian optimization",
                "likelihood function",
                "shadowless image",
                "Poisson equation",
                "texture appearance",
                "shadow removal",
                "shadow compositing",
                "image repair"
            ]
        },
        "id": 61,
        "cited_by": [
            {
                "year": "2009",
                "id": 23
            },
            {
                "year": "2007",
                "id": 249
            }
        ]
    },
    {
        "title": "8-point algorithm revisited: factorized 8-point algorithm",
        "authors": [
            "F.C. Wu",
            "Z.Y. Hu",
            "F.Q. Duan"
        ],
        "abstract": "In this paper, a novel algorithm for the fundamental matrix estimation, called factorized 8-point algorithm, is presented. The factorized 8-point algorithm is composed of three steps: (1) The measurement matrix in the traditional 8-point algorithm is decomposed into two factor matrices; (2) By introducing some auxiliary variables, a new linear minimization problem is formed, where every element of its associated measurement matrix is simply either a measurement datum or a constant; (3) The fundamental matrix is determined by solving this minimization problem by a least squares method. Like the traditional 8-point algorithm and Hartley's normalized 8-point algorithm, the factorized 8-point algorithm is also completely linear. But unlike the normalized 8-point algorithm, the factorized 8-point algorithm does not need any pre-normalization step. Since every element of the measurement matrix in the factorized 8-point algorithm is a measurement datum or a constant, no amplification of measurement error is involved; the factorized 8-point algorithm can boost effectively the robustness of the estimation. Large numbers of experiments show that the factorized 8-point algorithm consistently outperforms the traditional 8-point algorithm. In addition, although the factorized 8-point algorithm is specially designed for fundamental matrix estimation, its basic principle can be generalized to other estimation problems in computer vision, such as camera projection matrix estimation, homography estimation, focus of expansion estimation, and trifocal tensor estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541294",
        "reference_list": [
            {
                "year": "2003",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Matrix decomposition",
                "Measurement errors",
                "Computer vision",
                "Minimization methods",
                "Geometry",
                "Laboratories",
                "Pattern recognition",
                "Automation",
                "Least squares methods",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "least squares approximations",
                "minimisation",
                "tensors",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "factorized 8-point algorithm",
                "fundamental matrix estimation",
                "measurement matrix",
                "factor matrix",
                "linear minimization",
                "least squares method",
                "computer vision",
                "camera projection matrix estimation",
                "homography estimation",
                "expansion estimation",
                "trifocal tensor estimation"
            ]
        },
        "id": 62,
        "cited_by": []
    },
    {
        "title": "Shape classifier based on generalized probabilistic descent method with hidden Markov descriptor",
        "authors": [
            "N. Thakoor",
            "J. Gao"
        ],
        "abstract": "The goal of this paper is to present a weighted likelihood discriminant for minimum error shape classification. Different from traditional maximum likelihood (ML) methods, in which classification is based on probabilities from independent individual class models as is the case for general hidden Markov model (HMM) methods, proposed method utilizes information from all classes to minimize classification error. The proposed approach uses a HMM for shape curvature as its 2D shape descriptor. In this contribution, we introduce a weighted likelihood discriminant function and present a minimum error classification strategy based on generalized probabilistic descent (GPD) method. We believe our sound theory based implementation reduces classification error by combining HMM with GPD theory. We show comparative results obtained with our approach and classic ML classification along with Fourier descriptor and Zernike moments based classification for fighter planes and vehicle shapes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541295",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Hidden Markov models",
                "Topology",
                "Computer errors",
                "Object recognition",
                "Computer vision",
                "Feature extraction",
                "Robustness",
                "Computer science",
                "Vehicles"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "hidden Markov models",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape classifier",
                "generalized probabilistic descent method",
                "minimum error shape classification",
                "maximum likelihood method",
                "hidden Markov model",
                "classification error",
                "shape curvature",
                "2D shape descriptor",
                "weighted likelihood discriminant function",
                "Fourier descriptor",
                "Zernike moments",
                "fighter plane",
                "vehicle shape"
            ]
        },
        "id": 63,
        "cited_by": [
            {
                "year": "2007",
                "id": 186
            }
        ]
    },
    {
        "title": "Contour-based learning for object detection",
        "authors": [
            "J. Shotton",
            "A. Blake",
            "R. Cipolla"
        ],
        "abstract": "We present a novel categorical object detection scheme that uses only local contour-based features. A two-stage, partially supervised learning architecture is proposed: a rudimentary detector is learned from a very small set of segmented images and applied to a larger training set of un-segmented images; the second stage bootstraps these detections to learn an improved classifier while explicitly training against clutter. The detectors are learned with a boosting algorithm which creates a location-sensitive classifier using a discriminative set of features from a randomly chosen dictionary of contour fragments. We present results that are very competitive with other state-of-the-art object detection schemes and show robustness to object articulations, clutter, and occlusion. Our major contributions are the application of boosted local contour-based features for object detection in a partially supervised learning framework, and an efficient new boosting procedure for simultaneously selecting features and estimating per-feature parameters.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541296",
        "reference_list": [
            {
                "year": "2003",
                "id": 140
            }
        ],
        "citation": {
            "ieee": 115,
            "other": 57,
            "total": 172
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Computer vision",
                "Detectors",
                "Supervised learning",
                "Boosting",
                "Object recognition",
                "Shape",
                "Humans",
                "Image segmentation",
                "Dictionaries"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "image segmentation",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contour-based learning",
                "object detection",
                "local contour-based feature",
                "supervised learning architecture",
                "rudimentary detector",
                "image segmentation",
                "boosting algorithm",
                "location-sensitive classifier",
                "object articulation",
                "object clutter",
                "object occlusion"
            ]
        },
        "id": 64,
        "cited_by": [
            {
                "year": "2009",
                "id": 61
            },
            {
                "year": "2009",
                "id": 294
            },
            {
                "year": "2007",
                "id": 80
            },
            {
                "year": "2007",
                "id": 185
            },
            {
                "year": "2007",
                "id": 195
            },
            {
                "year": "2007",
                "id": 202
            }
        ]
    },
    {
        "title": "Multi-view AAM fitting and camera calibration",
        "authors": [
            "S. Koterba",
            "S. Baker",
            "I. Matthews",
            "Changbo Hu",
            "Jing Xiao",
            "J. Cohn",
            "T. Kanade"
        ],
        "abstract": "In this paper, we study the relationship between multi-view active appearance model (AAM) fitting and camera calibration. In the first part of the paper we propose an algorithm to calibrate the relative orientation of a set of N > 1 cameras by fitting an AAM to sets of N images. In essence, we use the human face as a (non-rigid) calibration grid. Our algorithm calibrates a set of 2 /spl times/ 3 weak-perspective camera projection matrices, protections of the world coordinate system origin into the images, depths of the world coordinate system origin, and focal lengths. We demonstrate that the performance of this algorithm is comparable to a standard algorithm using a calibration grid. In the second part of the paper, we show how calibrating the cameras improves tile performance of multi-view AAM fitting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541297",
        "reference_list": [
            {
                "year": "2003",
                "id": 7
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 13,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Cameras",
                "Calibration",
                "Robot vision systems",
                "Humans",
                "Face recognition",
                "Robot kinematics",
                "Surveillance",
                "Legged locomotion",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "curve fitting",
                "cameras",
                "calibration",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera calibration",
                "multiview active appearance model fitting",
                "human face",
                "nonrigid calibration grid",
                "weak-perspective camera projection matrix",
                "world coordinate system",
                "focal length"
            ]
        },
        "id": 65,
        "cited_by": [
            {
                "year": "2007",
                "id": 158
            }
        ]
    },
    {
        "title": "Using eye reflections for face recognition under varying illumination",
        "authors": [
            "K. Nishino",
            "P.N. Belhumeur",
            "S.K. Nayar"
        ],
        "abstract": "Face recognition under varying illumination remains a challenging problem. Much progress has been made toward a solution through methods that require multiple gallery images of each subject under varying illumination. Yet for many applications, this requirement is too severe. In this paper, we propose a novel method that requires only a single gallery image per subject taken under unknown lighting. The method builds upon two contributions. We first estimate the lighting from its reflection in the eyes. This allows us to explicitly recover the illumination in the single gallery images as well as the probe image. Next, we exploit the local linearity of face appearance variation across different people. We represent the gallery images as locally linear montages of images of many different faces taken under the same lighting (bootstrap images). Then, we transfer the estimated combination of bootstrap images to synthesize each subject's face under tile probe lighting to accomplish recognition. Finally, we show through tests on the CMU PIE database that we can achieve better recognition results using our lighting estimation method and locally linear montages than the current state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541298",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 7,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Lighting",
                "Optical reflection",
                "Probes",
                "Eyes",
                "Linearity",
                "Tiles",
                "Image recognition",
                "Testing",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image reconstruction",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "eye reflection",
                "face recognition",
                "varying illumination",
                "single gallery image",
                "unknown lighting",
                "face appearance variation",
                "linear montage",
                "tile probe lighting",
                "CMU PIE database",
                "lighting estimation"
            ]
        },
        "id": 66,
        "cited_by": []
    },
    {
        "title": "Mutual information-based 3D surface matching with applications to face recognition and brain mapping",
        "authors": [
            "Yalin Wang",
            "Ming-Chang Chiang",
            "P.M. Thompson"
        ],
        "abstract": "Face recognition and many medical imaging applications require the computation of dense correspondence vector fields that match one surface with another. In brain imaging, surface-based registration is useful for tracking brain change, and for creating statistical shape models of anatomy. Based on surface correspondences, metrics can also be designed to measure differences in facial geometry and expressions. To avoid the need for a large set of manually-defined landmarks to constrain these surface correspondences, we developed an algorithm to automate the matching of surface features. It extends the mutual information method to automatically match general 3D surfaces (including surfaces with a branching topology). We use diffeomorphic flows to optimally align the Riemann surface structures of two surfaces. First, we use holomorphic I-forms to induce consistent conformal grids on both surfaces. High genus surfaces are mapped to a set of rectangles in the Euclidean plane and closed genus-zero surfaces are mapped to the sphere. Next, we compute stable geometric features (mean curvature and conformal factor) and pull them back as scalar fields onto the 2D parameter domains. Mutual information is used as a cost functional to drive a fluid flow in the parameter domain that optimally aligns these surface features. A diffeomorphic surface-to-surface mapping is then recovered that matches surfaces in 3D. Lastly, we present a spectral method that ensures that the grids induced on the target surface remain conformal when pulled through the correspondence field. Using the chain rule, we express the gradient of the mutual information between surfaces in the conformal basis of the source surface. This finite-dimensional linear space generates all conformal reparameterizations of the surface. Illustrative experiments apply the method to face recognition and to the registration of brain structures, such as the hippocampus in 3D MRI scans, a key step in understanding brain shape alt...",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541299",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 0,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Brain mapping",
                "Mutual information",
                "Shape",
                "Biomedical imaging",
                "Computer applications",
                "Brain modeling",
                "Anatomy",
                "Geometry",
                "Topology"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "medical image processing",
                "image registration",
                "face recognition",
                "computational geometry",
                "spectral analysis",
                "gradient methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mutual information",
                "surface matching",
                "face recognition",
                "brain mapping",
                "medical imaging",
                "dense correspondence vector field",
                "brain imaging",
                "surface-based registration",
                "surface correspondence",
                "branching topology",
                "diffeomorphic flow",
                "Riemann surface structure",
                "holomorphic I-form",
                "Euclidean plane",
                "closed genus-zero surface",
                "stable geometric feature",
                "mean curvature",
                "conformal factor",
                "fluid flow",
                "diffeomorphic surface-to-surface mapping",
                "spectral method",
                "finite-dimensional linear space",
                "MRI scan",
                "brain shape alteration",
                "Alzheimer disease",
                "schizophrenia"
            ]
        },
        "id": 67,
        "cited_by": []
    },
    {
        "title": "Real-time interactively distributed multi-object tracking using a magnetic-inertia potential model",
        "authors": [
            "Wei Qu",
            "D. Schonfeld",
            "M. Mohamed"
        ],
        "abstract": "This paper breaks with the common practice of using a joint state space representation and performing the joint data association in multi-object tracking. Instead, we present an interactively distributed framework with linear complexity for real-time applications. When objects do not interact on each other our approach performs like multiple independent trackers. When, the objects are in close proximity or present occlusions, we propose a magnetic-inertia potential model to handle the \"error merge\" and \"labeling\" problems in a particle filtering framework. Specifically we propose to model the interactive likelihood densities by a \"gravitation\" and \"magnetic\" repulsion scheme and relax the common first-order Markov chain assumption by using an \"inertia\" Markov chain. Our model represents the cumulative effect of virtual physical forces that objects undergo while interacting with others. It implicitly handles the \"error merge\" and \"labeling\" problems and thus solves the difficult object occlusion and data association problems using an innovative scheme. Our preliminary work has demonstrated that the proposed approach is far superior to existing methods not only in robustness but also in speed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541300",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 2,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "State-space methods",
                "Particle tracking",
                "Particle filters",
                "Magnetic separation",
                "Switches",
                "Filtering",
                "Robustness",
                "Computational efficiency",
                "Collaboration",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "Markov processes",
                "distributed tracking",
                "particle filtering (numerical methods)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time interactively distributed multiobject tracking",
                "magnetic-inertia potential model",
                "joint state space representation",
                "joint data association",
                "linear complexity",
                "Markov chain",
                "particle filtering",
                "interactive likelihood density",
                "error merge problem",
                "labeling problem",
                "object occlusion"
            ]
        },
        "id": 68,
        "cited_by": [
            {
                "year": "2011",
                "id": 314
            }
        ]
    },
    {
        "title": "Image based regression using boosting method",
        "authors": [
            "Shaohua Kevin Zhou",
            "B. Georgescu",
            "Xiang Sean Zhou",
            "D. Comaniciu"
        ],
        "abstract": "We present a general algorithm of image based regression that is applicable to many vision problems. The proposed regressor that targets a multiple-output setting is learned using boosting method. We formulate a multiple-output regression problem in such a way that overfitting is decreased and an analytic solution is admitted. Because we represent the image via a set of highly redundant Haar-like features that can be evaluated very quickly and select relevant features through boosting to absorb the knowledge of the training data, during testing we require no storage of the training data and evaluate the regression function almost in no time. We also propose an efficient training algorithm that breaks the computational bottleneck in the greedy feature selection process. We validate the efficiency of the proposed regressor using three challenging tasks of age estimation, tumor detection, and endocardial wall localization and achieve the best performance with a dramatic speed, e.g., more than 1000 times faster than conventional data-driven techniques such as support vector regressor in the experiment of endocardial wall localization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541301",
        "reference_list": [
            {
                "year": "2001",
                "id": 131
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 1,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Humans",
                "Kernel",
                "Training data",
                "Image storage",
                "Tumors",
                "Computer vision",
                "Neoplasms",
                "Anisotropic magnetoresistance",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "learning (artificial intelligence)",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image based regression",
                "boosting method",
                "Haar-like feature",
                "training algorithm",
                "greedy feature selection",
                "age estimation",
                "tumor detection",
                "endocardial wall localization",
                "support vector regressor"
            ]
        },
        "id": 69,
        "cited_by": [
            {
                "year": "2013",
                "id": 358
            },
            {
                "year": "2009",
                "id": 255
            },
            {
                "year": "2007",
                "id": 229
            },
            {
                "year": "2007",
                "id": 266
            }
        ]
    },
    {
        "title": "Efficient block noise removal based on nonlinear manifolds",
        "authors": [
            "Haoying Fu",
            "Hongyuan Zha",
            "J. Barlow"
        ],
        "abstract": "The problem of block noise removal is considered. It is assumed that the original image is on or close to a sub-space of admissible images in the form of a low dimensional nonlinear manifold. We propose to use a close variant of the total variation regularizer for measuring block noise. Based on this noise measure, we present an effective approach that reconstructs the original image in the presence of block noise. Our main computational task is the solution of a quadratic programming problem, for which we propose a very efficient interior point method. The effectiveness and efficiency of our approach is demonstrated by an example.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541302",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise measurement",
                "Principal component analysis",
                "Image reconstruction",
                "Kernel",
                "Manifolds",
                "Video sequences",
                "Computer science",
                "Quadratic programming",
                "Propagation losses",
                "Internet"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image denoising"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "block noise removal",
                "nonlinear manifold",
                "image reconstruction",
                "computational task",
                "quadratic programming",
                "interior point method"
            ]
        },
        "id": 70,
        "cited_by": []
    },
    {
        "title": "Improved sub-pixel stereo correspondences through symmetric refinement",
        "authors": [
            "D. Nehab",
            "S. Rusinkiewiez",
            "J. Davis"
        ],
        "abstract": "Most dense stereo correspondence algorithms start by establishing discrete pixel matches and later refine these matches to sub-pixel precision. Traditional sub-pixel refinement methods attempt to determine the precise location of points, in the secondary image, that correspond to discrete positions in the reference image. We show that this strategy can lead to a systematic bias associated with the violation of the general symmetry of matching cost functions. This bias produces random or coherent noise in the final reconstruction, but can be avoided by refining both image coordinates simultaneously, in a symmetric way. We demonstrate that the symmetric sub-pixel refinement strategy results in more accurate correspondences by avoiding bias while preserving detail.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541303",
        "reference_list": [
            {
                "year": "2001",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 9,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Cost function",
                "Cameras",
                "Refining",
                "Rendering (computer graphics)",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image reconstruction",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subpixel stereo correspondence",
                "symmetric refinement",
                "discrete pixel matching",
                "subpixel refinement",
                "image reconstruction"
            ]
        },
        "id": 71,
        "cited_by": [
            {
                "year": "2007",
                "id": 381
            }
        ]
    },
    {
        "title": "What metrics can be approximated by geo-cuts, or global optimization of length/area and flux",
        "authors": [
            "V. Kolmogorov",
            "Y. Boykov"
        ],
        "abstract": "In the work of the authors (2003), we showed that graph cuts can find hypersurfaces of globally minimal length (or area) under any Riemannian metric. Here we show that graph cuts on directed regular grids can approximate a significantly more general class of continuous non-symmetric metrics. Using submodularity condition (Boros and Hammer, 2002 and Kolmogorov and Zabih, 2004), we obtain a tight characterization of graph-representable metrics. Such \"submodular\" metrics have an elegant geometric interpretation via hypersurface functionals combining length/area and flux. Practically speaking, we attend 'geo-cuts' algorithm to a wider class of geometrically motivated hypersurface functionals and show how to globally optimize any combination of length/area and flux of a given vector field. The concept of flux was recently introduced into computer vision by Vasilevskiy and Siddiqi (2002) but it was mainly studied within variational framework so far. We are first to show that flux can be integrated into graph cuts as well. Combining geometric concepts of flux and length/area within the global optimization framework of graph cuts allows principled discrete segmentation models and advances the slate of the art for the graph cuts methods in vision. In particular we address the \"shrinking\" problem of graph cuts, improve segmentation of long thin objects, and introduce useful shape constraints.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541304",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 59,
            "other": 45,
            "total": 104
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Computer vision",
                "Image segmentation",
                "Solid modeling",
                "Image edge detection",
                "Optimization methods",
                "Shape",
                "Biomedical imaging",
                "Robustness",
                "Extraterrestrial measurements"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "graph theory",
                "geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geo-cuts",
                "global optimization",
                "graph cuts",
                "directed regular grid",
                "continuous nonsymmetric metrics",
                "submodularity condition",
                "graph-representable metrics",
                "submodular metrics",
                "geometric interpretation",
                "hypersurface functionals",
                "vector field",
                "flux",
                "discrete segmentation model",
                "shrinking problem",
                "object segmentation",
                "shape constraint"
            ]
        },
        "id": 72,
        "cited_by": [
            {
                "year": "2011",
                "id": 211
            }
        ]
    },
    {
        "title": "Incorporating visual knowledge representation in stereo reconstruction",
        "authors": [
            "A. Barbu",
            "Song-Chun Zhu"
        ],
        "abstract": "In this paper, we present a two-layer generative model that incorporates generic middle-level visual knowledge for dense stereo reconstruction. The visual knowledge is represented by a dictionary of surface primitives including various categories of boundary discontinuities and junctions in parametric form. Given a stereo pair, we first compute a primal sketch representation which decomposes the image into a structural part for object boundaries and high intensity contrast represented by a 2D sketch graph, and a structure less part represented by Markov random field on pixels. Then we label the sketch graph and compute the 3D sketch (like a wire-frame) by fitting the primitive dictionary to the sketch graph. The surfaces between the 3D sketches are filled in by computing the depth of the MRP model on the structureless part. These two levels interact closely since the MRF is used to propagate information between the primitives, and at the same time, the primitives act as boundary conditions for the MRF. The two processes maximize a Bayesian posterior probability jointly. We propose an MCMC algorithm that simultaneously infers the 3D primitive types and parameters and estimates the depth of the scene. Our experiments show that this representation can infer the depth map with sharp boundaries and junctions for textureless images, curve objects and free-form shapes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541305",
        "reference_list": [
            {
                "year": "2001",
                "id": 70
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Knowledge representation",
                "Stereo image processing",
                "Image reconstruction",
                "Dictionaries",
                "Surface reconstruction",
                "Markov random fields",
                "Pixel",
                "Surface fitting",
                "Materials requirements planning",
                "Boundary conditions"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image reconstruction",
                "image texture",
                "knowledge representation",
                "Markov processes",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual knowledge representation",
                "stereo reconstruction",
                "two-layer generative model",
                "primal sketch representation",
                "image decomposition",
                "object boundary",
                "Markov random field",
                "sketch graph",
                "Bayesian posterior probability",
                "MCMC algorithm",
                "textureless image",
                "curve object",
                "free-form shape"
            ]
        },
        "id": 73,
        "cited_by": []
    },
    {
        "title": "Multiperspective projection and collineation",
        "authors": [
            "Jingyi Yu",
            "L. McMillan"
        ],
        "abstract": "We present theories of multiperspective projection and collineation. Given an arbitrary multiperspective imaging system that captures smoothly varying set of rays, we show how to map the rays onto a 2D ray manifold embedded in a 4D linear vector space. The characteristics of this imaging system, such as its projection, collineation, and image distortions can be analyzed by studying the 2D tangent planes of this ray manifold. These tangent planes correspond to the recently proposed general linear camera (GLC) model. In this paper, we study the imaging process of the GLCs. We show the GLC imaging process can be broken down into two separate stages: the mapping of 3D geometry to rays and the sampling of those rays over an image plane. We derive a closed-form solution to projecting 3D points in a scene to rays in a GLC. A GLC image is created by sampling these rays over an image plane. We develop a notion of GLC collineation analogous to pinhole cameras. GLC collineation describes the transformation between the images of a single GLC due to changes in sampling and image plane selection. We show that general GLC collineations can be characterized by a quartic (4th order) rational function. GLC projection and collineation provides a basis for developing new computer vision algorithms suitable for analyzing a wider range of imaging systems than current methods, based on simple pinhole projection models, permit.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541306",
        "reference_list": [
            {
                "year": "2001",
                "id": 104
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image analysis",
                "Geometry",
                "Image sampling",
                "Closed-form solution",
                "Layout",
                "Computer vision",
                "Mirrors",
                "Vectors",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sampling",
                "cameras",
                "ray tracing",
                "geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiperspective projection",
                "multiperspective collineation",
                "multiperspective imaging system",
                "2D ray manifold",
                "4D linear vector space",
                "image distortion",
                "2D tangent plane",
                "general linear camera",
                "imaging process",
                "3D geometry mapping",
                "image sampling",
                "pinhole camera",
                "image plane selection",
                "computer vision"
            ]
        },
        "id": 74,
        "cited_by": [
            {
                "year": "2009",
                "id": 234
            }
        ]
    },
    {
        "title": "Recovering facial shape and albedo using a statistical model of surface normal direction",
        "authors": [
            "W.A.P. Smith",
            "E.R. Hancock"
        ],
        "abstract": "This paper describes how facial shape can be modelled using a statistical model that captures variations in surface normal direction. To construct this model, we make use of the azimuthal equidistant projection to map surface normals from the unit sphere to points on a local tangent plane. The variations in surface normal direction are captured using the covariance matrix for the projected point positions. This allows us to model variations in face shape using a standard point distribution model. We train the model on fields of surface normals extracted from range data and show how to fit the model to intensity data using constraints on the surface normal direction provided by Lambert's law. We demonstrate that this process yields accurate facial shape recovery and allows an estimate of the albedo map to be made from single, real world face images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541307",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 10,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Surface fitting",
                "Principal component analysis",
                "Azimuth",
                "Computer science",
                "Covariance matrix",
                "Data mining",
                "Yield estimation",
                "Nose",
                "Photometry"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "statistical analysis",
                "covariance matrices",
                "albedo"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial shape recovery",
                "albedo",
                "statistical model",
                "surface normal direction",
                "azimuthal equidistant projection",
                "covariance matrix",
                "standard point distribution model",
                "Lambert law",
                "real world face image"
            ]
        },
        "id": 75,
        "cited_by": []
    },
    {
        "title": "Linear approaches to camera calibration from sphere images or active intrinsic calibration using vanishing points",
        "authors": [
            "Xianghua Ying",
            "Hongbin Zha"
        ],
        "abstract": "Spherical objects and vanish points are often used for camera calibration. An occluding contour of a sphere is projected to a conic in the perspective image, and using a moving active camera, the trajectory of a vanishing point in the perspective images is also a conic when the camera is rotated about a fixed 3D axis whereas the translation of the camera is arbitrary. In fact, the problems of camera calibration using conics from spheres or vanishing points can be described by same mathematic representations. Two linear approaches to the problems are proposed in this paper: one based on the geometric interpretation of the relation between image conics and the image of the absolute conic, and the other using the special structure of the problems in algebra. Only three such conics are needed for the two linear approaches, and the minimum number for previous nonlinear optimization methods is also three. All five intrinsic parameters are recovered linearly without making assumptions, such as, zero-skew or unitary aspect ratio which are often used in previous methods. The two linear algorithms have been tested in extensive experiments with respect to noise sensitivity and also made comparisons with recent calibration techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541308",
        "reference_list": [
            {
                "year": "2003",
                "id": 103
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Mathematics",
                "Laboratories",
                "Algebra",
                "Optimization methods",
                "Testing",
                "Optical imaging",
                "Optical sensors",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "cameras",
                "calibration",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera calibration",
                "sphere image",
                "active intrinsic calibration",
                "vanishing point",
                "spherical object",
                "occluding sphere contour",
                "moving active camera",
                "geometric interpretation",
                "image conics",
                "absolute conic image",
                "algebra",
                "nonlinear optimization",
                "linear algorithm",
                "noise sensitivity"
            ]
        },
        "id": 76,
        "cited_by": []
    },
    {
        "title": "Creating efficient codebooks for visual recognition",
        "authors": [
            "F. Jurie",
            "B. Triggs"
        ],
        "abstract": "Visual codebook based quantization of robust appearance descriptors extracted from local image patches is an effective means of capturing image statistics for texture analysis and scene classification. Codebooks are usually constructed by using a method such as k-means to cluster the descriptor vectors of patches sampled either densely ('textons') or sparsely ('bags of features' based on key-points or salience measures) from a set of training images. This works well for texture analysis in homogeneous images, but the images that arise in natural object recognition tasks have far less uniform statistics. We show that for dense sampling, k-means over-adapts to this, clustering centres almost exclusively around the densest few regions in descriptor space and thus failing to code other informative regions. This gives suboptimal codes that are no better than using randomly selected centres. We describe a scalable acceptance-radius based clusterer that generates better codebooks and study its performance on several image classification tasks. We also show that dense representations outperform equivalent keypoint based ones on these tasks and that SVM or mutual information based feature selection starting from a dense codebook further improves the performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541309",
        "reference_list": [
            {
                "year": "2003",
                "id": 84
            },
            {
                "year": "2003",
                "id": 60
            },
            {
                "year": "2003",
                "id": 38
            }
        ],
        "citation": {
            "ieee": 218,
            "other": 210,
            "total": 428
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistical analysis",
                "Image analysis",
                "Image texture analysis",
                "Quantization",
                "Robustness",
                "Layout",
                "Object recognition",
                "Image sampling",
                "Image classification",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image coding",
                "feature extraction",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual recognition",
                "visual codebook based quantization",
                "robust appearance descriptor",
                "image patch",
                "dense sampling",
                "scalable acceptance-radius based clusterer",
                "image classification",
                "support vector machine",
                "mutual information",
                "feature selection",
                "dense codebook"
            ]
        },
        "id": 77,
        "cited_by": [
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2013",
                "id": 211
            },
            {
                "year": "2011",
                "id": 316
            },
            {
                "year": "2009",
                "id": 80
            },
            {
                "year": "2007",
                "id": 13
            },
            {
                "year": "2007",
                "id": 27
            },
            {
                "year": "2007",
                "id": 34
            },
            {
                "year": "2007",
                "id": 198
            },
            {
                "year": "2007",
                "id": 225
            }
        ]
    },
    {
        "title": "Photometric stereo under perspective projection",
        "authors": [
            "A. Tankus",
            "N. Kiryati"
        ],
        "abstract": "Photometric stereo is a fundamental approach in computer vision. At its core lies a set of image irradiance equations each taken with a different illumination. The vast majority of studies in this field have assumed orthography as the projection model. This paper re-examines the basic set of equations of photometric stereo, under an assumption of perspective projection. We show that the resulting system is linear (as is the case under the orthographic model; Nevertheless, the unknowns are different in the perspective case). We then suggest a simple reconstruction algorithm based on the perspective formulae, and compare it to its orthographic counterpart on synthetic as well as real images. This algorithm obtained lower error rates than the orthographic one in all of the error measures. These findings strengthen the hypothesis that a more realistic set of assumptions, the perspective one, improves reconstruction significantly.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541310",
        "reference_list": [
            {
                "year": "2003",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 12,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Stereo vision",
                "Equations",
                "Lighting",
                "Computer vision",
                "Reconstruction algorithms",
                "Layout",
                "Reflectivity",
                "Light sources",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image reconstruction",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photometric stereo",
                "perspective projection",
                "computer vision",
                "image irradiance equation",
                "orthographic model",
                "reconstruction algorithm",
                "synthetic image",
                "real image",
                "image reconstruction"
            ]
        },
        "id": 78,
        "cited_by": [
            {
                "year": "2015",
                "id": 381
            }
        ]
    },
    {
        "title": "An expectation maximization approach to the synergy between image segmentation and object categorization",
        "authors": [
            "I. Kokkinos",
            "P. Maragos"
        ],
        "abstract": "In this work, we deal with the problem of modelling and exploiting the interaction between the processes of image segmentation and object categorization. We propose a novel framework to address this problem that is based on the combination of the expectation maximization (EM) algorithm and generative models for object categories. Using a concise formulation of the interaction between these two processes, segmentation is interpreted as the E step, assigning observations to models, whereas object detection/analysis is modelled as the M-step, fitting models to observations. We present in detail the segmentation and detection processes comprising the E and M steps and demonstrate results on the joint detection and segmentation of the object categories of faces and cars.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541311",
        "reference_list": [
            {
                "year": "2003",
                "id": 43
            },
            {
                "year": "2001",
                "id": 101
            },
            {
                "year": "2003",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Computer vision",
                "Object detection",
                "Face detection",
                "Biological system modeling",
                "Signal processing",
                "Oral communication",
                "Signal processing algorithms",
                "Fitting",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "image segmentation",
                "image classification",
                "expectation-maximisation algorithm"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "object categorization",
                "expectation maximization algorithm",
                "object detection",
                "object analysis",
                "model fitting"
            ]
        },
        "id": 79,
        "cited_by": []
    },
    {
        "title": "Fundamental matrix for cameras with radial distortion",
        "authors": [
            "J.P. Barreto",
            "K. Daniilidis"
        ],
        "abstract": "When deploying a heterogeneous camera network or when we use cheap zoom cameras like in cell-phones, it is not practical, if not impossible to off-line calibrate the radial distortion of each camera using reference objects. It is rather desirable to have an automatic procedure without strong assumptions about the scene. In this paper, we present a new algorithm for estimating the epipolar geometry of two views where the two views can be radially distorted with different distortion factors. It is the first algorithm in the literature solving the case of different distortion in the left and right view linearly and without assuming the existence of lines in the scene. Points in the projective plane are lifted to a quadric in three-dimensional projective space. A radial distortion of the projective plane results to a matrix transformation in the space of lifted coordinates. The new epipolar constraint depends linearly on a 4 /spl times/ 4 radial fundamental matrix which has 9 degrees of freedom. A complete algorithm is presented and tested on real imagery.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541312",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 18,
            "total": 51
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Nonlinear distortion",
                "Calibration",
                "Lenses",
                "Layout",
                "Robot vision systems",
                "Optical distortion",
                "Geometry",
                "Video sequences",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "cameras",
                "calibration",
                "optical distortion",
                "matrix algebra",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "radial distortion",
                "heterogeneous camera network",
                "zoom camera",
                "cell phone camera",
                "epipolar geometry",
                "3D projective space",
                "matrix transformation",
                "lifted coordinate space",
                "real imagery",
                "camera calibration"
            ]
        },
        "id": 80,
        "cited_by": [
            {
                "year": "2017",
                "id": 244
            },
            {
                "year": "2015",
                "id": 257
            },
            {
                "year": "2013",
                "id": 66
            },
            {
                "year": "2007",
                "id": 297
            }
        ]
    },
    {
        "title": "KALMANSAC: robust filtering by consensus",
        "authors": [
            "A. Vedaldi",
            "Hailin Jin",
            "P. Favaro",
            "S. Soatto"
        ],
        "abstract": "We propose an algorithm to perform causal inference of the state of a dynamical model when the measurements are corrupted by outliers. While the optimal (maximum-likelihood) solution has doubly exponential complexity due to the combinatorial explosion of possible choices of inliers, we exploit the structure of the problem to design a sampling-based algorithm that has constant complexity. We derive our algorithm from the equations of the optimal filter, which makes our approximation explicit. Our work is motivated by real-time tracking and the estimation of structure from motion (SFM). We test our algorithm for on-line outlier rejection both for tracking and for SFM. We show that our approach can tolerate a large proportion of outliers, whereas previous causal robust statistical inference methods failed with less than half as many. Our work can be thought of as the extension of random sample consensus algorithms to dynamic data, or as the implementation of pseudo-Bayesian filtering algorithms in a sampling framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541313",
        "reference_list": [
            {
                "year": "2001",
                "id": 92
            },
            {
                "year": "2003",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 4,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Inference algorithms",
                "Filtering algorithms",
                "Performance evaluation",
                "Maximum likelihood estimation",
                "Explosions",
                "Algorithm design and analysis",
                "Approximation algorithms",
                "Equations",
                "Filters"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "inference mechanisms",
                "filtering theory",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "KALMANSAC",
                "robust filtering",
                "causal inference",
                "maximum-likelihood solution",
                "combinatorial explosion",
                "sampling-based algorithm",
                "computational complexity",
                "optimal filter",
                "real-time tracking",
                "structure from motion",
                "online outlier rejection",
                "statistical inference",
                "random sample consensus algorithm",
                "pseudo-Bayesian filtering"
            ]
        },
        "id": 81,
        "cited_by": []
    },
    {
        "title": "Mixtures of dynamic textures",
        "authors": [
            "A.B. Chan",
            "N. Vasconcelos"
        ],
        "abstract": "A dynamic texture is a linear dynamical system used to model a single video as a sample from a spatio-temporal stochastic process. In this work, we introduce the mixture of dynamic textures, which models a collection of videos consisting of different visual processes as samples from a set of dynamic textures. We derive the EM algorithm for learning a mixture of dynamic textures, and relate the learning algorithm and the dynamic texture mixture model to previous works. Finally, we demonstrate the applicability of the proposed model to problems that have traditionally been challenging for computer vision.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541314",
        "reference_list": [
            {
                "year": "2003",
                "id": 161
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 15,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicle dynamics",
                "Computer vision",
                "Stochastic processes",
                "Biological system modeling",
                "Application software",
                "Vegetation",
                "Birds",
                "Remote monitoring",
                "Fires",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "video signal processing",
                "image texture",
                "expectation-maximisation algorithm",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic texture",
                "videos collection",
                "visual process",
                "EM algorithm",
                "learning algorithm",
                "computer vision"
            ]
        },
        "id": 82,
        "cited_by": []
    },
    {
        "title": "Manifold clustering",
        "authors": [
            "R. Souvenir",
            "R. Pless"
        ],
        "abstract": "Manifold learning has become a vital tool in data driven methods for interpretation of video, motion capture, and handwritten character data when they lie on a low dimensional, nonlinear manifold. This work extends manifold learning to classify and parameterize unlabeled data which lie on multiple, intersecting manifolds. This approach significantly increases the domain to which manifold learning methods can be applied, allowing parameterization of example manifolds such as figure eights and intersecting paths which are quite common in natural data sets. This approach introduces several technical contributions which may be of broader interest, including node-weighted multidimensional scaling and a fast algorithm for weighted low-rank approximation for rank-one weight matrices. We show examples for intersecting manifolds of mixed topology and dimension and demonstrations on human motion capture data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541315",
        "reference_list": [],
        "citation": {
            "ieee": 39,
            "other": 39,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Clustering algorithms",
                "Multidimensional systems",
                "Approximation algorithms",
                "Independent component analysis",
                "Humans",
                "Computer science",
                "Data engineering",
                "Drives",
                "Learning systems"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image classification",
                "learning (artificial intelligence)",
                "approximation theory",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "manifold clustering",
                "manifold learning",
                "video interpretation",
                "handwritten character data",
                "nodeweighted multidimensional scaling",
                "weighted low-rank approximation",
                "rank-one weight matrix",
                "human motion capture data"
            ]
        },
        "id": 83,
        "cited_by": [
            {
                "year": "2009",
                "id": 67
            },
            {
                "year": "2009",
                "id": 217
            },
            {
                "year": "2007",
                "id": 363
            }
        ]
    },
    {
        "title": "Geometric context from a single image",
        "authors": [
            "D. Hoiem",
            "A.A. Efros",
            "M. Hebert"
        ],
        "abstract": "Many computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label. These confidences can then be used to improve the performance of many other applications. We provide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single-view reconstruction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541316",
        "reference_list": [
            {
                "year": "2003",
                "id": 151
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 246,
            "other": 100,
            "total": 346
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Object detection",
                "Computer vision",
                "Surface reconstruction",
                "Cameras",
                "Application software",
                "Image reconstruction",
                "Humans",
                "Solid modeling",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image reconstruction",
                "learning (artificial intelligence)",
                "natural scenes",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "3D geometric image structure",
                "coarse geometric property",
                "appearance-based model",
                "geometric class",
                "cluttered natural scene",
                "scene structure estimation",
                "object detection",
                "automatic single-view reconstruction"
            ]
        },
        "id": 84,
        "cited_by": [
            {
                "year": "2017",
                "id": 81
            },
            {
                "year": "2017",
                "id": 485
            },
            {
                "year": "2015",
                "id": 111
            },
            {
                "year": "2015",
                "id": 117
            },
            {
                "year": "2015",
                "id": 188
            },
            {
                "year": "2015",
                "id": 382
            },
            {
                "year": "2015",
                "id": 445
            },
            {
                "year": "2013",
                "id": 34
            },
            {
                "year": "2013",
                "id": 48
            },
            {
                "year": "2013",
                "id": 94
            },
            {
                "year": "2013",
                "id": 177
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2011",
                "id": 15
            },
            {
                "year": "2011",
                "id": 47
            },
            {
                "year": "2011",
                "id": 56
            },
            {
                "year": "2011",
                "id": 81
            },
            {
                "year": "2011",
                "id": 96
            },
            {
                "year": "2011",
                "id": 328
            },
            {
                "year": "2009",
                "id": 17
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2009",
                "id": 114
            },
            {
                "year": "2009",
                "id": 212
            },
            {
                "year": "2009",
                "id": 224
            },
            {
                "year": "2009",
                "id": 254
            },
            {
                "year": "2007",
                "id": 216
            },
            {
                "year": "2007",
                "id": 251
            },
            {
                "year": "2007",
                "id": 291
            },
            {
                "year": "2007",
                "id": 292
            },
            {
                "year": "2007",
                "id": 388
            }
        ]
    },
    {
        "title": "Opaque document imaging: building images of inaccessible texts",
        "authors": [
            "Yun Lin",
            "W.B. Seales"
        ],
        "abstract": "This paper introduces a method for building a readable image of an opaque, rolled or folded text from a volumetric, penetrating scan. The problem is framed by localizing, constructing, and manipulating an image induced by a surface embedded in a 3D voxel space. There are two central contributions that lead to the demonstrated results. First is an energy-based texture formation algorithm, which is a function of voxel intensities and the geometry of the embedded surface. Second is a regularization algorithm based on a constrained mapping of the embedded surface to a regularized image plane. The mapping preserves angles and lengths, which minimizes the distortion of text in the image. The experimental results show readable images derived from custom, high resolution (X-ray-based) CT scans of rolled papyrus and ink samples. These methods are significant for scholars seeking to study inaccessible texts, and may lead to viable techniques for scanning everyday opaque objects (books) without opening them.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541317",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Document image processing",
                "Ink",
                "Surface reconstruction",
                "Computer science",
                "Computed tomography",
                "Books",
                "Cameras",
                "Space technology",
                "Cities and towns",
                "Buildings"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "image reconstruction",
                "image resolution",
                "computerised tomography",
                "geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "opaque document imaging",
                "inaccessible text imaging",
                "readable image building",
                "rolled text",
                "folded text",
                "volumetric penetrating scan",
                "3D voxel space",
                "energy-based texture formation algorithm",
                "embedded surface geometry",
                "regularization algorithm",
                "constrained mapping",
                "regularized image plane",
                "text distortion",
                "high resolution CT scan"
            ]
        },
        "id": 85,
        "cited_by": []
    },
    {
        "title": "An integrated framework for image segmentation and perceptual grouping",
        "authors": [
            "Zhuowen Tu"
        ],
        "abstract": "This paper presents an efficient algorithm for image segmentation and a framework for perceptual grouping. It makes an attempt to provide one way of combining bottom-up and top-down approaches. In image segmentation, it generalizes the Swendsen-Wang cut algorithm (SWC) by Barbu and Zhu (2003) to make both 2-way and m-way cuts, and includes topology change processes (graph repartitioning and boundary diffusion). The method directly works at a low temperature without using annealing. We show that it is much faster than the DDMCMC approach (Tu and Zhu, 2002) and more robust than the SWC method. The results are demonstrated on the Berkeley data set. In perceptual grouping, it integrates discriminative model learning/computing, a belief propagation algorithm (BP) by Yedidia et al. (2000), and SWC into a three-layer computing framework. These methods are realized as different levels of approximation to an \"ideal\" generative model. We demonstrate the algorithm on the problem of human body configuration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541318",
        "reference_list": [
            {
                "year": "2003",
                "id": 151
            },
            {
                "year": "2003",
                "id": 45
            },
            {
                "year": "2005",
                "id": 207
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 0,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Topology",
                "Biological system modeling",
                "Humans",
                "Inference algorithms",
                "Temperature",
                "Annealing",
                "Belief propagation",
                "Shape measurement",
                "Data systems"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "belief maintenance",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "perceptual grouping",
                "bottom-up approach",
                "top-down approach",
                "Swendsen-Wang cut algorithm",
                "topology change process",
                "graph repartitioning",
                "boundary diffusion",
                "DDMCMC approach",
                "discriminative model learning",
                "belief propagation algorithm",
                "human body configuration"
            ]
        },
        "id": 86,
        "cited_by": []
    },
    {
        "title": "Learning a sparse, corner-based representation for time-varying background modelling",
        "authors": [
            "Qiang Zhu",
            "S. Avidan",
            "Kwang-Ting Cheng"
        ],
        "abstract": "Time-varying phenomenon, such as ripples on water, trees waving in the wind and illumination changes, produces false motions, which significantly compromises the performance of an outdoor-surveillance system. In this paper, we propose a corner-based background model to effectively detect moving-objects in challenging dynamic scenes. Specifically, the method follows a three-step process. First, we detect feature points using a Harris corner detector and represent them as SIFT-like descriptors. Second, we dynamically learn a background model and classify each extracted feature as either a background or a foreground feature. Last, a \"Lucas-Kanade\" feature tracker is integrated into this framework to differentiate motion-consistent foreground objects from background objects with random or repetitive motion. The key insight of our work is that a collection of SIFT-like features can effectively represent the environment and account for variations caused by natural effects with dynamic movements. Features that do not correspond to the background must therefore correspond to foreground moving objects. Our method is computational efficient and works in real-time. Experiments on challenging video clips demonstrate that the proposed method achieves a higher accuracy in detecting the foreground objects than the existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541319",
        "reference_list": [
            {
                "year": "2003",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 1,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicle dynamics",
                "Layout",
                "Kernel",
                "Time varying systems",
                "Lighting",
                "Computer vision",
                "Object detection",
                "Surveillance",
                "Robustness",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image representation",
                "object detection",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "corner-based representation",
                "time-varying background modelling",
                "false motion detection",
                "outdoor-surveillance system",
                "moving object detection",
                "feature point detection",
                "Harris corner detector",
                "SIFT-like descriptor",
                "Lucas-Kanade feature tracker",
                "foreground moving object",
                "video clips",
                "feature extraction"
            ]
        },
        "id": 87,
        "cited_by": []
    },
    {
        "title": "How hard is 3-view triangulation really?",
        "authors": [
            "H. Stewenius",
            "F. Schaffalitzky",
            "D. Nister"
        ],
        "abstract": "We present a solution for optimal triangulation in three views. The solution is guaranteed to find the optimal solution because it computes all the stationary points of the (maximum likelihood) objective function. Internally, the solution is found by computing roots of multivariate polynomial equations, directly solving the conditions for stationarity. The solver makes use of standard methods from computational commutative algebra to convert the root-finding problem into a 47 /spl times/ 47 nonsymmetric eigenproblem. Although there are in general 47 roots, counting both real and complex ones, the number of real roots is usually much smaller. We also show experimentally that the number of stationary points that are local minima and lie in front of each camera is small but does depend on the scene geometry.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541320",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 31,
            "total": 62
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Equations",
                "Cameras",
                "Computer vision",
                "Iterative algorithms",
                "Robots",
                "Visualization",
                "Algebra",
                "Layout",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "computational geometry",
                "mesh generation",
                "algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3-view triangulation",
                "optimal triangulation",
                "maximum likelihood objective function",
                "multivariate polynomial equation",
                "computational commutative algebra",
                "root-finding problem",
                "nonsymmetric eigenproblem",
                "scene geometry",
                "image motion analysis"
            ]
        },
        "id": 88,
        "cited_by": [
            {
                "year": "2017",
                "id": 241
            },
            {
                "year": "2015",
                "id": 88
            },
            {
                "year": "2015",
                "id": 101
            }
        ]
    },
    {
        "title": "Fixed point probability field for complex occlusion handling",
        "authors": [
            "F. Fleuret",
            "R. Lengagne",
            "P. Fua"
        ],
        "abstract": "In this paper, we show that in a multi-camera context, we can effectively handle occlusions in real-time at each frame independently, even when the only available data comes from the binary output of a simple blob detector, and the number of present individuals is a priori unknown. We start from occupancy probability estimates in a top view and rely on a generative model to yield probability images to be compared with the actual input images. We then refine the estimates so that the probability images match the binary input images as well as possible. We demonstrate the quality of our results on several sequences involving complex occlusions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541321",
        "reference_list": [
            {
                "year": "2001",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Hidden Markov models",
                "Detectors",
                "Impedance matching",
                "Probability distribution",
                "Computer vision",
                "Yield estimation",
                "Image segmentation",
                "Bayesian methods",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "hidden feature removal",
                "real-time systems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fixed point probability",
                "complex occlusion handling",
                "multicamera system",
                "real-time system",
                "blob detector",
                "occupancy probability estimation",
                "probability image",
                "actual input image",
                "binary input image",
                "multiview multipeople detection"
            ]
        },
        "id": 89,
        "cited_by": [
            {
                "year": "2007",
                "id": 4
            },
            {
                "year": "2007",
                "id": 192
            }
        ]
    },
    {
        "title": "Visual learning given sparse data of unknown complexity",
        "authors": [
            "Tao Xiang",
            "Shaogang Gong"
        ],
        "abstract": "This study addresses the problem of unsupervised visual learning. It examines existing popular model order selection criteria before proposes two novel criteria for improving visual learning given sparse data and without any knowledge about model complexity. In particular, a rectified Bayesian information criterion (BICr) and a completed likelihood Akaike's information criterion (CL-AIC) are formulated to estimate the optimal model order (complexity) for learning the dynamic structure of a visual scene. Both criteria are designed to overcome poor model selection by existing popular criteria when the data sample size varies from very small to large. Extensive experiments on learning a dynamic scene structure are carried out to demonstrate the effectiveness of BICr and CL-AIC, compared to that of BIC (Schwarz, 1978), AIC (Akaike, 1973), ICL (Biernacki, 2000) and a MML (Figueiredo and Jain, 2002) based criterion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541322",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Bayesian methods",
                "Kernel",
                "Computer science",
                "Humans",
                "Prototypes",
                "Face recognition",
                "Codes",
                "Predictive models",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "natural scenes",
                "Bayes methods",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised visual learning",
                "model order selection criteria",
                "sparse data",
                "model complexity",
                "Bayesian information criterion",
                "completed likelihood Akaike information criterion",
                "optimal model order",
                "data sample size",
                "dynamic scene structure"
            ]
        },
        "id": 90,
        "cited_by": []
    },
    {
        "title": "Patch based blind image super resolution",
        "authors": [
            "Qiang Wang",
            "Xiaoou Tang",
            "H. Shum"
        ],
        "abstract": "In this paper, a novel method for learning based image super resolution (SR) is presented. The basic idea is to bridge the gap between a set of low resolution (LR) images and the corresponding high resolution (HR) image using both the SR reconstruction constraint and a patch based image synthesis constraint in a general probabilistic framework. We show that in this framework, the estimation of the LR image formation parameters is straightforward. The whole framework is implemented via an annealed Gibbs sampling method. Experiments on SR on both single image and image sequence input show that the proposed method provides an automatic and stable way to compute super-resolution and the achieved result is encouraging for both synthetic and real LR images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541323",
        "reference_list": [],
        "citation": {
            "ieee": 58,
            "other": 19,
            "total": 77
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Strontium",
                "Image reconstruction",
                "Maximum likelihood estimation",
                "Markov random fields",
                "Asia",
                "Bridges",
                "Image generation",
                "Annealing",
                "Sampling methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "learning (artificial intelligence)",
                "image reconstruction",
                "image sampling",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning based image super resolution",
                "image reconstruction",
                "patch based image synthesis",
                "annealed Gibbs sampling method",
                "image sequence input"
            ]
        },
        "id": 91,
        "cited_by": [
            {
                "year": "2015",
                "id": 58
            },
            {
                "year": "2013",
                "id": 117
            },
            {
                "year": "2013",
                "id": 353
            }
        ]
    },
    {
        "title": "Bidirectional tracking using trajectory segment analysis",
        "authors": [
            "Jian Sun",
            "Weiwei Zhang",
            "Xiaoou Tang",
            "Heung-Yeung Shum"
        ],
        "abstract": "In this paper, we present a novel approach to keyframe-based tracking, called bi-directional tracking. Given two object templates in the beginning and ending keyframes, the bi-directional tracker outputs the MAP (maximum a posterior) solution of the whole state sequence of the target object in the Bayesian framework. First, a number of 3D trajectory segments of the object are extracted from the input video, using a novel trajectory segment analysis. Second, these disconnected trajectory segments due to occlusion are linked by a number of inferred occlusion segments. Last, the MAP solution is obtained by trajectory optimization in a coarse-to-fine manner. Experimental results show the robustness of our approach with respect to sudden motion, ambiguity, and short and long periods of occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541324",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Bidirectional control",
                "Trajectory",
                "Target tracking",
                "Video sequences",
                "Recursive estimation",
                "Video compression",
                "Bayesian methods",
                "Video surveillance",
                "Hidden Markov models",
                "Sun"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "video signal processing",
                "maximum likelihood estimation",
                "image sequences",
                "image segmentation",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bidirectional tracking",
                "trajectory segment analysis",
                "keyframe-based tracking",
                "object templates",
                "maximum a posterior solution",
                "image sequence",
                "target object",
                "Bayesian framework",
                "3D trajectory segment extraction",
                "occlusion"
            ]
        },
        "id": 92,
        "cited_by": [
            {
                "year": "2013",
                "id": 286
            },
            {
                "year": "2011",
                "id": 233
            },
            {
                "year": "2007",
                "id": 110
            }
        ]
    },
    {
        "title": "Bayesian body localization using mixture of nonlinear shape models",
        "authors": [
            "Jiayong Zhang",
            "R. Collins",
            "Yanxi Liu"
        ],
        "abstract": "We present a 2D model-based approach to localizing human body in images viewed from arbitrary and unknown angles. The central component is a statistical shape representation of the nonrigid and articulated body contours, where a nonlinear deformation is decomposed based on the concept of parts. Several image cues are combined to relate the body configuration to the observed image, with self occlusion explicitly treated. To accommodate large viewpoint changes, a mixture of view-dependent models is employed. Inference is done by direct sampling of the posterior mixture, using Sequential Monte Carlo (SMC) simulation enhanced with annealing and kernel move. The fitting method is independent of the number of mixture components, and does not require the preselection of a \"correct\" viewpoint. The models were trained on a large number of interactively labeled gait images. Preliminary tests demonstrated the feasibility of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541325",
        "reference_list": [
            {
                "year": "2003",
                "id": 85
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Shape",
                "Biological system modeling",
                "Humans",
                "Image sampling",
                "Monte Carlo methods",
                "Sliding mode control",
                "Simulated annealing",
                "Kernel",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "Bayes methods",
                "Monte Carlo methods",
                "object recognition",
                "image representation",
                "image sampling",
                "inference mechanisms",
                "simulated annealing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian body localization",
                "nonlinear shape models",
                "2D model",
                "human body localization",
                "statistical shape representation",
                "nonrigid body contours",
                "articulated body contours",
                "nonlinear deformation decomposition",
                "view-dependent models",
                "inference",
                "image sampling",
                "posterior mixture",
                "sequential Monte Carlo simulation",
                "annealing",
                "kernel move"
            ]
        },
        "id": 93,
        "cited_by": []
    },
    {
        "title": "TemporalBoost for event recognition",
        "authors": [
            "P. Smith",
            "N. da Vitoria Lobo",
            "M. Shah"
        ],
        "abstract": "This paper contributes a new boosting paradigm to achieve detection of events in video. Previous boosting paradigms in vision focus on single frame detection and do not scale to video events. Thus new concepts need to be introduced to address questions such as determining if an event has occurred, localizing the event, handling same action performed at different speeds, incorporating previous classifier responses into current decision, using temporal consistency of data to aid detection and recognition. The proposed method has the capability to improve weak classifiers by allowing them to use previous history in evaluating the current frame. A learning mechanism built into the boosting paradigm is also given which allows event level decisions to be made. This is contrasted with previous work in boosting which uses limited higher level temporal reasoning and essentially makes object detection decisions at the frame level. Our approach makes extensive use of temporal continuity of video at the classifier and detector levels. We also introduce a relevant set of activity features. Features are evaluated at multiple zoom levels to improve detection. We show results for a system that is able to recognize 11 actions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541326",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 11,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Event detection",
                "Boosting",
                "Computer vision",
                "History",
                "Object detection",
                "Computer science",
                "Learning systems",
                "Detectors",
                "Application software",
                "Surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "video signal processing",
                "object detection",
                "learning (artificial intelligence)",
                "image classification",
                "temporal reasoning",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "TemporalBoost",
                "event recognition",
                "video events",
                "frame detection",
                "event localization",
                "data temporal consistency",
                "weak classifiers",
                "learning mechanism",
                "temporal reasoning",
                "video temporal continuity",
                "activity features"
            ]
        },
        "id": 94,
        "cited_by": [
            {
                "year": "2007",
                "id": 8
            },
            {
                "year": "2007",
                "id": 84
            },
            {
                "year": "2007",
                "id": 265
            }
        ]
    },
    {
        "title": "Detection, analysis and matching of hair",
        "authors": [
            "Y. Yacoob",
            "L. Davis"
        ],
        "abstract": "We develop computational models for measuring hair appearance for comparing different people. The models and methods developed have applications to person recognition and face image indexing. An automatic hair detection algorithm is described and results reported. A multidimensional representation of hair appearance is presented and computational algorithms are described. Results on a dataset of 524 subjects are reported. Identification of people using hair attributes is compared to eigenface-based recognition along with a joint, eigenface-hair based identification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541327",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Hair",
                "Face recognition",
                "Computer vision",
                "Humans",
                "Face detection",
                "Application software",
                "Indexing",
                "Image processing",
                "Forehead",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image matching",
                "image representation",
                "eigenvalues and eigenfunctions",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hair analysis",
                "hair matching",
                "hair appearance",
                "person recognition",
                "face image indexing",
                "automatic hair detection",
                "multidimensional representation",
                "hair attributes",
                "eigenface-based recognition",
                "eigenface-hair based identification"
            ]
        },
        "id": 95,
        "cited_by": []
    },
    {
        "title": "Towards ultimate motion estimation: combining highest accuracy with real-time performance",
        "authors": [
            "A. Bruhn",
            "J. Weickert"
        ],
        "abstract": "Although variational methods are among the most accurate techniques for estimating the optical flow, they have not yet entered the field of real-time vision. Main reason is the great popularity of standard numerical schemes that are easy to implement, however, at the expense of being too slow for real-time performance. In our paper we address this problem in two ways: (i) we present an improved version of the highly accurate technique of Brox et al. (2004). Thereby we show that a separate robustification of the constancy assumptions is very useful, in particular if the I-norm is used as penalizer. As a result, a method is obtained that yields the lowest angular errors in the literature, (ii) We develop an efficient numerical scheme for the proposed approach that allows real-time performance for sequences of size 160 /spl times/ 720. To this end, we combine two hierarchical strategies: a coarse-to-fine warping strategy as implementation of a fixed point iteration for a non-convex optimisation problem and a nonlinear full multigrid method - a so called full approximation scheme (FAS) - for solving the highly nonlinear equation systems at each warping level. In the experimental section the advantage of the proposed approach becomes obvious: Outperforming standard numerical schemes by two orders of magnitude frame rates of six high quality flow fields per second are obtained on a 3.06 GHz Pentium4 PC.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541328",
        "reference_list": [],
        "citation": {
            "ieee": 38,
            "other": 29,
            "total": 67
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Image motion analysis",
                "Nonlinear optics",
                "Optimization methods",
                "Optical computing",
                "Equations",
                "Iterative methods",
                "Image sequence analysis",
                "Mathematics",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "motion estimation",
                "iterative methods",
                "differential equations",
                "nonlinear equations",
                "approximation theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion estimation",
                "optical flow",
                "real-time vision",
                "real-time performance",
                "image sequences",
                "coarse-to-fine warping strategy",
                "fixed point iteration",
                "nonconvex optimisation problem",
                "nonlinear full multigrid method",
                "full approximation scheme",
                "nonlinear equation systems"
            ]
        },
        "id": 96,
        "cited_by": [
            {
                "year": "2017",
                "id": 325
            },
            {
                "year": "2017",
                "id": 343
            },
            {
                "year": "2013",
                "id": 172
            },
            {
                "year": "2011",
                "id": 141
            },
            {
                "year": "2011",
                "id": 249
            }
        ]
    },
    {
        "title": "LOCUS: learning object classes with unsupervised segmentation",
        "authors": [
            "J. Winn",
            "N. Jojic"
        ],
        "abstract": "We address the problem of learning object class models and object segmentations from unannotated images. We introduce LOCUS (learning object classes with unsupervised segmentation) which uses a generative probabilistic model to combine bottom-up cues of color and edge with top-down cues of shape and pose. A key aspect of this model is that the object appearance is allowed to vary from image to image, allowing for significant within-class variation. By iteratively updating the belief in the object's position, size, segmentation and pose, LOCUS avoids making hard decisions about any of these quantities and so allows for each to be refined at any stage. We show that LOCUS successfully learns an object class model from unlabeled images, whilst also giving segmentation accuracies that rival existing supervised methods. Finally, we demonstrate simultaneous recognition and segmentation in novel images using the learned models for a number of object classes, as well as unsupervised object discovery and tracking in video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541329",
        "reference_list": [],
        "citation": {
            "ieee": 129,
            "other": 61,
            "total": 190
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Object segmentation",
                "Object recognition",
                "Humans",
                "Training data",
                "Image recognition",
                "Computer vision",
                "Lighting",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image segmentation",
                "learning (artificial intelligence)",
                "image colour analysis",
                "edge detection",
                "target tracking",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "LOCUS",
                "object class learning",
                "unsupervised segmentation",
                "object segmentation",
                "unannotated images",
                "bottom-up cues",
                "object color",
                "object edge",
                "object shape",
                "object pose",
                "object appearance",
                "object position",
                "object recognition",
                "unsupervised object discovery",
                "unsupervised object tracking"
            ]
        },
        "id": 97,
        "cited_by": [
            {
                "year": "2015",
                "id": 473
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2013",
                "id": 161
            },
            {
                "year": "2013",
                "id": 162
            },
            {
                "year": "2013",
                "id": 216
            },
            {
                "year": "2011",
                "id": 81
            },
            {
                "year": "2011",
                "id": 228
            },
            {
                "year": "2011",
                "id": 328
            },
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2007",
                "id": 25
            },
            {
                "year": "2007",
                "id": 30
            },
            {
                "year": "2007",
                "id": 125
            },
            {
                "year": "2007",
                "id": 199
            },
            {
                "year": "2007",
                "id": 283
            }
        ]
    },
    {
        "title": "A multiscale hybrid linear model for lossy image representation",
        "authors": [
            "Wei Hong",
            "J. Wright",
            "Kun Huang",
            "Yi Ma"
        ],
        "abstract": "This paper introduces a simple and efficient representation for natural images. We partition an image into blocks and treat the blocks as vectors in a high-dimensional space. We then fit a piecewise linear model (i.e. a union of affine subspaces) to the vectors at each down-sampling scale. We call this a multiscale hybrid linear model of the image. The hybrid and hierarchical structure of this model allows us effectively to extract and exploit multimodal correlations among the imagery data at different scales. It conceptually and computationally remedies limitations of many existing image representation methods that are based on either a fixed linear transformation (e.g. DCT, wavelets), an adaptive unimodal linear transformation (e.g. PCA), or a multi-modal model at a single scale. We will justify both analytically and experimentally why and how such a simple multiscale hybrid model is able to reduce simultaneously the model complexity and computational cost. Despite a small overhead for the model, our results show that this new model gives more compact representations for a wide variety of natural images under a wide range of signal-to-noise ratio than many existing methods, including wavelets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541330",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image representation",
                "Discrete cosine transforms",
                "Principal component analysis",
                "Vectors",
                "Computer vision",
                "Parametric statistics",
                "Biomedical informatics",
                "Piecewise linear techniques",
                "Data mining",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "computer vision",
                "discrete cosine transforms",
                "wavelet transforms",
                "principal component analysis",
                "piecewise linear techniques",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale hybrid linear model",
                "lossy image representation",
                "natural image",
                "image partitioning",
                "piecewise linear model",
                "multimodal imagery data correlation",
                "fixed linear transformation",
                "discrete cosine transform",
                "wavelet transform",
                "adaptive unimodal linear transformation",
                "principal component analysis",
                "signal-to-noise ratio"
            ]
        },
        "id": 98,
        "cited_by": [
            {
                "year": "2005",
                "id": 20
            }
        ]
    },
    {
        "title": "Robust structure from motion and identified dynamics",
        "authors": [
            "B.R. Fransen",
            "O.I. Camps",
            "M. Sznaier"
        ],
        "abstract": "This paper addresses robust recovery of structure and motion for rigid bodies in video sequences. For small inter-frame motion, feature appearance is commonly estimated according to the previous frame. However, in the case of occlusion or corrupt frames, the small interframe motion model fails. In this paper, we propose to use robust identification techniques to estimate the motion dynamics based on a set of previous frames. These dynamics are then recursively used to robustly estimate object structure and motion and to predict the object appearance in future frames. Results are tested for 3D reconstructions of rigid bodies in real and synthetic image sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541331",
        "reference_list": [
            {
                "year": "2003",
                "id": 138
            },
            {
                "year": "2001",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Motion estimation",
                "Tracking",
                "Transmission line matrix methods",
                "Maximum likelihood detection",
                "Matrix decomposition",
                "Computer science",
                "Video sequences",
                "Recursive estimation",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "image sequences",
                "image reconstruction",
                "computer vision",
                "video signal processing",
                "stereo image processing",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video sequence",
                "motion estimation",
                "object structure estimation",
                "rigid body motion",
                "interframe motion model",
                "feature appearance",
                "occlusion",
                "frame object appearance",
                "3D image reconstruction",
                "real image sequence",
                "synthetic image sequence"
            ]
        },
        "id": 99,
        "cited_by": []
    },
    {
        "title": "Regular polygon detection",
        "authors": [
            "N. Barnesi",
            "G. Loy",
            "D. Shaw",
            "A. Robles-Kelly"
        ],
        "abstract": "This paper describes a new robust regular polygon detector. The regular polygon transform is posed as a mixture of regular polygons in a five dimensional space. Given the edge structure of an image, we derive the a posteriori probability for a mixture of regular polygons, and thus the probability density function for the appearance of a mixture of regular polygons. Likely regular polygons can be isolated quickly by discretising and collapsing the search space into three dimensions. The remaining dimensions may be efficiently recovered subsequently using maximum likelihood at the locations of the most likely polygons in the subspace. This leads to an efficient algorithm. Also the a posteriori formulation facilitates inclusion of additional a priori information leading to real-time application to road sign detection. The use of gradient information also reduces noise compared to existing approaches such as the generalised Hough transform. Results are presented for images with noise to show stability. The detector is also applied to two separate applications: real-time road sign detection for on-line driver assistance; and feature detection, recovering stable features in rectilinear environments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541332",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 8,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Roads",
                "Working environment noise",
                "Computer vision",
                "Robustness",
                "Image edge detection",
                "Probability density function",
                "Maximum likelihood detection",
                "Noise reduction",
                "Stability"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "computational geometry",
                "edge detection",
                "feature extraction",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "regular polygon detection",
                "regular polygon transform",
                "5D space",
                "edge structure",
                "a posteriori probability",
                "probability density function",
                "maximum likelihood",
                "a priori information",
                "real-time road sign detection",
                "on-line driver assistance",
                "feature detection",
                "feature recovery"
            ]
        },
        "id": 100,
        "cited_by": []
    },
    {
        "title": "Local Gabor binary pattern histogram sequence (LGBPHS): a novel non-statistical model for face representation and recognition",
        "authors": [
            "Wenchao Zhang",
            "Shiguang Shan",
            "Wen Gao",
            "Xilin Chen",
            "Hongming Zhang"
        ],
        "abstract": "For years, researchers in face recognition area have been representing and recognizing faces based on subspace discriminant analysis or statistical learning. Nevertheless, these approaches are always suffering from the generalizability problem. This paper proposes a novel non-statistics based face representation approach, local Gabor binary pattern histogram sequence (LGBPHS), in which training procedure is unnecessary to construct the face model, so that the generalizability problem is naturally avoided. In this approach, a face image is modeled as a \"histogram sequence\" by concatenating the histograms of all the local regions of all the local Gabor magnitude binary pattern maps. For recognition, histogram intersection is used to measure the similarity of different LGBPHSs and the nearest neighborhood is exploited for final classification. Additionally, we have further proposed to assign different weights for each histogram piece when measuring two LGBPHSes. Our experimental results on AR and FERET face database show the validity of the proposed approach especially for partially occluded face images, and more impressively, we have achieved the best result on FERET face database.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541333",
        "reference_list": [],
        "citation": {
            "ieee": 203,
            "other": 40,
            "total": 243
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Face recognition",
                "Pattern recognition",
                "Statistical learning",
                "Image databases",
                "Support vector machines",
                "Noise robustness",
                "Computer science",
                "Research and development",
                "Content addressable storage"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "Gabor filters",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local Gabor binary pattern histogram sequence",
                "face representation",
                "face recognition",
                "subspace discriminant analysis",
                "statistical learning",
                "histogram intersection",
                "nearest neighborhood",
                "AR face database",
                "FERET face database",
                "partially occluded face image"
            ]
        },
        "id": 101,
        "cited_by": [
            {
                "year": "2015",
                "id": 415
            },
            {
                "year": "2015",
                "id": 432
            },
            {
                "year": "2013",
                "id": 14
            },
            {
                "year": "2007",
                "id": 220
            }
        ]
    },
    {
        "title": "Shape recovery of 3D data obtained from a moving range sensor by using image sequences",
        "authors": [
            "A. Banno",
            "K. Ikeuchi"
        ],
        "abstract": "For a large object, scanning from the air is one of the most efficient methods of obtaining 3D data. In the case of large cultural heritage objects, there are some difficulties in scanning with respect to safety and efficiency. To remedy these problems, we have been developing a novel 3D measurement system, the floating laser range sensor (FLRS), in which a range sensor is suspended beneath a balloon. The obtained data, however, have some distortions due to sensor-movements during the scanning process. In this paper, we propose a method to recover 3D range data obtained by a moving laser range sensor. This method is applicable not only to our FLRS, but also to a general moving range sensor. Using image sequences from a video camera mounted on the FLRS enables us to estimate the motion of the FLRS without any physical sensors such as gyros or GPS. In the first stage, the initial values of camera motion parameters are estimated by full-perspective factorization. The next stage refines camera motion parameters using the relationships between camera images and range data distortion. Finally, by using the refined parameters, the distorted range data are recovered. In addition, our method is applicable with an uncalibrated video camera and range sensor system. We applied this method to an actual scanning project, and the results showed the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541334",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image sensors",
                "Image sequences",
                "Cameras",
                "Sensor systems",
                "Laser noise",
                "Motion estimation",
                "Cultural differences",
                "Safety",
                "Distortion measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image reconstruction",
                "motion compensation",
                "stereo image processing",
                "laser ranging"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D range data recovery",
                "moving laser range sensor",
                "image sequence",
                "large cultural heritage object",
                "floating laser range sensor",
                "sensor movement",
                "image scanning process",
                "FLRS",
                "gyros",
                "GPS",
                "camera motion parameter",
                "range data distortion",
                "uncalibrated video camera"
            ]
        },
        "id": 102,
        "cited_by": []
    },
    {
        "title": "Evaluation of features detectors and descriptors based on 3D objects",
        "authors": [
            "P. Moreels",
            "P. Perona"
        ],
        "abstract": "We explore the performance of a number of popular feature detectors and descriptors in matching 3D object features across viewpoints and lighting conditions. To this end we design a method, based on intersecting epipolar constraints, for providing ground truth correspondence automatically. We collect a database of 100 objects viewed from 144 calibrated viewpoints under three different lighting conditions. We find that the combination of Hessian-affine feature finder and SIFT features is most robust to viewpoint change. Harris-affine combined with SIFT and Hessian-affine combined with shape context descriptors were best respectively for lighting changes and scale changes. We also find that no detector-descriptor combination performs well with viewpoint changes of more than 25-30/spl deg/.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541335",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 32,
            "total": 68
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Object detection",
                "Detectors",
                "Layout",
                "Robustness",
                "Shape",
                "Simultaneous localization and mapping",
                "Frequency",
                "Object recognition",
                "Stereo vision"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "object detection",
                "feature extraction",
                "stereo image processing",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "features detection",
                "feature descriptor",
                "3D object feature matching",
                "lighting condition",
                "intersecting epipolar constraint",
                "Hessian-affine feature finder",
                "SIFT feature",
                "shape context descriptor"
            ]
        },
        "id": 103,
        "cited_by": [
            {
                "year": "2007",
                "id": 39
            }
        ]
    },
    {
        "title": "Behaviour understanding in video: a combined method",
        "authors": [
            "N. Robertson",
            "I. Reid"
        ],
        "abstract": "In this paper we develop a system for human behaviour recognition in video sequences. Human behaviour is modelled as a stochastic sequence of actions. Actions are described by a feature vector comprising both trajectory information (position and velocity), and a set of local motion descriptors. Action recognition is achieved via probabilistic search of image feature databases representing previously seen actions. A HMM which encodes the rules of the scene is used to smooth sequences of actions. High-level behaviour recognition is achieved by computing the likelihood that a set of predefined hidden Markov models explains the current action sequence. Thus, human actions and behaviour are represented using a hierarchy of abstraction: from simple actions, to actions with spatio-temporal context, to action sequences and finally general behaviours. While the upper levels all use (parametric) Bayes networks and belief propagation, the lowest level uses nonparametric sampling from a previously learned database of actions. The combined method represents a general framework for human behaviour modelling. In this paper we demonstrate the results chiefly on broadcast tennis sequences for automated video annotation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541336",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 13,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Hidden Markov models",
                "Video sequences",
                "Stochastic processes",
                "Image recognition",
                "Image databases",
                "Spatial databases",
                "Layout",
                "Belief propagation",
                "Sampling methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image motion analysis",
                "video signal processing",
                "hidden Markov models",
                "image representation",
                "belief maintenance",
                "belief networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human behaviour recognition",
                "video sequence",
                "feature vector",
                "trajectory information",
                "local motion descriptor",
                "action recognition",
                "probabilistic search",
                "image feature database",
                "image sequence",
                "hidden Markov models",
                "Bayes network",
                "belief propagation",
                "nonparametric sampling",
                "learned action database",
                "broadcast tennis sequence",
                "automated video annotation"
            ]
        },
        "id": 104,
        "cited_by": []
    },
    {
        "title": "Periodic motion detection and segmentation via approximate sequence alignment",
        "authors": [
            "I. Laptev",
            "S.J. Belongie",
            "P. Perez",
            "J. Wills"
        ],
        "abstract": "A method for detecting and segmenting periodic motion is presented. We exploit periodicity as a cue and detect periodic motion in complex scenes where common methods for motion segmentation are likely to fail. We note that periodic motion detection can be seen as an approximate case of sequence alignment where an image sequence is matched to itself over one or more periods of time. To use this observation, we first consider alignment of two video sequences obtained by independently moving cameras. Under assumption of constant translation, the fundamental matrices and the homographies are shown to be time-linear matrix functions. These dynamic quantities can be estimated by matching corresponding space-time points with similar local motion and shape. For periodic motion, we match corresponding points across periods and develop a RANSAC procedure to simultaneously estimate the period and the dynamic geometric transformations between periodic views. Using this method, we demonstrate detection and segmentation of human periodic motion in complex scenes with nonrigid backgrounds, moving camera and motion parallax.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541337",
        "reference_list": [
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2003",
                "id": 123
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 14,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion detection",
                "Layout",
                "Cameras",
                "Motion estimation",
                "Image segmentation",
                "Motion segmentation",
                "Computer vision",
                "Image sequences",
                "Video sequences",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "image segmentation",
                "image matching",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "periodic motion detection",
                "motion segmentation",
                "sequence alignment",
                "image sequence",
                "video sequence",
                "independently moving camera",
                "homography",
                "time-linear matrix function",
                "space-time point matching",
                "RANSAC procedure",
                "dynamic geometric transformation",
                "human periodic motion",
                "motion parallax"
            ]
        },
        "id": 105,
        "cited_by": [
            {
                "year": "2015",
                "id": 337
            },
            {
                "year": "2009",
                "id": 14
            },
            {
                "year": "2009",
                "id": 211
            }
        ]
    },
    {
        "title": "Recovering human body configurations using pairwise constraints between parts",
        "authors": [
            "Xiaofeng Ren",
            "A.C. Berg",
            "J. Malik"
        ],
        "abstract": "The goal of this work is to recover human body configurations from static images. Without assuming a priori knowledge of scale, pose or appearance, this problem is extremely challenging and demands the use of all possible sources of information. We develop a framework which can incorporate arbitrary pairwise constraints between body parts, such as scale compatibility, relative position, symmetry of clothing and smooth contour connections between parts. We detect candidate body parts from bottom-up using parallelism, and use various pairwise configuration constraints to assemble them together into body configurations. To find the most probable configuration, we solve an integer quadratic programming problem with a standard technique using linear approximations. Approximate IQP allows us to incorporate much more information than the traditional dynamic programming and remains computationally efficient. 15 hand-labeled images are used to train the low-level part detector and learn the pairwise constraints. We show test results on a variety of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541338",
        "reference_list": [
            {
                "year": "2005",
                "id": 159
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 1,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Information resources",
                "Clothing",
                "Parallel processing",
                "Assembly",
                "Quadratic programming",
                "Linear approximation",
                "Dynamic programming",
                "Detectors",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image matching",
                "object recognition",
                "image reconstruction",
                "integer programming",
                "quadratic programming",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human body parts configuration",
                "pairwise configuration constraint",
                "static image",
                "scale compatibility",
                "relative position",
                "clothing symmetry",
                "smooth contour connection",
                "integer quadratic programming",
                "linear approximation",
                "dynamic programming",
                "hand-labeled image",
                "pairwise constraint learning"
            ]
        },
        "id": 106,
        "cited_by": [
            {
                "year": "2017",
                "id": 134
            },
            {
                "year": "2009",
                "id": 173
            },
            {
                "year": "2009",
                "id": 174
            },
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2007",
                "id": 20
            },
            {
                "year": "2007",
                "id": 40
            }
        ]
    },
    {
        "title": "A maximum entropy framework for part-based texture and object recognition",
        "authors": [
            "S. Lazebnik",
            "C. Schmid",
            "J. Ponce"
        ],
        "abstract": "This paper presents a probabilistic part-based approach for texture and object recognition. Textures are represented using a part dictionary found by quantizing the appearance of scale- or affine- invariant keypoints. Object classes are represented using a dictionary of composite semi-local parts, or groups of neighboring keypoints with stable and distinctive appearance and geometric layout. A discriminative maximum entropy framework is used to learn the posterior distribution of the class label given the occurrences of parts from the dictionary in the training set. Experiments on two texture and two object databases demonstrate the effectiveness of this framework for visual classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541339",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 54,
            "other": 54,
            "total": 108
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Object recognition",
                "Dictionaries",
                "Image databases",
                "Spatial databases",
                "Visual databases",
                "Image retrieval",
                "Image representation",
                "Image recognition",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image texture",
                "image classification",
                "maximum entropy methods",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative maximum entropy",
                "texture recognition",
                "object recognition",
                "probabilistic part-based approach",
                "scale-invariant keypoints",
                "affine-invariant keypoints",
                "object class",
                "geometric layout",
                "object database",
                "visual classification"
            ]
        },
        "id": 107,
        "cited_by": [
            {
                "year": "2011",
                "id": 207
            },
            {
                "year": "2011",
                "id": 321
            },
            {
                "year": "2007",
                "id": 20
            },
            {
                "year": "2007",
                "id": 128
            }
        ]
    },
    {
        "title": "Realtime IBR with omnidirectional crossed-slits projection",
        "authors": [
            "D. Feldman",
            "D. Weinshall"
        ],
        "abstract": "The crossed-slits (X-Slits) projection can be used to generate new views of a scene from a sequence of perspective images. Compared with other image-based rendering (IBR) techniques, X-Slits image generation is simple and requires a relatively small number of input images, which makes it suitable for realtime IBR. In this paper we extend this model to omnidirectional cameras and a circular slit. We show how it can be used for realtime image-based rendering of omnidirectional images, and how to optimize it for speed and quality. We analyze the inherent geometric distortions of the circular X-Slits projection, and describe a normalization mechanism to reduce distortions, creating a realistic virtual environment. Essentially the same mechanism is used to augment the X-Slits images with artificial objects, when using standard graphics tools which assume perspective projection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541340",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Rendering (computer graphics)",
                "Image generation",
                "Virtual environment",
                "Graphics",
                "Optical distortion",
                "Computer science",
                "Optical reflection",
                "Image sampling"
            ],
            "INSPEC: Controlled Indexing": [
                "rendering (computer graphics)",
                "realistic images",
                "real-time systems",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "realtime image-based rendering",
                "crossed-slits projection",
                "circular X-Slits projection",
                "X-Slits image generation",
                "omnidirectional camera",
                "circular slit",
                "geometric distortion",
                "realistic virtual environment",
                "artificial object",
                "standard graphics tools"
            ]
        },
        "id": 108,
        "cited_by": []
    },
    {
        "title": "A probabilistic semantic model for image annotation and multimodal image retrieval",
        "authors": [
            "Ruofei Zhang",
            "Zhongfei Zhang",
            "Mingjing Li",
            "Wei-Ying Ma",
            "Hong-Jiang Zhang"
        ],
        "abstract": "This paper addresses automatic image annotation problem and its application to multi-modal image retrieval. The contribution of our work is three-fold. (1) We propose a probabilistic semantic model in which the visual features and the textual words are connected via a hidden layer which constitutes the semantic concepts to be discovered to explicitly exploit the synergy among the modalities. (2) The association of visual features and textual words is determined in a Bayesian framework such that the confidence of the association can be provided. (3) Extensive evaluation on a large-scale, visually and semantically diverse image collection crawled from Web is reported to evaluate the prototype system based on the model. In the proposed probabilistic model, a hidden concept layer which connects the visual feature and the word layer is discovered by fitting a generative model to the training image and annotation words through an Expectation-Maximization (EM) based iterative learning procedure. The evaluation of the prototype system on 17,000 images and 7,736 automatically extracted annotation words from crawled Web pages for multi-modal image retrieval has indicated that the proposed semantic model and the developed Bayesian framework are superior to a state-of-the-art peer system in the literature.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541341",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Image retrieval",
                "Bayesian methods",
                "Prototypes",
                "Information retrieval",
                "Content based retrieval",
                "Image databases",
                "Computer science",
                "Asia",
                "Application software",
                "Large-scale systems"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "content-based retrieval",
                "semantic networks",
                "Web sites",
                "expectation-maximisation algorithm",
                "feature extraction",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic semantic model",
                "automatic image annotation",
                "multimodal image retrieval",
                "visual features",
                "textual words",
                "Bayesian framework",
                "image collection",
                "word layer",
                "expectation-maximization algorithm",
                "iterative learning procedure",
                "crawled Web pages"
            ]
        },
        "id": 109,
        "cited_by": []
    },
    {
        "title": "Image statistics based on diffeomorphic matching",
        "authors": [
            "G. Charpiat",
            "O. Faugeras",
            "R. Keriven"
        ],
        "abstract": "We propose a new approach to deal with the first and second order statistics of a set of images. These statistics take into account the images characteristic deformations and their variations in intensity. The central algorithm is based on nonsupervised diffeomorphic image matching (without landmarks or human intervention). As they convey the notion of the mean shape and colors of an object and the one of its common variations, such statistics of sets of images may be relevant in the context of object recognition, both in the segmentation of any of its representations and in the classification of them. The proposed approach has been tested on a small database of face images to compute a mean face and second order statistics. The results are very encouraging since, whereas the algorithm does not need any human intervention and is not specific to face image databases, the mean image looks like a real face and the characteristic modes of variation (deformation and intensity changes) are sensible.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541342",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Humans",
                "Image databases",
                "Face",
                "Image matching",
                "Shape",
                "Object recognition",
                "Image segmentation",
                "Testing",
                "Statistical analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image matching",
                "image segmentation",
                "image representation",
                "image classification",
                "image morphing",
                "statistics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image statistics",
                "nonsupervised diffeomorphic image matching",
                "images characteristic deformation",
                "landmarks",
                "human intervention",
                "object recognition",
                "image segmentation",
                "image representation",
                "image classification",
                "face image database"
            ]
        },
        "id": 110,
        "cited_by": [
            {
                "year": "2009",
                "id": 167
            }
        ]
    },
    {
        "title": "Space-time scene manifolds",
        "authors": [
            "Y. Wexler",
            "D. Simakov"
        ],
        "abstract": "The space of images is known to be a nonlinear sub-space that is difficult to model. This paper derives an algorithm that walks within this space. We seek a manifold through the video volume that is constrained to lie locally in this space. Every local neighborhood within the manifold resembles some image patch. We call this the scene manifold because the solution traces the scene outline. For a broad class of inputs the problem can be posed as finding the shortest path in a graph and can thus be solved efficiently to produce the globally optimal solution. Constraining appearance rather than geometry gives rise to numerous new capabilities. Here we demonstrate the usefulness of this approach by posing the well-studied problem of mosaicing in a new way. Instead of treating it as geometrical alignment, we pose it as an appearance optimization. Since the manifold is constrained to lie in the space of valid image patches, the resulting mosaic is guaranteed to have the least distortions possible. Any small part of it can be seen in some image even though the manifold spans the whole video. Thus it can deal seamlessly with both static and dynamic scenes, with or without 3D parallax. Essentially, the method simultaneously solves two problems that have been solved only separately until now: alignment and mosaicing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541343",
        "reference_list": [
            {
                "year": "2003",
                "id": 154
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Nonlinear distortion",
                "Geometry",
                "Computer science",
                "Pixel",
                "Concrete",
                "Cameras",
                "Testing",
                "Video sequences",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "image segmentation",
                "computational geometry",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "space-time scene manifold",
                "nonlinear subspace",
                "video volume",
                "local neighborhood",
                "image patches",
                "shortest path",
                "computational geometry",
                "geometrical alignment",
                "appearance optimization",
                "3D parallax",
                "image mosaicing"
            ]
        },
        "id": 111,
        "cited_by": []
    },
    {
        "title": "Non-orthogonal binary subspace and its applications in computer vision",
        "authors": [
            "Hai Tao",
            "R. Crabb",
            "Feng Tang"
        ],
        "abstract": "This paper presents a novel approach that represents an image or a set of images using a non-orthogonal binary subspace (NBS) spanned by box-like base vectors. These base vectors possess the property that the inner product operation with them can be computed very efficiently. We investigate the optimized orthogonal matching pursuit method for finding the best NBS base vectors. It is demonstrated in this paper how the NBS based expansion can be applied to speed up several common computer vision algorithms, including normalized cross correlation (NCC), sum of squared difference (SSD) matching, appearance subspace projection and subspace-based object recognition. Promising experimental results on facial and natural images are demonstrated in this paper.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541344",
        "reference_list": [
            {
                "year": "2003",
                "id": 194
            },
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 0,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Application software",
                "Computer vision",
                "NIST",
                "Face detection",
                "Object recognition",
                "Image reconstruction",
                "Matching pursuit algorithms",
                "Pixel",
                "Principal component analysis",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "object recognition",
                "face recognition",
                "image matching",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonorthogonal binary subspace",
                "computer vision",
                "box-like base vector",
                "inner product operation",
                "orthogonal matching",
                "normalized cross correlation",
                "sum of squared difference matching",
                "appearance subspace projection",
                "subspace-based object recognition",
                "facial image",
                "natural image"
            ]
        },
        "id": 112,
        "cited_by": []
    },
    {
        "title": "An ensemble prior of image structure for cross-modal inference",
        "authors": [
            "S. Ravela",
            "A. Torralba",
            "W.T. Freeman"
        ],
        "abstract": "In cross-modal inference, we estimate complete fields from noisy and missing observations of one sensory modality using structure found in another sensory modality. This inference problem occurs in several areas including texture reconstruction and reconstruction of geophysical fields. We propose a method for cross-modal inference that simultaneously learns shape recipes between two modalities and estimates missing information by using a prior on image structure gleaned from the alternate modality. In the absence of a physical basis for representing image priors, we use a statistical one that represents correlations in differential features. This is done efficiently using a perturbation sampling scheme. Using just one example of the alternate modality, we produce a factorized ensemble representation of feature correlations that yields efficient solutions to large-sized spatial inference problems. We demonstrate the utility of this approach on cross-modal inference with depth and spectral data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541345",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Satellites",
                "Remote sensing",
                "Parameter estimation",
                "State estimation",
                "Uncertainty",
                "Noise shaping",
                "Shape",
                "Image sampling",
                "Geology"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image texture",
                "image representation",
                "image sampling",
                "inference mechanisms",
                "feature extraction",
                "geophysics computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image structure",
                "cross-modal inference",
                "sensory modality",
                "image texture reconstruction",
                "geophysical field reconstruction",
                "image representation",
                "perturbation sampling"
            ]
        },
        "id": 113,
        "cited_by": []
    },
    {
        "title": "Efficient model-based 3D tracking of deformable objects",
        "authors": [
            "E. Munoz",
            "J.M. Buenaposada",
            "L. Baumela"
        ],
        "abstract": "Efficient incremental image alignment is a topic of renewed interest in the computer vision community because of its applications in model fitting and model-based object tracking. Successful compositional procedures for aligning 2D and 3D models under weak-perspective imaging conditions have already been proposed. Here we present a mixed compositional and additive algorithm which is applicable to the full projective camera case.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541346",
        "reference_list": [
            {
                "year": "2003",
                "id": 7
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 5,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Target tracking",
                "Active appearance model",
                "Cameras",
                "Humans",
                "Computer vision",
                "Head",
                "Facial animation",
                "Application software",
                "Face detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "object recognition",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based 3D tracking",
                "deformable object tracking",
                "incremental image alignment",
                "computer vision",
                "model fitting",
                "weak-perspective imaging",
                "full projective camera"
            ]
        },
        "id": 114,
        "cited_by": []
    },
    {
        "title": "Modeling scenes with local descriptors and latent aspects",
        "authors": [
            "P. Quelhas",
            "F. Monay",
            "J.-M. Odobez",
            "D. Gatica-Perez",
            "T. Tuytelaars",
            "L. Van Gool"
        ],
        "abstract": "We present a new approach to model visual scenes in image collections, based on local invariant features and probabilistic latent space models. Our formulation provides answers to three open questions:(l) whether the invariant local features are suitable for scene (rather than object) classification; (2) whether unsupennsed latent space models can be used for feature extraction in the classification task; and (3) whether the latent space formulation can discover visual co-occurrence patterns, motivating novel approaches for image organization and segmentation. Using a 9500-image dataset, our approach is validated on each of these issues. First, we show with extensive experiments on binary and multi-class scene classification tasks, that a bag-of-visterm representation, derived from local invariant descriptors, consistently outperforms state-of-the-art approaches. Second, we show that probabilistic latent semantic analysis (PLSA) generates a compact scene representation, discriminative for accurate classification, and significantly more robust when less training data are available. Third, we have exploited the ability of PLSA to automatically extract visually meaningful aspects, to propose new algorithms for aspect-based image ranking and context-sensitive image segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541347",
        "reference_list": [
            {
                "year": "2003",
                "id": 84
            },
            {
                "year": "2003",
                "id": 151
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2003",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 124,
            "other": 86,
            "total": 210
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image segmentation",
                "Robustness",
                "Image retrieval",
                "Computer vision",
                "Feature extraction",
                "Data mining",
                "Object recognition",
                "Training data",
                "Content based retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "natural scenes",
                "probability",
                "image classification",
                "image segmentation",
                "feature extraction",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual scene modelling",
                "image collection",
                "local invariant feature",
                "probabilistic latent space model",
                "multiclass scene classification",
                "feature extraction",
                "visual cooccurrence pattern",
                "image organization",
                "bag-of-visterm representation",
                "probabilistic latent semantic analysis",
                "aspect-based image ranking",
                "context-sensitive image segmentation",
                "local descriptors"
            ]
        },
        "id": 115,
        "cited_by": [
            {
                "year": "2007",
                "id": 13
            },
            {
                "year": "2007",
                "id": 27
            },
            {
                "year": "2007",
                "id": 31
            },
            {
                "year": "2007",
                "id": 121
            },
            {
                "year": "2007",
                "id": 216
            }
        ]
    },
    {
        "title": "Shadow flow: a recursive method to learn moving cast shadows",
        "authors": [
            "F. Porikli",
            "J. Thornton"
        ],
        "abstract": "We present a novel algorithm to detect and remove cast shadows in a video sequence by taking advantage of the statistical prevalence of the shadowed regions over the object regions. We model shadows using multivariate Gaussians. We apply a weak classifier as a pre-filter. We project shadow models into a quantized color space to update a shadow flow function. We use shadow flow, background models, and current frame to determine the shadow and object regions. This method has several advantages: It does not require a color space transformation. We pose the problem in the RGB color space, and we can carry out the same analysis in other Cartesian spaces as well. It is data-driven and adapts to the changing shadow conditions. In other words, accuracy of our method is not limited by the preset values. Furthermore, it does not assume any 3D models for the target objects or tracking of the cast shadows between frames. Our results show that the detection performance is superior than the benchmark method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541348",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 10,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Image segmentation",
                "Color",
                "Target tracking",
                "Reflectivity",
                "Object detection",
                "Video sequences",
                "Gaussian processes",
                "Shape",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image colour analysis",
                "learning (artificial intelligence)",
                "video signal processing",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shadow flow",
                "cast shadow detection",
                "cast shadow removal",
                "video sequence",
                "recursive method",
                "multivariate Gaussian",
                "color space transformation",
                "RGB color space",
                "Cartesian space",
                "moving cast shadow learning"
            ]
        },
        "id": 116,
        "cited_by": []
    },
    {
        "title": "Vignette and exposure calibration and compensation",
        "authors": [
            "D.B. Goldman",
            "Jiun-Hung Chen"
        ],
        "abstract": "We discuss calibration and removal of \"vignetting\" (radial falloff) and exposure (gain) variations from sequences of images. Unique solutions for vignetting, exposure and scene radiances are possible when the response curve is known. When the response curve is unknown, an exponential ambiguity prevents us from recovering these parameters uniquely. However, the vignetting and exposure variations can nonetheless be removed from the images without resolving this ambiguity. Applications include panoramic image mosaics, photometry for material reconstruction, image-based rendering, and preprocessing for correlation-based vision algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541349",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 20,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Lenses",
                "Brightness",
                "Cameras",
                "Optical sensors",
                "Layout",
                "Optical materials",
                "Rendering (computer graphics)",
                "Apertures",
                "Focusing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image enhancement",
                "image reconstruction",
                "rendering (computer graphics)",
                "calibration",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "vignetting",
                "exposure calibration",
                "exposure compensation",
                "radial falloff",
                "image sequence",
                "scene radiance",
                "response curve",
                "panoramic image mosaic",
                "photometry",
                "material reconstruction",
                "image-based rendering",
                "correlation-based vision algorithm"
            ]
        },
        "id": 117,
        "cited_by": []
    },
    {
        "title": "An enhanced correlation-based method for stereo correspondence with subpixel accuracy",
        "authors": [
            "E.Z. Psarakis",
            "G.D. Evangelidis"
        ],
        "abstract": "The invariance of the similarity measure in photometric distortions as well as its capability in producing subpixel accuracy are two desired and often required features in most stereo vision applications. In this paper we propose a new correlation-based measure which incorporates both mentioned requirements. Specifically, by using an appropriate interpolation scheme in the candidate windows of the matching image, and using the classical zero mean normalized cross correlation function, we introduce a suitable measure. Although the proposed measure is a nonlinear function of the sub-pixel displacement parameter, its maximization results in a closed form solution, resulting in reduced complexity for its use in matching techniques. Application of the proposed measure in a number of benchmark stereo pair images reveals its superiority over existing correlation-based techniques used for subpixel accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1541350",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 9,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Interpolation",
                "Distortion measurement",
                "Photometry",
                "Stereo vision",
                "Layout",
                "Computer vision",
                "Application software",
                "Informatics",
                "Electronic mail",
                "Nonlinear distortion"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image reconstruction",
                "stereo image processing",
                "computer vision",
                "photometry",
                "interpolation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "similarity measure",
                "photometric distortion",
                "correlation-based method",
                "subpixel accuracy",
                "stereo vision",
                "interpolation",
                "image matching",
                "zero mean normalized cross correlation function",
                "subpixel displacement parameter",
                "stereo pair image",
                "stereo correspondence"
            ]
        },
        "id": 118,
        "cited_by": [
            {
                "year": "2007",
                "id": 381
            }
        ]
    },
    {},
    {
        "title": "Efficiently solving dynamic Markov random fields using graph cuts",
        "authors": [
            "P. Kohli",
            "P.H.S. Torr"
        ],
        "abstract": "In this paper, we present a fast new fully dynamic algorithm for the st-mincut/max-flow problem. We show how this algorithm can be used to efficiently compute MAP estimates for dynamically changing MRF models of labeling problems in computer vision, such as image segmentation. Specifically, given the solution of the max-flow problem on a graph, we show how to efficiently compute the maximum flow in a modified version of the graph. Our experiments showed that the time taken by our algorithm is roughly proportional to the number of edges whose weights were different in the two graphs. We test the performance of our algorithm on one particular problem: the object-background segmentation problem for video and compare it with the best known st-mincut algorithm. The results show that the dynamic graph cut algorithm is much faster than its static counterpart and enables real time image segmentation. It should be noted that our method is generic and can be used to yield similar improvements in many other cases that involve dynamic change in the graph",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544820",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 65,
            "other": 44,
            "total": 109
        },
        "keywords": {
            "IEEE Keywords": [
                "Markov random fields",
                "Heuristic algorithms",
                "Image segmentation",
                "Computer vision",
                "Inference algorithms",
                "Computational geometry",
                "Tree graphs",
                "Belief propagation",
                "Performance analysis",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "image segmentation",
                "Markov processes",
                "maximum likelihood estimation",
                "optimisation",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic Markov random fields",
                "dynamic algorithm",
                "st-mincut algorithm",
                "max-flow algorithm",
                "maximum a-posteriori estimates",
                "computer vision",
                "object-background segmentation",
                "dynamic graph cut algorithm",
                "real time image segmentation"
            ]
        },
        "id": 120,
        "cited_by": [
            {
                "year": "2009",
                "id": 35
            },
            {
                "year": "2007",
                "id": 370
            }
        ]
    },
    {
        "title": "A shape-based segmentation approach: an improved technique using level sets",
        "authors": [
            "H.E. Abd El Munim",
            "A.A. Farag"
        ],
        "abstract": "We propose a novel approach for shape-based segmentation based on a specially designed level set function format. This format permits us to better control the process of object registration which is an important part in the shape-based segmentation framework. The method depends on a set of training shapes used to build a parametric shape model. The color is taken into consideration besides the shape prior information. The shape model is fitted to the image volume by registration through an energy minimization problem. The approach overcomes the conventional methods problems like point correspondences and weighing coefficients tuning of the partial differential equations (PDE's). Also it is suitable for multidimensional data and computationally efficient. Results of extracting the 2D star fish and the brain ventricles in 3D demonstrate the efficiency of the approach",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544821",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 3,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Level set",
                "Deformable models",
                "Shape control",
                "Process control",
                "Partial differential equations",
                "Data mining",
                "Marine animals",
                "Anatomical structure",
                "Biomedical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image colour analysis",
                "image registration",
                "image segmentation",
                "partial differential equations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape-based segmentation",
                "level set function",
                "object registration",
                "parametric shape model",
                "partial differential equations",
                "2D star fish",
                "brain ventricles"
            ]
        },
        "id": 121,
        "cited_by": [
            {
                "year": "2007",
                "id": 186
            }
        ]
    },
    {
        "title": "An iterative optimization approach for unified image segmentation and matting",
        "authors": [
            "J. Wang",
            "M.F. Cohen"
        ],
        "abstract": "Separating a foreground object from the background in a static image involves determining both full and partial pixel coverages, also known as extracting a matte. Previous approaches require the input image to be presegmented into three regions: foreground, background and unknown, which are called a trimap. Partial opacity values are then computed only for pixels inside the unknown region. This presegmentation based approach fails for images with large portions of semitransparent foreground where the trimap is difficult to create even manually. In this paper, we combine the segmentation and matting problem together and propose a unified optimization approach based on belief propagation. We iteratively estimate the opacity value for every pixel in the image, based on a small sample of foreground and background pixels marked by the user. Experimental results show that compared with previous approaches, our method is more efficient to extract high quality mattes for foregrounds with significant semitransparent regions",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544822",
        "reference_list": [],
        "citation": {
            "ieee": 113,
            "other": 49,
            "total": 162
        },
        "keywords": {
            "IEEE Keywords": [
                "Iterative methods",
                "Image segmentation",
                "Pixel",
                "Bayesian methods",
                "Belief propagation",
                "Degradation",
                "Statistics",
                "Optimization methods",
                "Statistical analysis",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "iterative methods",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "iterative optimization",
                "image segmentation",
                "image matting",
                "trimap",
                "belief propagation",
                "image pixel opacity",
                "foreground pixels",
                "background pixels"
            ]
        },
        "id": 122,
        "cited_by": [
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2015",
                "id": 47
            },
            {
                "year": "2009",
                "id": 113
            },
            {
                "year": "2007",
                "id": 62
            },
            {
                "year": "2007",
                "id": 92
            }
        ]
    },
    {
        "title": "Convex grouping combining boundary and region information",
        "authors": [
            "J.S. Stahl",
            "Song Wang"
        ],
        "abstract": "Convexity is an important geometric property of many natural and man-made structures. Prior research has shown that it is imperative to many perceptual-organization and image-understanding tasks. This paper presents a new grouping method for detecting convex structures from noisy images in a globally optimal fashion. Particularly, this method combines both region and boundary information: the detected structural boundary is closed and well aligned with detected edges while the enclosed region has good intensity homogeneity. We introduce a ratio-form cost function for measuring the structural desirability, which avoids a possible bias to detect small structures. A new fragment-pruning algorithm is developed to achieve the structural convexity. The proposed method can also be extended to detect open boundaries, which correspond to the structures that are partially cropped by the image perimeter and incorporate a human-computer interaction for detecting a convex boundary around a specified point. We test the proposed method on a set of real images and compare it with the Jacobs'convex-grouping method",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544823",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 5,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Jacobian matrices",
                "Psychology",
                "Face detection",
                "Image segmentation",
                "Computer science",
                "Cost function",
                "Testing",
                "Computer vision",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convex grouping",
                "boundary information",
                "region information",
                "geometric property",
                "convex structure detection",
                "noisy images",
                "fragment-pruning algorithm",
                "structural convexity",
                "image perimeter",
                "human-computer interaction",
                "perceptual organization"
            ]
        },
        "id": 123,
        "cited_by": []
    },
    {
        "title": "Detecting rotational symmetries",
        "authors": [
            "V. Shiv Naga Prasad",
            "L.S. Davis"
        ],
        "abstract": "We present an algorithm for detecting multiple rotational symmetries in natural images. Given an image, its gradient magnitude field is computed, and information from the gradients is spread using a diffusion process in the form of a gradient vector flow (GVF) field. We construct a graph whose nodes correspond to pixels in tire image, connecting points that are likely to be rotated versions of one another The n-cycles present in tire graph are made to vote for C/sub n/ symmetries, their votes being weighted by the errors in transformation between GVF in the neighborhood of the voting points, and the irregularity of the n-sided polygons formed by the voters. The votes are accumulated at tire centroids of possible rotational symmetries, generating a confidence map for each order of symmetry. We tested the method with several natural images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544824",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 1,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Voting",
                "Computer vision",
                "Educational institutions",
                "Robustness",
                "Diffusion processes",
                "Pixel",
                "Joining processes",
                "Testing",
                "Reflection",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "image recognition",
                "graph theory",
                "computational geometry",
                "tyres",
                "axial symmetry",
                "gradient methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple rotational symmetries",
                "gradient magnitude field",
                "gradient vector flow",
                "tire image",
                "tire graph",
                "n-sided polygons"
            ]
        },
        "id": 124,
        "cited_by": []
    },
    {
        "title": "Learning nongenerative grammatical models for document analysis",
        "authors": [
            "M. Shilman",
            "P. Liang",
            "P. Viola"
        ],
        "abstract": "We present a general approach for the hierarchical segmentation and labeling of document layout structures. This approach models document layout as a grammar and performs a global search for the optimal parse based on a grammatical cost function. Our contribution is to utilize machine learning to discriminatively select features and set all parameters in the parsing process. Therefore, and unlike many other approaches for layout analysis, ours can easily adapt itself to a variety of document analysis problems. One need only specify the page grammar and provide a set of correctly labeled pages. We apply this technique to two document image analysis tasks: page layout structure extraction and mathematical expression interpretation. Experiments demonstrate that the learned grammars can be used to extract the document structure in 57 files from the UWIII document image database. We also show that the same framework can be used to automatically interpret printed mathematical expressions so as to recreate the original LaTeX",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544825",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Text analysis",
                "Machine learning",
                "Libraries",
                "Parameter estimation",
                "Labeling",
                "Cost function",
                "Image analysis",
                "Image databases",
                "Dynamic programming",
                "Computer languages"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "grammars",
                "learning (artificial intelligence)",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nongenerative grammatical models",
                "document layout structures",
                "grammatical cost function",
                "machine learning",
                "feature selection",
                "parsing",
                "document analysis",
                "page grammar",
                "document image analysis",
                "page layout structure extraction",
                "mathematical expression interpretation",
                "LaTeX"
            ]
        },
        "id": 125,
        "cited_by": []
    },
    {
        "title": "Phase field models and higher-order active contours",
        "authors": [
            "M. Rochery",
            "I. Jermyn",
            "J. Zerubia"
        ],
        "abstract": "The representation and modelling of regions is an important topic in computer vision. In this paper, we represent a region via a level set of a 'phase field' function. The function is not constrained, e.g. to be a distance function; nevertheless, phase field energies equivalent to classical active contour energies can be defined. They represent an advantageous alternative to other methods: a linear representation space; ease of implementation (a PDE with no reinitialization); neutral initialization; greater topological freedom. We extend the basic phase field model with terms that reproduce 'higher-order active contour' energies, a powerful way of including prior geometric knowledge in the active contour framework via nonlocal interactions between contour points, in addition to the above advantages, the phase field greatly simplifies the analysis and implementation of the higher-order terms. We define a phase field model that favours regions composed of thin arms meeting at junctions, combine this with image terms, and apply the model to the extraction of line networks from remote sensing images",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544826",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 10,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Active contours",
                "Level set",
                "Computer vision",
                "Solid modeling",
                "Layout",
                "Equations",
                "Shape control",
                "Arm",
                "Remote sensing",
                "Muscles"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "phase field models",
                "higher-order active contours",
                "region representation",
                "region modelling",
                "computer vision",
                "remote sensing images"
            ]
        },
        "id": 126,
        "cited_by": []
    },
    {
        "title": "Globally optimal estimates for geometric reconstruction problems",
        "authors": [
            "F. Kahl",
            "D. Henrion"
        ],
        "abstract": "We introduce a framework for computing statistically optimal estimates of geometric reconstruction problems. While traditional algorithms often suffer from either local minima or nonoptimality - or a combination of both - we pursue the goal of achieving global solutions of the statistically optimal cost-function. Our approach is based on a hierarchy of convex relaxations to solve nonconvex optimization problems with polynomials. These convex relaxations generate a monotone sequence of lower bounds and we show how one can detect whether the global optimum is attained at a given relaxation. The technique is applied to a number of classical vision problems: triangulation, camera pose, homography estimation and last, but not least, epipolar geometry estimation. Experimental validation on both synthetic and real data is provided. In practice, only a few relaxations are needed for attaining the global optimum",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544827",
        "reference_list": [],
        "citation": {
            "ieee": 19,
            "other": 17,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Computer vision",
                "Cameras",
                "Geometry",
                "Linear matrix inequalities",
                "Computer science",
                "Explosions",
                "Reconstruction algorithms",
                "Optimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "concave programming",
                "convex programming",
                "estimation theory",
                "mesh generation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric reconstruction",
                "statistically optimal estimates",
                "statistically optimal cost-function",
                "convex relaxations",
                "nonconvex optimization",
                "monotone sequence",
                "triangulation",
                "camera pose",
                "homography estimation",
                "epipolar geometry estimation"
            ]
        },
        "id": 127,
        "cited_by": [
            {
                "year": "2009",
                "id": 171
            },
            {
                "year": "2007",
                "id": 46
            },
            {
                "year": "2007",
                "id": 201
            },
            {
                "year": "2007",
                "id": 255
            },
            {
                "year": "2007",
                "id": 274
            },
            {
                "year": "2005",
                "id": 130
            }
        ]
    },
    {
        "title": "Quasiconvex optimization for robust geometric reconstruction",
        "authors": [
            "Q. Ke",
            "T. Kanade"
        ],
        "abstract": "Geometric reconstruction problems in computer vision are often solved by minimizing a cost function that combines the reprojection errors in the 2D images. In this paper, we show that, for various geometric reconstruction problems, their reprojection error functions share a common and quasiconvex formulation. Based on the quasiconvexity, we present a novel quasiconvex optimization framework in which the geometric reconstruction problems are formulated as a small number of small-scale convex programs that are ready to solve. Our final reconstruction algorithm is simple and has intuitive geometric interpretation. In contrast to existing random sampling or local minimization approaches. Our algorithm is deterministic and guarantees a predefined accuracy of the minimization result. We demonstrate the effectiveness of our algorithm by experiments on both synthetic and real data",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544828",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 28,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Image reconstruction",
                "Cameras",
                "Cost function",
                "Computer errors",
                "Computer vision",
                "Minimization methods",
                "Computer science",
                "Reconstruction algorithms",
                "Image sampling"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "convex programming",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "quasiconvex optimization",
                "geometric reconstruction",
                "computer vision",
                "cost function minimization",
                "reprojection error functions"
            ]
        },
        "id": 128,
        "cited_by": [
            {
                "year": "2011",
                "id": 50
            },
            {
                "year": "2011",
                "id": 116
            },
            {
                "year": "2009",
                "id": 12
            },
            {
                "year": "2007",
                "id": 57
            },
            {
                "year": "2007",
                "id": 74
            },
            {
                "year": "2007",
                "id": 75
            },
            {
                "year": "2007",
                "id": 191
            },
            {
                "year": "2007",
                "id": 247
            },
            {
                "year": "2007",
                "id": 255
            }
        ]
    },
    {
        "title": "Shapelets correlated with surface normals produce surfaces",
        "authors": [
            "P. Kovesi"
        ],
        "abstract": "This paper addresses the problem of deducing the surface shape of an object given just the surface normals. Many shape measurement algorithms such as shape from shading and shape from texture only return the surface normals of an object, often with an ambiguity of pi in the surface tilt. The surface shape has to be inferred from these normals, typically via some integration process. However; reconstruction through the integration of surface gradients is sensitive to noise and the choice of integration paths across the surface. In addition, existing techniques cannot accommodate ambiguities in tilt. This paper presents a new approach to the reconstruction of surfaces from surface normals using basis functions, referred to here as shapelets. The surface gradients of the shapelets are correlated with the gradients of the surface and the correlations summed to form the reconstruction. This results in a simple reconstruction process that is very robust to noise. Where there is an ambiguity of it in the surface tilt, reconstructions of reduced quality are still possible up to a positive/negative shape ambiguity. Intriguingly, some form of reconstruction is also possible using just slant information",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544829",
        "reference_list": [
            {
                "year": "2001",
                "id": 164
            }
        ],
        "citation": {
            "ieee": 48,
            "other": 30,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Noise shaping",
                "Shape measurement",
                "Surface texture",
                "Noise robustness",
                "Computer vision",
                "Stability",
                "Computer science",
                "Software engineering",
                "Australia"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shapelets",
                "surface normals",
                "surface shape",
                "shape measurement algorithms",
                "surface tilt",
                "surface gradients",
                "surface reconstruction",
                "basis functions"
            ]
        },
        "id": 129,
        "cited_by": [
            {
                "year": "2009",
                "id": 217
            },
            {
                "year": "2007",
                "id": 263
            }
        ]
    },
    {
        "title": "Multiple view geometry and the L/sub /spl infin//-norm",
        "authors": [
            "F. Kahl"
        ],
        "abstract": "This paper presents a new framework for solving geometric structure and motion problems based on L/sub /spl infin//-norm. Instead of using the common sum-of-squares cost-function, that is, the L/sub /spl infin//-norm, the model-fitting errors are measured using the L/sub /spl infin//-norm. Unlike traditional methods based on L/sub 2/ our framework allows for efficient computation of global estimates. We show that a variety of structure and motion problems, for example, triangulation, camera resectioning and homography estimation can be recast as a quasiconvex optimization problem within this framework. These problems can be efficiently solved using second order cone programming (SOCP) which is a standard technique in convex optimization. The proposed solutions have been validated on real data in different settings with small and large dimensions and with excellent performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544830",
        "reference_list": [
            {
                "year": "2005",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 25,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Cameras",
                "Image reconstruction",
                "Computer vision",
                "Computer science",
                "Motion estimation",
                "Layout",
                "Motion measurement",
                "Polynomials",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "convex programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple view geometry",
                "L/sub /spl infin//-norm",
                "geometric structure problem",
                "geometric motion problem",
                "sum-of-squares cost function",
                "model-fitting errors",
                "quasiconvex optimization",
                "second order cone programming"
            ]
        },
        "id": 130,
        "cited_by": []
    },
    {
        "title": "Object detection in aerial imagery based on enhanced semi-supervised learning",
        "authors": [
            "Jian Yao",
            "Zhongfei Zhang"
        ],
        "abstract": "Object detection in aerial imagery has been well studied in computer vision for years. However, given the complexity of large variations of the appearance of the object and the background in a typical aerial image, a robust and efficient detection is still considered as an open and challenging problem. In this paper, we present the enhanced semi-supervised learning (ESL) framework and apply this framework to revising an object detection methodology we have developed in a previous effort. Theoretic analysis and experimental evaluation using the UCI machine learning repository clearly indicate the superiority of the ESL framework. The performance evaluations of the revised object detection methodology against the original one clearly demonstrate the promise and superiority of this approach",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544831",
        "reference_list": [
            {
                "year": "2003",
                "id": 69
            },
            {
                "year": "2001",
                "id": 103
            },
            {
                "year": "2003",
                "id": 2
            },
            {
                "year": "2001",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Semisupervised learning",
                "Computer vision",
                "Iterative algorithms",
                "Machine learning algorithms",
                "Machine learning",
                "Supervised learning",
                "Labeling",
                "Computer science",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object detection",
                "aerial imagery",
                "semisupervised learning",
                "computer vision"
            ]
        },
        "id": 131,
        "cited_by": []
    },
    {
        "title": "A new framework for approximate labeling via graph cuts",
        "authors": [
            "N. Komodakis",
            "G. Tziritas"
        ],
        "abstract": "A new framework is presented that uses tools from duality theory of linear programming to derive graph-cut based combinatorial algorithms for approximating NP-hard classification problems. The derived algorithms include alpha-expansion graph cut techniques merely as a special case, have guaranteed optimality properties even in cases where alpha-expansion techniques fail to do so and can provide very tight per-instance suboptimality bounds in all occasions",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544832",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 12,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Costs",
                "Linear programming",
                "Approximation algorithms",
                "Computer science",
                "Electronic mail",
                "Image restoration",
                "Bridges",
                "Vectors",
                "NP-complete problem"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "graph theory",
                "image classification",
                "linear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "approximate labeling",
                "duality theory",
                "linear programming",
                "graph cut based combinatorial algorithms",
                "NP-hard classification problems",
                "alpha-expansion graph cut"
            ]
        },
        "id": 132,
        "cited_by": []
    },
    {
        "title": "Okapi-Chamfer matching for articulate object recognition",
        "authors": [
            "Hanning Zhou",
            "Thomas Huang"
        ],
        "abstract": "Recent years have witnessed the rise of many effective text information retrieval systems. By treating local visual features as terms, training images as documents and input images as queries, we formulate the problem of object recognition into that of text retrieval. Our formulation opens up the opportunity to integrate some powerful text retrieval tools with computer vision techniques. In this paper, we propose to improve the efficiency of articulated object recognition by an Okapi-Chamfer matching algorithm. The algorithm is based on the inverted index technique. The inverted index is a widely used way to effectively organize a collection of text documents. With the inverted index, only documents that contain query terms are accessed and used for matching. To enable inverted indexing in an image database, we build a lexicon of local visual features by clustering the features extracted from the training images. Given a query image, we extract visual features and quantize them based on the lexicon, and then look up the inverted index to identify the subset of training images with non-zero matching score. To evaluate the matching scores in the subset, we combined the modified Okapi weighting formula with the Chamfer distance. The performance of the Okapi-Chamfer matching algorithm is evaluated on a hand posture recognition system. We test the system with both synthesized and real world images. Quantitative results demonstrate the accuracy and efficiency of our system",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544833",
        "reference_list": [
            {
                "year": "2003",
                "id": 45
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2003",
                "id": 140
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Image databases",
                "Computer vision",
                "Shape",
                "Image retrieval",
                "Feature extraction",
                "Information retrieval",
                "Indexing",
                "System testing",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image retrieval",
                "information retrieval systems",
                "object recognition",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Okapi-Chamfer matching",
                "object recognition",
                "text information retrieval systems",
                "inverted index",
                "text documents",
                "query terms",
                "features extraction",
                "visual feature extraction",
                "Okapi weighting formula",
                "Chamfer distance",
                "hand posture recognition"
            ]
        },
        "id": 133,
        "cited_by": []
    },
    {
        "title": "Background estimation as a labeling problem",
        "authors": [
            "S. Cohen"
        ],
        "abstract": "We present a new background estimation algorithm that constructs the background of an image sequence with moving objects by copying areas from input frames. The background estimation problem is formulated as an optimal labeling problem in which the label at an output pixel is the frame number from which to copy the background color. The costs of assigning labels encourage seamless copying from regions that are stationary over a period of time in such a way that implied motion boundaries occur at intensity edges. This is accomplished without explicitly tracking the moving objects or computing optical flow. Experiments demonstrate that our algorithm is effective in difficult areas where the background is visible for only a small fraction of time, and on inputs with both moving objects that are not always in motion and moving objects with textureless areas",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544834",
        "reference_list": [
            {
                "year": "2001",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 3,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Image motion analysis",
                "Optical computing",
                "Color",
                "Layout",
                "Motion estimation",
                "Image sequences",
                "Costs",
                "Image segmentation",
                "Video surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image colour analysis",
                "image motion analysis",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "background estimation",
                "image sequence background",
                "moving objects",
                "optimal labeling problem",
                "background color"
            ]
        },
        "id": 134,
        "cited_by": [
            {
                "year": "2007",
                "id": 95
            }
        ]
    },
    {
        "title": "Bayesian structural content abstraction for region-level image authentication",
        "authors": [
            "Wei Feng",
            "Zhi-Qiang Liu"
        ],
        "abstract": "We present a hierarchical representation of image structure and use it for image content authentication. Firstly, we model the image with the Markov pixon random field. Within the Bayesian framework, the optimal label map and regional pixon map can be obtained, based on which we define an undirected graph, namely Bayesian structural content abstraction (BaSCA). This representation captures the spatial topology information of homogeneous regions as well as their finest scale and interactions. Then, an efficient optimization scheme has been proposed to iteratively minimize the distance (or learning error) to all content-identical image samples generated by an acceptable operation set defined by the user. In addition, we use the regional pixon map to remove spurious vertices and thus to establish a BaSCA hierarchy naturally The BaSCA itself and its features can act as the signature of the protected image. Our experimental results show that the proposed approach has much less false positive and comparable false negative probability compared with the existing methods",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544835",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Authentication",
                "Protection",
                "Image coding",
                "Image generation",
                "Digital images",
                "Transform coding",
                "Robustness",
                "Topology",
                "Image processing"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "image representation",
                "security of data"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian structural content abstraction",
                "region-level image authentication",
                "image structure representation",
                "image content authentication",
                "Markov pixon random field",
                "optimal label map",
                "regional pixon map",
                "undirected graph",
                "optimization",
                "protected image signature"
            ]
        },
        "id": 135,
        "cited_by": []
    },
    {
        "title": "A graph cut algorithm for generalized image deconvolution",
        "authors": [
            "A. Raj",
            "R. Zabih"
        ],
        "abstract": "The goal of deconvolution is to recover an image x from its convolution with a known blurring function. This is equivalent to inverting the linear system y = Hx. In this paper, we consider the generalized problem where the system matrix H is an arbitrary nonnegative matrix. Linear inverse problems can be solved by adding a regularization term to impose spatial smoothness. To avoid oversmoothing, the regularization term must preserve discontinuities; this results in a particularly challenging energy minimization problem. Where H is diagonal, as occurs in image denoising, the energy function can be solved by techniques such as graph cuts, which have proven to be very effective for problems in early vision. When H is nondiagonal, however, the data cost for a pixel to have a intensity depends on the hypothesized intensities of nearby pixels, so existing graph cut methods cannot be applied. This paper shows how to use graph cuts to obtain a discontinuity preserving solution to a linear inverse system with an arbitrary non-negative system matrix. We use a dynamically chosen approximation to the energy which can he minimized by graph cuts; minimizing this approximation also decreases the original energy. Experimental results are shown for MRI reconstruction from Fourier data",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544836",
        "reference_list": [
            {
                "year": "2003",
                "id": 118
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 5,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Deconvolution",
                "Inverse problems",
                "Costs",
                "Convolution",
                "Image denoising",
                "Magnetic resonance imaging",
                "Image reconstruction",
                "Equations",
                "Linear systems",
                "Sparse matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image restoration",
                "inverse problems",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph cut algorithm",
                "image deconvolution",
                "image recovery",
                "blurring function",
                "system matrix",
                "linear inverse problems",
                "MRI reconstruction"
            ]
        },
        "id": 136,
        "cited_by": []
    },
    {
        "title": "Reconstructing the geometry of flowing water",
        "authors": [
            "I. Ihrke",
            "B. Goidluecke",
            "M. Magnor"
        ],
        "abstract": "We present a recording scheme, image formation model and reconstruction method that enables image-based modeling of flowing bodies of water from multivideo input data. The recorded water is dyed with a fluorescent chemical to measure the thickness of a column of water which leads to an image formation model based on integrated emissivities along a viewing ray. This model allows for a photo-consistency based error measure for a weighted minimal surface, which is recovered using a PDE obtained from the Euler-Lagrangian formulation of the problem. The resulting equation is solved using the level set method",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544837",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 10,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Image reconstruction",
                "Surface reconstruction",
                "Optical surface waves",
                "Cameras",
                "Fluorescence",
                "Level set",
                "Tomography",
                "Light sources",
                "Refractive index"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "flow",
                "image reconstruction",
                "surface reconstruction",
                "water"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "flowing water geometry reconstruction",
                "image formation",
                "image reconstruction",
                "image-based modeling",
                "photo-consistency based error",
                "weighted minimal surface",
                "partial differential equations",
                "Euler-Lagrangian formulation",
                "level set method"
            ]
        },
        "id": 137,
        "cited_by": [
            {
                "year": "2015",
                "id": 377
            },
            {
                "year": "2011",
                "id": 149
            }
        ]
    },
    {
        "title": "Surface parameterization using Riemann surface structure",
        "authors": [
            "Yalin Wang",
            "Xianfeng Gu",
            "K.M. Hayashi",
            "T.F. Chan",
            "P.M. Thompson",
            "Shing-Tung Yau"
        ],
        "abstract": "We propose a general method that parameterizes general surfaces with complex (possible branching) topology using Riemann surface structure. Rather than evolve the surface geometry to a plane or sphere, we instead use the fact that all orientable surfaces are Riemann surfaces and admit conformal structures, which induce special curvilinear coordinate systems on the surfaces. We can then automatically partition the surface using a critical graph that connects zero points in the global conformal structure on the surface. The trajectories of iso-parametric curves canonically partition a surface into patches. Each of these patches is either a topological disk or a cylinder and can be conformally mapped to a parallelogram by integrating a holomorphic I-form defined on the surface. The resulting surface subdivision and the parameterizations of the components are intrinsic and stable. For surfaces with similar topology and geometry, we show that the parameterization results are consistent and the subdivided surfaces can be matched to each other using constrained harmonic maps. The surface similarity can be measured by direct computation of distance between each pair of corresponding points on two surfaces. To illustrate the technique, we computed conformal structures for anatomical surfaces in MRI scans of the brain and human face surfaces. We found that the resulting parameterizations were consistent across subjects, even for branching structures such as the ventricles, which are otherwise difficult to parameterize. Our method provides a surface-based framework for statistical comparison of surfaces and for generating grids on surfaces for PDE-based signal processing",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544838",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface structures",
                "Shape",
                "Mathematics",
                "Biomedical imaging",
                "Topology",
                "Neuroimaging",
                "Geometry",
                "Grid computing",
                "Conformal mapping",
                "Brain"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image segmentation",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface parameterization",
                "Riemann surface structure",
                "conformal structures",
                "curvilinear coordinate systems",
                "surface partition",
                "critical graph",
                "iso-parametric curves",
                "surface topology",
                "surface geometry",
                "constrained harmonic maps",
                "surface similarity",
                "brain surface",
                "human face surface"
            ]
        },
        "id": 138,
        "cited_by": [
            {
                "year": "2011",
                "id": 74
            },
            {
                "year": "2009",
                "id": 215
            },
            {
                "year": "2007",
                "id": 69
            }
        ]
    },
    {
        "title": "When does a camera see rain?",
        "authors": [
            "K. Garg",
            "S.K. Nayar"
        ],
        "abstract": "Rain produces sharp intensity fluctuations in images and videos, which degrade the performance of outdoor vision systems. These intensity fluctuations depend on various factors, such as the camera parameters, the properties of rain, and the brightness of the scene. We show that the properties of rain - its small drop size, high velocity, and low density - make its visibility strongly dependent on camera parameters such as exposure time and depth of field. We show that these parameters can be selected so as to reduce or even remove the effects of rain without altering the appearance of the scene. Conversely, the parameters of a camera can also be set to enhance the visual effects of rain. This can be used to develop an inexpensive and portable camera-based rain gauge that provides instantaneous rain rate measurements. The proposed methods serve to make vision algorithms more robust to rain without any necessity for post-processing. In addition, they can be used to control the visual effects of rain during the filming of movies",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544839",
        "reference_list": [],
        "citation": {
            "ieee": 47,
            "other": 20,
            "total": 67
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Rain",
                "Fluctuations",
                "Layout",
                "Visual effects",
                "Videos",
                "Degradation",
                "Machine vision",
                "Brightness",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "natural scenes",
                "rain"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sharp intensity fluctuations",
                "outdoor vision systems",
                "camera parameters",
                "rain visual effects",
                "camera-based rain gauge"
            ]
        },
        "id": 139,
        "cited_by": [
            {
                "year": "2017",
                "id": 265
            },
            {
                "year": "2017",
                "id": 266
            }
        ]
    },
    {
        "title": "Uncalibrated perspective reconstruction of deformable structures",
        "authors": [
            "Jing Xiao",
            "T. Kanade"
        ],
        "abstract": "Reconstruction of 3D structures from uncalibrated image sequences has a wealthy history. Most work has been focused on rigid objects or static scenes. This paper studies the problem of perspective reconstruction of deformable structures such as dynamic scenes from an uncalibrated image sequence. The task requires decomposing the image measurements into a composition of three factors: 3D deformable structures, rigid rotations and translations, and intrinsic camera parameters. We develop a factorization algorithm that consists of two steps. In the first step we recover the protective depths iteratively using the sub-space constraints embedded in the image measurements of the deformable structures. In the second step, we scale the image measurements by the reconstructed projective depths. We then extend the linear closed-form solution for weak-perspective reconstruction by J. Xiao, et al. (2004) to factorize the scaled measurements and simultaneously reconstruct the deformable shapes and underlying shape model, the rigid motions, and the varying camera parameters such as focal lengths. The accuracy and robustness of the proposed method is demonstrated quantitatively on synthetic data and qualitatively on real image sequences",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544840",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 2,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Image sequences",
                "Shape measurement",
                "Layout",
                "Cameras",
                "History",
                "Rotation measurement",
                "Iterative algorithms",
                "Protection",
                "Subspace constraints"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "uncalibrated perspective reconstruction",
                "3D deformable structure",
                "image sequence",
                "image measurement",
                "rigid rotation",
                "rigid translation",
                "intrinsic camera parameter",
                "factorization algorithm",
                "reconstructed projective depth",
                "linear closed-form solution",
                "shape model",
                "focal length"
            ]
        },
        "id": 140,
        "cited_by": [
            {
                "year": "2009",
                "id": 232
            },
            {
                "year": "2007",
                "id": 191
            }
        ]
    },
    {
        "title": "Fitting globally stabilized algebraic surfaces to range data",
        "authors": [
            "T. Sahin",
            "M. Unel"
        ],
        "abstract": "Linear fitting of implicit algebraic models to data usually suffers from global stability problems. Complicated object structures can accurately be modeled by closed-bounded surfaces of higher degrees using ridge regression. This paper derives an explicit formula for computing a Euclidean invariant 3D ridge regression matrix and applies it for the global stabilization of a particular linear fitting method. Experiments show that the proposed approach improves global stability of resulting surfaces significantly",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544841",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 6,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface fitting",
                "Polynomials",
                "Shape",
                "Data engineering",
                "Stability",
                "Noise shaping",
                "Noise robustness",
                "Topology",
                "Level set",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "curve fitting",
                "matrix algebra",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear fitting",
                "implicit algebraic model",
                "object structure",
                "closed-bounded surface",
                "Euclidean invariant 3D ridge regression matrix",
                "globally stabilized algebraic surface fitting"
            ]
        },
        "id": 141,
        "cited_by": []
    },
    {
        "title": "Lighting normalization with generic intrinsic illumination subspace for face recognition",
        "authors": [
            "Chia-Ping Chen",
            "Chu-Song Chen"
        ],
        "abstract": "In this paper, we introduce the concept of intrinsic illumination subspace which is based on the intrinsic images. This intrinsic illumination subspace enables an analytic generation of the illumination images under varying lighting conditions. When objects of the same class are concerned, our method allows a class-based generic intrinsic illumination subspace to be constructed in advance. We propose a lighting normalization method based on the generic intrinsic illumination subspace, which is used as a bootstrap subspace for novel images. Face recognition experiments are performed to demonstrate the effectiveness of our method",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544842",
        "reference_list": [
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 2,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Face recognition",
                "Reflectivity",
                "Face detection",
                "Maximum likelihood estimation",
                "Image recognition",
                "Information science",
                "Image analysis",
                "Geometry",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "lighting normalization",
                "face recognition",
                "intrinsic image",
                "class-based generic intrinsic illumination subspace",
                "bootstrap subspace"
            ]
        },
        "id": 142,
        "cited_by": []
    },
    {
        "title": "Layered active appearance models",
        "authors": [
            "E. Jones",
            "S. Soatto"
        ],
        "abstract": "Active appearance models (AAMs) provide a framework for modeling the joint shape and texture of an image. An AAM is a compact representation of both factors in a conditionally linear model. However, the standard AAM framework does not handle images which have missing features, or allow modification of certain structures in the image while leaving neighboring ones undeformed. We introduce the layered active appearance model (LAAM), which allows for missing features, occlusion, substantial spatial rearrangement of features, and which provides a more general representation that extends the applicability of the active appearance model",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544843",
        "reference_list": [
            {
                "year": "2003",
                "id": 195
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 1,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Principal component analysis",
                "Licenses",
                "Active shape model",
                "Computer science",
                "Eyes",
                "Nose",
                "Mouth",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "layered active appearance model",
                "image joint shape",
                "image texture",
                "linear model",
                "missing feature",
                "feature substantial spatial rearrangement"
            ]
        },
        "id": 143,
        "cited_by": []
    },
    {
        "title": "Identifying individuals in video by combining 'generative' and discriminative head models",
        "authors": [
            "M. Everingham",
            "A. Zisserman"
        ],
        "abstract": "The objective of this work is automatic detection and identification of individuals in unconstrained consumer video, given a minimal number of labelled faces as training data. Whilst much work has been done on (mainly frontal) face detection and recognition, current methods are not sufficiently robust to deal with the wide variations in pose and appearance found in such video. These include variations in scale, illumination, expression, partial occlusion, motion blur, etc. We describe two areas of innovation: the first is to capture the 3-D appearance of the entire head, rather than just the face region, so that visual features such as the hairline can be exploited. The second is to combine discriminative and 'generative' approaches for detection and recognition. Images rendered using the head model are used to train a discriminative tree-structured classifier giving efficient detection and pose estimates over a very wide pose range with three degrees of freedom. Subsequent verification of the identity is obtained using the head model in a 'generative' framework. We demonstrate excellent performance in detecting and identifying three characters and their poses in a TV situation comedy",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544844",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 6,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Head",
                "Face detection",
                "Training data",
                "Face recognition",
                "Robustness",
                "Lighting",
                "Technological innovation",
                "Rendering (computer graphics)",
                "Classification tree analysis",
                "TV"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "object detection",
                "pattern classification",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unconstrained consumer video",
                "generative head model",
                "discriminative head model",
                "automatic detection",
                "automatic identification",
                "labelled faces",
                "face detection",
                "face recognition",
                "3D appearance",
                "visual feature",
                "discriminative tree-structured classifier",
                "degrees of freedom",
                "TV situation comedy"
            ]
        },
        "id": 144,
        "cited_by": []
    },
    {
        "title": "A general framework for temporal video scene segmentation",
        "authors": [
            "Yun Zhai",
            "M. Shah"
        ],
        "abstract": "Videos are composed of many shots caused by different camera operations, e.g., on/off operations and switching between cameras. One important goal in video analysis is to group the shots into temporal scenes, such that all the shots in a single scene are related to a particular physical setting, an on-going action or a theme. In this paper, we present a general framework for temporal scene segmentation for various video types. The proposed method is formulated in a statistical fashion and uses the Markov chain Monte Carlo (MCMC) technique to determine the boundaries between video scenes. In this approach, an arbitrary number of scene boundaries are randomly initialized and automatically updated using two types of updates: diffuse and jumps. The posterior probability on the number of scenes and their boundary locations is computed based on the model priors and the data likelihood. The updates of the model parameters are controlled by the hypothesis ratio test in the MCMC process. The proposed framework has been experimented on two types of videos, home videos and feature films, and accurate results have been obtained",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544845",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Motion pictures",
                "Production",
                "Computer science",
                "Multimedia communication",
                "TV broadcasting",
                "Hidden Markov models",
                "Monte Carlo methods",
                "Probability"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "Markov processes",
                "Monte Carlo methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporal video scene segmentation",
                "camera operation",
                "video analysis",
                "Markov chain Monte Carlo technique",
                "scene boundary",
                "hypothesis ratio test",
                "home video"
            ]
        },
        "id": 145,
        "cited_by": []
    },
    {
        "title": "Geometric and photometric restoration of distorted documents",
        "authors": [
            "Mingxuan Sun",
            "Ruigang Yang",
            "Lin Yun",
            "G. Landon",
            "B. Seales",
            "M.S. Brown"
        ],
        "abstract": "We present a system to restore the 2D content printed on distorted documents. Our system works by acquiring a 3D scan of the document's surface together with a high-resolution image. Using the 3D surface information and the 2D image, we can ameliorate unwanted surface distortion and effects from non-uniform illumination. Our system can process arbitrary geometric distortions, not requiring any pre-assumed parametric models for the document's geometry. The illumination correction uses the 3D shape to distinguish content edges from illumination edges to recover the 2D content's reflectance image while making no assumptions about light sources and their positions. Results are shown for real objects, demonstrating a complete framework capable of restoring geometric and photometric artifacts on distorted documents",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544846",
        "reference_list": [
            {
                "year": "2003",
                "id": 31
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 0,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Image restoration",
                "Lighting",
                "Shape",
                "Geometry",
                "Reflectivity",
                "Light sources",
                "Sun",
                "Computer science",
                "Parametric statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "document image processing",
                "image restoration",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric restoration",
                "photometric restoration",
                "distorted document",
                "2D content reflectance image",
                "3D scan",
                "high-resolution image",
                "3D surface information",
                "surface distortion",
                "arbitrary geometric distortion",
                "document geometry",
                "illumination correction",
                "3D shape"
            ]
        },
        "id": 146,
        "cited_by": []
    },
    {
        "title": "A model-based vehicle segmentation method for tracking",
        "authors": [
            "Xuefeng Song",
            "R. Nevatia"
        ],
        "abstract": "Our goal is to detect and track moving vehicles on a road observed from cameras placed on poles or buildings. Inter-vehicle occlusion is significant under these conditions and traditional blob tracking methods is unable to separate the vehicles in the merged blobs. We use vehicle shape models, in addition to camera calibration and ground plane knowledge, to detect, track and classify moving vehicles in presence of occlusion. We use a 2-stage approach. In the first stage, hypothesis for vehicle types, positions and orientations are formed by a coarse search, which is then refined by a data driven Markov chain Monte Carlo (DDMCMC) process. We show results and evaluations on some real urban traffic video sequence using three types of vehicle models",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544847",
        "reference_list": [
            {
                "year": "2001",
                "id": 96
            },
            {
                "year": "2003",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 0,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Road vehicles",
                "Vehicle detection",
                "Cameras",
                "Land vehicles",
                "Shape",
                "Calibration",
                "Vehicle driving",
                "Monte Carlo methods",
                "Traffic control",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image motion analysis",
                "image segmentation",
                "image sequences",
                "Markov processes",
                "object detection",
                "road vehicles",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based vehicle segmentation",
                "moving vehicles detection",
                "inter-vehicle occlusion",
                "blob tracking method",
                "vehicle shape model",
                "camera calibration",
                "moving vehicle tracking",
                "moving vehicle classification",
                "vehicle type",
                "vehicle position",
                "vehicle orientation",
                "Markov chain Monte Carlo process",
                "urban traffic video sequence"
            ]
        },
        "id": 147,
        "cited_by": []
    },
    {
        "title": "Finding tree structures by grouping symmetries",
        "authors": [
            "H. Ishikawa",
            "D. Geiger",
            "R. Cole"
        ],
        "abstract": "The representation of objects in images as tree structures is of great interest to vision, as they can represent articulated objects such as people as well as other structured objects like arteries in human bodies, roads, circuit board patterns, etc. Tree structures are often related to the symmetry axis representation of shapes, which captures their local symmetries. Algorithms have been introduced to detect (i) open contours in images in quadratic time (ii) closed contours in images in cubic time, and (iii) tree structures from contours in quadratic time. The algorithms are based on dynamic programming and single source shortest path algorithms. However, in this paper, we show that the problem of finding tree structures in images in a principled manner is a much harder problem. We argue that the optimization problem of finding tree structures in images is essentially equivalent to a variant of the Steiner tree problem, which is NP-hard. Nevertheless, an approximate polynomial-time algorithm for this problem exists: we apply a fast implementation of the Goemans-Williamson approximate algorithm to the problem of finding a tree representation after an image is transformed by a local symmetry mapping. Examples of extracting tree structures from images illustrate the idea and applicability of the approximate method",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544848",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Tree data structures",
                "Shape",
                "Image edge detection",
                "Arteries",
                "Humans",
                "Roads",
                "Printed circuits",
                "Computer vision",
                "Biology",
                "Dynamic programming"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "dynamic programming",
                "image representation",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tree structure",
                "image object representation",
                "symmetry axis representation",
                "open contour",
                "dynamic programming",
                "single source shortest path algorithm",
                "optimization problem",
                "Steiner tree problem",
                "NP-hard problem",
                "polynomial-time algorithm",
                "Goemans-Williamson approximate algorithm",
                "tree representation",
                "symmetry mapping"
            ]
        },
        "id": 148,
        "cited_by": []
    },
    {
        "title": "Learning the probability of correspondences without ground truth",
        "authors": [
            "Qingxiong Yang",
            "R.M. Steele",
            "D. Nister",
            "C. Jayne"
        ],
        "abstract": "We present a quality assessment procedure for correspondence estimation based on geometric coherence rather than ground truth. The procedure can be used for performance evaluation of correspondence extraction schemes developed by researchers, as well as for online learning and adaptation aimed at better system performance. A very important aspect of the proposed procedure is that it considers uncertainty in the correspondence extraction, and encourages the evaluated methods to deal correctly with uncertainty. Other important strengths of the procedure are that it does not use any manual work, and that it does not put any strong constraints on the scene, but rather relies on geometric coherence in the motion. Thanks to these strengths, it can therefore be used with large amounts of real, potentially application specific data, or even data acquired during system operation. In the evaluation the correspondence extractor is handled as a black box producing a probability distribution for the local motion vector between a pair of image patches. The procedure is therefore quite general. We are making the evaluation procedure available for public use",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544849",
        "reference_list": [
            {
                "year": "2003",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Uncertainty",
                "Layout",
                "Data mining",
                "Computer vision",
                "Quality assessment",
                "Probability distribution",
                "Visualization",
                "Virtual environment",
                "Computer science",
                "System performance"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probability distribution",
                "quality assessment procedure",
                "correspondence estimation",
                "geometric coherence",
                "correspondence extraction scheme",
                "uncertainty handling",
                "local motion vector",
                "image patch"
            ]
        },
        "id": 149,
        "cited_by": []
    },
    {
        "title": "Mesh optimization using an inconsistency detection template",
        "authors": [
            "A. Nakatuji",
            "Y. Sugaya",
            "K. Kanatani"
        ],
        "abstract": "We propose a new technique for optimizing a triangular mesh for polyhedral representation of the scene in a video stream. We introduce a specially designed template that can effectively detect color and texture discontinuities. Using real images, we demonstrate that our method is superior to existing methods",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544850",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Layout",
                "Three dimensional displays",
                "Surface reconstruction",
                "Internet",
                "National electric code",
                "Computer science",
                "Streaming media",
                "Image reconstruction",
                "Stereo vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image representation",
                "image texture",
                "mesh generation",
                "object detection",
                "optimisation",
                "video streaming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "inconsistency detection template",
                "triangular mesh optimization",
                "polyhedral representation",
                "video stream",
                "color detection",
                "texture discontinuity detection"
            ]
        },
        "id": 150,
        "cited_by": []
    },
    {
        "title": "Fast texture-based tracking and delineation using texture entropy",
        "authors": [
            "A. Shahrokni",
            "T. Drummond",
            "P. Fua"
        ],
        "abstract": "We propose a fast texture-segmentation approach to the problem of 2D and 3D model-based contour tracking, which is suitable for real-time or interactive applications. Our approach relies on detecting texture boundaries in the direction normal to the contour boundaries and on using a hidden Markov model to link these boundary points in the other direction. The probabilities that appear in this computation closely relate to texture entropy and Kullback-Leibler divergence, a property we use to compute and update dynamic texture models. We demonstrate results both in the context of interactive 2D delineation and fast 3D tracking",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544851",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Hidden Markov models",
                "Target tracking",
                "Computer vision",
                "Application software",
                "Image segmentation",
                "Laboratories",
                "Robustness",
                "Real time systems",
                "Aggregates"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden Markov models",
                "image segmentation",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "texture-based tracking",
                "texture entropy",
                "texture-segmentation approach",
                "contour tracking",
                "texture boundary",
                "contour boundary",
                "Hidden Markov model",
                "Kullback-Leibler divergence",
                "dynamic texture model",
                "interactive 2D delineation",
                "fast 3D tracking"
            ]
        },
        "id": 151,
        "cited_by": []
    },
    {
        "title": "Learning and inference in parametric switching linear dynamic systems",
        "authors": [
            "Sang Min Oh",
            "J.M. Rehg",
            "T. Balch",
            "F. Dellaert"
        ],
        "abstract": "We introduce parametric switching linear dynamic systems (P-SLDS) for learning and interpretation of parametrized motion, i.e., motion that exhibits systematic temporal and spatial variations. Our motivating example is the honeybee dance: bees communicate the orientation and distance to food sources through the dance angles and waggle lengths of their stylized dances. Switching linear dynamic systems (SLDS) are a compelling way to model such complex motions. However, SLDS does not provide a means to quantify systematic variations in the motion. Previously, Wilson & Bobick (1999) presented parametric HMMs, an extension to HMMs with which they successfully interpreted human gestures. Inspired by their work, we similarly extend the standard SLDS model to obtain parametric SLDS. We introduce additional global parameters that represent systematic variations in the motion, and present general expectation-maximization (EM) methods for learning and inference. In the learning phase, P-SLDS learns canonical SLDS model from data. In the inference phase, P-SLDS simultaneously quantifies the global parameters and labels the data. We apply these methods to the automatic interpretation of honey-bee dances, and present both qualitative and quantitative experimental results on actual bee-tracks collected from noisy video data",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544852",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Superluminescent diodes",
                "Hidden Markov models",
                "Humans",
                "Computer vision",
                "Target tracking",
                "Parameter estimation",
                "Educational institutions",
                "Food technology",
                "Trajectory",
                "Computer errors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "expectation-maximisation algorithm",
                "hidden Markov models",
                "image motion analysis",
                "inference mechanisms",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parametric switching linear dynamic system",
                "parametrized motion",
                "systematic temporal variation",
                "systematic spatial variation",
                "honeybee dance",
                "complex motion",
                "parametric HMM",
                "human gesture",
                "global parameter",
                "expectation-maximization method",
                "inference phase",
                "hidden Markov model",
                "learning"
            ]
        },
        "id": 152,
        "cited_by": []
    },
    {
        "title": "Face recognition in the presence of multiple illumination sources",
        "authors": [
            "G. Aggarwal",
            "R. Chellappa"
        ],
        "abstract": "Most existing face recognition algorithms work well for controlled images but are quite susceptible to changes in illumination and pose. This has led to the rise of analysis-by-synthesis approaches due to their inherent potential to handle these external factors. Though these approaches work quite well, most of them assume that the face is illuminated by a single light source which is usually not true in realistic conditions. In this paper, we propose an algorithm to recognize faces illuminated by arbitrarily placed, multiple light sources. The algorithm does not need to know the number of light sources and works extremely well even while recognizing faces illuminated by different number of light sources. Results using this algorithm are reported on multiple-illumination datasets generated from PIE by T. Sim, et al. (2003) and Yale Face Database B by W. Zhao, et al. (2003). We also highlight the importance of the hard non-linearity in the Lambert's law which is often ignored, probably to linearize the estimation process",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544853",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Lighting",
                "Shape",
                "Light sources",
                "Educational institutions",
                "Reflectivity",
                "Computer science",
                "Image databases",
                "Focusing",
                "Image processing"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "light sources"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "multiple illumination source",
                "analysis-by-synthesis approach",
                "hard nonlinearity",
                "Lambert law",
                "estimation process"
            ]
        },
        "id": 153,
        "cited_by": []
    },
    {
        "title": "A bilinear illumination model for robust face recognition",
        "authors": [
            "J. Lee",
            "B. Moghaddam",
            "H. Pfister",
            "R. Machiraju"
        ],
        "abstract": "We present a technique to generate an illumination subspace for arbitrary 3D faces based on the statistics of measured illuminations under variable lighting conditions from many subjects. A bilinear model based on the higher-order singular value decomposition is used to create a compact illumination subspace given arbitrary shape parameters from a parametric 3D face model. Using a fitting procedure based on minimizing the distance of the input image to the dynamically changing illumination subspace, we reconstruct a shape-specific illumination subspace from a single photograph. We use the reconstructed illumination subspace in various face recognition experiments with variable lighting conditions and obtain accuracies which are very competitive with previous methods that require specific training sessions or multiple images of the subject",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544854",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 17,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Robustness",
                "Face recognition",
                "Shape",
                "Face detection",
                "Image reconstruction",
                "Humans",
                "Geometry",
                "Image databases",
                "DC generators"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "lighting",
                "singular value decomposition",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bilinear illumination model",
                "face recognition",
                "lighting condition",
                "singular value decomposition",
                "compact illumination subspace",
                "shape parameter",
                "parametric 3D face model",
                "fitting procedure",
                "shape-specific illumination subspace"
            ]
        },
        "id": 154,
        "cited_by": []
    },
    {
        "title": "Edge-based rich representation for vehicle classification",
        "authors": [
            "Xiaoxu Ma",
            "W.E.L. Grimson"
        ],
        "abstract": "In this paper, we propose an approach to vehicle classification under a mid-field surveillance framework. We develop a repeatable and discriminative feature based on edge points and modified SIFT descriptors, and introduce a rich representation for object classes. Experimental results show the proposed approach is promising for vehicle classification in surveillance videos despite great challenges such as limited image size and quality and large intra-class variations. Comparisons demonstrate the proposed approach outperforms other methods",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544855",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 4,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicles",
                "Object recognition",
                "Video surveillance",
                "Cameras",
                "Monitoring",
                "Protection",
                "Image recognition",
                "Error analysis",
                "Computer science",
                "Artificial intelligence"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "surveillance",
                "vehicles"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "edge-based rich representation",
                "vehicle classification",
                "mid-field surveillance framework",
                "modified SIFT descriptor",
                "surveillance video"
            ]
        },
        "id": 155,
        "cited_by": []
    },
    {
        "title": "Automatic 3D face modeling from video",
        "authors": [
            "Le Xin",
            "Qiang Wang",
            "Jianhua Tao",
            "Xiaoou Tang",
            "Tieniu Tan",
            "H. Shum"
        ],
        "abstract": "In this paper, we develop an efficient technique for fully automatic recovery of accurate 3D face shape from videos captured by a low cost camera. The method is designed to work with a short video containing a face rotating from frontal view to profile view. The whole approach consists of three components. First, automatic initialization is performed in the first frame with approximately frontal face. Then, to handle the case of low quality image captured by low cost camera, the 2D feature matching, head poses and underlying 3D face shape are estimated and refined iteratively in an efficient way based on image sequence segmentation. Finally, to take advantage of the sparse structure of the proposed algorithm, sparse bundle adjustment technique is further employed to speed up the computation. We demonstrate the accuracy and robustness of the algorithm using a set of experiments",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544856",
        "reference_list": [
            {
                "year": "2001",
                "id": 192
            },
            {
                "year": "2003",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Lighting",
                "Costs",
                "Cameras",
                "Head",
                "Iterative algorithms",
                "Robustness",
                "Image reconstruction",
                "Human computer interaction"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image segmentation",
                "image sequences",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic 3D face modeling",
                "video image",
                "automatic initialization",
                "low quality image",
                "feature matching",
                "head pose",
                "3D face shape",
                "image sequence segmentation",
                "sparse bundle adjustment technique"
            ]
        },
        "id": 156,
        "cited_by": []
    },
    {
        "title": "Consistent surface color for texturing large objects in outdoor scenes",
        "authors": [
            "R. Kawakami",
            "K. Ikeuchi",
            "R.T. Tan"
        ],
        "abstract": "Color appearance of an object is significantly influenced by the color of the illumination. When the illumination color changes, the color appearance of the object change accordingly, causing its appearance to be inconsistent. To arrive at color constancy, we have developed a physics-based method of estimating and removing the illumination color. In this paper, we focus on the use of this method to deal with outdoor scenes, since very few physics-based methods have successfully handled outdoor color constancy. Our method is principally based on shadowed and non-shadowed regions. Previously researchers have discovered that shadowed regions are illuminated by sky light, while non-shadowed regions are illuminated by a combination of sky light and sunlight. Based on this difference of illumination, we estimate the illumination colors (both the sunlight and the sky light) and then remove them. To reliably estimate the illumination colors in outdoor scenes, we include the analysis of noise, since the presence of noise is inevitable in natural images. As a result, compared to existing methods, the proposed method is more effective and robust in handling outdoor scenes. In addition, the proposed method requires only a single input image, making it useful for many applications of computer vision",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544857",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 9,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface texture",
                "Layout",
                "Lighting",
                "Colored noise",
                "Computer vision",
                "Image color analysis",
                "Optical reflection",
                "Image analysis",
                "Noise robustness",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image texture",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface color",
                "object texturing",
                "outdoor scene",
                "illumination color",
                "color appearance",
                "outdoor color constancy",
                "physics-based method",
                "shadowed region",
                "noise analysis",
                "natural image",
                "computer vision"
            ]
        },
        "id": 157,
        "cited_by": []
    },
    {
        "title": "Neighborhood preserving embedding",
        "authors": [
            "Xiaofei He",
            "Deng Cai",
            "Shuicheng Yan",
            "Hong-Jiang Zhang"
        ],
        "abstract": "Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is drawn from sampling a probability distribution that has support on or near a submanifold of Euclidean space. In this paper, we propose a novel subspace learning algorithm called neighborhood preserving embedding (NPE). Different from principal component analysis (PCA) which aims at preserving the global Euclidean structure, NPE aims at preserving the local neighborhood structure on the data manifold. Therefore, NPE is less sensitive to outliers than PCA. Also, comparing to the recently proposed manifold learning algorithms such as Isomap and locally linear embedding, NPE is defined everywhere, rather than only on the training data points. Furthermore, NPE may be conducted in the original space or in the reproducing kernel Hilbert space into which data points are mapped. This gives rise to kernel NPE. Several experiments on face database demonstrate the effectiveness of our algorithm",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544858",
        "reference_list": [
            {
                "year": "2003",
                "id": 51
            }
        ],
        "citation": {
            "ieee": 100,
            "other": 31,
            "total": 131
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis",
                "Kernel",
                "Linear discriminant analysis",
                "Computer science",
                "Training data",
                "Computer vision",
                "Helium",
                "Asia",
                "Data analysis",
                "Sampling methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "data analysis",
                "learning (artificial intelligence)",
                "principal component analysis",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "neighborhood preserving embedding",
                "probability distribution",
                "Euclidean space",
                "subspace learning algorithm",
                "principal component analysis",
                "data manifold learning algorithm",
                "kernel Hilbert space",
                "face database"
            ]
        },
        "id": 158,
        "cited_by": [
            {
                "year": "2015",
                "id": 210
            },
            {
                "year": "2011",
                "id": 183
            },
            {
                "year": "2011",
                "id": 187
            },
            {
                "year": "2011",
                "id": 204
            },
            {
                "year": "2007",
                "id": 16
            },
            {
                "year": "2007",
                "id": 218
            }
        ]
    },
    {
        "title": "Scale-invariant contour completion using conditional random fields",
        "authors": [
            "Xiaofeng Ren",
            "C.C. Fowlkes",
            "J. Malik"
        ],
        "abstract": "We present a model of curvilinear grouping using piece-wise linear representations of contours and a conditional random field to capture continuity and the frequency of different junction types. Potential completions are generated by building a constrained Delaunay triangulation (CDT) over the set of contours found by a local edge detector. Maximum likelihood parameters for the model are learned from human labeled ground truth. Using held out test data, we measure how the model, by incorporating continuity structure, improves boundary detection over the local edge detector. We also compare performance with a baseline local classifier that operates on pairs of edgels. Both algorithms consistently dominate the low-level boundary detector at all thresholds. To our knowledge, this is the first time that curvilinear continuity has been shown quantitatively useful for a large variety of natural images. Better boundary detection has immediate application in the problem of object detection and recognition",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544859",
        "reference_list": [
            {
                "year": "2001",
                "id": 60
            },
            {
                "year": "2003",
                "id": 151
            },
            {
                "year": "2003",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 4,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Object detection",
                "Detectors",
                "Piecewise linear techniques",
                "Humans",
                "Layout",
                "Computer vision",
                "Face detection",
                "Image segmentation",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "edge detection",
                "maximum likelihood estimation",
                "mesh generation",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale-invariant contour completion",
                "conditional random field",
                "curvilinear grouping",
                "piecewise linear representation",
                "constrained Delaunay triangulation",
                "local edge detector",
                "maximum likelihood parameter",
                "boundary detection",
                "baseline local classifier",
                "curvilinear continuity",
                "natural image",
                "object detection",
                "object recognition"
            ]
        },
        "id": 159,
        "cited_by": [
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2009",
                "id": 1
            },
            {
                "year": "2007",
                "id": 90
            },
            {
                "year": "2005",
                "id": 106
            }
        ]
    },
    {
        "title": "Common pattern discovery using earth mover's distance and local flow maximization",
        "authors": [
            "Hung-Khoon Tan",
            "Chong-Wah Ngo"
        ],
        "abstract": "In this paper, we present a novel segmentation-insensitive approach for mining common patterns from 2 images. We develop an algorithm using the earth movers distance (EMD) framework, unary and adaptive neighborhood color similarity. We then propose a novel local flow maximization approach to provide the best estimation of location and scale of the common pattern. This is achieved by performing an iterative optimization in search of the most stable flows' centroid. Common pattern discovery is difficult owing to the huge search space and problem domain. We intend to solve this problem by reducing the search space through identifying the location and a reduced spatial space for common pattern discovery. Experimental results justify the effectiveness and the potential of the approach",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544860",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 0,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Earth",
                "Image segmentation",
                "Image databases",
                "Visual databases",
                "Computer science",
                "Iterative algorithms",
                "Digital images",
                "Spatial databases",
                "Data mining",
                "Indexing"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image segmentation",
                "optimisation",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pattern discovery",
                "earth movers distance",
                "local flow maximization",
                "segmentation-insensitive approach",
                "color similarity",
                "flow maximization approach",
                "iterative optimization"
            ]
        },
        "id": 160,
        "cited_by": [
            {
                "year": "2007",
                "id": 30
            }
        ]
    },
    {
        "title": "Progressive surface reconstruction from images using a local prior",
        "authors": [
            "Gang Zeng",
            "S. Paris",
            "L. Quan",
            "F. Sillion"
        ],
        "abstract": "This paper introduces a new method for surface reconstruction from multiple calibrated images. The primary contribution of this work is the notion of local prior to combine the flexibility of the carving approach with the accuracy of graph-cut optimization. A progressive refinement scheme is used to recover the topology and reason the visibility of the object. Within each voxel, a detailed surface patch is optimally reconstructed using a graph-cut method. The advantage of this technique is its ability to handle complex shape similarly to level sets while enjoying a higher precision. Compared to carving techniques, the addressed problem is well-posed, and the produced surface does not suffer from aliasing. In addition, our approach seamlessly handles complete and partial reconstructions: If the scene is only partially visible, the process naturally produces an open surface; otherwise, if the scene is fully visible, it creates a complete shape. These properties are demonstrated on real image sequences",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544861",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2001",
                "id": 52
            },
            {
                "year": "2003",
                "id": 174
            },
            {
                "year": "2003",
                "id": 171
            },
            {
                "year": "2003",
                "id": 118
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 3,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Shape",
                "Layout",
                "Geometry",
                "Level set",
                "Computer science",
                "Topology",
                "Image sequences",
                "Head"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "graph theory",
                "image reconstruction",
                "image sequences",
                "optimisation",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface reconstruction",
                "multiple calibrated image",
                "voxel carving approach",
                "graph-cut optimization",
                "refinement scheme",
                "image sequence"
            ],
            "Author Keywords": [
                "Surface Reconstruction",
                "Local Prior",
                "Voxel Carving",
                "Graph Cut",
                "Complete/Partial Reconstruction"
            ]
        },
        "id": 161,
        "cited_by": []
    },
    {
        "title": "Video behaviour profiling and abnormality detection without manual labelling",
        "authors": [
            "Tao Xiang",
            "Shaogang Gong"
        ],
        "abstract": "A novel framework is developed for automatic behaviour profiling and abnormality sampling/detection without any manual labelling of the training dataset. Natural grouping of behaviour patterns is discovered through unsupervised model selection and feature selection on the eigenvectors of a normalised affinity matrix. Our experiments demonstrate that a behaviour model trained using an unlabelled dataset is superior to those trained using the same but labelled dataset in detecting abnormality from an unseen video",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544862",
        "reference_list": [
            {
                "year": "2003",
                "id": 98
            },
            {
                "year": "2003",
                "id": 42
            }
        ],
        "citation": {
            "ieee": 42,
            "other": 17,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Layout",
                "Sampling methods",
                "Pattern recognition",
                "Robustness",
                "Humans",
                "Feature extraction",
                "Computer science",
                "Prototypes",
                "Bayesian methods"
            ],
            "INSPEC: Controlled Indexing": [
                "behavioural sciences computing",
                "eigenvalues and eigenfunctions",
                "matrix algebra",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video behaviour profiling",
                "video abnormality detection",
                "video abnormality sampling",
                "training dataset",
                "unsupervised model selection",
                "feature selection",
                "eigenvector",
                "affinity matrix"
            ]
        },
        "id": 162,
        "cited_by": [
            {
                "year": "2011",
                "id": 307
            }
        ]
    },
    {
        "title": "A robust algorithm for point set registration using mixture of Gaussians",
        "authors": [
            "Bing Jian",
            "B.C. Vemuri"
        ],
        "abstract": "This paper proposes a novel and robust approach to the point set registration problem in the presence of large amounts of noise and outliers. Each of the point sets is represented by a mixture of Gaussians and the point set registration is treated as a problem of aligning the two mixtures. We derive a closed-form expression for the L/sub 2/distance between two Gaussian mixtures, which in turn leads to a computationally efficient registration algorithm. This new algorithm has an intuitive interpretation, is simple to implement and exhibits inherent statistical robustness. Experimental results indicate that our algorithm achieves very good performance in terms of both robustness and accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544863",
        "reference_list": [
            {
                "year": "2003",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 59,
            "other": 16,
            "total": 75
        },
        "keywords": {
            "IEEE Keywords": [
                "Gaussian processes",
                "Noise robustness",
                "Iterative closest point algorithm",
                "Kernel",
                "Closed-form solution",
                "Application software",
                "Iterative methods",
                "Shape",
                "Cost function",
                "Information science"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "Gaussian processes",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "point set registration",
                "Gaussian mixture",
                "closed-form expression",
                "intuitive interpretation",
                "statistical robustness"
            ]
        },
        "id": 163,
        "cited_by": [
            {
                "year": "2013",
                "id": 181
            },
            {
                "year": "2011",
                "id": 273
            },
            {
                "year": "2009",
                "id": 171
            },
            {
                "year": "2007",
                "id": 320
            }
        ]
    },
    {
        "title": "Appearance modeling under geometric context",
        "authors": [
            "Jian Li",
            "S.K. Zhou",
            "R. Chellappa"
        ],
        "abstract": "We propose a unified framework based on a general definition of geometric transform (GeT) for modeling appearance. GeT represents the appearance by applying designed functionals over certain geometric sets. We show that image warping, Radon transform, trace transform, etc. are special cases of our definition. Moreover, three different types of GeTs are designed to handle deformation, articulation and occlusion and applied to fingerprinting the appearance inside a contour. They include the contour-driven GeT, the feature curve based GeT and selecting functionals to model the appearance inside the convex hull of the contour. A multi-resolution representation that combines both shape and appearance information is also proposed. We apply our approach to image synthesis and object recognition. The proposed approach produces promising results when applied to fingerprinting the appearance of human and body parts despite the challenges due to articulated motion and deformations",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544864",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Context modeling",
                "Active appearance model",
                "Biological system modeling",
                "Shape",
                "Humans",
                "Fingerprint recognition",
                "Automation",
                "Data systems",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image representation",
                "object recognition",
                "Radon transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "appearance modeling",
                "geometric transform",
                "image warping",
                "Radon transform",
                "trace transform",
                "fingerprinting",
                "contour-driven GeT",
                "feature curve based GeT",
                "convex hull",
                "multi-resolution representation",
                "shape information",
                "image synthesis",
                "object recognition"
            ]
        },
        "id": 164,
        "cited_by": [
            {
                "year": "2009",
                "id": 209
            },
            {
                "year": "2007",
                "id": 111
            }
        ]
    },
    {
        "title": "Shape and appearance repair for incomplete point surfaces",
        "authors": [
            "Seyoun Park",
            "Xiaohu Guo",
            "Hayong Shin",
            "Hong Qin"
        ],
        "abstract": "This paper presents a new surface content completion framework that can restore both shape and appearance from scanned, incomplete point set inputs. First, the geometric holes can be robustly identified from noisy and defective data sets without the need of any normal or orientation information, using the method of active deformable models. The geometry and texture information of the holes can then be determined either automatically from the models' context, or semi-automatically with minimal users' intervention. The central idea for this repair process is to establish a quantitative similarity measurement among local surface patches based on their local parameterizations and curvature computation. The geometry and texture information of each hole can be completed by warping the candidate region and gluing it to the hole. The displacement for the alignment process is computed by solving a Poisson equation in 2D. Our experiments show that the unified framework, founded upon the techniques of deformable models, local parameterization, and PDE modeling, can provide a robust and elegant solution for content completion of defective, complex point surfaces",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544865",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Robustness",
                "Deformable models",
                "Context modeling",
                "Information geometry",
                "Solid modeling",
                "Noise shaping",
                "Computer vision",
                "Clouds",
                "Active noise reduction"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image denoising",
                "image restoration",
                "image texture",
                "Poisson equation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape repair",
                "appearance repair",
                "incomplete point surface",
                "surface content completion",
                "geometric hole",
                "active deformable model",
                "texture information",
                "geometry information",
                "model context",
                "minimal user intervention",
                "quantitative similarity measurement",
                "local surface patch",
                "local parameterization",
                "curvature computation",
                "alignment process",
                "Poisson equation",
                "PDE modeling"
            ]
        },
        "id": 165,
        "cited_by": [
            {
                "year": "2009",
                "id": 21
            },
            {
                "year": "2007",
                "id": 243
            }
        ]
    },
    {
        "title": "Ensuring color consistency across multiple cameras",
        "authors": [
            "A. Ilie",
            "G. Welch"
        ],
        "abstract": "Most multi-camera vision applications assume a single common color response for all cameras. However different cameras - even of the same type - can exhibit radically different color responses, and the differences can cause significant errors in scene interpretation. To address this problem we have developed a robust system aimed at inter-camera color consistency. Our method consists of two phases: an iterative closed-loop calibration phase that searches for the per-camera hardware register settings that best balance linearity and dynamic range, followed by a refinement phase that computes the per-camera parametric values for an additional software-based color mapping",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544866",
        "reference_list": [],
        "citation": {
            "ieee": 32,
            "other": 30,
            "total": 62
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Hardware",
                "Image reconstruction",
                "Calibration",
                "Color",
                "Application software",
                "Layout",
                "Colored noise",
                "Registers",
                "Linearity"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "image colour analysis",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multi-camera vision",
                "color response",
                "scene interpretation",
                "inter-camera color consistency",
                "iterative closed-loop calibration phase",
                "software-based color mapping"
            ]
        },
        "id": 166,
        "cited_by": []
    },
    {
        "title": "More-than-topology-preserving flows for active contours and polygons",
        "authors": [
            "G. Sundaramoorthi",
            "A. Yezzi"
        ],
        "abstract": "Active contour and active polygon models have been used widely for image segmentation. In some applications, the topology of the object(s) to be detected from an image is known a priori, despite an unknown complex geometry, and it is important that the active contour or polygon maintain the desired topology. In this work, we construct a novel geometric flow that can be added to image based evolutions of active contours and polygons so that the topology of the initial contour or polygon is preserved. Indeed, the proposed geometric flow ensures more than just correct topology; it ensures that the active contour or polygon is, in some sense, kept far away from a topology change. Smoothness properties similar to curvature flow are also guaranteed by the proposed geometric flow. The proposed topology preserving geometric flow is the gradient flow arising from an energy that is based on electrostatic principles. The evolution of a single point on the contour depends on all other points of the contour, which is different from traditional curve evolutions in computer vision literature",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544867",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 10,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Active contours",
                "Topology",
                "Image segmentation",
                "Application software",
                "Electrostatics",
                "Computer vision",
                "Geometry",
                "Level set",
                "Potential energy",
                "Strips"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active contours",
                "active polygons",
                "image segmentation",
                "geometric flow",
                "image based evolution",
                "image smoothness propertiy",
                "gradient flow",
                "computer vision",
                "more-than-topology-preserving flows",
                "object topology"
            ]
        },
        "id": 167,
        "cited_by": []
    },
    {
        "title": "A hierarchical field framework for unified context-based classification",
        "authors": [
            "S. Kumar",
            "M. Hebert"
        ],
        "abstract": "We present a two-layer hierarchical formulation to exploit different levels of contextual information in images for robust classification. Each layer is modeled as a conditional field that allows one to capture arbitrary observation-dependent label interactions. The proposed framework has two main advantages. First, it encodes both the short-range interactions (e.g., pixelwise label smoothing) as well as the long-range interactions (e.g., relative configurations of objects or regions) in a tractable manner. Second, the formulation is general enough to be applied to different domains ranging from pixelwise image labeling to contextual object detection. The parameters of the model are learned using a sequential maximum-likelihood approximation. The benefits of the proposed framework are demonstrated on four different datasets and comparison results are presented",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544868",
        "reference_list": [],
        "citation": {
            "ieee": 90,
            "other": 45,
            "total": 135
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Pixel",
                "Object detection",
                "Layout",
                "Keyboards",
                "Mice",
                "Context modeling",
                "Robots",
                "Robustness",
                "Smoothing methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "maximum likelihood estimation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unified context-based classification",
                "pixelwise label smoothing",
                "pixelwise image labeling",
                "contextual object detection",
                "sequential maximum-likelihood approximation",
                "observation-dependent label interaction"
            ]
        },
        "id": 168,
        "cited_by": [
            {
                "year": "2013",
                "id": 453
            },
            {
                "year": "2011",
                "id": 64
            },
            {
                "year": "2009",
                "id": 29
            },
            {
                "year": "2009",
                "id": 30
            },
            {
                "year": "2009",
                "id": 119
            },
            {
                "year": "2009",
                "id": 254
            },
            {
                "year": "2007",
                "id": 121
            }
        ]
    },
    {
        "title": "Squaring the circle in panoramas",
        "authors": [
            "L. Zelnik-Manor",
            "G. Peters",
            "P. Perona"
        ],
        "abstract": "Pictures taken by a rotating camera cover the viewing sphere surrounding the center of rotation. Having a set of images registered and blended on the sphere what is left to be done, in order to obtain a flat panorama, is projecting the spherical image onto a picture plane. This step is unfortunately not obvious - the surface of the sphere may not be flattened onto a page without some form of distortion. The objective of this paper is discussing the difficulties and opportunities that are connected to the projection from viewing sphere to image plane. We first explore a number of alternatives to the commonly used linear perspective projection. These are 'global' projections and do not depend on image content. We then show that multiple projections may coexist successfully in the same mosaic: these projections are chosen locally and depend on what is present in the pictures. We show that such multi-view projections can produce more compelling results than the global projections",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544869",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 9,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Pixel",
                "Head",
                "Computer displays",
                "Lenses",
                "Geometrical optics",
                "Eyes",
                "Digital cameras",
                "Video sequences",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image registration",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "panoramas",
                "rotating camera",
                "image registration",
                "linear perspective projection",
                "image content",
                "mosaic",
                "multi-view projections",
                "global projections"
            ]
        },
        "id": 169,
        "cited_by": []
    },
    {
        "title": "Efficiently registering video into panoramic mosaics",
        "authors": [
            "D. Steedly",
            "C. Pal",
            "R. Szeliski"
        ],
        "abstract": "We present an automatic and efficient method to register and stitch thousands of video frames into a large panoramic mosaic. Our method preserves the robustness and accuracy of image stitchers that match all pairs of images while utilizing the ordering information provided by video. We reduce the cost of searching for matches between video frames by adaptively identifying key frames based on the amount of image-to-image overlap. Key frames are matched to all other key frames, but intermediate video frames are only matched to temporally neighboring key frames and intermediate frames. Image orientations can be estimated from this sparse set of matches in time quadratic to cubic in the number of key frames but only linear in the number of intermediate frames. Additionally, the matches between pairs of images are compressed by replacing measurements within small windows in the image with a single representative measurement. We show that this approach substantially reduces the time required to estimate the image orientations with minimal loss of accuracy. Finally, we demonstrate both the efficiency and quality of our results by registering several long video sequences",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544870",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 18,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Video sequences",
                "Video compression",
                "Robustness",
                "Image coding",
                "Computer vision",
                "Costs",
                "Software quality",
                "Digital cameras",
                "Computational efficiency",
                "Dynamic range"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "panoramic mosaics",
                "video frames",
                "image stitchers",
                "image-to-image overlap",
                "image orientations",
                "video sequences",
                "video registration"
            ]
        },
        "id": 170,
        "cited_by": []
    },
    {
        "title": "Consistent segmentation for optical flow estimation",
        "authors": [
            "C.W. Zitnick",
            "N. Jojic",
            "Sing Bing Kang"
        ],
        "abstract": "In this paper, we propose a method for jointly computing optical flow and segmenting video while accounting for mixed pixels (matting). Our method is based on statistical modeling of an image pair using constraints on appearance and motion. Segments are viewed as overlapping regions with fractional (alpha) contributions. Bidirectional motion is estimated based on spatial coherence and similarity of segment colors. Our model is extended to video by chaining the pairwise models to produce a joint probability distribution to be maximized. To make the problem more tractable, we factorize the posterior distribution and iteratively minimize its parts. We demonstrate our method on frame interpolation",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544871",
        "reference_list": [
            {
                "year": "2001",
                "id": 70
            }
        ],
        "citation": {
            "ieee": 49,
            "other": 22,
            "total": 71
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Image segmentation",
                "Optical computing",
                "Motion estimation",
                "Interpolation",
                "Spatial coherence",
                "Probability distribution",
                "Nonlinear optics",
                "Shape",
                "Image converters"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image sequences",
                "motion estimation",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optical flow estimation",
                "video segmentation",
                "mixed pixels",
                "statistical modeling",
                "segment colors",
                "probability distribution",
                "posterior distribution",
                "frame interpolation"
            ]
        },
        "id": 171,
        "cited_by": [
            {
                "year": "2011",
                "id": 104
            },
            {
                "year": "2009",
                "id": 200
            },
            {
                "year": "2007",
                "id": 64
            }
        ]
    },
    {
        "title": "A symmetric patch-based correspondence model for occlusion handling",
        "authors": [
            "Yi Deng",
            "Qiong Yang",
            "Xueyin Lin",
            "Xiaoou Tang"
        ],
        "abstract": "Occlusion is one of the challenging problems in stereo. In this paper, we solve the problem in a segment-based style. Both images are segmented, and we propose a novel patch-based stereo algorithm that cuts the segments of one image using the segments of the other, and handles occlusion areas in a proper way. A symmetric graph-cuts optimization framework is used to find correspondence and occlusions simultaneously. The experimental results show superior performance of the proposed algorithm, especially on occlusions, untextured areas and discontinuities",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544872",
        "reference_list": [
            {
                "year": "2001",
                "id": 70
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 12,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Layout",
                "Pervasive computing",
                "Computer science",
                "Asia",
                "Pixel",
                "Image sampling",
                "Robustness",
                "Image analysis",
                "Information analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image segmentation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "occlusion handling",
                "image segmentation",
                "patch-based stereo algorithm"
            ]
        },
        "id": 172,
        "cited_by": [
            {
                "year": "2015",
                "id": 299
            }
        ]
    },
    {
        "title": "Non-negative lighting and specular object recognition",
        "authors": [
            "S. Shirdhonkar",
            "D.W. Jacobs"
        ],
        "abstract": "Recognition of specular objects is particularly difficult because their appearance is much more sensitive to lighting changes than that of Lambertian objects. We consider an approach in which we use a 3D model to deduce the lighting that best matches the model to the image. In this case, an important constraint is that incident lighting should be non-negative everywhere. In this paper, we propose a new method to enforce this constraint and explore its usefulness in specular object recognition, using the spherical harmonic representation of lighting. The method follows from a novel extension of Szego's eigenvalue distribution theorem to spherical harmonics, and uses semidefinite programming to perform a constrained optimization. The new method is faster as well as more accurate than previous methods. Experiments on both synthetic and real data indicate that the constraint can improve recognition of specular objects by better separating the correct and incorrect models",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544873",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Power harmonic filters",
                "Frequency",
                "Jacobian matrices",
                "Subspace constraints",
                "Automation",
                "Educational institutions",
                "Eigenvalues and eigenfunctions",
                "Constraint theory",
                "Constraint optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "image matching",
                "object recognition",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonnegative lighting",
                "specular object recognition",
                "Lambertian objects",
                "incident lighting",
                "spherical harmonic representation",
                "Szego eigenvalue distribution theorem",
                "semidefinite programming",
                "constrained optimization"
            ]
        },
        "id": 173,
        "cited_by": [
            {
                "year": "2009",
                "id": 74
            }
        ]
    },
    {
        "title": "Learning hierarchical models of scenes, objects, and parts",
        "authors": [
            "E.B. Sudderth",
            "A. Torralba",
            "W.T. Freeman",
            "A.S. Willsky"
        ],
        "abstract": "We describe a hierarchical probabilistic model for the detection and recognition of objects in cluttered, natural scenes. The model is based on a set of parts which describe the expected appearance and position, in an object centered coordinate frame, of features detected by a low-level interest operator. Each object category then has its own distribution over these parts, which are shared between objects. We learn the parameters of this model via a Gibbs sampler which uses the graphical model's structure to analytically average over many parameters. Applied to a database of images of isolated objects, the sharing of parts among objects improves detection accuracy when few training examples are available. We also extend this hierarchical framework to scenes containing multiple objects",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544874",
        "reference_list": [
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2003",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 97,
            "other": 61,
            "total": 158
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Object detection",
                "Computer vision",
                "Graphical models",
                "Random variables",
                "Dictionaries",
                "Computer science",
                "Image databases",
                "Spatial databases",
                "Visual databases"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "natural scenes",
                "object detection",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical probabilistic model",
                "object detection",
                "object recognition",
                "natural scenes",
                "object centered coordinate frame",
                "feature detection",
                "low-level interest operator",
                "image database"
            ]
        },
        "id": 174,
        "cited_by": [
            {
                "year": "2013",
                "id": 104
            },
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2011",
                "id": 64
            }
        ]
    },
    {
        "title": "An axis-based representation for recognition",
        "authors": [
            "C. Asian",
            "S. Tari"
        ],
        "abstract": "We present a new axis-based shape representation scheme along with a matching framework to address the problem of generic shape recognition. The main idea is to define the relative spatial arrangement of local symmetry axes and their metric properties in a shape centered coordinate frame. The resulting descriptions are invariant to scale, rotation, small changes in viewpoint and articulations. Symmetry points are extracted from a surface whose level curves roughly mimic the motion by curvature. By increasing the amount of smoothing on the evolving curve, only those symmetry axes that correspond to the most prominent parts of a shape are extracted. The representation does not suffer from the common instability problems of the traditional connected skeletons. It captures the perceptual qualities of shapes well. 'Therefore finding the similarities and the differences among shapes becomes easier. The matching process gives highly successful results on a diverse database of 2D shapes",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544875",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 20,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Skeleton",
                "Smoothing methods",
                "Electric shock",
                "Computer vision",
                "Rough surfaces",
                "Surface roughness",
                "Databases",
                "Lighting",
                "Guidelines"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "axis-based shape representation",
                "shape recognition",
                "spatial arrangement",
                "symmetry axes"
            ]
        },
        "id": 175,
        "cited_by": []
    },
    {
        "title": "A supervised learning framework for generic object detection in images",
        "authors": [
            "Saad Ali",
            "Mubarak Shah"
        ],
        "abstract": "In recent years kernel principal component analysis (kernel PCA) has gained much attention because of its ability to capture nonlinear image features, which are particularly important for encoding image structure. Boosting has been established as a powerful learning algorithm that can be used for feature selection. In this paper we present a novel framework for object class detection that combines the feature reduction and feature selection abilities of kernel PCA and AdaBoost respectively. The classifier obtained in this way is able to handle change in object appearance, illumination conditions, and surrounding clutter. A nonlinear subspace is learned for positive and negative object classes using Kernel PCA. Features are derived by projecting example images onto the learned subspaces. Base learners are modeled using Bayes classifier. AdaBoost is then employed to discover the features that are most relevant for the object detection task at hand. The proposed method has been successfully tested on wide range of object classes (cars, air-planes, pedestrians, motorcycles, etc) using standard data sets and has shown remarkable performance. Using a small training set, a classifier learned in this way was able to generalize the intra-class variation while still maintaining high detection rate. In most object categories we achieved detection rates of above 95% with minimal false alarm rates. We demonstrate the effectiveness of our approach in terms of absolute performance parameters and comparative performance against current state of the art approaches",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544876",
        "reference_list": [
            {
                "year": "2001",
                "id": 196
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 1,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Supervised learning",
                "Object detection",
                "Kernel",
                "Principal component analysis",
                "Image coding",
                "Boosting",
                "Change detection algorithms",
                "Lighting",
                "Testing",
                "Motorcycles"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "feature extraction",
                "learning (artificial intelligence)",
                "object detection",
                "pattern classification",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "supervised learning",
                "object detection",
                "kernel principal component analysis",
                "nonlinear image features",
                "image structure encoding",
                "learning algorithm",
                "feature selection",
                "object class detection",
                "feature reduction",
                "AdaBoost",
                "Bayes classifier"
            ]
        },
        "id": 176,
        "cited_by": []
    },
    {
        "title": "Retrieval with knowledge-driven kernel design: an approach to improving SVM-based CBIR with relevance feedback",
        "authors": [
            "Lei Wang",
            "Yan Gao",
            "Kap Luk Chan",
            "Ping Xue",
            "Wei-Yun Yau"
        ],
        "abstract": "The performance of SVM-based image retrieval is often constrained by the scarcity of training samples. The total number of image samples labeled by users in a retrieval session is very limited, and this small number of labeled samples cannot effectively represent the true distributions of positive and negative image classes, especially for the negative image class. This paper proposes a novel approach to deal with this problem. Instead of treating it as a problem, the mere existence of the small number of labeled images and their desired distribution in the kernel space is considered as prior knowledge from image retrieval to aid the design of the kernel used by SVMs. This is achieved by maximizing a criterion, such as one based on scatter matrices, through gradient-based search methods, incurring very little computational overhead to real-time retrieval process. Experimental results on two benchmark image databases demonstrate the improved retrieval performance by the dynamically designed kernel and hence the effectiveness of the proposed approach for SVM based image retrieval",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544877",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Feedback",
                "Image retrieval",
                "Support vector machines",
                "Information retrieval",
                "Training data",
                "Scattering",
                "Search methods",
                "Image databases",
                "Content based retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "gradient methods",
                "image retrieval",
                "learning (artificial intelligence)",
                "relevance feedback",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image retrieval",
                "knowledge-driven kernel design",
                "SVM",
                "CBIR",
                "relevance feedback",
                "gradient-based search",
                "real-time retrieval process",
                "image databases"
            ]
        },
        "id": 177,
        "cited_by": []
    },
    {
        "title": "Integrating representative and discriminant models for object category detection",
        "authors": [
            "M. Fritz",
            "B. Leibe",
            "B. Caputo",
            "B. Schiele"
        ],
        "abstract": "Category detection is a lively area of research. While categorization algorithms tend to agree in using local descriptors, they differ in the choice of the classifier, with some using generative models and others discriminative approaches. This paper presents a method for object category detection which integrates a generative model with a discriminative classifier. For each object category, we generate an appearance codebook, which becomes a common vocabulary for the generative and discriminative methods. Given a query image, the generative part of the algorithm finds a set of hypotheses and estimates their support in location and scale. Then, the discriminative part verifies each hypothesis on the same codebook activations. The new algorithm exploits the strengths of both original methods, minimizing their weaknesses. Experiments on several databases show that our new approach performs better than its building blocks taken separately. Moreover, experiments on two challenging multi-scale databases show that our new algorithm outperforms previously reported results",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544878",
        "reference_list": [
            {
                "year": "2001",
                "id": 58
            },
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2003",
                "id": 195
            },
            {
                "year": "2003",
                "id": 2
            },
            {
                "year": "2003",
                "id": 97
            },
            {
                "year": "2003",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 27,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Kernel",
                "Image segmentation",
                "Shape",
                "Voting",
                "Support vector machines",
                "Support vector machine classification",
                "Computer vision",
                "Object recognition",
                "Visual databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminant models",
                "object category detection",
                "categorization algorithms",
                "discriminative classifier",
                "appearance codebook",
                "query image",
                "multiscale databases"
            ]
        },
        "id": 178,
        "cited_by": [
            {
                "year": "2007",
                "id": 265
            }
        ]
    },
    {
        "title": "Learning effective image metrics from few pairwise examples",
        "authors": [
            "Hwann-Tzong Chen",
            "Tyng-Luh Liu",
            "Chiou-Shann Fuh"
        ],
        "abstract": "We present a new approach to learning image metrics. The main advantage of our method lies in a formulation that requires only a few pairwise examples. Apparently, based on the little amount of side-information, it would take a very effective learning scheme to yield a useful image metric. Our algorithm achieves this goal by addressing two key issues. First, we establish a global-local (glocal) image representation that induces two structure-meaningful vector spaces to respectively describe the global and the local image properties. Second, we develop a metric optimization framework that finds an optimal bilinear transform to best explain the given side-information. We emphasize it is the glocal image representation that makes the use of bilinear transform more powerful. Experimental results on classifications of face images and visual tracking are included to demonstrate the contributions of the proposed method",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544879",
        "reference_list": [
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2003",
                "id": 33
            },
            {
                "year": "2001",
                "id": 110
            },
            {
                "year": "2003",
                "id": 50
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Image representation",
                "Face detection",
                "Image recognition",
                "Image retrieval",
                "Computer vision",
                "Face recognition",
                "Nearest neighbor searches",
                "Euclidean distance",
                "Training data",
                "Information science"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image matching",
                "image representation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image metrics",
                "pairwise examples",
                "global-local image representation",
                "metric optimization",
                "optimal bilinear transform",
                "glocal image representation",
                "face images",
                "visual tracking"
            ]
        },
        "id": 179,
        "cited_by": []
    },
    {
        "title": "Large deformation diffeomorphic metric mapping of fiber orientations",
        "authors": [
            "Yan Cao",
            "M.I. Miller",
            "R.L. Winslow",
            "L. Younes"
        ],
        "abstract": "This paper proposes a method to match diffusion tensor magnetic resonance images (DT-MRI) through the large deformation diffeomorphic metric mapping of vector fields, focusing on the fiber orientations, considered as unit vector fields on the image volume. We study a suitable action of diffeomorphisms on such vector fields, and provide an extension of the large deformation diffeomorphic metric-mapping framework to this type of dataset, resulting in optimizing for geodesies on the space of diffeomorphisms connecting two images. Two different distance function of vector fields are considered. Existence of the minimizers under smoothness assumptions on the compared vector fields is proved, and coarse to fine hierarchical strategies are detailed, to reduce both ambiguities and computation load. This is illustrated by numerical experiments on DT-MRI heart and brain images",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544880",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Diffusion tensor imaging",
                "Tensile stress",
                "Heart",
                "Cardiac disease",
                "Magnetic resonance imaging",
                "Image registration",
                "Biomedical imaging",
                "Biological tissues",
                "Cardiovascular diseases",
                "Cardiology"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large deformation diffeomorphic metric mapping",
                "fiber orientations",
                "diffusion tensor magnetic resonance images",
                "image volume",
                "heart images",
                "brain images"
            ]
        },
        "id": 180,
        "cited_by": []
    },
    {
        "title": "Spherical matching for temporal correspondence of non-rigid surfaces",
        "authors": [
            "J. Starck",
            "A. Hilton"
        ],
        "abstract": "This paper introduces spherical matching to estimate dense temporal correspondence of non-rigid surfaces with genus-zero topology. The spherical domain gives a consistent 1D parameterization of non-rigid surfaces for matching. Non-rigid 3D surface correspondence is formulated as the recovery of a bijective mapping between two surfaces in the 2D domain. Formulating matching as a 2D bijection guarantees a continuous one-to-one surface correspondence without overfolding. This overcomes limitations of direct estimation of non-rigid surface correspondence in the 3D domain. A multiple resolution coarse-to-fine algorithm is introduced to robustly estimate the dense correspondence which minimizes the disparity in shape and appearance between two surfaces. Spherical matching is applied to derive the temporal correspondence between non-rigid surfaces reconstructed at successive frames from multiple view video sequences of people. Dense surface correspondence is recovered across complete motion sequences for both textured and uniform regions, without the requirement for a prior model of human shape or kinematics structure for tracking",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544881",
        "reference_list": [
            {
                "year": "2003",
                "id": 79
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 15,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Layout",
                "Shape",
                "Image reconstruction",
                "Topology",
                "Robustness",
                "Video sequences",
                "Humans",
                "Kinematics",
                "Sampling methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "image reconstruction",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spherical matching",
                "temporal correspondence",
                "genus-zero topology",
                "3D surface correspondence",
                "bijective mapping",
                "non-rigid surface correspondence",
                "resolution coarse-to-fine algorithm",
                "video sequences",
                "motion sequences"
            ]
        },
        "id": 181,
        "cited_by": [
            {
                "year": "2007",
                "id": 268
            }
        ]
    },
    {
        "title": "Actions as space-time shapes",
        "authors": [
            "M. Blank",
            "L. Gorelick",
            "E. Shechtman",
            "M. Irani",
            "R. Basri"
        ],
        "abstract": "Human action in video sequences can be seen as silhouettes of a moving torso and protruding limbs undergoing articulated motion. We regard human actions as three-dimensional shapes induced by the silhouettes in the space-time volume. We adopt a recent approach by Gorelick et al. (2004) for analyzing 2D shapes and generalize it to deal with volumetric space-time action shapes. Our method utilizes properties of the solution to the Poisson equation to extract space-time features such as local space-time saliency, action dynamics, shape structure and orientation. We show that these features are useful for action recognition, detection and clustering. The method is fast, does not require video alignment and is applicable in (but not limited to) many scenarios where the background is known. Moreover, we demonstrate the robustness of our method to partial occlusions, non-rigid deformations, significant changes in scale and viewpoint, high irregularities in the performance of an action and low quality video",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544882",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2003",
                "id": 57
            }
        ],
        "citation": {
            "ieee": 497,
            "other": 335,
            "total": 832
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Humans",
                "Video sequences",
                "Torso",
                "Poisson equations",
                "Computer vision",
                "Information analysis",
                "Optical computing",
                "Image motion analysis",
                "Motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image motion analysis",
                "image sequences",
                "Poisson equation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "space-time shapes",
                "video sequences",
                "space-time volume",
                "volumetric space-time action shapes",
                "Poisson equation",
                "space-time feature extraction",
                "local space-time saliency",
                "action dynamics",
                "shape structure",
                "shape orientation",
                "action recognition",
                "action detection",
                "video alignment",
                "partial occlusions",
                "2D shape analysis"
            ]
        },
        "id": 182,
        "cited_by": [
            {
                "year": "2015",
                "id": 356
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2013",
                "id": 228
            },
            {
                "year": "2013",
                "id": 396
            },
            {
                "year": "2011",
                "id": 66
            },
            {
                "year": "2011",
                "id": 72
            },
            {
                "year": "2011",
                "id": 89
            },
            {
                "year": "2011",
                "id": 131
            },
            {
                "year": "2011",
                "id": 179
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2009",
                "id": 121
            },
            {
                "year": "2009",
                "id": 127
            },
            {
                "year": "2009",
                "id": 204
            },
            {
                "year": "2009",
                "id": 248
            },
            {
                "year": "2009",
                "id": 262
            },
            {
                "year": "2007",
                "id": 10
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 206
            },
            {
                "year": "2007",
                "id": 265
            }
        ]
    },
    {
        "title": "Designing spatially coherent minimizing flows for variational problems based on active contours",
        "authors": [
            "G. Charpiat",
            "R. Keriven",
            "J.-P. Pons",
            "O. Faugeras"
        ],
        "abstract": "This paper tackles an important aspect of the variational problems involving active contours, which has been largely overlooked so far: the optimization by gradient flows. Classically, the definition of a gradient depends directly on the choice of an inner product structure. This consideration is largely absent from the active contours literature. Most authors, overtly or covertly, assume that the space of admissible deformations is ruled by the canonical L 2 inner product. The classical gradient flows reported in the literature are relative to this particular choice. In this paper, we investigate the relevance of using other inner products, yielding other gradient descents, and some other minimizing flows not deriving from any inner product. In particular we, show how to induce different degrees of spatial coherence into the minimizing flow, in order to decrease the probability of getting trapped into irrelevant local minima. We show with some numerical experiments that the sensitivity of the active contours method to initial conditions, which seriously limits its applicability and its efficiency, is alleviated by our application-specific spatially coherent minimizing flows",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544883",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2003",
                "id": 117
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 11,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatial coherence",
                "Active contours",
                "Computer vision",
                "Image segmentation",
                "Image reconstruction",
                "Laboratories",
                "Minimization methods",
                "Deformable models",
                "Image analysis",
                "Surface reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatially coherent minimizing flows",
                "variational problems",
                "active contours",
                "gradient flows",
                "gradient descents",
                "spatial coherence"
            ]
        },
        "id": 183,
        "cited_by": [
            {
                "year": "2007",
                "id": 163
            }
        ]
    },
    {
        "title": "Recovering photometric properties of multiple strongly-reflective, partially-transparent surfaces from a single image",
        "authors": [
            "J.P. Queiroz-Neto",
            "R.L. Carceroni",
            "L.C.R. Coelho"
        ],
        "abstract": "This paper introduces a method to recover photometric parameters of a set of 3D surfaces from a single image with significant global-illumination effects such as inter-reflections and transparencies. Since this problem is ambiguous for arbitrary unknown scenes, our formulation assumes that the scene consists of a small set of photometrically homogeneous surfaces with known 3D shapes, illuminated by known light sources. We show that under these conditions, the system of nonlinear equations that defines how the image is formed may be factorized into a vector composed only of products of some photometric parameters, and a matrix, whose elements depend non-linearly on both the known illumination, the known 3D shapes and the remaining photometric parameters. This factorization leads to an efficient optimization-based algorithm to compute all unknown photometric parameters from a single input image. Experiments with real data show that this algorithm is more stable and efficient than simpler alternatives",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544884",
        "reference_list": [
            {
                "year": "2003",
                "id": 74
            },
            {
                "year": "2003",
                "id": 23
            },
            {
                "year": "2003",
                "id": 180
            },
            {
                "year": "2003",
                "id": 129
            },
            {
                "year": "2001",
                "id": 80
            },
            {
                "year": "2003",
                "id": 114
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Layout",
                "Light sources",
                "Lighting",
                "Cultural differences",
                "Mirrors",
                "Nonlinear equations",
                "Virtual environment",
                "Reflectometry",
                "Polarization"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "image restoration",
                "nonlinear equations",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photometric property",
                "global-illumination effects",
                "photometrically homogeneous surfaces",
                "light sources",
                "nonlinear equations",
                "illumination",
                "optimization-based algorithm"
            ]
        },
        "id": 184,
        "cited_by": []
    },
    {
        "title": "Guiding model search using segmentation",
        "authors": [
            "G. Mori"
        ],
        "abstract": "In this paper we show how segmentation as preprocessing paradigm can be used to improve the efficiency and accuracy of model search in an image. We operationalize this idea using an over-segmentation of an image into superpixels. The problem domain we explore is human body pose estimation from still images. The superpixels prove useful in two ways. First, we restrict the joint positions in our human body model to lie at centers of superpixels, which reduces the size of the model search space. In addition, accurate support masks for computing features on half-limbs of the body model are obtained by using agglomerations of superpixels as half limb segments. We present results on a challenging dataset of people in sports news images",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544885",
        "reference_list": [
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2001",
                "id": 70
            },
            {
                "year": "2003",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 103,
            "other": 75,
            "total": 178
        },
        "keywords": {
            "IEEE Keywords": [
                "Biological system modeling",
                "Humans",
                "Image segmentation",
                "Pixel",
                "Joints",
                "State-space methods",
                "Testing",
                "Wrist",
                "Elbow",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "superpixels",
                "human body pose estimation",
                "still images",
                "model search space"
            ]
        },
        "id": 185,
        "cited_by": [
            {
                "year": "2011",
                "id": 56
            },
            {
                "year": "2009",
                "id": 98
            },
            {
                "year": "2007",
                "id": 3
            },
            {
                "year": "2007",
                "id": 171
            }
        ]
    },
    {
        "title": "Visual speech recognition with loosely synchronized feature streams",
        "authors": [
            "K. Saenko",
            "K. Livescu",
            "M. Siracusa",
            "K. Wilson",
            "J. Glass",
            "T. Darrell"
        ],
        "abstract": "We present an approach to detecting and recognizing spoken isolated phrases based solely on visual input. We adopt an architecture that first employs discriminative detection of visual speech and articulate features, and then performs recognition using a model that accounts for the loose synchronization of the feature streams. Discriminative classifiers detect the subclass of lip appearance corresponding to the presence of speech, and further decompose it into features corresponding to the physical components of articulate production. These components often evolve in a semi-independent fashion, and conventional viseme-based approaches to recognition fail to capture the resulting co-articulation effects. We present a novel dynamic Bayesian network with a multi-stream structure and observations consisting of articulate feature classifier scores, which can model varying degrees of co-articulation in a principled way. We evaluate our visual-only recognition system on a command utterance task. We show comparative results on lip detection and speech/non-speech classification, as well as recognition performance against several baseline systems",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544886",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 9,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Speech recognition",
                "Face detection",
                "Hidden Markov models",
                "Bayesian methods",
                "Switches",
                "Support vector machines",
                "Support vector machine classification",
                "Computer vision",
                "Detectors",
                "Glass"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "feature extraction",
                "image classification",
                "speech recognition",
                "synchronisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual speech recognition",
                "feature streams",
                "visual speech detection",
                "Bayesian network",
                "feature classifier",
                "visual-only recognition system",
                "command utterance",
                "lip detection",
                "speech classification",
                "nonspeech classification",
                "spoken isolated phrases detection",
                "spoken isolated phrases recognition"
            ]
        },
        "id": 186,
        "cited_by": [
            {
                "year": "2013",
                "id": 16
            }
        ]
    },
    {
        "title": "N-dimensional probability density function transfer and its application to color transfer",
        "authors": [
            "F. Pitie",
            "A.C. Kokaram",
            "R. Dahyot"
        ],
        "abstract": "This article proposes an original method to estimate a continuous transformation that maps one N-dimensional distribution to another. The method is iterative, non-linear, and is shown to converge. Only 1D marginal distribution is used in the estimation process, hence involving low computation costs. As an illustration this mapping is applied to color transfer between two images of different contents. The paper also serves as a central focal point for collecting together the research activity in this area and relating it to the important problem of automated color grading",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544887",
        "reference_list": [],
        "citation": {
            "ieee": 32,
            "other": 52,
            "total": 84
        },
        "keywords": {
            "IEEE Keywords": [
                "Density functional theory",
                "Rendering (computer graphics)",
                "Statistics",
                "Iterative methods",
                "Statistical distributions",
                "Color",
                "Educational institutions",
                "Image converters",
                "Distributed computing",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probability density function",
                "color transfer",
                "continuous transformation",
                "1D marginal distribution",
                "automated color grading"
            ]
        },
        "id": 187,
        "cited_by": [
            {
                "year": "2009",
                "id": 25
            }
        ]
    },
    {
        "title": "A theory of inverse light transport",
        "authors": [
            "S.M. Seitz",
            "Y. Matsushita",
            "K.N. Kutulakos"
        ],
        "abstract": "In this paper we consider the problem of computing and removing interreflections in photographs of real scenes. Towards this end, we introduce the problem of inverse light transport - given a photograph of an unknown scene, decompose it into a sum of n-bounce images, where each image records the contribution of light that bounces exactly n times before reaching the camera. We prove the existence of a set of interreflection cancelation operators that enable computing each n-bounce image by multiplying the photograph by a matrix. This matrix is derived from a set of \"impulse images\" obtained by probing the scene with a narrow beam of light. The operators work under unknown and arbitrary illumination, and exist for scenes that have arbitrary spatially-varying BRDFs. We derive a closed-form expression for these operators in the Lambertian case and present experiments with textured and untextured Lambertian scenes that confirm our theory's predictions",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544888",
        "reference_list": [
            {
                "year": "2003",
                "id": 23
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 27,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Lighting",
                "Computer vision",
                "Asia",
                "Matrix decomposition",
                "Optical propagation",
                "Computer graphics",
                "Inverse problems",
                "Shape measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image processing",
                "photography",
                "reflectivity"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "inverse light transport theory",
                "interreflections",
                "photographs",
                "n-bounce images",
                "image records",
                "interreflection cancelation operators",
                "arbitrary illumination",
                "Lambertian case",
                "Lambertian scenes"
            ]
        },
        "id": 188,
        "cited_by": [
            {
                "year": "2011",
                "id": 23
            },
            {
                "year": "2009",
                "id": 242
            },
            {
                "year": "2007",
                "id": 43
            }
        ]
    },
    {
        "title": "A theory of refractive and specular 3D shape by light-path triangulation",
        "authors": [
            "K.N. Kutulakos",
            "E. Steger"
        ],
        "abstract": "We investigate the feasibility of reconstructing an arbitrarily-shaped specular scene (refractive or mirror-like) from one or more viewpoints. By reducing shape recovery to the problem of reconstructing individual 3D light paths that cross the image plane, we obtain three key results. First, we show how to compute the depth map of a specular scene from a single viewpoint, when the scene redirects incoming light just once. Second, for scenes where incoming light undergoes two refractions or reflections, we show that three viewpoints are sufficient to enable reconstruction in the general case. Third, we show that it is impossible to reconstruct individual light paths when light is redirected more than twice. Our analysis assumes that, for every point on the image plane, we know at least one 3D point on its light path. This leads to reconstruction algorithms that rely on an \"environment matting\" procedure to establish pixel-to-point correspondences along a light path. Preliminary results for a variety of scenes (mirror, glass, etc) are also presented",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544889",
        "reference_list": [],
        "citation": {
            "ieee": 37,
            "other": 20,
            "total": 57
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical refraction",
                "Shape",
                "Layout",
                "Image reconstruction",
                "Reconstruction algorithms",
                "Mirrors",
                "Optical reflection",
                "Glass",
                "Liquids",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "light reflection",
                "light refraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "light-path triangulation",
                "arbitrarily-shaped specular scene",
                "shape recovery",
                "reconstruction algorithms",
                "environment matting",
                "pixel-to-point correspondences"
            ]
        },
        "id": 189,
        "cited_by": [
            {
                "year": "2013",
                "id": 88
            },
            {
                "year": "2011",
                "id": 149
            },
            {
                "year": "2009",
                "id": 24
            },
            {
                "year": "2007",
                "id": 43
            }
        ]
    },
    {
        "title": "The pyramid match kernel: discriminative classification with sets of image features",
        "authors": [
            "K. Grauman",
            "T. Darrell"
        ],
        "abstract": "Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences epsivnerally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This \"pyramid match\" computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544890",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2003",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 460,
            "other": 329,
            "total": 789
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Histograms",
                "Face detection",
                "Computer vision",
                "Image edge detection",
                "Shape",
                "Support vector machines",
                "Support vector machine classification",
                "Learning systems",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pyramid match kernel",
                "discriminative classification",
                "image features",
                "discriminative learning",
                "decision boundary",
                "kernel function",
                "multiresolution histograms",
                "weighted histogram intersection",
                "resolution histogram cell",
                "Mercer kernels",
                "object recognition"
            ]
        },
        "id": 190,
        "cited_by": [
            {
                "year": "2015",
                "id": 11
            },
            {
                "year": "2013",
                "id": 43
            },
            {
                "year": "2013",
                "id": 216
            },
            {
                "year": "2011",
                "id": 185
            },
            {
                "year": "2009",
                "id": 5
            },
            {
                "year": "2009",
                "id": 61
            },
            {
                "year": "2009",
                "id": 76
            },
            {
                "year": "2009",
                "id": 182
            },
            {
                "year": "2009",
                "id": 212
            },
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2007",
                "id": 20
            },
            {
                "year": "2007",
                "id": 146
            },
            {
                "year": "2007",
                "id": 213
            },
            {
                "year": "2007",
                "id": 225
            },
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2007",
                "id": 289
            }
        ]
    },
    {
        "title": "Deformation invariant image matching",
        "authors": [
            "Haibin Ling",
            "D.W. Jacobs"
        ],
        "abstract": "We propose a novel framework to build descriptors of local intensity that are invariant to general deformations. In this framework, an image is embedded as a 2D surface in 3D space, with intensity weighted relative to distance in x-y. We show that as this weight increases, geodesic distances on the embedded surface are less affected by image deformations. In the limit, distances are deformation invariant. We use geodesic sampling to get neighborhood samples for interest points, and then use a geodesic-intensity histogram (GIH) as a deformation invariant local descriptor. In addition to its invariance, the new descriptor automatically finds its support region. This means it can safely gather information from a large neighborhood to improve discriminability. Furthermore, we propose a matching method for this descriptor that is invariant to affine lighting changes. We have tested this new descriptor on interest point matching for two data sets, one with synthetic deformation and lighting change, and another with real non-affine deformations. Our method shows promising matching results compared to several other approaches",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544891",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 3,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Image matching",
                "Image sampling",
                "Histograms",
                "Surface treatment",
                "Jacobian matrices",
                "Automation",
                "Computer science",
                "Educational institutions",
                "Testing",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "differential geometry",
                "image matching",
                "image morphing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformation invariant image matching",
                "geodesic distances",
                "image deformations",
                "geodesic sampling",
                "geodesic-intensity histogram",
                "deformation invariant local descriptor",
                "point matching",
                "nonaffine deformations"
            ]
        },
        "id": 191,
        "cited_by": [
            {
                "year": "2015",
                "id": 74
            },
            {
                "year": "2013",
                "id": 320
            },
            {
                "year": "2011",
                "id": 4
            },
            {
                "year": "2011",
                "id": 320
            },
            {
                "year": "2009",
                "id": 167
            },
            {
                "year": "2007",
                "id": 268
            }
        ]
    },
    {
        "title": "Features for recognition: viewpoint invariance for non-planar scenes",
        "authors": [
            "A. Vedaldi",
            "S. Soatto"
        ],
        "abstract": "Most current local feature detectors/descriptors implicitly assume that the scene is (locally) planar, an assumption that is violated at surface discontinuities. We show that this restriction is, at least in theory, unnecessary, as one can construct local features that are viewpoint-invariant for generic non-planar scenes. However, we show that any such feature necessarily sacrifices shape information, in the sense of being non shape-discriminative. Finally, we show that if viewpoint is factored out as part of the matching process, rather than explicitly in the representation, then shape is discriminative indeed. We illustrate our theoretical results empirically by showing that, even for simple scenes, current affine descriptors fail where even a naive 3-D viewpoint invariant succeeds in matching",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544892",
        "reference_list": [
            {
                "year": "2001",
                "id": 60
            },
            {
                "year": "2003",
                "id": 160
            },
            {
                "year": "2001",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 15,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Shape",
                "Lighting",
                "Computer vision",
                "Reflectivity",
                "Computer science",
                "Detectors",
                "Image recognition",
                "Object recognition",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "viewpoint invariance",
                "nonplanar scenes",
                "feature detectors",
                "feature descriptors",
                "surface discontinuity"
            ]
        },
        "id": 192,
        "cited_by": [
            {
                "year": "2007",
                "id": 20
            }
        ]
    },
    {
        "title": "A spectral technique for correspondence problems using pairwise constraints",
        "authors": [
            "M. Leordeanu",
            "M. Hebert"
        ],
        "abstract": "We present an efficient spectral method for finding consistent correspondences between two sets of features. We build the adjacency matrix M of a graph whose nodes represent the potential correspondences and the weights on the links represent pairwise agreements between potential correspondences. Correct assignments are likely to establish links among each other and thus form a strongly connected cluster. Incorrect correspondences establish links with the other correspondences only accidentally, so they are unlikely to belong to strongly connected clusters. We recover the correct assignments based on how strongly they belong to the main cluster of M, by using the principal eigenvector of M and imposing the mapping constraints required by the overall correspondence mapping (one-to-one or one-to-many). The experimental evaluation shows that our method is robust to outliers, accurate in terms of matching rate, while being much faster than existing methods",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544893",
        "reference_list": [],
        "citation": {
            "ieee": 325,
            "other": 202,
            "total": 527
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Computer vision",
                "Geometry",
                "Robots",
                "Robustness",
                "Object recognition",
                "Stereo vision",
                "Application software",
                "Feature extraction",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "graph theory",
                "image matching",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spectral technique",
                "pairwise constraints",
                "adjacency matrix",
                "pairwise agreements",
                "potential correspondences",
                "principal eigenvector",
                "correspondence mapping"
            ]
        },
        "id": 193,
        "cited_by": [
            {
                "year": "2017",
                "id": 479
            },
            {
                "year": "2015",
                "id": 6
            },
            {
                "year": "2015",
                "id": 12
            },
            {
                "year": "2015",
                "id": 22
            },
            {
                "year": "2015",
                "id": 248
            },
            {
                "year": "2015",
                "id": 450
            },
            {
                "year": "2013",
                "id": 3
            },
            {
                "year": "2013",
                "id": 22
            },
            {
                "year": "2013",
                "id": 105
            },
            {
                "year": "2013",
                "id": 165
            },
            {
                "year": "2013",
                "id": 205
            },
            {
                "year": "2013",
                "id": 214
            },
            {
                "year": "2013",
                "id": 248
            },
            {
                "year": "2013",
                "id": 278
            },
            {
                "year": "2013",
                "id": 279
            },
            {
                "year": "2013",
                "id": 427
            },
            {
                "year": "2011",
                "id": 4
            },
            {
                "year": "2011",
                "id": 110
            },
            {
                "year": "2011",
                "id": 209
            },
            {
                "year": "2011",
                "id": 330
            },
            {
                "year": "2009",
                "id": 164
            },
            {
                "year": "2009",
                "id": 203
            },
            {
                "year": "2009",
                "id": 293
            },
            {
                "year": "2007",
                "id": 0
            },
            {
                "year": "2007",
                "id": 13
            },
            {
                "year": "2007",
                "id": 121
            },
            {
                "year": "2007",
                "id": 237
            }
        ]
    },
    {
        "title": "On-line density-based appearance modeling for object tracking",
        "authors": [
            "B. Han",
            "L. Davis"
        ],
        "abstract": "Object tracking is a challenging problem in real-time computer vision due to variations of lighting condition, pose, scale, and view-point over time. However, it is exceptionally difficult to model appearance with respect to all of those variations in advance; instead, on-line update algorithms are employed to adapt to these changes. We present a new on-line appearance modeling technique which is based on sequential density approximation. This technique provides accurate and compact representations using Gaussian mixtures, in which the number of Gaussians is automatically determined. This procedure is performed in linear time at each time step, which we prove by amortized analysis. Features for each pixel and rectangular region are modeled together by the proposed sequential density approximation algorithm, and the target model is updated in scale robustly. We show the performance of our method by simulations and tracking in natural videos",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544894",
        "reference_list": [
            {
                "year": "2001",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 42,
            "other": 22,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Density functional theory",
                "Performance analysis",
                "Noise robustness",
                "Histograms",
                "Computer science",
                "Educational institutions",
                "Computer vision",
                "Approximation algorithms",
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Gaussian processes",
                "image sequences",
                "object detection",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online density-based appearance modeling",
                "object tracking",
                "real-time computer vision",
                "online appearance modeling technique",
                "sequential density approximation",
                "Gaussian mixtures"
            ]
        },
        "id": 194,
        "cited_by": [
            {
                "year": "2011",
                "id": 151
            },
            {
                "year": "2011",
                "id": 196
            },
            {
                "year": "2007",
                "id": 109
            }
        ]
    },
    {
        "title": "Dynamic measurement clustering to aid real time tracking",
        "authors": [
            "C. Kemp",
            "T. Drummond"
        ],
        "abstract": "We present a technique/or clustering measurements such that high-dimensional parameter estimation problems can be simplified. The key idea is to find rows of the measurement Jacobian whose rank is significantly less than its width. Such a set of rows gives a cluster of measurements which is affected only by a subset of the parameter space. This cluster can be used independently from other measurements to isolate parameter decisions. Unlike static partitioning techniques, the method presented dynamically generates clusters at each step of the estimation. This achieves substantial computational reductions, even for problems which cannot be partitioned in the traditional sense. The technique is applied to the task of tracking camera motions in real-time and video sequences are used to compare the resulting system to previous methods",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544895",
        "reference_list": [
            {
                "year": "2003",
                "id": 183
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Vehicles",
                "Tracking",
                "Jacobian matrices",
                "Search problems",
                "Navigation",
                "Working environment noise",
                "Parameter estimation",
                "Real time systems",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "parameter estimation",
                "pattern clustering",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic measurement clustering",
                "real time tracking",
                "parameter estimation",
                "static partitioning techniques",
                "computational reductions",
                "camera motion tracking",
                "video sequences"
            ]
        },
        "id": 195,
        "cited_by": []
    },
    {
        "title": "Fusing points and lines for high performance tracking",
        "authors": [
            "E. Rosten",
            "T. Drummond"
        ],
        "abstract": "This paper addresses the problem of real-time 3D model-based tracking by combining point-based and edge-based tracking systems. We present a careful analysis of the properties of these two sensor systems and show that this leads to some non -trivial design choices that collectively yield extremely high performance. In particular, we present a method for integrating the two systems and robustly combining the pose estimates they produce. Further we show how on-line learning can be used to improve the performance of feature tracking. Finally, to aid real-time performance, we introduce the FAST feature detector which can perform full-frame feature detection at 400Hz. The combination of these techniques results in a system which is capable of tracking average prediction errors of 200 pixels. This level of robustness allows us to track very rapid motions, such as 50deg camera shake at 6Hz",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544896",
        "reference_list": [],
        "citation": {
            "ieee": 261,
            "other": 166,
            "total": 427
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Robustness",
                "Tracking",
                "Real time systems",
                "Detectors",
                "Cameras",
                "Layout",
                "Performance analysis",
                "Sensor systems",
                "Acceleration"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction",
                "image motion analysis",
                "sensor fusion",
                "solid modelling",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high performance tracking",
                "3D model-based tracking",
                "point-based tracking system",
                "edge-based tracking system",
                "sensor systems",
                "online learning",
                "feature tracking",
                "FAST feature detector",
                "feature detection",
                "prediction errors",
                "400 Hz"
            ]
        },
        "id": 196,
        "cited_by": [
            {
                "year": "2015",
                "id": 134
            },
            {
                "year": "2015",
                "id": 260
            },
            {
                "year": "2015",
                "id": 490
            },
            {
                "year": "2007",
                "id": 197
            },
            {
                "year": "2007",
                "id": 258
            },
            {
                "year": "2007",
                "id": 275
            }
        ]
    },
    {
        "title": "Fast global kernel density mode seeking with application to localization and tracking",
        "authors": [
            "Chunhua Shen",
            "M.J. Brooks",
            "A. van den Hengel"
        ],
        "abstract": "We address the problem of seeking the global mode of a density function using the mean shift algorithm. Mean shift, like other gradient ascent optimization methods, is susceptible to local maxima, and hence often fails to find the desired global maximum. In this work, we propose a multi-bandwidth mean shift procedure that alleviates this problem, which we term annealed mean shift, as it shares similarities with the annealed importance sampling procedure. The bandwidth of the algorithm plays the same role as the temperature in annealing. We observe that the over-smoothed density function with a sufficiently large bandwidth is uni-modal. Using a continuation principle, the influence of the global peak in the density function is introduced gradually. In this way the global maximum is more reliably located. Generally, the price of this annealing-like procedure is that more iteration is required since it is imperative that the computation complexity is minimal in real-time applications such as visual tracking. We propose an accelerated version of the mean shift algorithm. Compared with the conventional mean shift algorithm, the accelerated mean shift can significantly decrease the number of iterations required for convergence. The proposed algorithm is applied to the problems of visual tracking and object localization. We empirically show on various data sets that the proposed algorithm can reliably find the true object location when the starting position of mean shift is far away from the global maximum, in contrast with the conventional mean shift algorithm that will usually get trapped in a spurious local maximum",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544897",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 2,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Target tracking",
                "Annealing",
                "Density functional theory",
                "Optimization methods",
                "Bandwidth",
                "Acceleration",
                "Convergence",
                "Particle tracking",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "gradient methods",
                "importance sampling",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "global kernel density mode seeking",
                "mean shift algorithm",
                "gradient ascent optimization",
                "multibandwidth mean shift",
                "annealed mean shift",
                "annealed importance sampling",
                "over-smoothed density function",
                "continuation principle",
                "computation complexity",
                "visual tracking",
                "object localization"
            ]
        },
        "id": 197,
        "cited_by": [
            {
                "year": "2013",
                "id": 364
            },
            {
                "year": "2007",
                "id": 138
            }
        ]
    },
    {
        "title": "Is Levenberg-Marquardt the most efficient optimization algorithm for implementing bundle adjustment?",
        "authors": [
            "M.L.A. Lourakis",
            "A.A. Argyros"
        ],
        "abstract": "In order to obtain optimal 3D structure and viewing parameter estimates, bundle adjustment is often used as the last step of feature-based structure and motion estimation algorithms. Bundle adjustment involves the formulation of a large scale, yet sparse minimization problem, which is traditionally solved using a sparse variant of the Levenberg-Marquardt optimization algorithm that avoids storing and operating on zero entries. This paper argues that considerable computational benefits can be gained by substituting the sparse Levenberg-Marquardt algorithm in the implementation of bundle adjustment with a sparse variant of Powell's dog leg non-linear least squares technique. Detailed comparative experimental results provide strong evidence supporting this claim",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544898",
        "reference_list": [],
        "citation": {
            "ieee": 41,
            "other": 32,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Least squares methods",
                "Large-scale systems",
                "Iterative algorithms",
                "Equations",
                "Motion estimation",
                "Minimization methods",
                "Leg",
                "Cameras",
                "Computer science",
                "Parameter estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "minimisation",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bundle adjustment",
                "parameter estimation",
                "feature-based structure",
                "motion estimation algorithms",
                "minimization problem",
                "Levenberg-Marquardt optimization algorithm",
                "sparse Levenberg-Marquardt algorithm",
                "nonlinear least squares technique"
            ]
        },
        "id": 198,
        "cited_by": [
            {
                "year": "2015",
                "id": 52
            }
        ]
    },
    {
        "title": "3D shape recognition and reconstruction based on line element geometry",
        "authors": [
            "M. Hofer",
            "B. Odehnal",
            "H. Pottmann",
            "T. Steiner",
            "J. Wallner"
        ],
        "abstract": "This paper presents a new method for the recognition and reconstruction of surfaces from 3D data. Line element geometry, which generalizes both line geometry and the Laguerre geometry of oriented planes, enables us to recognize a wide class of surfaces (spiral surfaces, cones, helical surfaces, rotational surfaces, cylinders, etc.), by fitting linear subspaces in an appropriate seven-dimensional image space. In combination with standard techniques such as PCA and RANSAC, line element geometry is employed to effectively perform the segmentation of complex objects according to surface type. Examples show applications in reverse engineering of CAD models and testing mathematical hypotheses concerning the exponential growth of sea shells",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544899",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 16,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Geometry",
                "Surface fitting",
                "Sea surface",
                "Surface reconstruction",
                "Image reconstruction",
                "Image recognition",
                "Spirals",
                "Engine cylinders",
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image recognition",
                "image reconstruction",
                "stochastic processes",
                "surface fitting",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape recognition",
                "shape reconstruction",
                "line element geometry",
                "Laguerre geometry",
                "linear subspaces",
                "RANSAC",
                "object segmentation",
                "reverse engineering",
                "CAD models",
                "exponential growth",
                "sea shells"
            ]
        },
        "id": 199,
        "cited_by": []
    },
    {
        "title": "Multi-view geometry of 1D radial cameras and its application to omnidirectional camera calibration",
        "authors": [
            "S. Thirthala",
            "M. Pollefeys"
        ],
        "abstract": "We study the multi-view geometry of 1D radial cameras. A broad a class of both central and non-central cameras, such as fish-eye and catadioptric cameras, can be reduced to 1D radial cameras under the assumption of known center of radial distortion. For cameras in general configuration, we introduce a quadrifocal tensor that can be computed linearly from 15 or more features seen in four views. From this tensor a metric reconstruction of the 1D cameras as well as the observed features can be obtained. In a second phase this reconstruction can then be used as a calibration object to estimate a non-parametric non-central model for the cameras. We study some degenerate cases, including pure rotation. In the case of a purely rotating camera we obtain a trifocal tensor that can be estimated linearly from 7 points in three views. This allows us to obtain a metric reconstruction of the plane at infinity. Next, we use the plane at infinity as a calibration device to non-parametrically estimate the radial distortion. We demonstrate the results of our approach on real and synthetic images",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544900",
        "reference_list": [
            {
                "year": "2001",
                "id": 16
            },
            {
                "year": "2001",
                "id": 117
            },
            {
                "year": "2003",
                "id": 176
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 16,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Layout",
                "Image reconstruction",
                "Tensile stress",
                "Robot vision systems",
                "Computational geometry",
                "H infinity control",
                "Lenses",
                "Mirrors"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview geometry",
                "1D radial cameras",
                "omnidirectional camera calibration",
                "radial distortion",
                "quadrifocal tensor"
            ]
        },
        "id": 200,
        "cited_by": [
            {
                "year": "2007",
                "id": 149
            }
        ]
    },
    {
        "title": "Geometric invariants and applications under catadioptric camera model",
        "authors": [
            "Yihong Wu",
            "Zhanyi Hu"
        ],
        "abstract": "This paper presents geometric invariants of points and their applications under central catadioptric camera model. Although the image has severe distortions under the model, we establish some accurate projective geometric invariants of scene points and their image points. These invariants, being functions of principal point, are useful, from which a method for calibrating the camera principal point and a method for recovering planar scene structures are proposed. The main advantage of using these in variants for plane reconstruction is that neither camera motion nor the intrinsic parameters, except for the principal point, is needed. The theoretical correctness of the established invariants and robustness of the proposed methods are demonstrated by experiments. In addition, our results are found to be applicable to some more general camera models other than the catadioptric one",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544901",
        "reference_list": [
            {
                "year": "2001",
                "id": 16
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Solid modeling",
                "Robot vision systems",
                "Equations",
                "Optical distortion",
                "Image reconstruction",
                "Layout",
                "Mirrors",
                "Calibration",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computational geometry",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric invariants",
                "catadioptric camera model",
                "image distortions",
                "planar scene structure recovery"
            ]
        },
        "id": 201,
        "cited_by": []
    },
    {
        "title": "Object tracking across multiple independently moving airborne cameras",
        "authors": [
            "Yaser Sheikh",
            "Mubarak Shah"
        ],
        "abstract": "A camera mounted on an aerial vehicle provides an excellent means for monitoring large areas of a scene. Utilizing several such cameras on different aerial vehicles allows further flexibility, in terms of increased visual scope and in the pursuit of multiple targets. In this paper, we address the problem of tracking objects across multiple moving airborne cameras. Since the cameras are moving and often widely separated, direct appearance-based or proximity-based constraints cannot be used. Instead, we exploit geometric constraints on the relationship between the motions of each object across cameras, to test multiple correspondence hypotheses, without assuming any prior calibration information. We propose a statistically and geometrically meaningful means of evaluating a hypothesized correspondence between two observations in different cameras. Second, since multiple cameras exist, ensuring coherency in correspondence, i.e. transitive closure is maintained between more than two cameras, is an essential requirement. To ensure such coherency we pose the problem of object tracking across cameras as a k-dimensional matching and use an approximation to find the maximum likelihood assignment of correspondence. Third, we show that as a result of tracking objects across the cameras, a concurrent visualization of multiple aerial video streams is possible. Results are shown on a number of real and controlled scenarios with multiple objects observed by multiple cameras, validating our qualitative models",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544902",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 0,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Visualization",
                "Computer vision",
                "Unmanned aerial vehicles",
                "Surveillance",
                "Reconnaissance",
                "Layout",
                "Object detection",
                "Terminology",
                "Graph theory"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computational geometry",
                "image matching",
                "image motion analysis",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object tracking",
                "airborne cameras",
                "geometric constraints",
                "object motion",
                "k-dimensional matching",
                "multiple aerial video streams"
            ]
        },
        "id": 202,
        "cited_by": []
    },
    {
        "title": "Robust point matching for two-dimensional nonrigid shapes",
        "authors": [
            "Yefeng Zheng",
            "D. Doermann"
        ],
        "abstract": "Recently, nonrigid shape matching has received more and more attention. For nonrigid shapes, most neighboring points cannot move independently under deformation due to physical constraints. Furthermore, the rough structure of a shape should be preserved under deformation otherwise even people cannot match shapes reliably. Therefore, though the absolute distance between two points may change significantly, the neighborhood of a point is well preserved in general. Based on this observation, we formulate point matching as a graph matching problem. Each point is a node in the graph, and two nodes are connected by an edge if their Euclidean distance is less than a threshold. The optimal match between two graphs is the one that maximizes the number of matched edges. The shape context distance is used to initialize the graph matching, followed by relaxation labeling for refinement. Nonrigid deformation is overcome by bringing one shape closer to the other in each iteration using deformation parameters estimated from the current point correspondence. Experiments demonstrate the effectiveness of our approach: it outperforms the shape context and TPS-RPM algorithms under nonrigid deformation and noise on a public data set",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544903",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Shape",
                "Optimal matching",
                "Pattern matching",
                "Computer vision",
                "Laboratories",
                "Educational institutions",
                "Euclidean distance",
                "Labeling",
                "Noise shaping"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "graph theory",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "point matching",
                "2D nonrigid shapes",
                "nonrigid shape matching",
                "graph matching",
                "Euclidean distance",
                "nonrigid deformation"
            ]
        },
        "id": 203,
        "cited_by": []
    },
    {
        "title": "Face recognition by stepwise nonparametric margin maximum criterion",
        "authors": [
            "Xipeng Qiu",
            "Lide Wu"
        ],
        "abstract": "Linear discriminant analysis (LDA) is a popular feature extraction technique in face recognition. However, it often suffers from the small sample size problem when dealing with the high dimensional data. Moreover, while LDA is guaranteed to find the best directions when each class has a Gaussian density with a common covariance matrix, it can fail if the class densities are more general. In this paper; a new nonparametric linear feature extraction method, stepwise nonparametric margin maximum criterion (SNMMC), is proposed to find the most discriminant directions, which does not assume that the class densities belong to any particular parametric family and does not depend on the non- singularity of the within-class scatter matrix neither. On three datasets from ATT and FERET face databases, our experimental results demonstrate that SNMMC outperforms other methods and is robust to variations of pose, illumination and expression",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544904",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 0,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Linear discriminant analysis",
                "Scattering",
                "Feature extraction",
                "Covariance matrix",
                "Character generation",
                "Computer science",
                "Databases",
                "Robustness",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "covariance matrices",
                "face recognition",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "stepwise nonparametric margin maximum criterion",
                "linear discriminant analysis",
                "feature extraction",
                "Gaussian density",
                "covariance matrix"
            ]
        },
        "id": 204,
        "cited_by": [
            {
                "year": "2007",
                "id": 218
            }
        ]
    },
    {
        "title": "Dynamic refraction stereo",
        "authors": [
            "N.J.W. Morris",
            "K.N. Kutulakos"
        ],
        "abstract": "In this paper we consider the problem of reconstructing the 3D position and surface normal of points on an unknown, arbitrarily-shaped refractive surface. We show that two viewpoints are sufficient to solve this problem in the general case, even if the refractive index is unknown. The key requirements are: (1) knowledge of a function that maps each point on the two image planes to a known 3D point that refracts to it; and (2) light is refracted only once. We apply this result to the problem of reconstructing the time-varying surface of a liquid from patterns placed below it. To do this, we introduce a novel stereo matching criterion called refractive disparity, appropriate for refractive scenes, and develop an optimization-based algorithm for individually reconstructing the position and normal of each point projecting to a pixel in the input views. Results on reconstructing a variety of complex, deforming liquid surfaces suggest that our technique can yield detailed reconstructions that capture the dynamic behavior of free-flowing liquids",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544905",
        "reference_list": [],
        "citation": {
            "ieee": 43,
            "other": 15,
            "total": 58
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Sea surface",
                "Image reconstruction",
                "Optical refraction",
                "Shape",
                "Layout",
                "Liquids",
                "Computer vision",
                "Stereo vision",
                "Refractive index"
            ],
            "INSPEC: Controlled Indexing": [
                "flow",
                "image reconstruction",
                "optimisation",
                "refractive index",
                "stereo image processing",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic refraction stereo",
                "3D position reconstruction",
                "time-varying surface reconstruction",
                "arbitrarily-shaped refractive surface",
                "stereo matching",
                "refractive disparity",
                "optimization",
                "free-flowing liquids"
            ]
        },
        "id": 205,
        "cited_by": [
            {
                "year": "2015",
                "id": 377
            },
            {
                "year": "2011",
                "id": 44
            },
            {
                "year": "2011",
                "id": 80
            },
            {
                "year": "2011",
                "id": 149
            },
            {
                "year": "2009",
                "id": 296
            },
            {
                "year": "2007",
                "id": 43
            }
        ]
    },
    {},
    {},
    {
        "title": "Class-specific material categorisation",
        "authors": [
            "B. Caputo",
            "E. Hayman",
            "P. Mallikarjuna"
        ],
        "abstract": "Although a considerable amount of work has been published on material classification, relatively little of it studies situations with considerable variation within each class. Many experiments use the exact same sample, or different patches from the same image, for training and test sets. Thus, such studies are vulnerable to effectively recognising one particular sample of a material as opposed to the material category. In contrast, this paper places firm emphasis on the capability to generalise to previously unseen instances of materials. We adopt an appearance-based strategy, and conduct experiments on a new database which contains several samples of each of eleven material categories, imaged under a variety of pose, illumination and scale conditions. Together, these sources of intra-class variation provide a stern challenge indeed for recognition. Somewhat surprisingly, the difference in performance between various state-of-the-art texture descriptors proves rather small in this task. On the other hand, we clearly demonstrate that very significant gains can be achieved via different SVM-based classification techniques. Selecting appropriate kernel parameters proves crucial. This motivates a novel recognition scheme based on a decision tree. Each node contains an SVM to split one class from all others with a kernel parameter optimal for that particular node. Hence, each decision is made using a different, optimal, class-specific metric. Experiments show the superiority of this approach over several state-of-the-art classifiers",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544908",
        "reference_list": [
            {
                "year": "2003",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 68,
            "other": 63,
            "total": 131
        },
        "keywords": {
            "IEEE Keywords": [
                "Conducting materials",
                "Image databases",
                "Lighting",
                "Kernel",
                "Robots",
                "Pattern recognition",
                "Image recognition",
                "Computer vision",
                "Laboratories",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "class-specific material categorisation",
                "appearance-based strategy",
                "decision tree",
                "material sample recognition"
            ]
        },
        "id": 208,
        "cited_by": [
            {
                "year": "2017",
                "id": 516
            },
            {
                "year": "2007",
                "id": 37
            }
        ]
    },
    {
        "title": "A generative/discriminative learning algorithm for image classification",
        "authors": [
            "Yi Li",
            "L.O. Shapiro",
            "J.A. Bilmes"
        ],
        "abstract": "We have developed a two-phase generative/discriminative learning procedure for the recognition of classes of objects and concepts in outdoor scenes. Our method uses both multiple types of object features and context within the image. The generative phase normalizes the description length of images, which can have an arbitrary number of extracted features of each type. In the discriminative phase, a classifier learns which images, as represented by this fixed-length description, contain the target object. We have tested the approach by comparing it to several other approaches in the literature and by experimenting with several different data sets and combinations of features. Our results, using color, texture, and structure features, show a significant improvement over previously published results in image retrieval. Using salient region features, we are competitive with recent results in object recognition",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544909",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Classification algorithms",
                "Image classification",
                "Image segmentation",
                "Videos",
                "Image recognition",
                "Layout",
                "Image retrieval",
                "Object recognition",
                "Computer vision",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image colour analysis",
                "image texture",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generative learning",
                "discriminative learning",
                "image classification",
                "object recognition",
                "object features",
                "feature extraction",
                "image retrieval",
                "salient region features"
            ]
        },
        "id": 209,
        "cited_by": []
    },
    {
        "title": "Learning models for predicting recognition performance",
        "authors": [
            "Rong Wang",
            "Bir Bhanu"
        ],
        "abstract": "This paper addresses one of the fundamental problems encountered in performance prediction for object recognition. In particular we address the problems related to estimation of small gallery size that can give good error estimates and their confidences on large probe sets and populations. We use a generalized two-dimensional prediction model that integrates a hypergeometric probability distribution model with a binomial model explicitly and considers the distortion problem in large populations. We incorporate learning in the prediction process in order to find the optimal small gallery size and to improve its performance. The Chernoff and Chebychev inequalities are used as a guide to obtain the small gallery size. During the prediction we use the expectation-maximum (EM) algorithm to learn the match score and the non-match score distributions (the number of components, their weights, means and covariances) that are represented as Gaussian mixtures. By learning we find the optimal size of small gallery and at the same time provide the upper bound and the lower bound for the prediction on large populations. Results are shown using real-world databases",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544910",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 1,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Probability distribution",
                "Biometrics",
                "Image recognition",
                "Uncertainty",
                "Intelligent systems",
                "Object recognition",
                "Probes",
                "Upper bound",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "Gaussian processes",
                "learning (artificial intelligence)",
                "object recognition",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning models",
                "object recognition",
                "error estimates",
                "2D prediction model",
                "hypergeometric probability distribution",
                "Chernoff inequalities",
                "Chebychev inequalities",
                "expectation-maximum algorithm",
                "Gaussian mixtures"
            ]
        },
        "id": 210,
        "cited_by": []
    },
    {
        "title": "Joint Haar-like features for face detection",
        "authors": [
            "T. Mita",
            "T. Kaneko",
            "O. Hori"
        ],
        "abstract": "In this paper, we propose a new distinctive feature, called joint Haar-like feature, for detecting faces in images. This is based on co-occurrence of multiple Haar-like features. Feature co-occurrence, which captures the structural similarities within the face class, makes it possible to construct an effective classifier. The joint Haar-like feature can be calculated very fast and has robustness against addition of noise and change in illumination. A face detector is learned by stagewise selection of the joint Haar-like features using AdaBoost. A small number of distinctive features achieve both computational efficiency and accuracy. Experimental results with 5, 676 face images and 30,000 nonface images show that our detector yields higher classification performance than Viola and Jones' detector; which uses a single feature for each weak classifier. Given the same number of features, our method reduces the error by 37%. Our detector is 2.6 times as fast as Viola and Jones' detector to achieve the same performance",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544911",
        "reference_list": [],
        "citation": {
            "ieee": 85,
            "other": 50,
            "total": 135
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Face detection",
                "Detectors",
                "Boosting",
                "Lighting",
                "Computational efficiency",
                "Error analysis",
                "Electronic mail",
                "Noise robustness",
                "Computer errors"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint Haar-like features",
                "face detection",
                "multiple Haar-like features",
                "feature cooccurrence",
                "AdaBoost"
            ]
        },
        "id": 211,
        "cited_by": [
            {
                "year": "2015",
                "id": 5
            },
            {
                "year": "2009",
                "id": 50
            }
        ]
    },
    {
        "title": "Multiple light sources and reflectance property estimation based on a mixture of spherical distributions",
        "authors": [
            "K. Hara",
            "K. Nishino",
            "K. Ikeuchi"
        ],
        "abstract": "In tins paper we propose a new method for simultaneously estimating the illumination of the scene and the reflectance property of an object from a single image. We assume that the illumination consists of multiple point sources and the shape of the object is known. Unlike previous methods, we will recover not only the direction and intensity of the light sources, but also the number of light sources and the specular reflection parameter of the object. First, we represent the illumination on the surface of a unit sphere as a finite mixture of von Mises-Fisher distributions by deriving a spherical specular reflection model. Next, we estimate this mixture and the number of distributions. Finally, using this result as initial estimates, we refine the estimates using the original specular reflection model. We can use the results to render the object under novel lighting conditions",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544912",
        "reference_list": [
            {
                "year": "2003",
                "id": 178
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 7,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Light sources",
                "Reflectivity",
                "Lighting",
                "Layout",
                "Optical reflection",
                "Rendering (computer graphics)",
                "Computer science",
                "Shape",
                "Computer vision",
                "Visual communication"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple light sources",
                "reflectance property estimation",
                "spherical distributions",
                "scene illumination",
                "object reflectance property",
                "von Mises-Fisher distributions",
                "spherical specular reflection model",
                "lighting conditions"
            ]
        },
        "id": 212,
        "cited_by": []
    },
    {
        "title": "Passive photometric stereo from motion",
        "authors": [
            "Jongwoo Lim",
            "J. Ho",
            "Ming-Hsuan Yang",
            "D. Kriegman"
        ],
        "abstract": "We introduce an iterative algorithm for shape reconstruction from multiple images of a moving (Lambertian) object illuminated by distant (and possibly time varying) lighting. Starting with an initial piecewise linear surface, the algorithm iteratively estimates a new surface based on the previous surface estimate and the photometric information available from the input image sequence. During each iteration, standard photometric stereo techniques are applied to estimate the surface normals up to an unknown generalized bas-relief transform, and a new surface is computed by integrating the estimated normals. The algorithm essentially consists of a sequence of matrix factorizations (of intensity values) followed by minimization using gradient descent (integration of the normals). Conceptually, the algorithm admits a clear geometric interpretation, which is used to provide a qualitative analysis of the algorithm's convergence. Implementation-wise, it is straightforward being based on several established photometric stereo and structure from motion algorithms. We demonstrate experimentally the effectiveness of our algorithm using several videos of hand-held objects moving in front of a fixed light and camera",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544913",
        "reference_list": [
            {
                "year": "2001",
                "id": 52
            },
            {
                "year": "2003",
                "id": 174
            },
            {
                "year": "2003",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 1,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Surface reconstruction",
                "Iterative algorithms",
                "Shape",
                "Image reconstruction",
                "Piecewise linear techniques",
                "Image sequences",
                "Minimization methods",
                "Algorithm design and analysis",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image motion analysis",
                "image reconstruction",
                "image sequences",
                "minimisation",
                "stereo image processing",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "passive photometric stereo",
                "iterative algorithm",
                "shape reconstruction",
                "moving Lambertian object",
                "piecewise linear surface",
                "surface estimation",
                "image sequence",
                "bas-relief transform",
                "matrix factorizations",
                "minimization",
                "gradient descent",
                "geometric interpretation"
            ]
        },
        "id": 213,
        "cited_by": [
            {
                "year": "2007",
                "id": 100
            },
            {
                "year": "2007",
                "id": 181
            },
            {
                "year": "2007",
                "id": 263
            }
        ]
    },
    {},
    {
        "title": "Eliminating structure and intensity misalignment in image stitching",
        "authors": [
            "Jiaya Jia",
            "Chi-Keung Tang"
        ],
        "abstract": "The aim of this paper is to achieve seamless image stitching for eliminating obvious visual artifact caused by severe intensity discrepancy, image distortion and structure misalignment, given that the input images are globally registered. Our approach is based on structure deformation and propagation while maintaining the overall appearance affinity of the result to the input images. This new approach is proven to be effective in solving the above problems, and has found applications in mosaic deghosting, image blending and intensity correction. Our new method consists of the following main processes. First, salient features or structures are robustly detected and aligned along the optimal partitioning boundary between the input images. From these features, we derive sparse deformation vectors to to uniformly encode the underlying structure and intensity misalignment. These sparse deformation cues will then be propagated robustly and smoothly into the interior of the target image by solving the associated Laplace equations in the image gradient domain. We present convincing results to show that our method can handle significant structure and intensity misalignment in image stitching",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544915",
        "reference_list": [
            {
                "year": "2003",
                "id": 21
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 10,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Computer vision",
                "Laplace equations",
                "Image generation",
                "Councils",
                "Pattern matching",
                "Image registration",
                "Videoconference"
            ],
            "INSPEC: Controlled Indexing": [
                "image denoising",
                "image restoration",
                "Laplace equations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure misalignment elimination",
                "intensity misalignment elimination",
                "seamless image stitching",
                "image distortion",
                "mosaic deghosting",
                "image blending",
                "intensity correction",
                "salient features",
                "Laplace equations",
                "image gradient"
            ]
        },
        "id": 215,
        "cited_by": []
    },
    {
        "title": "Modelling shapes with uncertainties: higher order polynomials, variable bandwidth kernels and non parametric density estimation",
        "authors": [
            "M. Taron",
            "N. Paragios",
            "M.-P. Jolly"
        ],
        "abstract": "In this paper, we introduce a new technique for shape modelling in the space of implicit polynomials. Registration consists of recovering an optimal one-to-one transformation of a higher order polynomial along with uncertainties measures that are determined according to the covariance matrix of the correspondences at the zero isosurface. In the modelling phase, these measures are used to weight the importance of the training samples phase according to a variable bandwidth non-parametric density estimation process. The selection of the most appropriate kernels to represent the training set is done through the maximum likelihood criterion. Excellent results for patterns of digits, related with the registration and the modelling aspects of our approach demonstrate the potentials of our method",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544916",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Polynomials",
                "Bandwidth",
                "Kernel",
                "Measurement uncertainty",
                "Covariance matrix",
                "Isosurfaces",
                "Density measurement",
                "Phase measurement",
                "Phase estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "covariance matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "higher order polynomials",
                "variable bandwidth kernels",
                "nonparametric density estimation",
                "shape modelling",
                "implicit polynomials",
                "covariance matrix",
                "zero isosurface"
            ]
        },
        "id": 216,
        "cited_by": []
    },
    {
        "title": "Learning non-negative sparse image codes by convex programming",
        "authors": [
            "M. Heiler",
            "C. Schnorr"
        ],
        "abstract": "Example-based learning of codes that statistically encode general image classes is of vital importance for computational vision. Recently non negative matrix factorization (NMF) was suggested to provide image code that was both sparse and localized, in contrast to established non local methods like PCA. In this paper, we adopt and generalize this approach to develop a novel learning framework that allows to efficiently compute sparsity-controlled invariant image codes by a well defined sequence of convex conic programs. Applying the corresponding parameter-free algorithm to various image classes results in semantically relevant and transformation-invariant image representations that are remarkably robust against noise and quantization",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544917",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Sparse matrices",
                "Robustness",
                "Signal processing algorithms",
                "Quantization",
                "Application software",
                "Constraint optimization",
                "Bayesian methods",
                "Mathematics",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image coding",
                "image representation",
                "learning by example",
                "matrix decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonnegative sparse image codes",
                "convex programming",
                "example-based code learning",
                "nonnegative matrix factorization",
                "sparsity-controlled invariant image codes",
                "transformation-invariant image representations"
            ]
        },
        "id": 217,
        "cited_by": []
    },
    {
        "title": "Integrating the effects of motion, illumination and structure in video sequences",
        "authors": [
            "Yilei Xu",
            "A.K. Roy-Chowdhury"
        ],
        "abstract": "Most work in computer vision has concentrated on studying the individual effect of motion and illumination on a 3D object. In this paper, we present a theory for combining the effects of motion, illumination, 3D structure, albedo, and camera parameters in a sequence of images obtained by a perspective camera. We show that the set of all Lambertian reflectance functions of a moving object, illuminated by arbitrarily distant light sources, lies close to a bilinear subspace consisting of nine illumination variables and six motion variables. This result implies that, given an arbitrary video sequence, it is possible to recover the 3D structure, motion and illumination conditions simultaneously using the bilinear subspace formulation. The derivation is based on the intuitive notion that, given an illumination direction, the images of a moving surface cannot change suddenly over a short time period. We experimentally compare the images obtained using our theory with ground truth data and show that the difference is small and acceptable. We also provide experimental results on real data by synthesizing video sequences of a 3D face with various combinations of motion and illumination directions",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544918",
        "reference_list": [
            {
                "year": "2003",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Video sequences",
                "Cameras",
                "Light sources",
                "Optical variables control",
                "Image motion analysis",
                "Reflectivity",
                "Shape",
                "Layout",
                "Photometry"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "illumination effect",
                "video sequences",
                "computer vision",
                "3D structure",
                "Lambertian reflectance functions",
                "3d object motion",
                "video sequence",
                "bilinear subspace formulation"
            ]
        },
        "id": 218,
        "cited_by": []
    },
    {},
    {
        "title": "Separating reflections in human iris images for illumination estimation",
        "authors": [
            "Huiqiong Wang",
            "S. Lin",
            "Xiaopei Liu",
            "Sing Bing Kang"
        ],
        "abstract": "A method is presented for separating corneal reflections in an image of human irises to estimate illumination from the surrounding scene. Previous techniques for reflection separation have demonstrated success in only limited cases, such as for uniform colored lighting and simple object textures, so they are not applicable to irises which exhibit intricate textures and complicated reflections of the environment. To make this problem feasible, we present a method that capitalizes on physical characteristics of human irises to obtain an illumination estimate that encompasses the prominent light contributors in the scene. Results of this algorithm are presented for eyes of different colors, including light colored eyes for which reflection separation is necessary to determine a valid illumination estimate",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544920",
        "reference_list": [
            {
                "year": "2003",
                "id": 22
            },
            {
                "year": "2003",
                "id": 114
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Iris",
                "Lighting",
                "Optical reflection",
                "Eyes",
                "Layout",
                "Waveguide discontinuities",
                "Brightness",
                "Asia",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "eye"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human iris images",
                "illumination estimation",
                "light colored eyes",
                "corneal reflections"
            ]
        },
        "id": 220,
        "cited_by": []
    },
    {
        "title": "Coupled space learning of image style transformation",
        "authors": [
            "Dahua Lin",
            "Xiaoou Tang"
        ],
        "abstract": "In this paper, we present a new learning framework for image style transforms. Considering that the images in different style representations constitute different vector spaces, we propose a novel framework called coupled space learning to learn the relations between different spaces and use them to infer the images from one style to another style. Observing that for each style, only the components correlated to the space of the target style are useful for inference, we first develop the correlative component analysis to pursue the embedded hidden subspaces that best preserve the inter-space correlation information. Then we develop the coupled bidirectional transform algorithm to estimate the transforms between the two embedded spaces, where the coupling between the forward transform and the backward transform is explicitly taken into account. To enhance the capability of modelling complex data, we further develop the coupled Gaussian mixture model to generalize our framework to a mixture-model architecture. The effectiveness of the framework is demonstrated in the applications including face super-resolution and bidirectional portrait style transforms",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544921",
        "reference_list": [
            {
                "year": "2003",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 0,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis",
                "Information analysis",
                "Inference algorithms",
                "Computer vision",
                "Statistical learning",
                "Image reconstruction",
                "Asia",
                "Computer architecture",
                "Application software",
                "Face detection"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image processing",
                "learning (artificial intelligence)",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coupled space learning",
                "image style transformation",
                "vector spaces",
                "correlative component analysis",
                "coupled bidirectional transform",
                "forward transform",
                "backward transform",
                "coupled Gaussian mixture model",
                "mixture-model architecture"
            ]
        },
        "id": 221,
        "cited_by": []
    },
    {
        "title": "On optimal light configurations in photometric stereo",
        "authors": [
            "O. Drbohlav",
            "M. Chantler"
        ],
        "abstract": "This paper develops new theory for the optimal placement of photometric stereo lighting in the presence of camera noise. We show that for three lights, any triplet of orthogonal light directions minimises the uncertainty in scaled normal computation. The assumptions are that the camera noise is additive and normally distributed, and uncertainty is defined as the expectation of squared distance of scale normal to the ground truth. If the camera noise is of zero mean and variance sigma 2 the optimal (minimum) uncertainty in the scaled normal is 3sigma 2 For case of n > 3 lights, we show that the minimum uncertainty is 9sigma 2 n, and identify sets of light configurations which reach this theoretical minimum",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544922",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 8,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Reflectivity",
                "Cameras",
                "Uncertainty",
                "Computer errors",
                "Lighting",
                "Light sources",
                "Additive noise",
                "Optical sensors",
                "Optical noise"
            ],
            "INSPEC: Controlled Indexing": [
                "statistical analysis",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optimal light configurations",
                "photometric stereo lighting",
                "camera noise",
                "orthogonal light directions"
            ]
        },
        "id": 222,
        "cited_by": [
            {
                "year": "2015",
                "id": 388
            }
        ]
    },
    {
        "title": "Integration of conditionally dependent object features for robust figure/background segmentation",
        "authors": [
            "F. Moreno-Noguer",
            "A. Sanfeliu",
            "D. Samaras"
        ],
        "abstract": "We propose a new technique for focusing multiple cues to robustly segment an object from its background in video sequences that suffer from abrupt changes of both illumination and position of the target. Robustness is achieved by tile integration of appearance and geometric object features and by their description using particle filters. Previous approaches assume independence of the object cues or apply the particle filter formulation to only one of the features, and assume a smooth change in the rest, which can prove is very limiting, especially when the state of some features needs to be updated using other cues or when their dynamics follow non-linear and unpredictable paths. Our technique offers a general framework to model the probabilistic relationship between features. The proposed method is analytically justified and applied to develop a robust tracking system that adapts online and simultaneously the color space where the image points are represented, the color distributions, and the contour of the object. Results with synthetic data and real video sequences demonstrate the robustness and versatility of our method",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544923",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Particle filters",
                "Video sequences",
                "Target tracking",
                "State estimation",
                "Image color analysis",
                "Image segmentation",
                "Particle tracking",
                "Particle measurements",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image segmentation",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "conditionally dependent object features",
                "robust figure-background segmentation",
                "object segmentation",
                "video sequences",
                "geometric object features",
                "particle filters"
            ]
        },
        "id": 223,
        "cited_by": []
    },
    {
        "title": "On the equivalence of common approaches to lighting insensitive recognition",
        "authors": [
            "M. Osadchy",
            "D.W. Jacobs",
            "M. Lindenbaum"
        ],
        "abstract": "Lighting variation is commonly handled by methods invariant to additive and multiplicative changes in image intensity. It has been demonstrated that comparing images using the direction of the gradient can produce broader insensitivity to changes in lighting conditions, even for 3D scenes. We analyze two common approaches to image comparison that are invariant, normalized correlation using small correlation windows, and comparison based on a large set of oriented difference of Gaussian filters. We show analytically that these methods calculate a monotonic (cosine) function of the gradient direction difference and hence are equivalent to the direction of gradient method. Our analysis is supported with experiments on both synthetic and real scenes",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544924",
        "reference_list": [
            {
                "year": "2003",
                "id": 181
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Computer vision",
                "Jacobian matrices",
                "Image analysis",
                "Gradient methods",
                "Image processing",
                "Gabor filters",
                "Object recognition",
                "Image texture analysis",
                "Frequency"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "lighting insensitive recognition",
                "lighting variation",
                "image intensity",
                "lighting conditions",
                "3D scenes",
                "Gaussian filters",
                "monotonic cosine function",
                "gradient direction difference"
            ]
        },
        "id": 224,
        "cited_by": []
    },
    {
        "title": "Randomized RANSAC with sequential probability ratio test",
        "authors": [
            "J. Matas",
            "O. Chum"
        ],
        "abstract": "A randomized model verification strategy for RANSAC is presented. The proposed method finds, like RANSAC, a solution that is optimal with user-controllable probability n. A provably optimal model verification strategy is designed for the situation when the contamination of data by outliers is known, i.e. the algorithm is the fastest possible (on average) of all randomized RANSAC algorithms guaranteeing 1 - n confidence in the solution. The derivation of the optimality property is based on Wald's theory of sequential decision making. The R-RANSAC with SPRT which does not require the a priori knowledge of the fraction of outliers and has results close to the optimal strategy is introduced. We show experimentally that on standard test data the method is 2 to 10 times faster than the standard RANSAC and up to 4 times faster than previously published methods",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544925",
        "reference_list": [
            {
                "year": "2003",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 37,
            "other": 29,
            "total": 66
        },
        "keywords": {
            "IEEE Keywords": [
                "Sequential analysis",
                "Standards publication",
                "Contamination",
                "Decision making",
                "Testing",
                "Cybernetics",
                "Algorithm design and analysis",
                "Robustness",
                "Computer vision",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "probability",
                "sequential estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sequential probability ratio test",
                "randomized model verification",
                "user-controllable probability",
                "optimal model verification",
                "randomized RANSAC algorithms",
                "sequential decision making",
                "R-RANSAC",
                "random sample consensus"
            ]
        },
        "id": 225,
        "cited_by": [
            {
                "year": "2015",
                "id": 254
            },
            {
                "year": "2009",
                "id": 282
            },
            {
                "year": "2007",
                "id": 378
            }
        ]
    },
    {
        "title": "Simultaneous facial action tracking and expression recognition using a particle filter",
        "authors": [
            "F. Dornaika",
            "F. Davoine"
        ],
        "abstract": "The recognition of facial gestures and expressions in image sequences is an important and challenging problem. Most of the existing methods adopt the following paradigm. First, facial actions/features are retrieved from the images, and then facial expressions are recognized based on the retrieved temporal parameters. Unlike this main strewn, this paper introduces a new approach allowing the simultaneous recovery of facial actions and expression using a particle filter adopting, multiclass dynamics that are conditioned on the expression. For each frame in the video sequence, our approach is split in two consecutive stages. In the first stage, the 3D head pose is recovered using a deterministic registration technique based on online appearance models. In the second stage, the facial actions as well as the facial expression are simultaneously recovered using the stochastic framework with, mixed states. The proposed fast scheme is either as robust as existing ones or more robust with respect to many regards. Experimental results show the feasibility and robustness of the proposed approach",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544926",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 10,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Particle tracking",
                "Face recognition",
                "Particle filters",
                "Robustness",
                "Image recognition",
                "Image retrieval",
                "Image sequences",
                "Video sequences",
                "Head",
                "Stochastic processes"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "gesture recognition",
                "image restoration",
                "image sequences",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial action tracking",
                "expression recognition",
                "particle filter",
                "facial gesture recognition",
                "image sequences",
                "facial features",
                "video sequence",
                "3D head pose recovery",
                "deterministic registration",
                "online appearance models"
            ]
        },
        "id": 226,
        "cited_by": []
    },
    {
        "title": "Adaptive enhancement of cardiac magnetic resonance (CMR) images",
        "authors": [
            "E.T.H. Leung",
            "J.K. Tsotsos"
        ],
        "abstract": "This paper presents a wavelet-based framework for enhancing the coherent structures attributable to the target organ in cardiac magnetic resonance (MR) images. Previous approaches focus on the Rician nature of noise in magnitude MR images. Image noise is but only one of the confounding factors that obscure the anatomical structures of the target organ. This paper models the image noise in a magnitude MR image in terms of two noise classes which occur over different ranges of signal intensity. An adaptive enhancement scheme is developed to achieve simultaneous attenuation of the effects of these factors and improvement in image contrast",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544927",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Magnetic resonance",
                "Magnetic noise",
                "Heart",
                "Rician channels",
                "Anatomical structure",
                "Data mining",
                "Shape",
                "Noise shaping",
                "Noise reduction",
                "Background noise"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "cardiology",
                "image denoising",
                "image enhancement",
                "wavelet transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cardiac magnetic resonance images",
                "wavelet-based framework",
                "magnitude MR images",
                "image noise",
                "adaptive enhancement scheme",
                "image contrast"
            ]
        },
        "id": 227,
        "cited_by": []
    },
    {
        "title": "Fusion of multiview silhouette cues using a space occupancy grid",
        "authors": [
            "J.-S. Franco",
            "E. Boyer"
        ],
        "abstract": "In this paper, we investigate what can be inferred from several silhouette probability maps, in multiview silhouette cue fusion. To this aim, we propose a new framework for multiview silhouette cue fusion. This framework work uses a space occupancy grid as a probabilistic 3D representation of scene contents. Such a representation is of great interest for various computer vision applications in perception, or localization for instance. Our main contribution is to introduce the occupancy grid concept, popular in the robotics, for multicamera environments. The idea is to consider each camera pixel as a statistical occupancy sensor. All pixel observations are then used jointly to infer where, and how likely, matter is present in the scene. As our results illustrate, this sample model has various advantages. Most sources of uncertainty are explicitly modeled, and no premature decisions about pixel labeling occur, thus preserving pixel knowledge. Consequently, optimal scene object localization, and robust volume reconstruction, can achieved, with no constraint on camera placement and object visibility. In addition, this representation allows to improve silhouette extraction in images",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544928",
        "reference_list": [
            {
                "year": "2003",
                "id": 174
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 24,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Computer vision",
                "Application software",
                "Robot sensing systems",
                "Orbital robotics",
                "Robot vision systems",
                "Uncertainty",
                "Labeling",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image reconstruction",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview silhouette cues",
                "space occupancy grid",
                "silhouette probability maps",
                "probabilistic 3D scene representation",
                "computer vision",
                "statistical occupancy sensor",
                "optimal scene object localization",
                "volume reconstruction",
                "image silhouette extraction"
            ]
        },
        "id": 228,
        "cited_by": [
            {
                "year": "2009",
                "id": 233
            },
            {
                "year": "2007",
                "id": 58
            },
            {
                "year": "2007",
                "id": 176
            }
        ]
    },
    {
        "title": "Incremental discovery of object parts in video sequences",
        "authors": [
            "S. Drouin",
            "P. Hebert",
            "M. Parizeau"
        ],
        "abstract": "This paper addresses the fundamental problem of automatically discovering an unknown moving deformable object in a monocular video sequence. No prior model of the object is used; it is only assumed that the object is composed of a set of apparently rigid parts that are not necessarily visible simultaneously, making it possible to circumvent the typical constraint of model initialization. A set of rigid parts describing the object is incrementally extracted in a modeling-tracking loop with reinforced memory. In this framework, low-level segmentation is considered as a necessary but non reliable process that helps initiating hypotheses. Motion-based layer segmentation from feature points and edges is applied only when and where no modeled parts can be tracked. Using the quantity of motion measure, it is further shown how to deal with temporal scale. The interest for this approach in applications such as human tracking is demonstrated for a set of various sequences including a rapidly evolving shape",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544929",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Video sequences",
                "Humans",
                "Tracking",
                "Shape",
                "Computer vision",
                "Biological system modeling",
                "Image segmentation",
                "Motion analysis",
                "Laboratories",
                "Motion measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "incremental object part discovery",
                "deformable moving object",
                "monocular video sequence",
                "motion-based layer segmentation"
            ]
        },
        "id": 229,
        "cited_by": []
    },
    {
        "title": "Efficient learning of relational object class models",
        "authors": [
            "A.B. Hillel",
            "D. Weinshall",
            "T. Hertz"
        ],
        "abstract": "We present an efficient method for learning part-based object class models. The models include location and scale relations between parts, as well as part appearance. Models are learnt from raw object and background images, represented as an unordered set of features extracted using an interest point detector. The object class is generatively modeled using a simple Bayesian network with a central hidden node containing location and scale information, and nodes describing object parts. The model's parameters, however are optimized to reduce a loss function which reflects training error as in discriminative methods. Specifically, the optimization is done using a boosting-like technique with complexity linear in the number of parts and the number of features per image. This efficiency allows our method to learn relational models with many parts and features, and leads to improved results when compared with other methods. Extensive experimental results are described, using some common bench-mark datasets and three sets of newly collected data, showing the relative advantage of our method",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544930",
        "reference_list": [
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2003",
                "id": 38
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 7,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Optimization methods",
                "Object recognition",
                "Computer science",
                "Feature extraction",
                "Data mining",
                "Object detection",
                "Detectors",
                "Humans",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relational object class models",
                "part-based object class models",
                "feature extraction",
                "interest point detector",
                "Bayesian network"
            ]
        },
        "id": 230,
        "cited_by": [
            {
                "year": "2007",
                "id": 12
            },
            {
                "year": "2007",
                "id": 224
            }
        ]
    },
    {
        "title": "Face recognition with MRC-boosting",
        "authors": [
            "Xun Xu",
            "T.S. Huang"
        ],
        "abstract": "In this paper, a novel classification algorithm called MRC-Boosting is proposed. Through aggregating maximal-rejection-classifier features under boosting framework, this algorithm can deal with complicated two-class classification problem, especially for the category called target detection problem where a target class should be discriminated from tile surrounding clutter class. MRC-Boosting is efficient since unlike many other boosting based algorithms, at each iteration the optimal feature is computed in closed-form, with neither exhaustive search nor time-consuming numerical optimization. Furthermore, a variant of MRC-Boosting is derived and applied to face recognition. This variant MRC-Boosting algorithm is able to utilize large amount of training samples efficiently overcoming the difficulty faced by other algorithms like AdaBoost. The effectiveness of the proposed algorithm is validated by face recognition experiments on CMU-PIE database",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544931",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Bayesian methods",
                "Boosting",
                "Object detection",
                "Face detection",
                "Databases",
                "Computer vision",
                "Gaussian distribution",
                "Classification algorithms",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "MRC-boosting",
                "classification algorithm",
                "maximal-rejection-classifier features",
                "target detection"
            ]
        },
        "id": 231,
        "cited_by": []
    },
    {
        "title": "Bottom-up/top-down image parsing by attribute graph grammar",
        "authors": [
            "Feng Han",
            "Song-Chun Zhu"
        ],
        "abstract": "In this paper, we present an attribute graph grammar for image parsing on scenes with man-made objects, such as buildings, hallways, kitchens, and living moms. We choose one class of primitives - 3D planar rectangles projected on images and six graph grammar production rules. Each production rule not only expands a node into its components, but also includes a number of equations that constrain the attributes of a parent node and those of its children. Thus our graph grammar is context sensitive. The grammar rules are used recursively to produce a large number of objects and patterns in images and thus the whole graph grammar is a type of generative model. The inference algorithm integrates bottom-up rectangle detection which activates top-down prediction using the grammar rules. The final results are validated in a Bayesian framework. The output of the inference is a hierarchical parsing graph with objects, surfaces, rectangles, and their spatial relations. In the inference, the acceptance of a grammar rule means recognition of an object, and actions are taken to pass the attributes between a node and its parent through the constraint equations associated with this production rule. When an attribute is passed from a child node to a parent node, it is called bottom-up, and the opposite is called top-down",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544932",
        "reference_list": [
            {
                "year": "2003",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 3,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Production",
                "Equations",
                "Inference algorithms",
                "Tree graphs",
                "Computer vision",
                "Computer science",
                "Statistics",
                "Bayesian methods",
                "Vocabulary"
            ],
            "INSPEC: Controlled Indexing": [
                "graph grammars",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bottom-up image parsing",
                "top-down image parsing",
                "attribute graph grammar",
                "3D planar rectangles",
                "graph grammar production rules",
                "image objects",
                "image patterns",
                "inference algorithm",
                "bottom-up rectangle detection",
                "object recognition"
            ]
        },
        "id": 232,
        "cited_by": [
            {
                "year": "2017",
                "id": 125
            },
            {
                "year": "2007",
                "id": 34
            },
            {
                "year": "2007",
                "id": 251
            }
        ]
    },
    {
        "title": "Variational-based method to extract parametric shapes from images",
        "authors": [
            "M.T. El-Melegy",
            "N.H. Al-Ashwal",
            "A.A. Farag"
        ],
        "abstract": "In this paper, we propose a variational method to segment image objects, which have a given parametric shape based on a level-set formulation of the Mumford-Shah functional, and the shape parameters. We define an energy functional composed by two complementary terms. The first one detects object boundaries using a Chan-Vese-like method. The second term constrains the contour to find a shape compatible with the parametric shape. The segmentation of the object of interest is given by the minimum of our energy functional. This minimum is computed with the calculus of variation and the gradient descent method that provide a system of evolution equations solved with the well-known level set method. We focus in this paper on the parametric category of image linear objects. Applications of the proposed model are presented on synthetic and real images",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544933",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image segmentation",
                "Level set",
                "Computer vision",
                "Focusing",
                "Image processing",
                "Object detection",
                "Calculus",
                "Application software",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "object detection",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parametric shape extraction",
                "variational method",
                "image object segmentation",
                "Mumford-Shah functional",
                "object boundary detection",
                "Chan-Vese-like method",
                "gradient descent method"
            ]
        },
        "id": 233,
        "cited_by": []
    },
    {
        "title": "Local features for object class recognition",
        "authors": [
            "K. Mikolajczyk",
            "B. Leibe",
            "B. Schiele"
        ],
        "abstract": "In this paper, we compare the performance of local detectors and descriptors in the context of object class recognition. Recently, many detectors/descriptors have been evaluated in the context of matching as well as invariance to viewpoint changes (Mikolajczyk and Schmid, 2004). However, it is unclear if these results can be generalized to categorization problems, which require different properties of features. We evaluate 5 state-of-the-art scale invariant region detectors and 5 descriptors. Local features are computed for 20 object classes and clustered using hierarchical agglomerative clustering. We measure the quality of appearance clusters and location distributions using entropy as well as precision. We also measure how the clusters generalize from training set to novel test data. Our results indicate that attended SIFT descriptors (Mikolajczyk and Schmid, 2005) computed on Hessian-Laplace regions perform best. Second score is obtained by salient regions (Kadir and Brady, 2001). The results also show that these two detectors provide complementary features. The new detectors/descriptors significantly improve the performance of a state-of-the art recognition approach (Leibe, et al., 2005) in pedestrian detection task",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544934",
        "reference_list": [
            {
                "year": "2003",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 56,
            "other": 50,
            "total": 106
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Computer vision",
                "Object detection",
                "Art",
                "Image recognition",
                "Interactive systems",
                "Entropy",
                "Testing",
                "Photometry",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "object recognition",
                "pattern classification",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object class recognition",
                "categorization problems",
                "scale invariant region detectors",
                "hierarchical agglomerative clustering",
                "SIFT descriptors",
                "Hessian-Laplace regions",
                "salient regions",
                "pedestrian detection"
            ]
        },
        "id": 234,
        "cited_by": [
            {
                "year": "2007",
                "id": 39
            }
        ]
    },
    {
        "title": "Object categorization by learned universal visual dictionary",
        "authors": [
            "J. Winn",
            "A. Criminisi",
            "T. Minka"
        ],
        "abstract": "This paper presents a new algorithm for the automatic recognition of object classes from images (categorization). Compact and yet discriminative appearance-based object class models are automatically learned from a set of training images. The method is simple and extremely fast, making it suitable for many applications such as semantic image retrieval, Web search, and interactive image editing. It classifies a region according to the proportions of different visual words (clusters in feature space). The specific visual words and the typical proportions in each object are learned from a segmented training set. The main contribution of this paper is twofold: i) an optimally compact visual dictionary is learned by pair-wise merging of visual words from an initially large dictionary. The final visual words are described by GMMs. ii) A novel statistical measure of discrimination is proposed which is optimized by each merge operation. High classification accuracy is demonstrated for nine object classes on photographs of real objects viewed under general lighting conditions, poses and viewpoints. The set of test images used for validation comprise: i) photographs acquired by us, ii) images from the Web and iii) images from the recently released Pascal dataset. The proposed algorithm performs well on both texture-rich objects (e.g. grass, sky, trees) and structure-rich ones (e.g. cars, bikes, planes)",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544935",
        "reference_list": [],
        "citation": {
            "ieee": 282,
            "other": 189,
            "total": 471
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Testing",
                "Bicycles",
                "Lighting",
                "Image recognition",
                "Deformable models",
                "Image retrieval",
                "Web search",
                "Image segmentation",
                "Merging"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object categorization",
                "universal visual dictionary",
                "object class recognition",
                "image categorization",
                "discriminative appearance-based object class models",
                "visual words"
            ]
        },
        "id": 235,
        "cited_by": [
            {
                "year": "2015",
                "id": 66
            },
            {
                "year": "2015",
                "id": 69
            },
            {
                "year": "2015",
                "id": 188
            },
            {
                "year": "2013",
                "id": 211
            },
            {
                "year": "2013",
                "id": 333
            },
            {
                "year": "2013",
                "id": 375
            },
            {
                "year": "2011",
                "id": 21
            },
            {
                "year": "2011",
                "id": 29
            },
            {
                "year": "2011",
                "id": 39
            },
            {
                "year": "2011",
                "id": 336
            },
            {
                "year": "2009",
                "id": 80
            },
            {
                "year": "2009",
                "id": 87
            },
            {
                "year": "2009",
                "id": 251
            },
            {
                "year": "2009",
                "id": 253
            },
            {
                "year": "2007",
                "id": 27
            },
            {
                "year": "2007",
                "id": 125
            },
            {
                "year": "2007",
                "id": 179
            }
        ]
    },
    {
        "title": "Conditional models for contextual human motion recognition",
        "authors": [
            "C. Sminchisescu",
            "A. Kanaujia",
            "Zhiguo Li",
            "D. Metaxas"
        ],
        "abstract": "We present algorithms for recognizing human motion in monocular video sequences, based on discriminative conditional random field (CRF) and maximum entropy Markov models (MEMM). Existing approaches to this problem typically use generative (joint) structures like the hidden Markov model (HMM). Therefore they have to make simplifying, often unrealistic assumptions on the conditional independence of observations given the motion class labels and cannot accommodate overlapping features or long term contextual dependencies in the observation sequence. In contrast, conditional models like the CRFs seamlessly represent contextual dependencies, support efficient, exact inference using dynamic programming, and their parameters can be trained using convex optimization. We introduce conditional graphical models as complementary tools for human motion recognition and present an extensive set of experiments that show how these typically outperform HMMs in classifying not only diverse human activities like walking, jumping. running, picking or dancing, but also for discriminating among subtle motion styles like normal walk and wander walk",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544936",
        "reference_list": [
            {
                "year": "2003",
                "id": 98
            },
            {
                "year": "2003",
                "id": 151
            }
        ],
        "citation": {
            "ieee": 92,
            "other": 48,
            "total": 140
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Character generation",
                "Artificial intelligence",
                "Chromium",
                "Image recognition",
                "Inference algorithms",
                "Hidden Markov models",
                "Feature extraction",
                "Optical filters",
                "Gold"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human motion recognition",
                "monocular video sequences",
                "discriminative conditional random field",
                "maximum entropy Markov models"
            ],
            "Author Keywords": [
                "Markov random fields",
                "discriminative models",
                "Hidden Markov Models",
                "human motion recognition",
                "multiclass logistic regression",
                "feature selection",
                "conditional models",
                "optimization"
            ]
        },
        "id": 236,
        "cited_by": [
            {
                "year": "2011",
                "id": 14
            }
        ]
    },
    {
        "title": "Learning object categories from Google's image search",
        "authors": [
            "R. Fergus",
            "L. Fei-Fei",
            "P. Perona",
            "A. Zisserman"
        ],
        "abstract": "Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by utilizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spatial information in a translation and scale invariant manner. Our approach can handle the high intra-class variability and large proportion of unrelated images returned by search engines. We evaluate tire models on standard test sets, showing performance competitive with existing methods trained on hand prepared datasets",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544937",
        "reference_list": [
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2001",
                "id": 69
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2003",
                "id": 38
            }
        ],
        "citation": {
            "ieee": 201,
            "other": 145,
            "total": 346
        },
        "keywords": {
            "IEEE Keywords": [
                "Testing",
                "Search engines",
                "Internet",
                "Computer vision",
                "Image segmentation",
                "Airplanes",
                "Motorcycles",
                "Wrist",
                "Watches",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "classification",
                "image classification",
                "image retrieval",
                "Internet",
                "search engines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object category learning",
                "Google",
                "object category recognition",
                "image search engines",
                "Internet",
                "TSI-pLSA"
            ]
        },
        "id": 237,
        "cited_by": [
            {
                "year": "2015",
                "id": 516
            },
            {
                "year": "2013",
                "id": 206
            },
            {
                "year": "2013",
                "id": 355
            },
            {
                "year": "2011",
                "id": 66
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2011",
                "id": 260
            },
            {
                "year": "2011",
                "id": 261
            },
            {
                "year": "2007",
                "id": 13
            },
            {
                "year": "2007",
                "id": 18
            },
            {
                "year": "2007",
                "id": 31
            },
            {
                "year": "2007",
                "id": 125
            },
            {
                "year": "2007",
                "id": 164
            },
            {
                "year": "2007",
                "id": 202
            },
            {
                "year": "2007",
                "id": 259
            },
            {
                "year": "2007",
                "id": 288
            }
        ]
    },
    {
        "title": "Shape from symmetry",
        "authors": [
            "S. Thrun",
            "B. Wegbreit"
        ],
        "abstract": "We describe a technique for reconstructing probable occluded surfaces from 3D range images. The technique exploits the fact that many objects possess shape symmetries that can be recognized even from partial 3D views. Our approach identifies probable symmetries and uses them to attend the partial 3D shape model into the occluded space. To accommodate objects consisting of multiple parts, we describe a technique for segmenting objects into parts characterized by different symmetries. Results are provided for a real-world database of 3D range images of common objects, acquired through an active stereo rig",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544938",
        "reference_list": [],
        "citation": {
            "ieee": 42,
            "other": 28,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Surface reconstruction",
                "Image reconstruction",
                "Computer vision",
                "Stereo vision",
                "Optical reflection",
                "Image segmentation",
                "Image databases",
                "Cameras",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image segmentation",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "occluded surface reconstruction",
                "3D range images",
                "3D shape model",
                "object segmentation"
            ]
        },
        "id": 238,
        "cited_by": []
    },
    {
        "title": "Parameter-free radial distortion correction with centre of distortion estimation",
        "authors": [
            "R.I. Hartley",
            "Sing Bing Kang"
        ],
        "abstract": "We propose a method of simultaneously calibrating the radial distortion function of a camera along with the other internal calibration parameters. The method relies on the use of a planar (or alternatively nonplanar) calibration grid, which is captured in several images. In this way, the determination of the radial distortion is an easy add-on to the popular calibration method proposed by Zhang [1999]. The method is entirely noniterative, and hence is extremely rapid and immune from the problem of local minima. Our method determines the radial distortion in a parameter-free way, not relying on any particular radial distortion model. This makes it applicable to a large range of cameras from narrow-angle to fish-eye lenses. The method also computes the centre of radial distortion, which we argue is important in obtaining optimal results. Experiments show that this point may be significantly displaced from the centre of the image, or the principal point of the camera",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544939",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 24,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Lenses",
                "Iterative methods",
                "Cost function",
                "Digital cameras",
                "Image analysis",
                "Digital images",
                "Polynomials",
                "Australia Council",
                "Curve fitting"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parameter-free radial distortion correction",
                "distortion estimation",
                "planar calibration grid",
                "nonplanar calibration grid"
            ]
        },
        "id": 239,
        "cited_by": [
            {
                "year": "2017",
                "id": 405
            },
            {
                "year": "2007",
                "id": 273
            },
            {
                "year": "2007",
                "id": 297
            }
        ]
    },
    {
        "title": "Inference of non-overlapping camera network topology by measuring statistical dependence",
        "authors": [
            "Kinh Tieu",
            "G. Dalley",
            "W.E.L. Grimson"
        ],
        "abstract": "We present an approach for inferring the topology of a camera network by measuring statistical dependence between observations in different cameras. Two cameras are considered connected if objects seen departing in one camera is seen arriving in the other. This is captured by the degree of statistical dependence between the cameras. The nature of dependence is characterized by the distribution of observation transformations between cameras, such as departure to arrival transition times, and color appearance. We show how to measure statistical dependence when the correspondence between observations in different cameras is unknown. This is accomplished by non-parametric estimates of statistical dependence and Bayesian integration of the unknown correspondence. Our approach generalizes previous work which assumed restricted parametric transition distributions and only implicitly dealt with unknown correspondence. Results are shown on simulated and real data. We also describe a technique for learning the absolute locations of the cameras with Global Positioning System (GPS) side information",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544940",
        "reference_list": [
            {
                "year": "2003",
                "id": 125
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 6,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Network topology",
                "Smart cameras",
                "Global Positioning System",
                "Surveillance",
                "Monitoring",
                "Computer science",
                "Artificial intelligence",
                "Laboratories",
                "Bayesian methods",
                "Traffic control"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image processing",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonoverlapping camera network topology",
                "statistical dependence",
                "Bayesian integration",
                "restricted parametric transition distributions",
                "Global Positioning System"
            ]
        },
        "id": 240,
        "cited_by": [
            {
                "year": "2009",
                "id": 15
            },
            {
                "year": "2007",
                "id": 98
            },
            {
                "year": "2007",
                "id": 155
            }
        ]
    },
    {
        "title": "Can two specular pixels calibrate photometric stereo?",
        "authors": [
            "O. Drbohlav",
            "M. Chaniler"
        ],
        "abstract": "Lambertian photometric stereo with unknown light source parameters is ambiguous. Provided that the object imaged constitutes a surface, the ambiguity is represented by the group of generalised bas-relief (GBR) transformations. We show that this ambiguity is resolved when specular reflection is present in two images taken under two different light source directions. We identify all configurations of the two directional lights which are singular and show that they can easily be tested for. While previous work used optimisation algorithms to apply the constraints implied by the specular reflectance component, we have developed a linear algorithm to achieve this goal. Our theory can be utilised to construct fast algorithms for automatic reconstruction of smooth glossy surfaces",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544941",
        "reference_list": [
            {
                "year": "2003",
                "id": 107
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 12,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Light sources",
                "Reflectivity",
                "Surface reconstruction",
                "Optical reflection",
                "Calibration",
                "Stereo vision",
                "Image reconstruction",
                "Image resolution",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "photometry",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "specular pixels",
                "Lambertian photometric stereo",
                "generalised bas-relief transformations",
                "light source",
                "smooth glossy surface reconstruction"
            ]
        },
        "id": 241,
        "cited_by": [
            {
                "year": "2007",
                "id": 263
            }
        ]
    },
    {
        "title": "Bayesian autocalibration for surveillance",
        "authors": [
            "N. Krahnstoever",
            "P.R.S. Mendonca"
        ],
        "abstract": "In the context of visual surveillance of human activity knowledge about a camera's internal and external parameters is useful, as it allows for the establishment of a connection between image and world measurements. Unfortunately, calibration information is rarely available and difficult to obtain after a surveillance system has been installed. In this paper, a method for camera autocalibration based on information gathered by tracking people is developed. It brings two main contributions: first, we show how a foot-to-head plane homology can be used to obtain the calibration parameters and then we show an approach how to efficiently estimate initial parameter estimates from measurements; second, we present a Bayesian solution to the calibration problem that can elegantly handle measurement uncertainties, outliers, as well as prior information. It is shown how the full posterior distribution of calibration parameters given the measurements can be estimated, which allows making statements about the accuracy of both the calibration parameters and the measurements involving them",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544942",
        "reference_list": [],
        "citation": {
            "ieee": 41,
            "other": 22,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Surveillance",
                "Cameras",
                "Calibration",
                "Layout",
                "Geometry",
                "Image segmentation",
                "Parameter estimation",
                "State estimation",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "cameras",
                "computational geometry",
                "image processing",
                "surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian autocalibration",
                "visual surveillance",
                "surveillance system",
                "camera autocalibration",
                "foot-to-head plane homology"
            ]
        },
        "id": 242,
        "cited_by": [
            {
                "year": "2007",
                "id": 168
            },
            {
                "year": "2007",
                "id": 242
            }
        ]
    },
    {
        "title": "Objective image fusion performance characterisation",
        "authors": [
            "V. Petrovic",
            "C. Xydeas"
        ],
        "abstract": "Image fusion as a way of combining multiple image signals into a single fused image has in recent years been extensively researched for a variety of multisensor applications. Choosing an optimal fusion approach for each application from the plethora of algorithms available however, remains a largely open issue. A small number of metrics proposed so far provide only a rough, numerical estimate of fusion performance with limited understanding of the relative merits of different fusion schemes. This paper proposes a method for comprehensive, objective, image fusion performance characterisation using a fusion evaluation framework based on gradient information representation. The method provides an in-depth analysis of fusion performance by quantifying: information contributions by each sensor, fusion gain, fusion information loss and fusion artifacts (artificial information created). It is demonstrated on the evaluation of an extensive dataset of multisensor images fused with a wide range of established image fusion algorithms. The results demonstrate and quantify a number of well known issues concerning the performance of these schemes and provide a useful insight into a number of more subtle yet important fusion performance effects not immediately accessible to an observer",
        "ieee_link": "https://ieeexplore.ieee.org/document/1544943",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 15,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Image fusion",
                "Biomedical imaging",
                "Information representation",
                "Aerospace electronics",
                "Application software",
                "Biomedical engineering",
                "Information analysis",
                "Performance analysis",
                "Performance loss",
                "Sensor fusion"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "sensor fusion"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image fusion",
                "image signals",
                "multisensor applications",
                "gradient information representation",
                "fusion gain",
                "fusion information loss",
                "fusion artifacts"
            ]
        },
        "id": 243,
        "cited_by": []
    }
]