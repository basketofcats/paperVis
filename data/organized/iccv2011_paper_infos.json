[
    {
        "title": "A nonparametric Riemannian framework on tensor field with application to foreground segmentation",
        "authors": [
            "Rui Caseiro",
            "Jo\u00e3o F. Henriques",
            "Pedro Martins",
            "Jorge Batista"
        ],
        "abstract": "Background modelling on tensor field has recently been proposed for foreground detection tasks. Taking into account the Riemannian structure of the tensor manifold, recent research has focused on developing parametric methods on the tensor domain e.g. gaussians mixtures (GMM) [7]. However, in some scenarios, simple parametric models do not accurately explain the physical processes. Kernel density estimators (KDE) have been successful to model, on Euclidean sample spaces the nonparametric nature of complex, time varying, and non-static backgrounds [8]. Founded on the mathematically rigorous KDE paradigm on general Riemannian manifolds [15], we define a KDE specifically to operate on the tensor manifold. We present a mathematically-sound framework for nonparametric modeling on tensor field to foreground segmentation. We endow the tensor manifold with two well-founded Riemannian metrics, i.e. Affine-Invariant and Log-Euclidean. Theoretical aspects are defined and the metrics are compared experimentally. Theoretic analysis and experimental results demonstrate the promise/effectiveness of the framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126218",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 4,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Manifolds",
                "Vectors",
                "Kernel",
                "Density functional theory",
                "Euclidean distance"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "matrix algebra",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric Riemannian framework",
                "tensor field",
                "foreground segmentation",
                "background modelling",
                "foreground detection tasks",
                "Gaussian mixtures",
                "Kernel density estimators",
                "Euclidean sample spaces",
                "nonstatic backgrounds",
                "nonparametric modeling",
                "Riemannian metrics"
            ]
        },
        "id": 0,
        "cited_by": []
    },
    {
        "title": "Are spatial and global constraints really necessary for segmentation?",
        "authors": [
            "Aur\u00e9lien Lucchi",
            "Yunpeng Li",
            "Xavier Boix",
            "Kevin Smith",
            "Pascal Fua"
        ],
        "abstract": "Many state-of-the-art segmentation algorithms rely on Markov or Conditional Random Field models designed to enforce spatial and global consistency constraints. This is often accomplished by introducing additional latent variables to the model, which can greatly increase its complexity. As a result, estimating the model parameters or computing the best maximum a posteriori (MAP) assignment becomes a computationally expensive task. In a series of experiments on the PASCAL and the MSRC datasets, we were unable to find evidence of a significant performance increase attributed to the introduction of such constraints. On the contrary, we found that similar levels of performance can be achieved using a much simpler design that essentially ignores these constraints. This more simple approach makes use of the same local and global features to leverage evidence from the image, but instead directly biases the preferences of individual pixels. While our investigation does not prove that spatial and consistency constraints are not useful in principle, it points to the conclusion that they should be validated in a larger context.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126219",
        "reference_list": [
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2007",
                "id": 145
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 9,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Computational modeling",
                "Image segmentation",
                "Support vector machines",
                "Image color analysis",
                "Data models",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatial constraint",
                "global constraint",
                "segmentation algorithm",
                "conditional random field model",
                "maximum a posteriori assignment"
            ]
        },
        "id": 1,
        "cited_by": [
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2013",
                "id": 288
            }
        ]
    },
    {
        "title": "Illumination demultiplexing from a single image",
        "authors": [
            "Christine Chen",
            "Daniel Vaquero",
            "Matthew Turk"
        ],
        "abstract": "A class of techniques in computer vision and graphics is based on capturing multiple images of a scene under different illumination conditions. These techniques explore variations in illumination from image to image to extract interesting information about the scene. However, their applicability to dynamic environments is limited due to the need for robust motion compensation algorithms. To overcome this issue, we propose a method to separate multiple illuminants from a single image. Given an image of a scene simultaneously illuminated by multiple light sources, our method generates individual images as if they had been illuminated by each of the light sources separately. To facilitate the illumination separation process, we encode each light source with a distinct sinusoidal pattern, strategically selected given the relative position of each light with respect to the camera, such that the observed sinusoids become independent of the scene geometry. The individual illuminants are then demultiplexed by analyzing local frequencies. We show applications of our approach in image-based relighting, photometric stereo, and multiflash imaging.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126220",
        "reference_list": [
            {
                "year": "2003",
                "id": 106
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Cameras",
                "Light sources",
                "Multiplexing",
                "Geometry",
                "Image coding"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer graphics",
                "computer vision",
                "demultiplexing",
                "image coding",
                "image retrieval",
                "lighting",
                "motion compensation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "illumination demultiplexing",
                "computer vision",
                "robust motion compensation algorithms",
                "information extraction",
                "illumination separation process",
                "light source",
                "scene geometry",
                "image-based relighting",
                "photometric stereo",
                "multiflash imaging"
            ]
        },
        "id": 2,
        "cited_by": []
    },
    {
        "title": "Silhouette-based object phenotype recognition using 3D shape priors",
        "authors": [
            "Yu Chen",
            "Tae-Kyun Kim",
            "Roberto Cipolla"
        ],
        "abstract": "This paper tackles the novel challenging problem of 3D object phenotype recognition from a single 2D silhouette. To bridge the large pose (articulation or deformation) and camera viewpoint changes between the gallery images and query image, we propose a novel probabilistic inference algorithm based on 3D shape priors. Our approach combines both generative and discriminative learning. We use latent probabilistic generative models to capture 3D shape and pose variations from a set of 3D mesh models. Based on these 3D shape priors, we generate a large number of projections for different phenotype classes, poses, and camera viewpoints, and implement Random Forests to efficiently solve the shape and pose inference problems. By model selection in terms of the silhouette coherency between the query and the projections of 3D shapes synthesized using the galleries, we achieve the phenotype recognition result as well as a fast approximate 3D reconstruction of the query. To verify the efficacy of the proposed approach, we present new datasets which contain over 500 images of various human and shark phenotypes and motions. The experimental results clearly show the benefits of using the 3D priors in the proposed method over previous 2D-based methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126221",
        "reference_list": [
            {
                "year": "2001",
                "id": 34
            },
            {
                "year": "2009",
                "id": 177
            },
            {
                "year": "2009",
                "id": 233
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three dimensional displays",
                "Cameras",
                "Solid modeling",
                "Humans",
                "Training",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "inference mechanisms",
                "object recognition",
                "pose estimation",
                "shape recognition",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single 2D silhouette-based object phenotype recognition",
                "3D object phenotype recognition",
                "pose viewpoint",
                "camera viewpoint",
                "gallery image",
                "query image",
                "inference algorithm",
                "discriminative learning",
                "probabilistic generative model",
                "3D mesh model",
                "pose variation",
                "random forests",
                "pose inference problem",
                "silhouette coherency",
                "3D shape projection",
                "fast approximate 3D reconstruction",
                "shark phenotype",
                "shark motion",
                "1D-based method"
            ]
        },
        "id": 3,
        "cited_by": []
    },
    {
        "title": "Optimal object matching via convexification and composition",
        "authors": [
            "Hongsheng Li",
            "Junzhou Huang",
            "Shaoting Zhang",
            "Xiaolei Huang"
        ],
        "abstract": "In this paper, we propose a novel object matching method to match an object to its instance in an input scene image, where both the object template and the input scene image are represented by groups of feature points. We relax each template point's discrete feature cost function to create a convex function that can be optimized efficiently. Such continuous and convex functions with different regularization terms are able to create different convex optimization models handling objects undergoing (i) global transformation, (ii) locally affine transformation, and (iii) articulated transformation. These models can better constrain each template point's transformation and therefore generate more robust matching results. Unlike traditional object or feature matching methods with \u201chard\u201d node-to-node results, our proposed method allows template points to be transformed to any location in the image plane. Such a property makes our method robust to feature point occlusion or mis-detection. Our extensive experiments demonstrate the robustness and flexibility of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126222",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2005",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Cost function",
                "Vectors",
                "Convex functions",
                "Junctions",
                "Robustness",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "affine transforms",
                "computer graphics",
                "convex programming",
                "feature extraction",
                "image matching",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optimal object matching",
                "input scene image",
                "object template",
                "feature point representation",
                "template point discrete feature cost function",
                "continuous function",
                "convex optimization",
                "affine transformation",
                "articulated transformation",
                "template point transformation",
                "robust matching",
                "feature matching method",
                "image plane",
                "feature point occlusion"
            ]
        },
        "id": 4,
        "cited_by": [
            {
                "year": "2015",
                "id": 103
            }
        ]
    },
    {
        "title": "Unsupervised learning of event AND-OR grammar and semantics from video",
        "authors": [
            "Zhangzhang Si",
            "Mingtao Pei",
            "Benjamin Yao",
            "Song-Chun Zhu"
        ],
        "abstract": "We study the problem of automatically learning event AND-OR grammar from videos of a certain environment, e.g. an office where students conduct daily activities. We propose to learn the event grammar under the information projection and minimum description length principles in a coherent probabilistic framework, without manual supervision about what events happen and when they happen. Firstly a predefined set of unary and binary relations are detected for each video frame: e.g. agent's position, pose and interaction with environment. Then their co-occurrences are clustered into a dictionary of simple and transient atomic actions. Recursively these actions are grouped into longer and complexer events, resulting in a stochastic event grammar. By modeling time constraints of successive events, the learned grammar becomes context-sensitive. We introduce a new dataset of surveillance-style video in office, and present a prototype system for video analysis integrating bottom-up detection, grammatical learning and parsing. On this dataset, the learning algorithm is able to automatically discover important events and construct a stochastic grammar, which can be used to accurately parse newly observed video. The learned grammar can be used as a prior to improve the noisy bottom-up detection of atomic actions. It can also be used to infer semantics of the scene. In general, the event grammar is an efficient way for common knowledge acquisition from video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126223",
        "reference_list": [
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2011",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 19,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Grammar",
                "Production",
                "Stochastic processes",
                "Semantics",
                "Data models",
                "Color",
                "Transient analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "grammars",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised learning",
                "event AND-OR grammar",
                "coherent probabilistic framework",
                "video frame",
                "stochastic event grammar",
                "time constraints",
                "surveillance-style video",
                "video analysis",
                "bottom-up detection",
                "grammatical learning",
                "parsing",
                "learning algorithm",
                "knowledge acquisition"
            ]
        },
        "id": 5,
        "cited_by": [
            {
                "year": "2017",
                "id": 308
            },
            {
                "year": "2013",
                "id": 168
            },
            {
                "year": "2011",
                "id": 61
            }
        ]
    },
    {
        "title": "A general preconditioning scheme for difference measures in deformable registration",
        "authors": [
            "Darko Zikic",
            "Maximilian Baust",
            "Ali Kamen",
            "Nassir Navab"
        ],
        "abstract": "We present a preconditioning scheme for improving the efficiency of optimization of arbitrary difference measures in deformable registration problems. This is of particular interest for high-dimensional registration problems with statistical difference measures such as MI, and the demons method, since in these cases the range of applicable optimization methods is limited. The proposed scheme is simple and computationally efficient: It performs an approximate normalization of the point-wise vectors of the difference gradient to unit length. The major contribution of this work is a theoretical analysis which demonstrates the improvement of the condition by our approach, which is furthermore shown to be an approximation to the optimal case for the analyzed model. Our scheme improves the convergence speed while adding only negligible computational cost, thus resulting in shorter effective runtimes. The theoretical findings are confirmed by experiments on 3D brain data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126224",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Approximation methods",
                "Force",
                "Convergence",
                "Vectors",
                "Erbium",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "general preconditioning scheme",
                "deformable registration",
                "optimization",
                "arbitrary difference measures"
            ]
        },
        "id": 6,
        "cited_by": []
    },
    {
        "title": "Source camera identification using Auto-White Balance approximation",
        "authors": [
            "Zhonghai Deng",
            "Arjan Gijsenij",
            "Jingyuan Zhang"
        ],
        "abstract": "Source camera identification finds many applications in real world. Although many identification methods have been proposed, they work with only a small set of cameras, and are weak at identifying cameras of the same model. Based on the observation that a digital image would not change if the same Auto-White Balance (AWB) algorithm is applied for the second time, this paper proposes to identify the source camera by approximating the AWB algorithm used inside the camera. To the best of our knowledge, this is the first time that a source camera identification method based on AWB has been reported. Experiments show near perfect accuracy in identifying cameras of different brands and models. Besides, proposed method performances quite well in distinguishing among camera devices of the same model, as AWB is done at the end of imaging pipeline, any small differences induced earlier will lead to different types of AWB output. Furthermore, the performance remains stable as the number of cameras grows large.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126225",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Feature extraction",
                "Light sources",
                "Accuracy",
                "Digital cameras",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "identification",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "source camera identification",
                "auto-white balance approximation",
                "digital image",
                "AWB algorithm",
                "source camera identification method",
                "imaging pipeline"
            ]
        },
        "id": 7,
        "cited_by": []
    },
    {
        "title": "Fusing visual and range imaging for object class recognition",
        "authors": [
            "Aharon Bar-Hillel",
            "Dmitri Hanukaev",
            "Dan Levi"
        ],
        "abstract": "Category level object recognition has improved significantly in the last few years, but machine performance remains unsatisfactory for most real-world applications. We believe this gap may be bridged using additional depth information obtained from range imaging, which was recently used to overcome similar problems in body shape interpretation. This paper presents a system which successfully fuses visual and range imaging for object category classification. We explore fusion at multiple levels: using depth as an attention mechanism, high-level fusion at the classifier level and low-level fusion of local descriptors, and show that each mechanism makes a unique contribution to performance. For low-level fusion we present a new algorithm for training of local descriptors, the Generalized Image Feature Transform (GIFT), which generalizes current representations such as SIFT and spatial pyramids and allows for the creation of new representations based on multiple channels of information. We show that our system improves state-of-the-art visual-only and depth-only methods on a diverse dataset of every-day objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126226",
        "reference_list": [
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 4,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Three dimensional displays",
                "Visualization",
                "Imaging",
                "Feature extraction",
                "Histograms",
                "Transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image fusion",
                "object recognition",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual imaging",
                "range imaging",
                "object class recognition",
                "image fusion",
                "category level object recognition",
                "object category classification",
                "generalized image feature transform",
                "SIFT",
                "spatial pyramid",
                "visual-only method",
                "depth-only method"
            ]
        },
        "id": 8,
        "cited_by": []
    },
    {
        "title": "Learning to cluster using high order graphical models with latent variables",
        "authors": [
            "Nikos Komodakis"
        ],
        "abstract": "This paper proposes a very general max-margin learning framework for distance-based clustering. To this end, it formulates clustering as a high order energy minimization problem with latent variables, and applies a dual decomposition approach for training this model. The resulting framework allows learning a very broad class of distance functions, permits an automatic determination of the number of clusters during testing, and is also very efficient. As an additional contribution, we show how our method can be generalized to handle the training of a very broad class of important models in computer vision: arbitrary high-order latent CRFs. Experimental results verify its effectiveness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126227",
        "reference_list": [
            {
                "year": "2007",
                "id": 51
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Clustering algorithms",
                "Computer vision",
                "Optimization",
                "Minimization",
                "Vectors",
                "Fasteners"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "minimisation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high order graphical models",
                "latent variables",
                "general max-margin learning framework",
                "distance-based clustering",
                "high order energy minimization problem",
                "latent variables",
                "dual decomposition approach",
                "distance functions",
                "computer vision",
                "arbitrary high-order latent CRF",
                "conditional random field"
            ]
        },
        "id": 9,
        "cited_by": [
            {
                "year": "2013",
                "id": 384
            }
        ]
    },
    {
        "title": "Hough-based tracking of non-rigid objects",
        "authors": [
            "Martin Godec",
            "Peter M. Roth",
            "Horst Bischof"
        ],
        "abstract": "Online learning has shown to be successful in tracking of previously unknown objects. However, most approaches are limited to a bounding-box representation with fixed aspect ratio. Thus, they provide a less accurate fore- ground/background separation and cannot handle highly non-rigid and articulated objects. This, in turn, increases the amount of noise introduced during online self-training. In this paper, we present a novel tracking-by-detection approach to overcome this limitation based on the generalized Hough-transform. We extend the idea of Hough Forests to the online domain and couple the voting- based detection and back-projection with a rough segmentation based on GrabCut. This significantly reduces the amount of noisy training samples during online learning and thus effectively prevents the tracker from drifting. In the experiments, we demonstrate that our method successfully tracks a variety of previously unknown objects even under heavy non-rigid transformations, partial occlusions, scale changes and rotations. Moreover, we compare our tracker to state-of-the-art methods (both bounding-box- based as well as part-based) and show robust and accurate tracking results on various challenging sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126228",
        "reference_list": [
            {
                "year": "2009",
                "id": 257
            }
        ],
        "citation": {
            "ieee": 73,
            "other": 51,
            "total": 124
        },
        "keywords": {
            "IEEE Keywords": [
                "Vegetation",
                "Training",
                "Robustness",
                "Detectors",
                "Vectors",
                "Training data",
                "Noise"
            ],
            "INSPEC: Controlled Indexing": [
                "Hough transforms",
                "image segmentation",
                "object detection",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Hough-based tracking",
                "nonrigid objects",
                "online learning",
                "bounding-box representation",
                "fixed aspect ratio",
                "foreground/background separation",
                "online self-training",
                "tracking-by-detection approach",
                "generalized Hough-transform",
                "Hough forests",
                "voting- based detection",
                "rough segmentation",
                "GrabCut",
                "nonrigid transformation",
                "partial occlusion",
                "scale changes"
            ]
        },
        "id": 10,
        "cited_by": [
            {
                "year": "2017",
                "id": 576
            },
            {
                "year": "2015",
                "id": 341
            },
            {
                "year": "2015",
                "id": 345
            },
            {
                "year": "2013",
                "id": 52
            },
            {
                "year": "2013",
                "id": 80
            },
            {
                "year": "2013",
                "id": 309
            },
            {
                "year": "2013",
                "id": 364
            }
        ]
    },
    {
        "title": "Ensemble of exemplar-SVMs for object detection and beyond",
        "authors": [
            "Tomasz Malisiewicz",
            "Abhinav Gupta",
            "Alexei A. Efros"
        ],
        "abstract": "This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar-SVMs is thus defined by a single positive instance and millions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generalization. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computational cost increase. But the central benefit of our approach is that it creates an explicit association between each detection and a single training exemplar. Because most detections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of overall scene understanding.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126229",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 256,
            "other": 137,
            "total": 393
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Support vector machines",
                "Detectors",
                "Calibration",
                "Object detection",
                "Vectors",
                "Three dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "exemplar-SVM",
                "support vector machines",
                "discriminative object detector",
                "nearest-neighbor approach",
                "linear SVM classifier",
                "PASCAL VOC detection task",
                "complex latent part-based model",
                "exemplar meta-data"
            ]
        },
        "id": 11,
        "cited_by": [
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2017",
                "id": 485
            },
            {
                "year": "2015",
                "id": 16
            },
            {
                "year": "2015",
                "id": 52
            },
            {
                "year": "2015",
                "id": 56
            },
            {
                "year": "2015",
                "id": 245
            },
            {
                "year": "2015",
                "id": 277
            },
            {
                "year": "2015",
                "id": 358
            },
            {
                "year": "2015",
                "id": 449
            },
            {
                "year": "2015",
                "id": 468
            },
            {
                "year": "2013",
                "id": 0
            },
            {
                "year": "2013",
                "id": 46
            },
            {
                "year": "2013",
                "id": 90
            },
            {
                "year": "2013",
                "id": 94
            },
            {
                "year": "2013",
                "id": 215
            },
            {
                "year": "2013",
                "id": 314
            },
            {
                "year": "2013",
                "id": 317
            },
            {
                "year": "2013",
                "id": 344
            },
            {
                "year": "2013",
                "id": 376
            },
            {
                "year": "2013",
                "id": 392
            },
            {
                "year": "2013",
                "id": 415
            }
        ]
    },
    {
        "title": "CARD: Compact And Real-time Descriptors",
        "authors": [
            "Mitsuru Ambai",
            "Yuichi Yoshida"
        ],
        "abstract": "We propose Compact And Real-time Descriptors (CARD) which can be computed very rapidly and be expressed by short binary codes. An efficient algorithm based on lookup tables is presented for extracting histograms of oriented gradients, which results in approximately 16 times faster computation time per descriptor than that of SIFT. Our lookup-table-based approach can handle arbitrary layouts of bins, such as the grid binning of SIFT and the log-polar binning of GLOH, thus yielding sufficient discrimination power. In addition, we introduce learning-based sparse hashing to convert the extracted descriptors to short binary codes. This conversion is achieved very rapidly by multiplying a very sparse integer weight matrix by the descriptors and aggregating signs of their multiplications. The weight matrix is optimized in a training phase so as to make Hamming distances between encoded training pairs reflect visual dissimilarities between them. Experimental results demonstrate that CARD outperforms previous methods in terms of both computation time and memory usage.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126230",
        "reference_list": [
            {
                "year": "2009",
                "id": 45
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 32,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Binary codes",
                "Training",
                "Histograms",
                "Real time systems",
                "Sparse matrices",
                "Interpolation",
                "Arrays"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "cryptography",
                "feature extraction",
                "grid computing",
                "real-time systems",
                "sparse matrices",
                "table lookup"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "CARD",
                "compact and real-time descriptor",
                "histogram extraction",
                "SIFT",
                "lookup-table-based approach",
                "grid binning",
                "GLOH",
                "learning-based sparse hashing",
                "short binary code",
                "sparse integer weight matrix",
                "training phase",
                "Hamming distance",
                "training pair encoding",
                "visual dissimilarity",
                "memory usage"
            ]
        },
        "id": 12,
        "cited_by": [
            {
                "year": "2015",
                "id": 65
            }
        ]
    },
    {
        "title": "Automatic salient object extraction with contextual cue",
        "authors": [
            "Le Wang",
            "Jianru Xue",
            "Nanning Zheng",
            "Gang Hua"
        ],
        "abstract": "We present a method for automatically extracting salient object from a single image, which is cast in an energy minimization framework. Unlike most previous methods that only leverage appearance cues, we employ an auto-context cue as a complementary data term. Benefitting from a generic saliency model for bootstrapping, the segmentation of the salient object and the learning of the auto-context model are iteratively performed without any user intervention. Upon convergence, we obtain not only a clear separation of the salient object, but also an auto-context classifier which can be used to recognize the same type of object in other images. Our experiments on four benchmarks demonstrated the efficacy of the added contextual cue. It is shown that our method compares favorably with the state-of-the-art, some of which even embraced user interactions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126231",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2007",
                "id": 145
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 18,
            "total": 58
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Context",
                "Minimization",
                "Context modeling",
                "Image segmentation",
                "Visualization",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image segmentation",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic salient object extraction",
                "contextual cue",
                "energy minimization framework",
                "auto-context cue",
                "generic saliency model",
                "bootstrapping",
                "salient object segmentation",
                "auto-context model learning",
                "auto-context classifier",
                "object recognition"
            ]
        },
        "id": 13,
        "cited_by": [
            {
                "year": "2013",
                "id": 190
            },
            {
                "year": "2013",
                "id": 207
            },
            {
                "year": "2013",
                "id": 246
            },
            {
                "year": "2013",
                "id": 415
            }
        ]
    },
    {
        "title": "Action recognition in cluttered dynamic scenes using Pose-Specific Part Models",
        "authors": [
            "Vivek Kumar Singh",
            "Ram Nevatia"
        ],
        "abstract": "We present an approach to recognizing single actor human actions in complex backgrounds. We adopt a Joint Tracking and Recognition approach, which track the actor pose by sampling from 3D action models. Most existing such approaches require large training data or MoCAP to handle multiple viewpoints, and often rely on clean actor silhouettes. The action models in our approach are obtained by annotating keyposes in 2D, lifting them to 3D stick figures and then computing the transformation matrices between the 3D keypose figures. Poses sampled from coarse action models may not fit the observations well; to overcome this difficulty, we propose an approach for efficiently localizing a pose by generating a Pose-Specific Part Model (PSPM) which captures appropriate kinematic and occlusion constraints in a tree-structure. In addition, our approach also does not require pose silhouettes. We show improvements to previous results on two publicly available datasets as well as on a novel, augmented dataset with dynamic backgrounds.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126232",
        "reference_list": [
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2005",
                "id": 60
            },
            {
                "year": "2009",
                "id": 13
            },
            {
                "year": "2005",
                "id": 236
            },
            {
                "year": "2009",
                "id": 240
            },
            {
                "year": "2007",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 11,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Solid modeling",
                "Computational modeling",
                "Kinematics",
                "Image edge detection",
                "Joints",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image sampling",
                "object recognition",
                "object tracking",
                "pose estimation",
                "solid modelling",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action recognition",
                "cluttered dynamic scene",
                "pose-specific part model",
                "single actor human action recognition",
                "actor pose recognition",
                "3D action model sampling",
                "actor pose tracking",
                "clean actor silhouettes",
                "3D stick figure",
                "transformation matrices",
                "3D keypose figure",
                "pose sampling",
                "occlusion constraint",
                "tree structure",
                "pose silhouette",
                "augmented dataset"
            ]
        },
        "id": 14,
        "cited_by": [
            {
                "year": "2013",
                "id": 398
            }
        ]
    },
    {
        "title": "Real-time indoor scene understanding using Bayesian filtering with motion cues",
        "authors": [
            "Grace Tsai",
            "Changhai Xu",
            "Jingen Liu",
            "Benjamin Kuipers"
        ],
        "abstract": "We present a method whereby an embodied agent using visual perception can efficiently create a model of a local indoor environment from its experience of moving within it. Our method uses motion cues to compute likelihoods of indoor structure hypotheses, based on simple, generic geometric knowledge about points, lines, planes, and motion. We present a single-image analysis, not to attempt to identify a single accurate model, but to propose a set of plausible hypotheses about the structure of the environment from an initial frame. We then use data from subsequent frames to update a Bayesian posterior probability distribution over the set of hypotheses. The likelihood function is efficiently computable by comparing the predicted location of point features on the environment model to their actual tracked locations in the image stream. Our method runs in real-time, and it avoids the need of extensive prior training and the Manhattan-world assumption, which makes it more practical and efficient for an intelligent robot to understand its surroundings compared to most previous scene understanding methods. Experimental results on a collection of indoor videos suggest that our method is capable of an unprecedented combination of accuracy and efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126233",
        "reference_list": [
            {
                "year": "2009",
                "id": 237
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2009",
                "id": 87
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 13,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Economic indicators"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image motion analysis",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time indoor scene understanding",
                "Bayesian filtering",
                "motion cues",
                "visual perception",
                "local indoor environment",
                "indoor structure hypotheses",
                "generic geometric knowledge",
                "single-image analysis",
                "Bayesian posterior probability distribution",
                "likelihood function",
                "Manhattan-world assumption",
                "intelligent robot"
            ]
        },
        "id": 15,
        "cited_by": []
    },
    {
        "title": "Revisiting radiometric calibration for color computer vision",
        "authors": [
            "Haiting Lin",
            "Seon Joo Kim",
            "Sabine S\u00fcsstrunk",
            "Michael S. Brown"
        ],
        "abstract": "We present a study of radiometric calibration and the in-camera imaging process through an extensive analysis of more than 10,000 images from over 30 cameras. The goal is to investigate if image values can be transformed to physically meaningful values and if so, when and how this can be done. From our analysis, we show that the conventional radiometric model fits well for image pixels with low color saturation but begins to degrade as color saturation level increases. This is due to the color mapping process which includes gamut mapping in the in-camera processing that cannot be modeled with conventional methods. To this end, we introduce a new imaging model for radiometric calibration and present an effective calibration scheme that allows us to compensate for the nonlinear color correction to convert non-linear sRGB images to CCD RAW responses.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126234",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 6,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Cameras",
                "Radiometry",
                "Calibration",
                "Computational modeling",
                "Clouds"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image colour analysis",
                "radiometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "radiometric calibration",
                "color computer vision",
                "in-camera imaging process",
                "extensive analysis",
                "image values",
                "radiometric model",
                "image pixels",
                "color saturation level",
                "color mapping process",
                "gamut mapping",
                "in-camera processing",
                "imaging model",
                "nonlinear color correction",
                "nonlinear sRGB image",
                "CCD RAW response"
            ]
        },
        "id": 16,
        "cited_by": []
    },
    {
        "title": "Tracking multiple people under global appearance constraints",
        "authors": [
            "Horesh Ben Shitrit",
            "J\u00e9r\u00f4me Berclaz",
            "Fran\u00e7ois Fleuret",
            "Pascal Fua"
        ],
        "abstract": "In this paper, we show that tracking multiple people whose paths may intersect can be formulated as a convex global optimization problem. Our proposed framework is designed to exploit image appearance cues to prevent identity switches. Our method is effective even when such cues are only available at distant time intervals. This is unlike many current approaches that depend on appearance being exploitable from frame to frame. We validate our approach on three multi-camera sport and pedestrian datasets that contain long and complex sequences. Our algorithm perseveres identities better than state-of-the-art algorithms while keeping similar MOTA scores.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126235",
        "reference_list": [
            {
                "year": "2005",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 68,
            "other": 39,
            "total": 107
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Trajectory",
                "Cameras",
                "Radar tracking",
                "Target tracking",
                "Linear programming",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple people tracking",
                "global appearance constraints",
                "convex global optimization problem",
                "image appearance cues",
                "pedestrian datasets",
                "multicamera sport datasets",
                "MOTA score"
            ]
        },
        "id": 17,
        "cited_by": [
            {
                "year": "2015",
                "id": 524
            },
            {
                "year": "2013",
                "id": 140
            },
            {
                "year": "2013",
                "id": 249
            },
            {
                "year": "2013",
                "id": 287
            },
            {
                "year": "2013",
                "id": 365
            }
        ]
    },
    {
        "title": "A new distance for scale-invariant 3D shape recognition and registration",
        "authors": [
            "Minh-Tri Pham",
            "Oliver J. Woodford",
            "Frank Perbet",
            "Atsuto Maki",
            "Bj\u00f6rn Stenger",
            "Roberto Cipolla"
        ],
        "abstract": "This paper presents a method for vote-based 3D shape recognition and registration, in particular using mean shift on 3D pose votes in the space of direct similarity transforms for the first time. We introduce a new distance between poses in this space-the SRT distance. It is left-invariant, unlike Euclidean distance, and has a unique, closed-form mean, in contrast to Riemannian distance, so is fast to compute. We demonstrate improved performance over the state of the art in both recognition and registration on a real and challenging dataset, by comparing our distance with others in a mean shift framework, as well as with the commonly used Hough voting approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126236",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 7,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Lead"
            ],
            "INSPEC: Controlled Indexing": [
                "Hough transforms",
                "image registration",
                "pose estimation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale-invariant 3D shape recognition",
                "scale-invariant 3D shape registration",
                "vote-based 3D shape recognition",
                "vote-based 3D shape registration",
                "direct similarity transform",
                "mean shift framework",
                "Hough voting",
                "pose estimation"
            ]
        },
        "id": 18,
        "cited_by": []
    },
    {
        "title": "Inferring human gaze from appearance via adaptive linear regression",
        "authors": [
            "Feng Lu",
            "Yusuke Sugano",
            "Takahiro Okabe",
            "Yoichi Sato"
        ],
        "abstract": "The problem of estimating human gaze from eye appearance is regarded as mapping high-dimensional features to low-dimensional target space. Conventional methods require densely obtained training samples on the eye appearance manifold, which results in a tedious calibration stage. In this paper, we introduce an adaptive linear regression (ALR) method for accurate mapping via sparsely collected training samples. The key idea is to adaptively find the subset of training samples where the test sample is most linearly representable. We solve the problem via l 1 -optimization and thoroughly study the key issues to seek for the best solution for regression. The proposed gaze estimation approach based on ALR is naturally sparse and low-dimensional, giving the ability to infer human gaze from variant resolution eye images using much fewer training samples than existing methods. Especially, the optimization procedure in ALR is extended to solve the subpixel alignment problem simultaneously for low resolution test eye images. Performance of the proposed method is evaluated by extensive experiments against various factors such as number of training samples, feature dimensionality and eye image resolution to verify its effectiveness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126237",
        "reference_list": [
            {
                "year": "2003",
                "id": 18
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 17,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Training",
                "Equations",
                "Image resolution",
                "Estimation",
                "Manifolds",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image resolution",
                "optimisation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "inferring human gaze",
                "adaptive linear regression",
                "human gaze estimation",
                "eye appearance",
                "calibration stage",
                "optimization",
                "eye image resolution"
            ]
        },
        "id": 19,
        "cited_by": [
            {
                "year": "2017",
                "id": 104
            },
            {
                "year": "2017",
                "id": 331
            }
        ]
    },
    {
        "title": "Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance",
        "authors": [
            "Ryan Farrell",
            "Om Oza",
            "Ning Zhang",
            "Vlad I. Morariu",
            "Trevor Darrell",
            "Larry S. Davis"
        ],
        "abstract": "Subordinate-level categorization typically rests on establishing salient distinctions between part-level characteristics of objects, in contrast to basic-level categorization, where the presence or absence of parts is determinative. We develop an approach for subordinate categorization in vision, focusing on an avian domain due to the fine-grained structure of the category taxonomy for this domain. We explore a pose-normalized appearance model based on a volumetric poselet scheme. The variation in shape and appearance properties of these parts across a taxonomy provides the cues needed for subordinate categorization. Training pose detectors requires a relatively large amount of training data per category when done from scratch; using a subordinate-level approach, we exploit a pose classifier trained at the basic-level, and extract part appearance and shape information to build subordinate-level models. Our model associates the underlying image pattern parameters used for detection with corresponding volumetric part location, scale and orientation parameters. These parameters implicitly define a mapping from the image pixels into a pose-normalized appearance space, removing view and pose dependencies, facilitating fine-grained categorization from relatively few training examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126238",
        "reference_list": [
            {
                "year": "2009",
                "id": 37
            },
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2005",
                "id": 37
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2009",
                "id": 68
            },
            {
                "year": "2007",
                "id": 224
            }
        ],
        "citation": {
            "ieee": 85,
            "other": 33,
            "total": 118
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Shape",
                "Birds",
                "Ellipsoids",
                "Visualization",
                "Feature extraction",
                "Taxonomy"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image resolution",
                "information retrieval",
                "object detection",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Birdlets",
                "subordinate-level categorization",
                "volumetric primitives",
                "pose-normalized appearance model",
                "salient distinctions",
                "category taxonomy",
                "volumetric poselet scheme",
                "pose detectors",
                "part appearance extraction",
                "shape information extraction",
                "subordinate-level models",
                "image pixels",
                "computer vision"
            ]
        },
        "id": 20,
        "cited_by": [
            {
                "year": "2017",
                "id": 141
            },
            {
                "year": "2015",
                "id": 109
            },
            {
                "year": "2015",
                "id": 161
            },
            {
                "year": "2015",
                "id": 267
            },
            {
                "year": "2013",
                "id": 1
            },
            {
                "year": "2013",
                "id": 90
            },
            {
                "year": "2013",
                "id": 213
            },
            {
                "year": "2013",
                "id": 231
            },
            {
                "year": "2013",
                "id": 259
            },
            {
                "year": "2013",
                "id": 314
            }
        ]
    },
    {
        "title": "Distributed cosegmentation via submodular optimization on anisotropic diffusion",
        "authors": [
            "Gunhee Kim",
            "Eric P. Xing",
            "Li Fei-Fei",
            "Takeo Kanade"
        ],
        "abstract": "The saliency of regions or objects in an image can be significantly boosted if they recur in multiple images. Leveraging this idea, cosegmentation jointly segments common regions from multiple images. In this paper, we propose CoSand, a distributed cosegmentation approach for a highly variable large-scale image collection. The segmentation task is modeled by temperature maximization on anisotropic heat diffusion, of which the temperature maximization with finite K heat sources corresponds to a K-way segmentation that maximizes the segmentation confidence of every pixel in an image. We show that our method takes advantage of a strong theoretic property in that the temperature under linear anisotropic diffusion is a submodular function; therefore, a greedy algorithm guarantees at least a constant factor approximation to the optimal solution for temperature maximization. Our theoretic result is successfully applied to scalable cosegmentation as well as diversity ranking and single-image segmentation. We evaluate CoSand on MSRC and ImageNet datasets, and show its competence both in competitive performance over previous work, and in much superior scalability.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126239",
        "reference_list": [
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 48,
            "other": 3,
            "total": 51
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Optimization",
                "Heating",
                "Anisotropic magnetoresistance",
                "Approximation algorithms",
                "Temperature",
                "Greedy algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "diffusion",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "submodular optimization",
                "object saliency",
                "distributed image cosegmentation approach",
                "CoSand",
                "large-scale image collection",
                "temperature maximization",
                "heat sources",
                "K-way segmentation",
                "linear anisotropic heat diffusion",
                "single-image segmentation",
                "ImageNet datasets"
            ]
        },
        "id": 21,
        "cited_by": [
            {
                "year": "2017",
                "id": 455
            },
            {
                "year": "2015",
                "id": 66
            },
            {
                "year": "2015",
                "id": 494
            },
            {
                "year": "2013",
                "id": 49
            },
            {
                "year": "2013",
                "id": 161
            },
            {
                "year": "2013",
                "id": 162
            },
            {
                "year": "2013",
                "id": 219
            },
            {
                "year": "2013",
                "id": 424
            }
        ]
    },
    {
        "title": "Segmentation fusion for connectomics",
        "authors": [
            "Amelio V\u00e1zquez-Reina",
            "Michael Gelbart",
            "Daniel Huang",
            "Jeff Lichtman",
            "Eric Miller",
            "Hanspeter Pfister"
        ],
        "abstract": "We address the problem of automatic 3D segmentation of a stack of electron microscopy sections of brain tissue. Unlike previous efforts, where the reconstruction is usually done on a section-to-section basis, or by the agglomerative clustering of 2D segments, we leverage information from the entire volume to obtain a globally optimal 3D segmentation. To do this, we formulate the segmentation as the solution to a fusion problem. We first enumerate multiple possible 2D segmentations for each section in the stack, and a set of 3D links that may connect segments across consecutive sections. We then identify the fusion of segments and links that provide the most globally consistent segmentation of the stack. We show that this two-step approach of pre-enumeration and posterior fusion yields significant advantages and provides state-of-the-art reconstruction results. Finally, as part of this method, we also introduce a robust rotationally-invariant set of features that we use to learn and enumerate the above 2D segmentations. Our features outperform previous connectomic-specific descriptors without relying on a large set of heuristics or manually designed filter banks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126240",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 24,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Neurons",
                "Image segmentation",
                "Brain",
                "Labeling",
                "Joining processes",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "biological tissues",
                "brain",
                "electron microscopy",
                "image fusion",
                "image reconstruction",
                "image segmentation",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "segmentation fusion",
                "automatic 3D segmentation",
                "electron microscopy stack",
                "brain tissue",
                "section-to-section basis",
                "agglomerative clustering",
                "2D segmentation",
                "3D link",
                "segment fusion",
                "two-step approach",
                "state-of-the-art reconstruction",
                "robust rotationally-invariant feature set",
                "connectomic-specific descriptor"
            ]
        },
        "id": 22,
        "cited_by": [
            {
                "year": "2015",
                "id": 35
            },
            {
                "year": "2015",
                "id": 73
            },
            {
                "year": "2013",
                "id": 430
            }
        ]
    },
    {
        "title": "Diffuse reflectance imaging with astronomical applications",
        "authors": [
            "Samuel W. Hasinoff",
            "Anat Levin",
            "Philip R. Goode",
            "William T. Freeman"
        ],
        "abstract": "Diffuse objects generally tell us little about the surrounding lighting, since the radiance they reflect blurs together incident lighting from many directions. In this paper we discuss how occlusion geometry can help invert diffuse reflectance to recover lighting or surface albedo. Self-occlusion in the scene can be regarded as a form of coding, creating high frequencies that improve the conditioning of diffuse light transport. Our analysis builds on a basic observation that diffuse reflectors with sufficiently detailed geometry can fully resolve the incident lighting. Using a Bayesian framework, we propose a novel reconstruction method based on high-resolution photography, taking advantage of visibility changes near occlusion boundaries. We also explore the limits of single-pixel observations as the diffuse reflector (and potentially the lighting) vary over time. Diffuse reflectance imaging is particularly relevant for astronomy applications, where diffuse reflectors arise naturally but the incident lighting and camera position cannot be controlled. To test our approaches, we first study the feasibility of using the moon as a diffuse reflector to observe the earth as seen from space. Next we present a reconstruction of Mars using historical photometry measurements not previously used for this purpose. As our results suggest, diffuse reflectance imaging expands our notion of what can qualify as a camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126241",
        "reference_list": [
            {
                "year": "2005",
                "id": 188
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Moon",
                "Earth",
                "Imaging",
                "Image reconstruction",
                "Geometry",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "astronomical image processing",
                "Earth",
                "image reconstruction",
                "image resolution",
                "Mars",
                "Moon",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "diffuse reflectance imaging",
                "astronomical application",
                "incident lighting",
                "occlusion geometry",
                "surface albedo",
                "diffuse light transport conditioning",
                "Bayesian framework",
                "reconstruction method",
                "high-resolution photography",
                "single-pixel observation",
                "diffuse reflector",
                "moon",
                "earth",
                "Mars reconstruction",
                "historical photometry measurement"
            ]
        },
        "id": 23,
        "cited_by": []
    },
    {
        "title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models",
        "authors": [
            "George Papandreou",
            "Alan L. Yuille"
        ],
        "abstract": "We propose a novel way to induce a random field from an energy function on discrete labels. It amounts to locally injecting noise to the energy potentials, followed by finding the global minimum of the perturbed energy function. The resulting Perturb-and-MAP random fields harness the power of modern discrete energy minimization algorithms, effectively transforming them into efficient random sampling algorithms, thus extending their scope beyond the usual deterministic setting. In this fashion we can enjoy the benefits of a sound probabilistic framework, such as the ability to represent the solution uncertainty or learn model parameters from training data, while completely bypassing costly Markov-chain Monte-Carlo procedures typically associated with discrete label Gibbs Markov random fields (MRFs). We study some interesting theoretical properties of the proposed model in juxtaposition to those of Gibbs MRFs and address the issue of principled design of the perturbation process. We present experimental results in image segmentation and scene labeling that illustrate the new qualitative aspects and the potential of the proposed model for practical computer vision applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126242",
        "reference_list": [
            {
                "year": "2009",
                "id": 298
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 13,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sampling",
                "image segmentation",
                "Markov processes",
                "Monte Carlo methods",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Perturb-and-MAP random field",
                "discrete optimization",
                "energy model",
                "random field",
                "discrete label",
                "energy potential",
                "perturbed energy function",
                "discrete energy minimization algorithm",
                "random sampling algorithm",
                "sound probabilistic framework",
                "model parameter",
                "Markov chain Monte Carlo procedure",
                "discrete label Gibbs Markov random field",
                "Gibbs MRF",
                "perturbation process",
                "image segmentation",
                "scene labeling"
            ]
        },
        "id": 24,
        "cited_by": [
            {
                "year": "2017",
                "id": 458
            },
            {
                "year": "2015",
                "id": 190
            },
            {
                "year": "2013",
                "id": 288
            }
        ]
    },
    {
        "title": "3D reconstruction of a smooth articulated trajectory from a monocular image sequence",
        "authors": [
            "Hyun Soo Park",
            "Yaser Sheikh"
        ],
        "abstract": "An articulated trajectory is defined as a trajectory that remains at a fixed distance with respect to a parent trajectory. In this paper, we present a method to reconstruct an articulated trajectory in three dimensions given the two dimensional projection of the articulated trajectory, the 3D parent trajectory, and the camera pose at each time instant. This is a core challenge in reconstructing the 3D motion of articulated structures such as the human body because endpoints of each limb form articulated trajectories. We simultaneously apply activity-independent spatial and temporal constraints, in the form of fixed 3D distance to the parent trajectory and smooth 3D motion. There exist two solutions that satisfy each instantaneous 2D projection and articulation constraint (a ray intersects a sphere at up to two locations) and we show that resolving this ambiguity by enforcing smoothness is equivalent to solving a binary quadratic programming problem. A geometric analysis of the reconstruction of articulated trajectories is also presented and a measure of the reconstructibility of an articulated trajectory is proposed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126243",
        "reference_list": [
            {
                "year": "2009",
                "id": 240
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 7,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Three dimensional displays",
                "Cameras",
                "Image reconstruction",
                "Equations",
                "Humans",
                "Optical variables measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image motion analysis",
                "image reconstruction",
                "image sequences",
                "quadratic programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D image reconstruction",
                "smooth articulated trajectory",
                "monocular image sequence",
                "dimensional projection",
                "3D parent trajectory",
                "camera pose",
                "human body",
                "limb form articulated trajectories",
                "activity-independent spatial constraints",
                "activity-independent temporal constraints",
                "binary quadratic programming problem",
                "geometric analysis"
            ]
        },
        "id": 25,
        "cited_by": [
            {
                "year": "2015",
                "id": 495
            }
        ]
    },
    {
        "title": "Contextual weighting for vocabulary tree based image retrieval",
        "authors": [
            "Xiaoyu Wang",
            "Ming Yang",
            "Timothee Cour",
            "Shenghuo Zhu",
            "Kai Yu",
            "Tony X. Han"
        ],
        "abstract": "In this paper we address the problem of image retrieval from millions of database images. We improve the vocabulary tree based approach by introducing contextual weighting of local features in both descriptor and spatial domains. Specifically, we propose to incorporate efficient statistics of neighbor descriptors both on the vocabulary tree and in the image spatial domain into the retrieval. These contextual cues substantially enhance the discriminative power of individual local features with very small computational overhead. We have conducted extensive experiments on benchmark datasets, i.e., the UKbench, Holidays, and our new Mobile dataset, which show that our method reaches state-of-the-art performance with much less computation. Furthermore, the proposed method demonstrates excellent scalability in terms of both retrieval accuracy and efficiency on large-scale experiments using 1.26 million images from the ImageNet database as distractors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126244",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 9,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Vocabulary",
                "Indexes",
                "Quantization",
                "Image retrieval",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "visual databases",
                "vocabulary"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contextual weighting",
                "vocabulary tree based image retrieval",
                "database image",
                "spatial domain",
                "descriptor domain",
                "image spatial domain",
                "computational overhead",
                "benchmark dataset",
                "UKbench",
                "Holidays",
                "mobile dataset",
                "ImageNet database"
            ]
        },
        "id": 26,
        "cited_by": [
            {
                "year": "2015",
                "id": 222
            },
            {
                "year": "2013",
                "id": 208
            }
        ]
    },
    {
        "title": "A theory of Coprime Blurred Pairs",
        "authors": [
            "Feng Li",
            "Zijia Li",
            "David Saunders",
            "Jingyi Yu"
        ],
        "abstract": "We present a new Coprime Blurred Pair (CBP) theory that may benefit a number of computer vision applications. A CBP is constructed by blurring the same latent image with two unknown kernels, where the two kernels are co-prime when mapped to bivariate polynomials under the z-transform. We first show that the blurred contents in a CBP are difficult to restore using conventional blind deconvolution methods based on sparsity priors. We therefore introduce a new coprime prior for recovering the latent image in a CBP. Our solution maps the CBP to bivariate polynomials and sample them on the unit circle in both dimension. We show that coprimality can be derived in terms of the rank of the B\u00e9zout Matrix [2] formed by the sampled polynomials and we present an efficient algorithm to factor the B\u00e9zout Matrix for recovering the latent image. Finally, we discuss applications of the CBP theory in privacy-preserving surveillance and motion deblurring, as well as physical implementations of CBPs using flutter shutter cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126245",
        "reference_list": [
            {
                "year": "2009",
                "id": 210
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Polynomials",
                "Estimation",
                "Streaming media",
                "Surveillance",
                "Vectors",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "deconvolution",
                "image motion analysis",
                "image restoration",
                "polynomials",
                "sparse matrices",
                "Z transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coprime blurred pairs",
                "CBP theory",
                "computer vision applications",
                "latent image blurring",
                "bivariate polynomials",
                "z-transform",
                "blurred contents",
                "image restoration",
                "conventional blind deconvolution methods",
                "sparsity priors",
                "coprime prior",
                "coprimality",
                "B\u00e9zout Matrix",
                "latent image recovery",
                "privacy-preserving surveillance",
                "motion deblurring",
                "flutter shutter cameras"
            ]
        },
        "id": 27,
        "cited_by": [
            {
                "year": "2015",
                "id": 395
            }
        ]
    },
    {
        "title": "Learning to predict the perceived visual quality of photos",
        "authors": [
            "Ou Wu",
            "Weiming Hu",
            "Jun Gao"
        ],
        "abstract": "Visual quality (VisQ) representation is a fundamental step in the learning of a VisQ prediction model for photos. It not only reflects how we understand VisQ but also determines the label type. Existing studies apply a scalar value (i.e., a categorical label or a score) to represent VisQ. As VisQ is a subjective property, only a scalar value is insufficient to represent human's perceived VisQ of a photo. This study represents VisQ by a distribution on pre-defined ordinal basic ratings in order to capture the subjectivity of VisQ better. When using the new representation, the label type is structural instead of scalar. Conventional learning algorithms cannot be directly applied in model learning. Meanwhile, for many photos, the numbers of users involved in the evaluation are limited, making some labels unreliable. In this study, a new algorithm called support vector distribution regression (SVDR) is presented to deal with the structural output learning. Two independent learning strategies (reliability-sensitive learning and label refinement) are proposed to alleviate the difficulty of insufficient involved users for rating. Combining SVDR with the two learning strategies, two separate structural-output regression algorithms (i.e., reliability-sensitive SVDR and label refinement-based SVDR) are produced. Experimental results demonstrate the effectiveness of our introduced learning strategies and learning algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126246",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 10,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Reliability",
                "Vectors",
                "Predictive models",
                "Training",
                "Prediction algorithms",
                "Correlation",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "learning (artificial intelligence)",
                "regression analysis",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual quality representation",
                "VisQ prediction model",
                "scalar value",
                "categorical label",
                "human perceived VisQ",
                "photos",
                "predefined ordinal basic ratings",
                "support vector distribution regression",
                "structural output learning",
                "structural-output regression algorithms",
                "reliability-sensitive SVDR",
                "label refinement-based algorithms"
            ]
        },
        "id": 28,
        "cited_by": []
    },
    {
        "title": "Salient Object Detection using concavity context",
        "authors": [
            "Yao Lu",
            "Wei Zhang",
            "Hong Lu",
            "Xiangyang Xue"
        ],
        "abstract": "Convexity (concavity) is a bottom-up cue to assign figure-ground relation in the perceptual organization [18]. It suggests that region on the convex side of a curved boundary tend to be figural. To explore the validity of this cue in the task of salient object detection, we segment the images in a test dataset into superpixels, and then locate the concave arcs and their bounding boxes along boundary of superpixels. Ecological statistics indicate that such bounding box contains salient object with a large probability. To utilize this spatial context information, i.e. concavity context, we follow the multi-scale analysis of human visual perception and design a hierarchical model. The model yields an affinity graph over candidate superpixels, in which weights between vertices are determined by the summation of concavity context on different scales in the hierarchy. Finally a graph-cut algorithm is performed to separate the salient and background objects. Evaluation on MSRA Salient Object Detection (SOD) dataset shows that concavity context is effective, and our approach provides improvement over state-of-the-art feature-based algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126247",
        "reference_list": [
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Context",
                "Object detection",
                "Image segmentation",
                "Context modeling",
                "Feature extraction",
                "Computational modeling",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "object detection",
                "statistics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph-cut algorithm",
                "human visual perception",
                "ecological statistics",
                "image segmentation",
                "perceptual organization",
                "figure-ground relation",
                "concavity context",
                "salient object detection"
            ]
        },
        "id": 29,
        "cited_by": [
            {
                "year": "2015",
                "id": 24
            },
            {
                "year": "2013",
                "id": 207
            },
            {
                "year": "2013",
                "id": 415
            }
        ]
    },
    {
        "title": "Learning universal multi-view age estimator using video context",
        "authors": [
            "Zheng Song",
            "Bingbing Ni",
            "Dong Guo",
            "Terence Sim",
            "Shuicheng Yan"
        ],
        "abstract": "Many existing techniques for analyzing face images assume that the faces are at nearly frontal. Generalizing to non-frontal faces is often difficult, due to a dearth of ground truth for non-frontal faces and also to the inherent challenges in handling pose variations. In this work, we investigate how to learn a universal multi-view age estimator by harnessing 1) unlabeled web videos, 2) a publicly available labeled frontal face corpus, and 3) zero or more non-frontal faces with age labels. First, a large diverse human-involved video corpus is collected from online video sharing website. Then, multi-view face detection and tracking are performed to build a large set of frontal-vs-profile face bundles, each of which is from the same tracking sequence, and thus exhibiting the same age. These unlabeled face bundles constitute the so-called video context, and the parametric multi-view age estimator is trained by 1) enforcing the face-to-age relation for the partially labeled faces, 2) imposing the consistency of the predicted ages for the non-frontal and frontal faces within each face bundle, and 3) mutually constraining the multi-view age models with the spatial correspondence priors derived from the face bundles. Our multi-view age estimator performs well on a realistic evaluation dataset that contains faces under varying poses, and whose ground truth age was manually annotated.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126248",
        "reference_list": [
            {
                "year": "2009",
                "id": 40
            },
            {
                "year": "2009",
                "id": 255
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 9,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Feature extraction",
                "Estimation",
                "Context",
                "Face detection",
                "Databases",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image sequences",
                "Internet",
                "learning (artificial intelligence)",
                "pose estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "universal multiview age estimator learning",
                "video context",
                "face image",
                "nonfrontal face",
                "pose variation handling",
                "unlabeled Web video",
                "human-involved video corpus",
                "multiview face detection",
                "multiview face tracking",
                "offrontal-vs-profileface bundle",
                "tracking sequence"
            ]
        },
        "id": 30,
        "cited_by": []
    },
    {
        "title": "Learning nonlinear distance functions using neural network for regression with application to robust human age estimation",
        "authors": [
            "Na Fan"
        ],
        "abstract": "In this paper, a robust regression method is proposed for human age estimation, in which, outlier samples are corrected by their neighbors, through asymptotically increasing the correlation coefficients between the desired distances and the distances of sample labels. As another extension, we adopt a nonlinear distance function and approximate it by neural network. For fair comparison, we also experiment on the regression problem of age estimation from face images, and the results are very competitive among the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126249",
        "reference_list": [
            {
                "year": "2007",
                "id": 210
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial neural networks",
                "Measurement",
                "Humans",
                "Estimation",
                "Semantics",
                "Training",
                "Face"
            ],
            "INSPEC: Controlled Indexing": [
                "age issues",
                "approximation theory",
                "computer vision",
                "face recognition",
                "learning (artificial intelligence)",
                "neural nets",
                "nonlinear functions",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonlinear distance function learning",
                "neural network",
                "robust regression method",
                "robust human age estimation",
                "correlation coefficients",
                "face images",
                "distance metric learning",
                "computer vision problems"
            ]
        },
        "id": 31,
        "cited_by": []
    },
    {
        "title": "Generalized roof duality for pseudo-boolean optimization",
        "authors": [
            "Fredrik Kahl",
            "Petter Strandmark"
        ],
        "abstract": "The number of applications in computer vision that model higher-order interactions has exploded over the last few years. The standard technique for solving such problems is to reduce the higher-order objective function to a quadratic pseudo-boolean function, and then use roof duality for obtaining a lower bound. Roof duality works by constructing the tightest possible lower-bounding submodular function, and instead of optimizing the original objective function, the relaxation is minimized. We generalize this idea to polynomials of higher degree, where quadratic roof duality appears as a special case. Optimal relaxations are defined to be the ones that give the maximum lower bound. We demonstrate that important properties such as persistency still hold and how the relaxations can be efficiently constructed for general cubic and quartic pseudo-boolean functions. From a practical point of view, we show that our relaxations perform better than state-of-the-art for a wide range of problems, both in terms of lower bounds and in the number of assigned variables.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126250",
        "reference_list": [
            {
                "year": "2009",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 9,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Tin",
                "Optimization",
                "Computational modeling",
                "Minimization",
                "Computer vision",
                "Stereo vision"
            ],
            "INSPEC: Controlled Indexing": [
                "Boolean functions",
                "computer vision",
                "duality (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generalized roof duality",
                "pseudoBoolean optimization",
                "computer vision",
                "higher-order interactions",
                "quadratic pseudoBoolean function",
                "lower-bounding submodular function",
                "cubic pseudoBoolean function"
            ]
        },
        "id": 32,
        "cited_by": []
    },
    {
        "title": "Struck: Structured output tracking with kernels",
        "authors": [
            "Sam Hare",
            "Amir Saffari",
            "Philip H. S. Torr"
        ],
        "abstract": "Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (accurate estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we are able to avoid the need for an intermediate classification step. Our method uses a kernelized structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow for real-time application, we introduce a budgeting mechanism which prevents the unbounded growth in the number of support vectors which would otherwise occur during tracking. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126251",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2003",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 683,
            "other": 357,
            "total": 1040
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Training",
                "Target tracking",
                "Kernel",
                "Robustness",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "learning (artificial intelligence)",
                "object tracking",
                "pose estimation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "adaptive tracking-by-detection method",
                "structured output tracking",
                "arbitrary object tracking",
                "classification task",
                "online learning technique",
                "object position estimation",
                "adaptive visual object tracking",
                "structured output prediction",
                "intermediate classification step",
                "kernelized structured output support vector machine",
                "real-time application",
                "state-of-the-art tracker"
            ]
        },
        "id": 33,
        "cited_by": [
            {
                "year": "2017",
                "id": 11
            },
            {
                "year": "2017",
                "id": 119
            },
            {
                "year": "2017",
                "id": 120
            },
            {
                "year": "2017",
                "id": 350
            },
            {
                "year": "2017",
                "id": 508
            },
            {
                "year": "2015",
                "id": 335
            },
            {
                "year": "2015",
                "id": 336
            },
            {
                "year": "2015",
                "id": 339
            },
            {
                "year": "2015",
                "id": 341
            },
            {
                "year": "2015",
                "id": 342
            },
            {
                "year": "2015",
                "id": 343
            },
            {
                "year": "2015",
                "id": 345
            },
            {
                "year": "2015",
                "id": 346
            },
            {
                "year": "2015",
                "id": 348
            },
            {
                "year": "2015",
                "id": 349
            },
            {
                "year": "2015",
                "id": 353
            },
            {
                "year": "2015",
                "id": 354
            },
            {
                "year": "2015",
                "id": 480
            },
            {
                "year": "2015",
                "id": 481
            },
            {
                "year": "2015",
                "id": 491
            },
            {
                "year": "2015",
                "id": 525
            },
            {
                "year": "2013",
                "id": 29
            },
            {
                "year": "2013",
                "id": 81
            },
            {
                "year": "2013",
                "id": 138
            },
            {
                "year": "2013",
                "id": 139
            },
            {
                "year": "2013",
                "id": 254
            },
            {
                "year": "2013",
                "id": 309
            },
            {
                "year": "2013",
                "id": 347
            }
        ]
    },
    {
        "title": "Simplification of 3D morphable models",
        "authors": [
            "Ankur Patel",
            "William A. P. Smith"
        ],
        "abstract": "In this paper we show how to simplify a 3D morphable model. Our method only requires knowledge of the original highest resolution statistical model and leads to low resolution models in which the model statistics are a subset of the original high resolution model. We employ an iterative edge collapse strategy, where the deleted edge is chosen as a function of the model statistics. We show that the expected value of the Quadric Error Metric can be computed in closed form for a PCA deformable model. Model parameters obtained using the model at any resolution (lower) can be used to reconstruct a high resolution surface, providing a route to super-resolution. We provide experimental results for a statistical face model, showing how the simplified models improve the efficiency of model fitting. We are able to decrease the model resolution and fitting time by factors of approximately 10 and 4 respectively whilst inducing an error which is only slightly larger than the fitting error of the original model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126252",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Shape",
                "Solid modeling",
                "Data models",
                "Vectors",
                "Mathematical model",
                "Three dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "iterative methods",
                "principal component analysis",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D morphable model simplification",
                "highest resolution statistical model",
                "low resolution model",
                "iterative edge collapse strategy",
                "quadric error metric",
                "PCA deformable model",
                "super-resolution",
                "statistical face model"
            ]
        },
        "id": 34,
        "cited_by": []
    },
    {
        "title": "A 3D Laplacian-driven parametric deformable model",
        "authors": [
            "Tian Shen",
            "Xiaolei Huang",
            "Hongsheng Li",
            "Edward Kim",
            "Shaoting Zhang",
            "Junzhou Huang"
        ],
        "abstract": "3D parametric deformable models have been used to extract volumetric object boundaries and they generate smooth boundary surfaces as results. However, in some segmentation cases, such as cerebral cortex with complex folds and creases, and human lung with high curvature boundary, parametric deformable models often suffer from over-smoothing or decreased mesh quality during model deformation. To address this problem, we propose a 3D Laplacian-driven parametric deformable model with a new internal force. Derived from a Mesh Laplacian, the internal force exerted on each control vertex can be decomposed into two orthogonal vectors based on the vertex's tangential plane. We then introduce a weighting function to control the contributions of the two vectors based on the model mesh's geometry. Deforming the new model is solving a linear system, so the new model can converge very efficiently. To validate the model's performance, we tested our method on various segmentation cases and compared our model with Finite Element and Level Set deformable models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126253",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 5,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Force",
                "Vectors",
                "Three dimensional displays",
                "Finite element methods",
                "Brain modeling",
                "Laplace equations"
            ],
            "INSPEC: Controlled Indexing": [
                "finite element analysis",
                "object detection",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D Laplacian-driven parametric deformable model",
                "volumetric object boundaries extraction",
                "smooth boundary surfaces",
                "mesh Laplacian",
                "finite element model",
                "level set deformable models"
            ]
        },
        "id": 35,
        "cited_by": []
    },
    {
        "title": "Video from a single coded exposure photograph using a learned over-complete dictionary",
        "authors": [
            "Yasunobu Hitomi",
            "Jinwei Gu",
            "Mohit Gupta",
            "Tomoo Mitsunaga",
            "Shree K. Nayar"
        ],
        "abstract": "Cameras face a fundamental tradeoff between the spatial and temporal resolution - digital still cameras can capture images with high spatial resolution, but most high-speed video cameras suffer from low spatial resolution. It is hard to overcome this tradeoff without incurring a significant increase in hardware costs. In this paper, we propose techniques for sampling, representing and reconstructing the space-time volume in order to overcome this tradeoff. Our approach has two important distinctions compared to previous works: (1) we achieve sparse representation of videos by learning an over-complete dictionary on video patches, and (2) we adhere to practical constraints on sampling scheme which is imposed by architectures of present image sensor devices. Consequently, our sampling scheme can be implemented on image sensors by making a straightforward modification to the control unit. To demonstrate the power of our approach, we have implemented a prototype imaging system with per-pixel coded exposure control using a liquid crystal on silicon (LCoS) device. Using both simulations and experiments on a wide range of scenes, we show that our method can effectively reconstruct a video from a single image maintaining high spatial resolution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126254",
        "reference_list": [
            {
                "year": "2009",
                "id": 292
            }
        ],
        "citation": {
            "ieee": 55,
            "other": 17,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Image reconstruction",
                "Spatial resolution",
                "Cameras",
                "Sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "dictionaries",
                "image representation",
                "image resolution",
                "image sampling",
                "learning (artificial intelligence)",
                "video cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single coded exposure photograph",
                "learned over-complete dictionary",
                "camera face",
                "temporal resolution",
                "spatial resolution",
                "digital still camera",
                "high-speed video camera",
                "space-time volume sampling",
                "space-time volume representation",
                "space-time volume reconstruction",
                "sparse representation",
                "image sensor device",
                "prototype imaging system",
                "per-pixel coded exposure control",
                "liquid crystal on silicon device"
            ]
        },
        "id": 36,
        "cited_by": [
            {
                "year": "2017",
                "id": 12
            },
            {
                "year": "2013",
                "id": 125
            },
            {
                "year": "2013",
                "id": 409
            }
        ]
    },
    {
        "title": "Generalized subgraph preconditioners for large-scale bundle adjustment",
        "authors": [
            "Yong-Dian Jian",
            "Doru C. Balcan",
            "Frank Dellaert"
        ],
        "abstract": "We present a generalized subgraph preconditioning (GSP) technique to solve large-scale bundle adjustment problems efficiently. In contrast with previous work which uses either direct or iterative methods as the linear solver, GSP combines their advantages and is significantly faster on large datasets. Similar to [11], the main idea is to identify a sub-problem (subgraph) that can be solved efficiently by sparse factorization methods and use it to build a preconditioner for the conjugate gradient method. The difference is that GSP is more general and leads to much more effective preconditioners. We design a greedy algorithm to build subgraphs which have bounded maximum clique size in the factorization phase, and also result in smaller condition numbers than standard preconditioning techniques. When applying the proposed method to the \u201cbal\u201d datasets [1], GSP displays promising performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126255",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2007",
                "id": 245
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Jacobian matrices",
                "Cameras",
                "Symmetric matrices",
                "Iterative methods",
                "Linear systems",
                "Gradient methods",
                "Uncertainty"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "graph theory",
                "greedy algorithms",
                "iterative methods",
                "sparse matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generalized subgraph preconditioning technique",
                "large-scale bundle adjustment",
                "iterative methods",
                "linear solver",
                "sparse factorization methods",
                "conjugate gradient method",
                "greedy algorithm",
                "bounded maximum clique size"
            ]
        },
        "id": 37,
        "cited_by": []
    },
    {
        "title": "Minimum near-convex decomposition for robust shape representation",
        "authors": [
            "Zhou Ren",
            "Junsong Yuan",
            "Chunyuan Li",
            "Wenyu Liu"
        ],
        "abstract": "Shape decomposition is a fundamental problem for part-based shape representation. We propose a novel shape decomposition method called Minimum Near-Convex Decomposition (MNCD), which decomposes 2D and 3D arbitrary shapes into minimum number of \u201cnear-convex\u201d parts. With the degree of near-convexity a user specified parameter, our decomposition is robust to large local distortions and shape deformation. The shape decomposition is formulated as a combinatorial optimization problem by minimizing the number of non-intersection cuts. Two major perception rules are also imposed into our scheme to improve the visual naturalness of the decomposition. The global optimal solution of this challenging discrete optimization problem is obtained by a dynamic subgradient-based branch-and-bound search. Both theoretical analysis and experiment results show that our approach outperforms the state-of-the-art results without introducing redundant parts. Finally we also show the superiority of our method in the application of hand gesture recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126256",
        "reference_list": [
            {
                "year": "2007",
                "id": 174
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 4,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Visualization",
                "Robustness",
                "Optimization",
                "Transform coding",
                "Heuristic algorithms",
                "Search problems"
            ],
            "INSPEC: Controlled Indexing": [
                "combinatorial mathematics",
                "deformation",
                "gesture recognition",
                "optimisation",
                "shape recognition",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "minimum near-convex decomposition",
                "shape decomposition",
                "part-based shape representation",
                "2D arbitrary shape",
                "3D arbitrary shape",
                "near-convexity",
                "local distortion",
                "shape deformation",
                "combinatorial optimization",
                "nonintersection cuts",
                "perception rule",
                "visual naturalness",
                "discrete optimization",
                "dynamic subgradient-based branch-and-bound search",
                "hand gesture recognition"
            ]
        },
        "id": 38,
        "cited_by": [
            {
                "year": "2013",
                "id": 108
            }
        ]
    },
    {
        "title": "Visual word disambiguation by semantic contexts",
        "authors": [
            "Yu Su",
            "Fr\u00e9d\u00e9ric Jurie"
        ],
        "abstract": "This paper presents a novel schema to address the polysemy of visual words in the widely used bag-of-words model. As a visual word may have multiple meanings, we show it is possible to use semantic contexts to disambiguate these meanings and therefore improve the performance of bag-of-words model. On one hand, for an image, multiple context-specific bag-of-words histograms are constructed, each of which corresponds to a semantic context. Then these histograms are merged by selecting only the most discriminative context for each visual word, resulting in a compact image representation. On the other hand, an image is represented by the occurrence probabilities of semantic contexts. Finally, when classifying an image, two image representations are combined at decision level to utilize the complementary information embedded in them. Experiments on three challenging image databases (PASCAL VOC 2007, Scene-15 and MSRCv2) show that our method significantly outperforms state-of-the-art classification methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126257",
        "reference_list": [
            {
                "year": "2009",
                "id": 30
            },
            {
                "year": "2009",
                "id": 125
            },
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2005",
                "id": 235
            },
            {
                "year": "2009",
                "id": 55
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 18,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Context",
                "Rain"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "natural languages",
                "probability",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual word disambiguation",
                "semantic contexts",
                "bag-of-words model",
                "performance improvement",
                "bag-of-words histograms",
                "compact image representation",
                "occurrence probabilities",
                "PASCAL VOC 2007 image database",
                "Scene-15 image database",
                "MSRCv2 image database",
                "classification methods"
            ]
        },
        "id": 39,
        "cited_by": [
            {
                "year": "2013",
                "id": 424
            }
        ]
    },
    {
        "title": "Viewpoint invariant 3D landmark model inference from monocular 2D images using higher-order priors",
        "authors": [
            "Chaohui Wang",
            "Yun Zeng",
            "Loic Simon",
            "Ioannis Kakadiaris",
            "Dimitris Samaras",
            "Nikos Paragios"
        ],
        "abstract": "In this paper, we propose a novel one-shot optimization approach to simultaneously determine both the optimal 3D landmark model and the corresponding 2D projections without explicit estimation of the camera viewpoint, which is also able to deal with misdetections as well as partial occlusions. To this end, a 3D shape manifold is built upon fourth-order interactions of landmarks from a training set where pose-invariant statistics are obtained in this space. The 3D-2D consistency is also encoded in such high-order interactions, which eliminate the necessity of viewpoint estimation. Furthermore, the modeling of visibility improves further the performance of the method by handling missing correspondences and occlusions. The inference is addressed through a MAP formulation which is naturally transformed into a higher-order MRF optimization problem and is solved using a dual-decomposition-based method. Promising results on standard face benchmarks demonstrate the potential of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126258",
        "reference_list": [
            {
                "year": "2007",
                "id": 51
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Solid modeling",
                "Estimation",
                "Cameras",
                "Computational modeling",
                "Shape",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "optimisation",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "viewpoint invariant 3D landmark model inference",
                "monocular 2D images",
                "one-shot optimization approach",
                "2D projections",
                "camera viewpoint",
                "3D shape manifold",
                "fourth-order interactions",
                "pose-invariant statistics",
                "3D-2D consistency",
                "higher-order interactions",
                "MAP formulation",
                "higher-order MRF optimization problem",
                "dual-decomposition-based method"
            ]
        },
        "id": 40,
        "cited_by": [
            {
                "year": "2015",
                "id": 412
            }
        ]
    },
    {
        "title": "Object recoloring based on intrinsic image estimation",
        "authors": [
            "Shida Beigpour",
            "Joost van de Weijer"
        ],
        "abstract": "Object recoloring is one of the most popular photo-editing tasks. The problem of object recoloring is highly under-constrained, and existing recoloring methods limit their application to objects lit by a white illuminant. Application of these methods to real-world scenes lit by colored illuminants, multiple illuminants, or interreflections, results in unrealistic recoloring of objects. In this paper, we focus on the recoloring of single-colored objects presegmented from their background. The single-color constraint allows us to fit a more comprehensive physical model to the object. We demonstrate that this permits us to perform realistic recoloring of objects lit by non-white illuminants, and multiple illuminants. Moreover, the model allows for more realistic handling of illuminant alteration of the scene. Recoloring results captured by uncalibrated cameras demonstrate that the proposed framework obtains realistic recoloring for complex natural images. Furthermore we use the model to transfer color between objects and show that the results are more realistic than existing color transfer methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126259",
        "reference_list": [
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 7,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Estimation",
                "Lighting",
                "Image segmentation",
                "Robustness",
                "Mathematical model",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image colour analysis",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "intrinsic image estimation",
                "photo editing task",
                "colored illuminant",
                "unrealistic recoloring",
                "single-colored object recoloring method",
                "single color constraint",
                "realistic object recoloring",
                "uncalibrated camera",
                "natural image",
                "color transfer method"
            ]
        },
        "id": 41,
        "cited_by": []
    },
    {
        "title": "Understanding scenes on many levels",
        "authors": [
            "Joseph Tighe",
            "Svetlana Lazebnik"
        ],
        "abstract": "This paper presents a framework for image parsing with multiple label sets. For example, we may want to simultaneously label every image region according to its basic-level object category (car, building, road, tree, etc.), superordinate category (animal, vehicle, manmade object, natural object, etc.), geometric orientation (horizontal, vertical, etc.), and material (metal, glass, wood, etc.). Some object regions may also be given part names (a car can have wheels, doors, windshield, etc.). We compute co-occurrence statistics between different label types of the same region to capture relationships such as \u201croads are horizontal,\u201d \u201ccars are made of metal,\u201d \u201ccars have wheels\u201d but \u201chorses have legs,\u201d and so on. By incorporating these constraints into a Markov Random Field inference framework and jointly solving for all the label sets, we are able to improve the classification accuracy for all the label sets at once, achieving a richer form of image understanding.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126260",
        "reference_list": [
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2007",
                "id": 145
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 9,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Wheels",
                "Labeling",
                "Vehicles",
                "Materials",
                "Animals",
                "Buildings",
                "Metals"
            ],
            "INSPEC: Controlled Indexing": [
                "category theory",
                "image classification",
                "inference mechanisms",
                "Markov processes",
                "set theory",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene understanding",
                "image parsing",
                "multiple label sets",
                "image region",
                "basic-level object category",
                "superordinate category",
                "geometric orientation",
                "object regions",
                "cooccurrence statistics",
                "Markov random field inference framework",
                "classification accuracy",
                "image understanding"
            ]
        },
        "id": 42,
        "cited_by": [
            {
                "year": "2013",
                "id": 104
            }
        ]
    },
    {
        "title": "Weakly supervised object detector learning with model drift detection",
        "authors": [
            "Parthipan Siva",
            "Tao Xiang"
        ],
        "abstract": "A conventional approach to learning object detectors uses fully supervised learning techniques which assumes that a training image set with manual annotation of object bounding boxes are provided. The manual annotation of objects in large image sets is tedious and unreliable. Therefore, a weakly supervised learning approach is desirable, where the training set needs only binary labels regarding whether an image contains the target object class. In the weakly supervised approach a detector is used to iteratively annotate the training set and learn the object model. We present a novel weakly supervised learning framework for learning an object detector. Our framework incorporates a new initial annotation model to start the iterative learning of a detector and a model drift detection method that is able to detect and stop the iterative learning when the detector starts to drift away from the objects of interest. We demonstrate the effectiveness of our approach on the challenging PASCAL 2007 dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126261",
        "reference_list": [
            {
                "year": "2009",
                "id": 247
            }
        ],
        "citation": {
            "ieee": 59,
            "other": 17,
            "total": 76
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Training",
                "Measurement",
                "Histograms",
                "Adaptation models",
                "Object detection",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "iterative methods",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "supervised object detector learning",
                "model drift detection",
                "image sets",
                "supervised learning approach",
                "iterative learning",
                "model drift detection method",
                "PASCAL 2007 dataset"
            ]
        },
        "id": 43,
        "cited_by": [
            {
                "year": "2017",
                "id": 356
            },
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2015",
                "id": 66
            },
            {
                "year": "2015",
                "id": 136
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2013",
                "id": 372
            }
        ]
    },
    {
        "title": "Multi-view 3D reconstruction for scenes under the refractive plane with known vertical direction",
        "authors": [
            "Yao-Jen Chang",
            "Tsuhan Chen"
        ],
        "abstract": "Images taken from scenes under water suffer distortion due to refraction. While refraction causes magnification with mild distortion on the observed images, severe distortions in geometry reconstruction would be resulted if the refractive distortion is not properly handled. Different from the radial distortion model, the refractive distortion depends on the scene depth seen from each light ray as well as the camera pose relative to the refractive surface. Therefore, it's crucial to obtain a good estimate of scene depth, camera pose and optical center to alleviate the impact of refractive distortion. In this work, we formulate the forward and back projections of light rays involving a refractive plane for the perspective camera model by explicitly modeling refractive distortion as a function of depth. Furthermore, for cameras with an inertial measurement unit (IMU), we show that a linear solution to the relative pose and a closed-form solution to the absolute pose can be derived with known camera vertical directions. We incorporate our formulations with the general structure from motion framework followed by the patch-based multiview stereo algorithm to obtain a 3D reconstruction of the scene. We show through experiments that the explicit modeling of depth-dependent refractive distortion physically leads to more accurate scene reconstructions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126262",
        "reference_list": [
            {
                "year": "2003",
                "id": 135
            },
            {
                "year": "2009",
                "id": 234
            },
            {
                "year": "2005",
                "id": 205
            },
            {
                "year": "2009",
                "id": 296
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 12,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three dimensional displays",
                "Image reconstruction",
                "Estimation",
                "Surface reconstruction",
                "Optical imaging",
                "Optical refraction"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "natural scenes",
                "pose estimation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview 3D reconstruction",
                "refractive plane",
                "image distortion",
                "geometry reconstruction",
                "refractive distortion",
                "radial distortion model",
                "scene depth",
                "camera pose",
                "optical center",
                "perspective camera model",
                "inertial measurement unit",
                "closed-form solution",
                "camera vertical direction",
                "patch-based multiview stereo algorithm",
                "depth-dependent refractive distortion",
                "scene reconstruction"
            ]
        },
        "id": 44,
        "cited_by": [
            {
                "year": "2013",
                "id": 7
            }
        ]
    },
    {
        "title": "Edge foci interest points",
        "authors": [
            "C. Lawrence Zitnick",
            "Krishnan Ramnath"
        ],
        "abstract": "In this paper, we describe an interest point detector using edge foci. Unlike traditional detectors that compute interest points directly from image intensities, we use normalized intensity edges and their orientations. We hypothesize that detectors based on the presence of oriented edges are more robust to non-linear lighting variations and background clutter than intensity based techniques. Specifically, we detect edge foci, which are points in the image that are roughly equidistant from edges with orientations perpendicular to the point. The scale of the interest point is defined by the distance between the edge foci and the edges. We quantify the performance of our detector using the interest point's repeatability, uniformity of spatial distribution, and the uniqueness of the resulting descriptors. Results are found using traditional datasets and new datasets with challenging non-linear lighting variations and occlusions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126263",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 17,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Detectors",
                "Laplace equations",
                "Entropy",
                "Lighting",
                "Feature extraction",
                "Clutter"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "edge detection",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interest point detector",
                "edge foci",
                "normalized intensity edge",
                "nonlinear lighting variation",
                "background clutter",
                "spatial distribution",
                "occlusion"
            ]
        },
        "id": 45,
        "cited_by": []
    },
    {
        "title": "Segmentation from a box",
        "authors": [
            "Leo Grady",
            "Marie-Pierre Jolly",
            "Aaron Seitz"
        ],
        "abstract": "Drawing a box around an intended segmentation target has become both a popular user interface and a common output for learning-driven detection algorithms. Despite the ubiquity of using a box to define a segmentation target, it is unclear in the literature whether a box is sufficient to define a unique segmentation or whether segmentation from a box is ill-posed without higher-level (semantic) knowledge of the intended target. We examine this issue by conducting a study of 14 subjects who are asked to segment a boxed target in a set of 50 real images for which they have no semantic attachment. We find that the subjects do indeed perceive and trace almost the same segmentations as each other, despite the inhomogeneity of the image intensities, irregular shapes of the segmentation targets and weakness of the target boundaries. Since the subjects produce the same segmentation, we conclude that the problem is well-posed and then provide a new segmentation algorithm from a box which achieves results close to the perceived target.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126264",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2009",
                "id": 35
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2007",
                "id": 88
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Algorithm design and analysis",
                "Semantics",
                "Training",
                "Probabilistic logic",
                "Ultrasonic imaging",
                "Indexes"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "object detection",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "box",
                "segmentation target",
                "user interface",
                "learning-driven detection algorithm",
                "real image",
                "image intensities",
                "irregular shape",
                "target boundaries"
            ]
        },
        "id": 46,
        "cited_by": []
    },
    {
        "title": "Smooth object retrieval using a bag of boundaries",
        "authors": [
            "Relja Arandjelovi\u0107",
            "Andrew Zisserman"
        ],
        "abstract": "We describe a scalable approach to 3D smooth object retrieval which searches for and localizes all the occurrences of a user outlined object in a dataset of images in real time. The approach is illustrated on sculptures. A smooth object is represented by its material appearance (sufficient for foreground/background segmentation) and imaged shape (using a set of semi-local boundary descriptors). The descriptors are tolerant to scale changes, segmentation failures, and limited viewpoint changes. Furthermore, we show that the descriptors may be vector quantized (into a bag-of-boundaries) giving a representation that is suited to the standard visual word architectures for immediate retrieval of specific objects. We introduce a new dataset of 6K images containing sculptures by Moore and Rodin, and annotated with ground truth for the occurrence of twenty 3D sculptures. It is demonstrated that recognition can proceed successfully de- spite changes in viewpoint, illumination and partial occlusion, and also that instances of the same shape can be retrieved even though they may be made of different materials.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126265",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 15,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Training",
                "Vectors",
                "Materials",
                "Image color analysis",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image retrieval",
                "lighting",
                "vector quantisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bag of boundaries",
                "3D smooth object retrieval",
                "user outlined object",
                "vector quantization",
                "standard visual word architectures",
                "3D sculptures",
                "partial occlusion",
                "illumination",
                "image recognition"
            ]
        },
        "id": 47,
        "cited_by": []
    },
    {
        "title": "A Direct Least-Squares (DLS) method for PnP",
        "authors": [
            "Joel A. Hesch",
            "Stergios I. Roumeliotis"
        ],
        "abstract": "In this work, we present a Direct Least-Squares (DLS) method for computing all solutions of the perspective-n-point camera pose determination (PnP) problem in the general case (n \u2265 3). Specifically, based on the camera measurement equations, we formulate a nonlinear least-squares cost function whose optimality conditions constitute a system of three third-order polynomials. Subsequently, we employ the multiplication matrix to determine all the roots of the system analytically, and hence all minima of the LS, without requiring iterations or an initial guess of the parameters. A key advantage of our method is scalability, since the order of the polynomial system that we solve is independent of the number of points. We compare the performance of our algorithm with the leading PnP approaches, both in simulation and experimentally, and demonstrate that DLS consistently achieves accuracy close to the Maximum-Likelihood Estimator (MLE).",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126266",
        "reference_list": [],
        "citation": {
            "ieee": 54,
            "other": 35,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Mathematical model",
                "Polynomials",
                "Cameras",
                "Cost function",
                "Noise measurement",
                "Matrix decomposition"
            ],
            "INSPEC: Controlled Indexing": [
                "least squares approximations",
                "matrix algebra",
                "maximum likelihood estimation",
                "polynomial approximation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "direct least-squares method",
                "perspective-n-point camera pose determination problem",
                "camera measurement equations",
                "nonlinear least-squares cost function",
                "third-order polynomials",
                "optimality conditions",
                "multiplication matrix",
                "PnP approaches",
                "maximum-likelihood estimator"
            ]
        },
        "id": 48,
        "cited_by": [
            {
                "year": "2017",
                "id": 0
            },
            {
                "year": "2013",
                "id": 292
            }
        ]
    },
    {
        "title": "Sorted Random Projections for robust texture classification",
        "authors": [
            "Li Liu",
            "Paul Fieguth",
            "Gangyao Kuang",
            "Hongbin Zha"
        ],
        "abstract": "This paper presents a simple and highly effective system for robust texture classification, based on (1) random local features, (2) a simple global Bag-of-Words (BoW) representation, and (3) Support Vector Machines (SVMs) based classification. The key contribution in this work is to apply a sorting strategy to a universal yet information-preserving random projection (RP) technique, then comparing two different texture image representations (histograms and signatures) with various kernels in the SVMs. We have tested our texture classification system on six popular and challenging texture databases for exemplar based texture classification, comparing with 12 recent state-of-the-art methods. Experimental results show that our texture classification system yields the best classification rates of which we are aware of 99.37% for CUReT, 97.16% for Brodatz, 99.30% for UMD and 99.29% for KTH-TIPS. Moreover, combining random features significantly outperforms the state-of-the-art descriptors in material categorization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126267",
        "reference_list": [
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 30,
            "other": 12,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Feature extraction",
                "Training",
                "Histograms",
                "Vectors",
                "Sorting",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "image texture",
                "sorting",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sorted random projection",
                "robust texture classification",
                "random local feature",
                "global bag-of-words representation",
                "support vector machines",
                "sorting strategy",
                "information-preserving random projection technique",
                "texture image representation",
                "histograms",
                "signatures",
                "SVM",
                "exemplar based texture classification",
                "CUReT",
                "Brodatz",
                "UMD",
                "KTH-TIPS"
            ]
        },
        "id": 49,
        "cited_by": [
            {
                "year": "2017",
                "id": 516
            }
        ]
    },
    {
        "title": "An adversarial optimization approach to efficient outlier removal",
        "authors": [
            "Jin Yu",
            "Anders Eriksson",
            "Tat-Jun Chin",
            "David Suter"
        ],
        "abstract": "This paper proposes a novel adversarial optimization approach to efficient outlier removal in computer vision. We characterize the outlier removal problem as a game that involves two players of conflicting interests, namely, optimizer and outlier. Such an adversarial view not only brings new insights into various existing methods, but also gives rise to a general optimization framework that provably unifies them. Under the proposed framework, we develop a new outlier removal approach that is able to offer a much needed control over the trade-off between reliability and speed, which is otherwise not available in previous methods. The proposed approach is driven by a mixed-integer minmax (convex-concave) optimization process. Although a minmax problem is generally not amenable to efficient optimization, we show that for some commonly used vision objective functions, an equivalent Linear Program reformulation exists. We demonstrate our method on two representative multiview geometry problems. Experiments on real image data illustrate superior practical performance of our method over recent techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126268",
        "reference_list": [
            {
                "year": "2009",
                "id": 166
            },
            {
                "year": "2005",
                "id": 128
            },
            {
                "year": "2009",
                "id": 137
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Vectors",
                "Games",
                "Cost function",
                "Estimation",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "integer programming",
                "linear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adversarial optimization approach",
                "outlier removal",
                "computer vision",
                "mixed-integer minmax optimization process",
                "minmax problem",
                "vision objective function",
                "equivalent linear program reformulation",
                "representative multiview geometry problem",
                "real image data"
            ]
        },
        "id": 50,
        "cited_by": [
            {
                "year": "2017",
                "id": 0
            }
        ]
    },
    {
        "title": "Understanding egocentric activities",
        "authors": [
            "Alireza Fathi",
            "Ali Farhadi",
            "James M. Rehg"
        ],
        "abstract": "We present a method to analyze daily activities, such as meal preparation, using video from an egocentric camera. Our method performs inference about activities, actions, hands, and objects. Daily activities are a challenging domain for activity recognition which are well-suited to an egocentric approach. In contrast to previous activity recognition methods, our approach does not require pre-trained detectors for objects and hands. Instead we demonstrate the ability to learn a hierarchical model of an activity by exploiting the consistent appearance of objects, hands, and actions that results from the egocentric context. We show that joint modeling of activities, actions, and objects leads to superior performance in comparison to the case where they are considered independently. We introduce a novel representation of actions based on object-hand interactions and experimentally demonstrate the superior performance of our representation in comparison to standard activity representations such as bag of words.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126269",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2009",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 101,
            "other": 55,
            "total": 156
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "inference mechanisms",
                "object detection",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "understanding egocentric activity",
                "daily activity",
                "meal preparation",
                "egocentric camera",
                "inference",
                "activity recognition methods",
                "pre-trained detectors",
                "object detection",
                "hand detection",
                "consistent appearance",
                "joint modeling",
                "object-hand interactions",
                "superior performance",
                "standard activity representations",
                "bag of words"
            ]
        },
        "id": 51,
        "cited_by": [
            {
                "year": "2017",
                "id": 5
            },
            {
                "year": "2017",
                "id": 304
            },
            {
                "year": "2017",
                "id": 308
            },
            {
                "year": "2017",
                "id": 389
            },
            {
                "year": "2015",
                "id": 502
            },
            {
                "year": "2015",
                "id": 521
            },
            {
                "year": "2013",
                "id": 401
            }
        ]
    },
    {
        "title": "Efficient regression of general-activity human poses from depth images",
        "authors": [
            "Ross Girshick",
            "Jamie Shotton",
            "Pushmeet Kohli",
            "Antonio Criminisi",
            "Andrew Fitzgibbon"
        ],
        "abstract": "We present a new approach to general-activity human pose estimation from depth images, building on Hough forests. We extend existing techniques in several ways: real time prediction of multiple 3D joints, explicit learning of voting weights, vote compression to allow larger training sets, and a comparison of several decision-tree training objectives. Key aspects of our work include: regression directly from the raw depth image, without the use of an arbitrary intermediate representation; applicability to general motions (not constrained to particular activities) and the ability to localize occluded as well as visible body joints. Experimental results demonstrate that our method produces state of the art results on several data sets including the challenging MSRC-5000 pose estimation test set, at a speed of about 200 frames per second. Results on silhouettes suggest broader applicability to other imaging modalities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126270",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2007",
                "id": 135
            }
        ],
        "citation": {
            "ieee": 105,
            "other": 92,
            "total": 197
        },
        "keywords": {
            "IEEE Keywords": [
                "Joints",
                "Training",
                "Vegetation",
                "Three dimensional displays",
                "Accuracy",
                "Estimation",
                "Regression tree analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "image representation",
                "learning (artificial intelligence)",
                "pose estimation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "general-activity human pose regression",
                "depth image",
                "Hough forest",
                "multiple 3D joint",
                "explicit learning",
                "voting weight",
                "decision-tree training objective",
                "arbitrary intermediate representation",
                "visible body joint",
                "MSRC-5000 pose estimation test set",
                "imaging modality"
            ]
        },
        "id": 52,
        "cited_by": [
            {
                "year": "2017",
                "id": 415
            },
            {
                "year": "2015",
                "id": 91
            },
            {
                "year": "2015",
                "id": 213
            },
            {
                "year": "2013",
                "id": 52
            },
            {
                "year": "2013",
                "id": 137
            },
            {
                "year": "2013",
                "id": 241
            },
            {
                "year": "2013",
                "id": 402
            },
            {
                "year": "2013",
                "id": 431
            }
        ]
    },
    {
        "title": "Diffusion runs low on persistence fast",
        "authors": [
            "Chao Chen",
            "Herbert Edelsbrunner"
        ],
        "abstract": "Interpreting an image as a function on a compact subset of the Euclidean plane, we get its scale-space by diffusion, spreading the image over the entire plane. This generates a 1-parameter family of functions alternatively defined as convolutions with a progressively wider Gaussian kernel. We prove that the corresponding 1-parameter family of persistence diagrams have norms that go rapidly to zero as time goes to infinity. This result rationalizes experimental observations about scale-space. We hope this will lead to targeted improvements of related computer vision methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126271",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Tin",
                "Kernel",
                "Topology",
                "Heating",
                "Feature extraction",
                "Face",
                "Bridges"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Gaussian processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Euclidean plane",
                "scale-space",
                "image diffusion",
                "Gaussian kernel",
                "computer vision"
            ]
        },
        "id": 53,
        "cited_by": []
    },
    {
        "title": "Automated articulated structure and 3D shape recovery from point correspondences",
        "authors": [
            "Jo\u00e3o Fayad",
            "Chris Russell",
            "Lourdes Agapito"
        ],
        "abstract": "In this paper we propose a new method for the simultaneous segmentation and 3D reconstruction of interest point based articulated motion. We decompose a set of point tracks into rigid-bodied overlapping regions which are associated with skeletal links, while joint centres can be derived from the regions of overlap. This allows us to formulate the problem of 3D reconstruction as one of model assignment, where each model corresponds to the motion and shape parameters of an articulated body part. We show how this labelling can be optimised using a combination of pre-existing graph-cut based inference, and robust structure from motion factorization techniques. The strength of our approach comes from viewing both the decomposition into parts, and the 3D reconstruction as the optimisation of a single cost function, namely the image re-projection error. We show results of full 3D shape recovery on challenging real-world sequences with one or more articulated bodies, in the presence of outliers and missing data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126272",
        "reference_list": [
            {
                "year": "2009",
                "id": 232
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 15,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Motion segmentation",
                "Solid modeling",
                "Shape",
                "Computer vision",
                "Data models",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image motion analysis",
                "image reconstruction",
                "image sequences",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automated articulated structure",
                "3D shape recovery",
                "point correspondences",
                "3D reconstruction",
                "rigid-bodied overlapping regions",
                "model assignment",
                "motion parameters",
                "shape parameters",
                "motion factorization techniques",
                "preexisting graph-cut based inference",
                "image reprojection error",
                "real-world sequences"
            ]
        },
        "id": 54,
        "cited_by": []
    },
    {
        "title": "Imaging via three-dimensional compressive sampling (3DCS)",
        "authors": [
            "Xianbiao Shu",
            "Narendra Ahuja"
        ],
        "abstract": "Compressive sampling (CS) aims at acquiring a signal at a sampling rate that is significantly below the Nyquist rate. Its main idea is that a signal can be decoded from incomplete linear measurements by seeking its sparsity in some domain. Despite the remarkable progress in the theory of CS, little headway has been made in the compressive imaging (CI) camera. In this paper, a three-dimensional compressive sampling (3DCS) approach is proposed to reduce the required sampling rate of the CI camera to a practical level. In 3DCS, a generic three-dimensional sparsity measure (3DSM) is presented, which decodes a video from incomplete samples by exploiting its 3D piecewise smoothness and temporal low-rank property. In addition, an efficient decoding algorithm is developed for this 3DSM with guaranteed convergence. The experimental results show that our 3DCS requires a much lower sampling rate than the existing CS methods without compromising recovery accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126273",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 11,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three dimensional displays",
                "Sensors",
                "Image coding",
                "Joints",
                "Decoding"
            ],
            "INSPEC: Controlled Indexing": [
                "image sampling",
                "image sensors",
                "signal detection",
                "video coding"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D compressive sampling",
                "signal acquisition",
                "3DCS",
                "incomplete linear measurements",
                "compressive imaging camera",
                "generic 3D sparsity measure",
                "video decoding",
                "3D piecewise smoothness",
                "temporal low-rank property"
            ]
        },
        "id": 55,
        "cited_by": []
    },
    {
        "title": "Structure-sensitive superpixels via geodesic distance",
        "authors": [
            "Gang Zeng",
            "Peng Wang",
            "Jingdong Wang",
            "Rui Gan",
            "Hongbin Zha"
        ],
        "abstract": "Over-segments (i.e. superpixels) have been commonly used as supporting regions for feature vectors and primitives to reduce computational complexity in various image analysis tasks. In this paper, we describe a structuresensitive over-segmentation technique by exploiting Lloyd's algorithm with a geodesic distance. It generates smaller superpixels to achieve lower under-segmentation in structure-dense regions with high intensity or color variation, and produces larger segments to increase computational efficiency in structure-sparse regions with homogeneous appearance. We adopt geometric flows to compute the geodesic distances amongst pixels, and in the segmentation procedure, the density of over-segments is automatically adjusted according to an energy functional that embeds color homogeneity, structure density and compactness constraints. Comparative experiments with the Berkeley database show that the proposed algorithm outperforms prior arts while offering a comparable computational efficiency with fast methods, such as TurboPixels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126274",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2009",
                "id": 278
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2009",
                "id": 98
            },
            {
                "year": "2005",
                "id": 185
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2009",
                "id": 87
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 5,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Lattices",
                "Density functional theory",
                "Complexity theory",
                "Shape",
                "Image edge detection",
                "Level measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "differential geometry",
                "image colour analysis",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure-sensitive superpixels",
                "geodesic distance",
                "feature vector",
                "computational complexity",
                "structure-sensitive over-segmentation technique",
                "Lloyd algorithm",
                "color variation",
                "computational efficiency",
                "structure-sparse region",
                "geometric flow",
                "energy functional",
                "color homogeneity structure density",
                "Berkeley database"
            ]
        },
        "id": 56,
        "cited_by": [
            {
                "year": "2017",
                "id": 380
            },
            {
                "year": "2013",
                "id": 48
            }
        ]
    },
    {
        "title": "Optimal landmark detection using shape models and branch and bound",
        "authors": [
            "Brian Amberg",
            "Thomas Vetter"
        ],
        "abstract": "Fitting statistical 2D and 3D shape models to images is necessary for a variety of tasks, such as video editing and face recognition. Much progress has been made on local fitting from an initial guess, but determining a close enough initial guess is still an open problem. One approach is to detect distinct landmarks in the image and initalize the model fit from these correspondences. This is difficult, because detection of landmarks based only on the local appearance is inherently ambiguous. This makes it necessary to use global shape information for the detections. We propose a method to solve the combinatorial problem of selecting out of a large number of candidate landmark detections the configuration which is best supported by a shape model. Our method, as opposed to previous approaches, always finds the globally optimal configuration. The algorithm can be applied to a very general class of shape models and is independent of the underlying feature point detector. Its theoretic optimality is shown, and it is evaluated on a large face dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126275",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 13,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Shape",
                "Face",
                "Three dimensional displays",
                "Solid modeling",
                "Cost function",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "object detection",
                "shape recognition",
                "solid modelling",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optimal landmark detection",
                "branch and bound detection",
                "3D shape model",
                "video editing",
                "face recognition",
                "shape information",
                "candidate landmark detection",
                "feature point detector",
                "large face dataset"
            ]
        },
        "id": 57,
        "cited_by": [
            {
                "year": "2015",
                "id": 351
            },
            {
                "year": "2013",
                "id": 241
            },
            {
                "year": "2013",
                "id": 314
            }
        ]
    },
    {
        "title": "Fast removal of non-uniform camera shake",
        "authors": [
            "Michael Hirsch",
            "Christian J. Schuler",
            "Stefan Harmeling",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "Camera shake leads to non-uniform image blurs. State-of-the-art methods for removing camera shake model the blur as a linear combination of homographically transformed versions of the true image. While this is conceptually interesting, the resulting algorithms are computationally demanding. In this paper we develop a forward model based on the efficient filter flow framework, incorporating the particularities of camera shake, and show how an efficient algorithm for blur removal can be obtained. Comprehensive comparisons on a number of real-world blurry images show that our approach is not only substantially faster, but it also leads to better deblurring results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126276",
        "reference_list": [
            {
                "year": "2007",
                "id": 65
            },
            {
                "year": "2007",
                "id": 83
            }
        ],
        "citation": {
            "ieee": 87,
            "other": 39,
            "total": 126
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Computational modeling",
                "Kernel",
                "Mathematical model",
                "Estimation",
                "Deconvolution",
                "Convolution"
            ],
            "INSPEC: Controlled Indexing": [
                "filtering theory",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonuniform camera shake fast removal",
                "nonuniform image blurs",
                "homographically transformed versions",
                "filter flow framework",
                "forward model",
                "blur removal"
            ]
        },
        "id": 58,
        "cited_by": [
            {
                "year": "2017",
                "id": 112
            },
            {
                "year": "2017",
                "id": 261
            },
            {
                "year": "2017",
                "id": 346
            },
            {
                "year": "2017",
                "id": 422
            },
            {
                "year": "2017",
                "id": 425
            },
            {
                "year": "2015",
                "id": 68
            },
            {
                "year": "2013",
                "id": 182
            },
            {
                "year": "2013",
                "id": 297
            },
            {
                "year": "2013",
                "id": 394
            }
        ]
    },
    {
        "title": "Sparse representation or collaborative representation: Which helps face recognition?",
        "authors": [
            "Lei Zhang",
            "Meng Yang",
            "Xiangchu Feng"
        ],
        "abstract": "As a recently proposed technique, sparse representation based classification (SRC) has been widely used for face recognition (FR). SRC first codes a testing sample as a sparse linear combination of all the training samples, and then classifies the testing sample by evaluating which class leads to the minimum representation error. While the importance of sparsity is much emphasized in SRC and many related works, the use of collaborative representation (CR) in SRC is ignored by most literature. However, is it really the l 1 -norm sparsity that improves the FR accuracy? This paper devotes to analyze the working mechanism of SRC, and indicates that it is the CR but not the l1-norm sparsity that makes SRC powerful for face classification. Consequently, we propose a very simple yet much more efficient face classification scheme, namely CR based classification with regularized least square (CRC_RLS). The extensive experiments clearly show that CRC_RLS has very competitive classification results, while it has significantly less complexity than SRC.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126277",
        "reference_list": [
            {
                "year": "2009",
                "id": 292
            }
        ],
        "citation": {
            "ieee": 521,
            "other": 276,
            "total": 797
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Face",
                "Dictionaries",
                "Encoding",
                "Minimization",
                "Collaboration",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image representation",
                "least squares approximations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "sparse representation based classification",
                "face classification",
                "collaborative representation based classification",
                "regularized least square"
            ]
        },
        "id": 59,
        "cited_by": [
            {
                "year": "2015",
                "id": 432
            },
            {
                "year": "2013",
                "id": 74
            },
            {
                "year": "2013",
                "id": 151
            },
            {
                "year": "2013",
                "id": 411
            },
            {
                "year": "2011",
                "id": 68
            },
            {
                "year": "2011",
                "id": 95
            }
        ]
    },
    {
        "title": "From learning models of natural image patches to whole image restoration",
        "authors": [
            "Daniel Zoran",
            "Yair Weiss"
        ],
        "abstract": "Learning good image priors is of utmost importance for the study of vision, computer vision and image processing applications. Learning priors and optimizing over whole images can lead to tremendous computational challenges. In contrast, when we work with small image patches, it is possible to learn priors and perform patch restoration very efficiently. This raises three questions - do priors that give high likelihood to the data also lead to good performance in restoration? Can we use such patch based priors to restore a full image? Can we learn better patch priors? In this work we answer these questions. We compare the likelihood of several patch models and show that priors that give high likelihood to data perform better in patch restoration. Motivated by this result, we propose a generic framework which allows for whole image restoration using any patch based prior for which a MAP (or approximate MAP) estimate can be calculated. We show how to derive an appropriate cost function, how to optimize it and how to use it to restore whole images. Finally, we present a generic, surprisingly simple Gaussian Mixture prior, learned from a set of natural images. When used with the proposed framework, this Gaussian Mixture Model outperforms all other generic prior methods for image denoising, deblurring and inpainting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126278",
        "reference_list": [],
        "citation": {
            "ieee": 311,
            "other": 128,
            "total": 439
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Noise reduction",
                "Equations",
                "Noise measurement",
                "Image reconstruction",
                "Mathematical model",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Gaussian processes",
                "image restoration",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning model",
                "natural image patches",
                "image restoration",
                "computer vision",
                "image processing",
                "learning priors",
                "patch restoration",
                "patch priors",
                "approximate MAP estimate",
                "cost function",
                "Gaussian mixture prior",
                "Gaussian mixture model"
            ]
        },
        "id": 60,
        "cited_by": [
            {
                "year": "2017",
                "id": 112
            },
            {
                "year": "2017",
                "id": 115
            },
            {
                "year": "2017",
                "id": 181
            },
            {
                "year": "2017",
                "id": 188
            },
            {
                "year": "2017",
                "id": 346
            },
            {
                "year": "2017",
                "id": 449
            },
            {
                "year": "2017",
                "id": 476
            },
            {
                "year": "2017",
                "id": 481
            },
            {
                "year": "2017",
                "id": 571
            },
            {
                "year": "2015",
                "id": 27
            },
            {
                "year": "2015",
                "id": 32
            },
            {
                "year": "2015",
                "id": 51
            },
            {
                "year": "2015",
                "id": 67
            },
            {
                "year": "2015",
                "id": 68
            },
            {
                "year": "2015",
                "id": 203
            },
            {
                "year": "2015",
                "id": 440
            },
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2013",
                "id": 78
            },
            {
                "year": "2013",
                "id": 182
            },
            {
                "year": "2013",
                "id": 297
            }
        ]
    },
    {
        "title": "Parsing video events with goal inference and intent prediction",
        "authors": [
            "Mingtao Pei",
            "Yunde Jia",
            "Song-Chun Zhu"
        ],
        "abstract": "In this paper, we present an event parsing algorithm based on Stochastic Context Sensitive Grammar (SCSG) for understanding events, inferring the goal of agents, and predicting their plausible intended actions. The SCSG represents the hierarchical compositions of events and the temporal relations between the sub-events. The alphabets of the SCSG are atomic actions which are defined by the poses of agents and their interactions with objects in the scene. The temporal relations are used to distinguish events with similar structures, interpolate missing portions of events, and are learned from the training data. In comparison with existing methods, our paper makes the following contributions. i) We define atomic actions by a set of relations based on the fluents of agents and their interactions with objects in the scene. ii) Our algorithm handles events insertion and multi-agent events, keeps all possible interpretations of the video to preserve the ambiguities, and achieves the globally optimal parsing solution in a Bayesian framework; iii) The algorithm infers the goal of the agents and predicts their intents by a top-down process; iv) The algorithm improves the detection of atomic actions by event contexts. We show satisfactory results of event recognition and atomic action detection on the data set we captured which contains 12 event categories in both indoor and outdoor videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126279",
        "reference_list": [
            {
                "year": "2011",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 22,
            "total": 62
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Context",
                "Grammar",
                "Prediction algorithms",
                "Portable computers",
                "Training data",
                "Atomic clocks"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "context-sensitive grammars",
                "inference mechanisms",
                "multi-agent systems",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video event parsing algorithm",
                "goal inference",
                "intent prediction",
                "stochastic context sensitive grammar",
                "temporal relations",
                "atomic actions",
                "event insertion",
                "multiagent events",
                "optimal parsing solution",
                "Bayesian framework",
                "top-down process",
                "event recognition",
                "outdoor videos",
                "indoor videos"
            ]
        },
        "id": 61,
        "cited_by": [
            {
                "year": "2017",
                "id": 67
            },
            {
                "year": "2017",
                "id": 122
            },
            {
                "year": "2015",
                "id": 264
            },
            {
                "year": "2015",
                "id": 502
            },
            {
                "year": "2015",
                "id": 507
            },
            {
                "year": "2013",
                "id": 168
            },
            {
                "year": "2013",
                "id": 277
            },
            {
                "year": "2013",
                "id": 335
            },
            {
                "year": "2013",
                "id": 391
            },
            {
                "year": "2013",
                "id": 408
            },
            {
                "year": "2011",
                "id": 5
            }
        ]
    },
    {
        "title": "Handling outliers in non-blind image deconvolution",
        "authors": [
            "Sunghyun Cho",
            "Jue Wang",
            "Seungyong Lee"
        ],
        "abstract": "Non-blind deconvolution is a key component in image deblurring systems. Previous deconvolution methods assume a linear blur model where the blurred image is generated by a linear convolution of the latent image and the blur kernel. This assumption often does not hold in practice due to various types of outliers in the imaging process. Without proper outlier handling, previous methods may generate results with severe ringing artifacts even when the kernel is estimated accurately. In this paper we analyze a few common types of outliers that cause previous methods to fail, such as pixel saturation and non-Gaussian noise. We propose a novel blur model that explicitly takes these outliers into account, and build a robust non-blind deconvolution method upon it, which can effectively reduce the visual artifacts caused by outliers. The effectiveness of our method is demonstrated by experimental results on both synthetic and real-world examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126280",
        "reference_list": [
            {
                "year": "2007",
                "id": 65
            }
        ],
        "citation": {
            "ieee": 55,
            "other": 23,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Deconvolution",
                "Noise",
                "Kernel",
                "Dynamic range",
                "Cameras",
                "Image restoration",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "deconvolution",
                "Gaussian noise",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "handling outliers",
                "nonblind image deconvolution",
                "image deblurring systems",
                "deconvolution methods",
                "linear blur model",
                "blurred image",
                "linear convolution",
                "latent image",
                "blur kernel",
                "imaging process",
                "outlier handling",
                "pixel saturation",
                "nonGaussian noise",
                "robust nonblind deconvolution method",
                "visual artifacts"
            ]
        },
        "id": 62,
        "cited_by": [
            {
                "year": "2017",
                "id": 174
            },
            {
                "year": "2017",
                "id": 261
            }
        ]
    },
    {
        "title": "Relative attributes",
        "authors": [
            "Devi Parikh",
            "Kristen Grauman"
        ],
        "abstract": "Human-nameable visual \u201cattributes\u201d can benefit various recognition tasks. However, existing techniques restrict these properties to categorical labels (for example, a person is `smiling' or not, a scene is `dry' or not), and thus fail to capture more general semantic relationships. We propose to model relative attributes. Given training data stating how object/scene categories relate according to different attributes, we learn a ranking function per attribute. The learned ranking functions predict the relative strength of each property in novel images. We then build a generative model over the joint space of attribute ranking outputs, and propose a novel form of zero-shot learning in which the supervisor relates the unseen object category to previously seen objects via attributes (for example, `bears are furrier than giraffes'). We further show how the proposed relative attributes enable richer textual descriptions for new images, which in practice are more precise for human interpretation. We demonstrate the approach on datasets of faces and natural scenes, and show its clear advantages over traditional binary attribute prediction for these new tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126281",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2009",
                "id": 68
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2009",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 285,
            "other": 132,
            "total": 417
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Visualization",
                "Humans",
                "Support vector machines",
                "Image recognition",
                "Machine learning",
                "Vocabulary"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human-name visual attribute",
                "recognition task",
                "categorical label",
                "training data",
                "scene category",
                "ranking function per attribute",
                "learned ranking function",
                "zero-shot learning",
                "object category",
                "image textual description",
                "human interpretation",
                "natural scene",
                "face dataset",
                "binary attribute prediction"
            ]
        },
        "id": 63,
        "cited_by": [
            {
                "year": "2017",
                "id": 40
            },
            {
                "year": "2017",
                "id": 43
            },
            {
                "year": "2017",
                "id": 224
            },
            {
                "year": "2017",
                "id": 514
            },
            {
                "year": "2017",
                "id": 584
            },
            {
                "year": "2015",
                "id": 43
            },
            {
                "year": "2015",
                "id": 118
            },
            {
                "year": "2015",
                "id": 162
            },
            {
                "year": "2015",
                "id": 269
            },
            {
                "year": "2015",
                "id": 273
            },
            {
                "year": "2015",
                "id": 414
            },
            {
                "year": "2015",
                "id": 426
            },
            {
                "year": "2015",
                "id": 474
            },
            {
                "year": "2015",
                "id": 512
            },
            {
                "year": "2013",
                "id": 31
            },
            {
                "year": "2013",
                "id": 37
            },
            {
                "year": "2013",
                "id": 45
            },
            {
                "year": "2013",
                "id": 90
            },
            {
                "year": "2013",
                "id": 91
            },
            {
                "year": "2013",
                "id": 92
            },
            {
                "year": "2013",
                "id": 152
            },
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2013",
                "id": 209
            },
            {
                "year": "2013",
                "id": 231
            },
            {
                "year": "2013",
                "id": 264
            },
            {
                "year": "2013",
                "id": 269
            },
            {
                "year": "2013",
                "id": 324
            },
            {
                "year": "2013",
                "id": 341
            },
            {
                "year": "2013",
                "id": 428
            },
            {
                "year": "2013",
                "id": 453
            }
        ]
    },
    {
        "title": "Extracting adaptive contextual cues from unlabeled regions",
        "authors": [
            "Congcong Li",
            "Devi Parikh",
            "Tsuhan Chen"
        ],
        "abstract": "Existing approaches to contextual reasoning for enhanced object detection typically utilize other labeled categories in the images to provide contextual information. As a consequence, they inadvertently commit to the granularity of information implicit in the labels. Moreover, large portions of the images may not belong to any of the manually-chosen categories, and these unlabeled regions are typically neglected. In this paper, we overcome both these drawbacks and propose a contextual cue that exploits unlabeled regions in images. Our approach adaptively determines the granularity (scene, inter-object, intra-object, etc.) at which contextual information is captured. In order to extract the proposed contextual cue, we consider a scene to be a structured configuration of objects and regions; just as an object is a composition of parts. We thus learn our proposed \u201ccontextual meta-objects\u201d using any off-the-shelf object detector, which makes our proposed cue widely accessible to the community. Our results show that incorporating our proposed cue provides a relative improvement of 12% over a state-of-the-art object detector on the challenging PASCAL dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126282",
        "reference_list": [
            {
                "year": "2009",
                "id": 29
            },
            {
                "year": "2005",
                "id": 168
            },
            {
                "year": "2009",
                "id": 254
            },
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2005",
                "id": 174
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 2,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Context",
                "Training",
                "Context modeling",
                "Object detection",
                "Adaptation models",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive contextual cues extraction",
                "unlabeled region",
                "object detection",
                "contextual information",
                "information granularity",
                "off-the-shelf object detector",
                "state-of-the-art object detector",
                "PASCAL dataset"
            ]
        },
        "id": 64,
        "cited_by": [
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2013",
                "id": 2
            }
        ]
    },
    {
        "title": "Recognizing jumbled images: The role of local and global information in image classification",
        "authors": [
            "Devi Parikh"
        ],
        "abstract": "The performance of current state-of-the-art computer vision algorithms at image classification falls significantly short as compared to human abilities. To reduce this gap, it is important for the community to know what problems to solve, and not just how to solve them. Towards this goal, via the use of jumbled images, we strip apart two widely investigated aspects: local and global information in images, and identify the performance bottleneck. Interestingly, humans have been shown to reliably recognize jumbled images. The goal of our paper is to determine a functional model that mimics how humans recognize jumbled images i.e. exploit local information alone, and further evaluate if existing implementations of this computational model suffice to match human performance. Surprisingly, in our series of human studies and machine experiments, we find that a simple bag-of-words based majority-vote-like strategy is an accurate functional model of how humans recognize jumbled images. Moreover, a straightforward machine implementation of this model achieves accuracies similar to human subjects at classifying jumbled images. This indicates that perhaps existing machine vision techniques already leverage local information from images effectively, and future research efforts should be focused on more advanced modeling of global information.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126283",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 12,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Accuracy",
                "Image recognition",
                "Computational modeling",
                "Reliability",
                "Face",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image classification",
                "jumbled image recognition",
                "global information",
                "local information",
                "computer vision algorithms",
                "human abilities",
                "computational model",
                "bag-of-words based majority-vote-like strategy",
                "functional model",
                "machine vision techniques"
            ]
        },
        "id": 65,
        "cited_by": []
    },
    {
        "title": "Automatic construction of an action video shot database using web videos",
        "authors": [
            "Do Hang Nga",
            "Keiji Yanai"
        ],
        "abstract": "There are a huge number of videos with text tags on the Web nowadays. In this paper, we propose a method of automatically extracting from Web videos video shots corresponding to specific actions with just only providing action keywords such as \u201cwalking\u201d and \u201ceating\u201d. The proposed method consists of three steps: (1) tag-based video selection, (2) segmenting videos into shots and extracting features from the shots, and (3) visual-feature-based video shot selection with tag-based scores taken into account. Firstly, we gather video IDs and tag lists for 1000 Web videos corresponding to given keywords via Web API, and we calculate tag relevance scores for each video using a tag-co-occurrence dictionary which is constructed in advance. Secondly, we fetch the top 200 videos from the Web in the descending order of the tag relevance scores, and segment each downloaded video into several shots. From each shot we extract spatio-temporal features, global motion features and appearance features, and convert them into the bag-of-features representation. Finally, we apply the VisualRank method to select the video shots which describe the actions corresponding to the given keywords best after calculating a similarity matrix between video shots. In the experiments, we achieved the 49.5% precision at 100 shots over six kinds of human actions by just providing keywords without any supervision. In addition, we made large-scale experiments on 100 kinds of action keywords.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126284",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2007",
                "id": 259
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Vectors",
                "Visualization",
                "YouTube",
                "Humans",
                "Dictionaries",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image motion analysis",
                "image segmentation",
                "matrix algebra",
                "video retrieval",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action video shot database",
                "Web videos",
                "action keyword",
                "tag-based video selection",
                "video segmentation",
                "feature extraction",
                "visual-feature-based video shot selection",
                "tag-based score",
                "Web API",
                "tag-cooccurrence dictionary",
                "spatio-temporal feature",
                "global motion feature",
                "appearance feature",
                "bag-of-features representation",
                "VisualRank method",
                "similarity matrix"
            ]
        },
        "id": 66,
        "cited_by": []
    },
    {
        "title": "Multi-view repetitive structure detection",
        "authors": [
            "Nianjuan Jiang",
            "Ping Tan",
            "Loong-Fah Cheong"
        ],
        "abstract": "Symmetry, especially repetitive structures in architecture are universally demonstrated across countries and cultures. Existing detection methods mainly focus on the detection of planar patterns from a single image. It is difficult to apply them to detect repetitive structures in architecture, which abounds with non-planar 3D repetitive elements (such as balconies and windows) and curved surfaces. We study the repetitive structure detection problem from multiple images of such architecture. Our method jointly analyzes these images and a set of 3D points reconstructed from them by structure-from-motion algorithms. 3D points help to rectify geometric deformations and hypothesize possible lattice structures, while images provide denser color and texture information to evaluate and confirm these hypotheses. In the experiments, we compare our method with existing algorithm. We also show how our results might be used to assist image-based modeling.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126285",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Buildings"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image recognition",
                "image reconstruction",
                "image texture",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview repetitive structure detection",
                "planar pattern detection",
                "nonplanar 3D repetitive element",
                "3D point set",
                "image reconstruction",
                "structure-from-motion algorithm",
                "geometric deformation",
                "hypothesize possible lattice structure",
                "texture information",
                "image-based modeling"
            ]
        },
        "id": 67,
        "cited_by": []
    },
    {
        "title": "Fisher Discrimination Dictionary Learning for sparse representation",
        "authors": [
            "Meng Yang",
            "Lei Zhang",
            "Xiangchu Feng",
            "David Zhang"
        ],
        "abstract": "Sparse representation based classification has led to interesting image recognition results, while the dictionary used for sparse coding plays a key role in it. This paper presents a novel dictionary learning (DL) method to improve the pattern classification performance. Based on the Fisher discrimination criterion, a structured dictionary, whose dictionary atoms have correspondence to the class labels, is learned so that the reconstruction error after sparse coding can be used for pattern classification. Meanwhile, the Fisher discrimination criterion is imposed on the coding coefficients so that they have small within-class scatter but big between-class scatter. A new classification scheme associated with the proposed Fisher discrimination DL (FDDL) method is then presented by using both the discriminative information in the reconstruction error and sparse coding coefficients. The proposed FDDL is extensively evaluated on benchmark image databases in comparison with existing sparse representation and DL based classification methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126286",
        "reference_list": [
            {
                "year": "2011",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 287,
            "other": 182,
            "total": 469
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Training",
                "Encoding",
                "Image reconstruction",
                "Testing",
                "Image coding",
                "Face"
            ],
            "INSPEC: Controlled Indexing": [
                "dictionaries",
                "image classification",
                "image coding",
                "image representation",
                "learning (artificial intelligence)",
                "object recognition",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Fisher discrimination dictionary learning",
                "sparse representation based classification",
                "image recognition",
                "sparse coding coefficients",
                "pattern classification performance",
                "structured dictionary",
                "reconstruction error",
                "coding coefficients",
                "image databases",
                "DL based classification methods"
            ]
        },
        "id": 68,
        "cited_by": [
            {
                "year": "2015",
                "id": 26
            },
            {
                "year": "2015",
                "id": 199
            },
            {
                "year": "2015",
                "id": 504
            },
            {
                "year": "2013",
                "id": 141
            },
            {
                "year": "2013",
                "id": 151
            },
            {
                "year": "2013",
                "id": 225
            },
            {
                "year": "2013",
                "id": 276
            }
        ]
    },
    {
        "title": "Dyadic transfer learning for cross-domain image classification",
        "authors": [
            "Hua Wang",
            "Feiping Nie",
            "Heng Huang",
            "Chris Ding"
        ],
        "abstract": "Because manual image annotation is both expensive and labor intensive, in practice we often do not have sufficient labeled images to train an effective classifier for the new image classification tasks. Although multiple labeled image data sets are publicly available for a number of computer vision tasks, a simple mixture of them cannot achieve good performance due to the heterogeneous properties and structures between different data sets. In this paper, we propose a novel nonnegative matrix tri-factorization based transfer learning framework, called as Dyadic Knowledge Transfer (DKT) approach, to transfer cross-domain image knowledge for the new computer vision tasks, such as classifications. An efficient iterative algorithm to solve the proposed optimization problem is introduced. We perform the proposed approach on two benchmark image data sets to simulate the real world cross-domain image classification tasks. Promising experimental results demonstrate the effectiveness of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126287",
        "reference_list": [
            {
                "year": "2009",
                "id": 261
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 14,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Knowledge transfer",
                "Optimization",
                "Computer vision",
                "Image color analysis",
                "Semantics",
                "Videos",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dyadic transfer learning",
                "cross-domain image classification",
                "manual image annotation",
                "data sets",
                "dyadic knowledge transfer",
                "computer vision tasks"
            ]
        },
        "id": 69,
        "cited_by": []
    },
    {
        "title": "Sparse multi-task regression and feature selection to identify brain imaging predictors for memory performance",
        "authors": [
            "Hua Wang",
            "Feiping Nie",
            "Heng Huang",
            "Shannon Risacher",
            "Chris Ding",
            "Andrew J Saykin",
            "Li Shen"
        ],
        "abstract": "Alzheimer's disease (AD) is a neurodegenerative disorder characterized by progressive impairment of memory and other cognitive functions, which makes regression analysis a suitable model to study whether neuroimaging measures can help predict memory performance and track the progression of AD. Existing memory performance prediction methods via regression, however, do not take into account either the interconnected structures within imaging data or those among memory scores, which inevitably restricts their predictive capabilities. To bridge this gap, we propose a novel Sparse Multi-tAsk Regression and feaTure selection (SMART) method to jointly analyze all the imaging and clinical data under a single regression framework and with shared underlying sparse representations. Two convex regularizations are combined and used in the model to enable sparsity as well as facilitate multi-task learning. The effectiveness of the proposed method is demonstrated by both clearly improved prediction performances in all empirical test cases and a compact set of selected RAVLT-relevant MRI predictors that accord with prior studies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126288",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 10,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Magnetic resonance imaging",
                "Particle measurements",
                "Atmospheric measurements",
                "Neuroimaging",
                "Weight measurement",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "biology computing",
                "brain",
                "diseases",
                "medical image processing",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparse multitask regression",
                "feature selection",
                "brain imaging predictors",
                "memory performance",
                "Alzheimer disease",
                "neurodegenerative disorder",
                "progressive impairment",
                "cognitive function",
                "regression analysis",
                "neuroimaging measures",
                "regression framework",
                "sparse representation",
                "convex regularization",
                "sparsity",
                "multitask learning",
                "compact set"
            ]
        },
        "id": 70,
        "cited_by": []
    },
    {
        "title": "Conditional Random Fields for multi-camera object detection",
        "authors": [
            "Gemma Roig",
            "Xavier Boix",
            "Horesh Ben Shitrit",
            "Pascal Fua"
        ],
        "abstract": "We formulate a model for multi-class object detection in a multi-camera environment. From our knowledge, this is the first time that this problem is addressed taken into account different object classes simultaneously. Given several images of the scene taken from different angles, our system estimates the ground plane location of the objects from the output of several object detectors applied at each viewpoint. We cast the problem as an energy minimization modeled with a Conditional Random Field (CRF). Instead of predicting the presence of an object at each image location independently, we simultaneously predict the labeling of the entire scene. Our CRF is able to take into account occlusions between objects and contextual constraints among them. We propose an effective iterative strategy that renders tractable the underlying optimization problem, and learn the parameters of the model with the max-margin paradigm. We evaluate the performance of our model on several challenging multi-camera pedestrian detection datasets namely PETS 2009 [5] and EPFL terrace sequence [9]. We also introduce a new dataset in which multiple classes of objects appear simultaneously in the scene. It is here where we show that our method effectively handles occlusions in the multi-class case.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126289",
        "reference_list": [
            {
                "year": "2009",
                "id": 29
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Cameras",
                "Random variables",
                "Detectors",
                "Object detection",
                "Inference algorithms",
                "Approximation algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "iterative methods",
                "minimisation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "conditional random fields",
                "multiclass object detection",
                "object classes",
                "ground plane location",
                "energy minimization",
                "image location",
                "contextual constraints",
                "iterative strategy",
                "optimization problem",
                "max-margin paradigm",
                "multicamera pedestrian detection datasets",
                "PETS 2009",
                "EPFL terrace sequence"
            ]
        },
        "id": 71,
        "cited_by": []
    },
    {
        "title": "Dynamic Manifold Warping for view invariant action recognition",
        "authors": [
            "Dian Gong",
            "G\u00e9rard Medioni"
        ],
        "abstract": "We address the problem of learning view-invariant 3D models of human motion from motion capture data, in order to recognize human actions from a monocular video sequence with arbitrary viewpoint. We propose a Spatio-Temporal Manifold (STM) model to analyze non-linear multivariate time series with latent spatial structure and apply it to recognize actions in the joint-trajectories space. Based on STM, a novel alignment algorithm Dynamic Manifold Warping (DMW) and a robust motion similarity metric are proposed for human action sequences, both in 2D and 3D. DMW extends previous works on spatio-temporal alignment by incorporating manifold learning. We evaluate and compare the approach to state-of-the-art methods on motion capture data and realistic videos. Experimental results demonstrate the effectiveness of our approach, which yields visually appealing alignment results, produces higher action recognition accuracy, and can recognize actions from arbitrary views with partial occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126290",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2003",
                "id": 123
            },
            {
                "year": "2007",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 12,
            "total": 39
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "image motion analysis",
                "image sequences",
                "solid modelling",
                "time series"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic manifold warping",
                "view invariant action recognition",
                "view-invariant 3D models",
                "motion capture data",
                "monocular video sequence",
                "spatiotemporal manifold model",
                "nonlinear multivariate time series",
                "latent spatial structure",
                "joint-trajectories space",
                "robust motion similarity metric",
                "human action sequences",
                "partial occlusion"
            ]
        },
        "id": 72,
        "cited_by": [
            {
                "year": "2015",
                "id": 514
            },
            {
                "year": "2013",
                "id": 390
            }
        ]
    },
    {
        "title": "Pose estimation from reflections for specular surface recovery",
        "authors": [
            "Miaomiao Liu",
            "Kwan-Yee K. Wong",
            "Zhenwen Dai",
            "Zhihu Chen"
        ],
        "abstract": "This paper addresses the problem of estimating the poses of a reference plane in specular shape recovery. Unlike existing methods which require an extra mirror or an extra reference plane and camera, our proposed method recovers the poses of the reference plane directly from its reflections on the specular surface. By establishing reflection correspondences on the reference plane in three distinct poses, our method estimates the poses of the reference plane in two steps. First, by applying a colinearity constraint to the reflection correspondences, a simple closed-form solution is derived for recovering the poses of the reference plane relative to its initial pose. Second, by applying a ray incidence constraint to the incident rays formed by the reflection correspondences and the visual rays cast from the image, a closed-form solution is derived for recovering the poses of the reference plane relative to the camera. The shape of the specular surface then follows. Experimental results on both synthetic and real data are presented, which demonstrate the feasibility and accuracy of our proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126291",
        "reference_list": [
            {
                "year": "2007",
                "id": 44
            },
            {
                "year": "2003",
                "id": 78
            },
            {
                "year": "2009",
                "id": 24
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 1,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Shape",
                "Visualization",
                "Closed-form solutions",
                "Encoding",
                "Estimation",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "mirrors",
                "pose estimation",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pose estimation",
                "specular surface recovery",
                "reference plane",
                "mirror",
                "camera",
                "reflection correspondences",
                "colinearity constraint",
                "closed-form solution",
                "ray incidence constraint",
                "incident rays",
                "visual rays",
                "synthetic data",
                "real data"
            ]
        },
        "id": 73,
        "cited_by": []
    },
    {
        "title": "Robust consistent correspondence between 3D non-rigid shapes based on \u201cDual Shape-DNA\u201d",
        "authors": [
            "Huai-Yu Wu",
            "Hongbin Zha"
        ],
        "abstract": "In this paper, we propose a novel framework to construct dense and high-quality consistent correspondence between non-rigid surfaces. Our correspondence framework exploits \u201cDual Shape-DNA\u201d (dual Laplace-Beltrami spectral embedding) to capture global characteristics of objects, and converts two originally different and complex shapes into two similar and simple shapes to facilitate the correspondence. Since our method avoids the computation of geodesic distances, it is robust to local topology changes. By exploiting the excellent properties of the dual domain, our dual spectral framework can robustly construct Laplace-Beltrami embeddings on highly non-regular 3D meshes. After performing initial non-rigid matching in the dual Laplace-Beltrami spectral domain, we return 3D spatial domain and apply a shape-preserving non-rigid deformation to produce the final dense consistent correspondence. We show that our framework is suitable for non-rigid consistent correspondence, and the high-quality correspondence results are achieved.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126292",
        "reference_list": [
            {
                "year": "2007",
                "id": 189
            },
            {
                "year": "2005",
                "id": 138
            },
            {
                "year": "2005",
                "id": 50
            },
            {
                "year": "2007",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three dimensional displays",
                "Harmonic analysis",
                "Spectral analysis",
                "Eigenvalues and eigenfunctions",
                "Robustness",
                "Manifolds"
            ],
            "INSPEC: Controlled Indexing": [
                "mesh generation",
                "shape recognition",
                "solid modelling",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D nonrigid shape",
                "dual shape-DNA",
                "nonrigid surface",
                "dual Laplace-Beltrami spectral embedding",
                "geodesic distance",
                "local topology",
                "dual domain",
                "dual spectral framework",
                "nonregular 3D meshes",
                "nonrigid matching",
                "3D spatial domain",
                "nonrigid consistent correspondence"
            ]
        },
        "id": 74,
        "cited_by": []
    },
    {
        "title": "Multi-observation visual recognition via joint dynamic sparse representation",
        "authors": [
            "Haichao Zhang",
            "Nasser M. Nasrabadi",
            "Yanning Zhang",
            "Thomas S. Huang"
        ],
        "abstract": "We address the problem of visual recognition from multiple observations of the same physical object, which can be generated under different conditions, such as frames at different time instances or snapshots from different viewpoints. We formulate the multi-observation visual recognition task as a joint sparse representation model and take advantage of the correlations among the multiple observations for classification using a novel joint dynamic sparsity prior. The proposed joint dynamic sparsity prior promotes shared joint sparsity pattern among the multiple sparse representation vectors at class-level, while allowing distinct sparsity patterns at atom-level within each class in order to facilitate a flexible representation. The proposed method can handle both homogenous as well as heterogenous data within the same framework. Extensive experiments on various visual classification tasks including face recognition and generic object classification demonstrate that the proposed method outperforms existing state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126293",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Joints",
                "Vectors",
                "Heuristic algorithms",
                "Face",
                "Training",
                "Visualization",
                "Face recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image representation",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multi-observation visual recognition",
                "joint dynamic sparse representation",
                "visual classification",
                "face recognition",
                "object classification"
            ]
        },
        "id": 75,
        "cited_by": []
    },
    {
        "title": "Local Intensity Order Pattern for feature description",
        "authors": [
            "Zhenhua Wang",
            "Bin Fan",
            "Fuchao Wu"
        ],
        "abstract": "This paper presents a novel method for feature description based on intensity order. Specifically, a Local Intensity Order Pattern(LIOP) is proposed to encode the local ordinal information of each pixel and the overall ordinal information is used to divide the local patch into subregions which are used for accumulating the LIOPs respectively. Therefore, both local and overall intensity ordinal information of the local patch are captured by the proposed LIOP descriptor so as to make it a highly discriminative descriptor. It is shown that the proposed descriptor is not only invariant to monotonic intensity changes and image rotation but also robust to many other geometric and photometric transformations such as viewpoint change, image blur and JEPG compression. The proposed descriptor has been evaluated on the standard Oxford dataset and four additional image pairs with complex illumination changes. The experimental results show that the proposed descriptor obtains a significant improvement over the existing state-of-the-art descriptors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126294",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 11,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Lighting",
                "Vectors",
                "Robustness",
                "Indexes",
                "Feature extraction",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "data compression",
                "feature extraction",
                "image coding",
                "image restoration",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local intensity order pattern",
                "feature description",
                "local patch",
                "LIOP descriptor",
                "discriminative descriptor",
                "monotonic intensity changes",
                "image rotation",
                "geometric transformations",
                "photometric transformations",
                "viewpoint change",
                "image blur",
                "JEPG compression",
                "Oxford dataset",
                "complex illumination changes",
                "computer vision"
            ]
        },
        "id": 76,
        "cited_by": [
            {
                "year": "2017",
                "id": 349
            },
            {
                "year": "2015",
                "id": 10
            },
            {
                "year": "2015",
                "id": 11
            },
            {
                "year": "2015",
                "id": 13
            },
            {
                "year": "2013",
                "id": 337
            }
        ]
    },
    {
        "title": "Large-scale image annotation using visual synset",
        "authors": [
            "David Tsai",
            "Yushi Jing",
            "Yi Liu",
            "Henry A. Rowley",
            "Sergey Ioffe",
            "James M. Rehg"
        ],
        "abstract": "We address the problem of large-scale annotation of web images. Our approach is based on the concept of visual synset, which is an organization of images which are visually-similar and semantically-related. Each visual synset represents a single prototypical visual concept, and has an associated set of weighted annotations. Linear SVM's are utilized to predict the visual synset membership for unseen image examples, and a weighted voting rule is used to construct a ranked list of predicted annotations from a set of visual synsets. We demonstrate that visual synsets lead to better performance than standard methods on a new annotation database containing more than 200 million images and 300 thousand annotations, which is the largest ever reported.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126295",
        "reference_list": [
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2009",
                "id": 39
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 12,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Semantics",
                "Support vector machines",
                "Vectors",
                "Training",
                "Facebook",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "Internet",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Web image annotation",
                "visually-similar images",
                "semantically-related images",
                "single prototypical visual concept",
                "weighted annotations",
                "linear SVM",
                "visual synset membership",
                "weighted voting rule",
                "annotation database"
            ]
        },
        "id": 77,
        "cited_by": [
            {
                "year": "2015",
                "id": 516
            },
            {
                "year": "2013",
                "id": 355
            },
            {
                "year": "2013",
                "id": 452
            }
        ]
    },
    {
        "title": "Multi-hypothesis motion planning for visual object tracking",
        "authors": [
            "Haifeng Gong",
            "Jack Sim",
            "Maxim Likhachev",
            "Jianbo Shi"
        ],
        "abstract": "In this paper, we propose a long-term motion model for visual object tracking. In crowded street scenes, persistent occlusions are a frequent challenge for tracking algorithm and a robust, long-term motion model could help in these situations. Motivated by progresses in robot motion planning, we propose to construct a set of `plausible' plans for each person, which are composed of multiple long-term motion prediction hypotheses that do not include redundancies, unnecessary loops or collisions with other objects. Constructing plausible plan is the key step in utilizing motion planning in object tracking, which has not been fully investigate in robot motion planning. We propose a novel method of efficiently constructing disjoint plans in different homotopy classes, based on winding numbers and winding angles of planned paths around all obstacles. As the goals can be specified by winding numbers and winding angles, we can avoid redundant plans in the same homotopy class and multiple whirls or loops around a single obstacle. We test our algorithm on a challenging, real-world dataset, and compare our algorithm with Linear Trajectory Avoidance and a simplified linear planning model. We find that our algorithm outperforms both algorithms in most sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126296",
        "reference_list": [
            {
                "year": "2009",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 3,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Windings",
                "Planning",
                "Tracking",
                "Trajectory",
                "Vectors",
                "Joining processes",
                "Robots"
            ],
            "INSPEC: Controlled Indexing": [
                "mobile robots",
                "motion control",
                "object tracking",
                "path planning",
                "prediction theory",
                "robot vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multihypothesis motion planning",
                "visual object tracking",
                "long-term motion model",
                "persistent occlusion",
                "robot motion planning",
                "long-term motion prediction hypothesis",
                "plausible plan",
                "homotopy class"
            ]
        },
        "id": 78,
        "cited_by": [
            {
                "year": "2015",
                "id": 485
            },
            {
                "year": "2013",
                "id": 277
            }
        ]
    },
    {
        "title": "Treat samples differently: Object tracking with semi-supervised online CovBoost",
        "authors": [
            "Guorong Li",
            "Lei Qin",
            "Qingming Huang",
            "Junbiao Pang",
            "Shuqiang Jiang"
        ],
        "abstract": "Most feature selection methods for object tracking assume that the labeled samples obtained in the next frames follow the similar distribution with the samples in the previous frame. However, this assumption is not true in some scenarios. As a result, the selected features are not suitable for tracking and the \u201cdrift\u201d problem happens. In this paper, we consider data's distribution in tracking from a new perspective. We classify the samples into three categories: auxiliary samples (samples in the previous frames), target samples (collected in the current frame) and unlabeled samples (obtained in the next frame). To make the best use of them for tracking, we propose a novel semi-supervised transfer learning approach. Specifically, we assume only target samples follow the same distribution as the unlabeled samples and develop a novel semi-supervised CovBoost method. It could utilize auxiliary samples and unlabeled samples effectively when training the best strong classifier for tracking. Furthermore, we develop a new online updating algorithm for semi-supervised CovBoost, making our tracker handle with significant variations of the tracked target and background successfully. We demonstrate the excellent performance of the proposed tracker on several challenging test videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126297",
        "reference_list": [
            {
                "year": "2007",
                "id": 115
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object tracking",
                "semisupervised online CovBoost",
                "feature selection method",
                "drift problem",
                "auxiliary sample",
                "target sample",
                "unlabeled sample",
                "semisupervised transfer learning approach",
                "online updating algorithm"
            ]
        },
        "id": 79,
        "cited_by": [
            {
                "year": "2013",
                "id": 195
            },
            {
                "year": "2013",
                "id": 347
            }
        ]
    },
    {
        "title": "Self-calibrating depth from refraction",
        "authors": [
            "Zhihu Chen",
            "Kwan-Yee K. Wong",
            "Yasuyuki Matsushita",
            "Xiaolong Zhu",
            "Miaomiao Liu"
        ],
        "abstract": "In this paper, we introduce a novel method for depth acquisition based on refraction of light. A scene is captured twice by a fixed perspective camera, with the first image captured directly by the camera and the second by placing a transparent medium between the scene and the camera. A depth map of the scene is then recovered from the displacements of scene points in the images. Unlike other existing depth from refraction methods, our method does not require the knowledge of the pose and refractive index of the transparent medium, but can recover them directly from the input images. We hence call our method self-calibrating depth from refraction. Experimental results on both synthetic and real-world data are presented, which demonstrate the effectiveness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126298",
        "reference_list": [
            {
                "year": "2005",
                "id": 205
            },
            {
                "year": "2009",
                "id": 41
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Refractive index",
                "Three dimensional displays",
                "Closed-form solutions",
                "Apertures",
                "Calibration",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "refraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "self-calibrating depth",
                "depth acquisition",
                "light refraction",
                "transparent medium",
                "depth map"
            ]
        },
        "id": 80,
        "cited_by": []
    },
    {
        "title": "Weakly supervised semantic segmentation with a multi-image model",
        "authors": [
            "Alexander Vezhnevets",
            "Vittorio Ferrari",
            "Joachim M. Buhmann"
        ],
        "abstract": "We propose a novel method for weakly supervised semantic segmentation. Training images are labeled only by the classes they contain, not by their location in the image. On test images instead, the method predicts a class label for every pixel. Our main innovation is a multi-image model (MIM) - a graphical model for recovering the pixel labels of the training images. The model connects superpixels from all training images in a data-driven fashion, based on their appearance similarity. For generalizing to new test images we integrate them into MIM using a learned multiple kernel metric, instead of learning conventional classifiers on the recovered pixel labels. We also introduce an \u201cobjectness\u201d potential, that helps separating objects (e.g. car, dog, human) from background classes (e.g. grass, sky, road). In experiments on the MSRC 21 dataset and the LabelMe subset of [18], our technique outperforms previous weakly supervised methods and achieves accuracy comparable with fully supervised methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126299",
        "reference_list": [
            {
                "year": "2009",
                "id": 39
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 67,
            "other": 22,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Measurement",
                "Image segmentation",
                "Semantics",
                "Kernel",
                "Roads",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "image segmentation",
                "learning (artificial intelligence)",
                "set theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weakly supervised semantic segmentation",
                "multiimage model",
                "pixel labels",
                "graphical model",
                "data-driven fashion",
                "appearance similarity",
                "learned multiple kernel metric",
                "objectness potential",
                "LabelMe subset",
                "training set"
            ]
        },
        "id": 81,
        "cited_by": [
            {
                "year": "2017",
                "id": 222
            },
            {
                "year": "2015",
                "id": 151
            },
            {
                "year": "2015",
                "id": 165
            },
            {
                "year": "2013",
                "id": 279
            },
            {
                "year": "2013",
                "id": 324
            }
        ]
    },
    {
        "title": "Correlative multi-label multi-instance image annotation",
        "authors": [
            "Xiangyang Xue",
            "Wei Zhang",
            "Jie Zhang",
            "Bin Wu",
            "Jianping Fan",
            "Yao Lu"
        ],
        "abstract": "In this paper, each image is viewed as a bag of local regions, as well as it is investigated globally. A novel method is developed for achieving multi-label multi-instance image annotation, where image-level (bag-level) labels and region-level (instance-level) labels are both obtained. The associations between semantic concepts and visual features are mined both at the image level and at the region level. Inter-label correlations are captured by a co-occurence matrix of concept pairs. The cross-level label coherence encodes the consistency between the labels at the image level and the labels at the region level. The associations between visual features and semantic concepts, the correlations among the multiple labels, and the cross-level label coherence are sufficiently leveraged to improve annotation performance. Structural max-margin technique is used to formulate the proposed model and multiple interrelated classifiers are learned jointly. To leverage the available image-level labeled samples for the model training, the region-level label identification on the training set is firstly accomplished by building the correspondences between the multiple bag-level labels and the image regions. JEC distance based kernels are employed to measure the similarities both between images and between regions. Experimental results on real image datasets MSRC and Corel demonstrate the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126300",
        "reference_list": [
            {
                "year": "2009",
                "id": 39
            },
            {
                "year": "2007",
                "id": 125
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "USA Councils"
            ],
            "INSPEC: Controlled Indexing": [
                "correlation methods",
                "image classification",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "correlative multilabel multi-instance image annotation",
                "local regions",
                "image-level labels",
                "semantic concepts",
                "visual features",
                "inter-label correlations",
                "cooccurence matrix",
                "concept pairs",
                "cross-level labels",
                "structural max-margin technique",
                "multiple related classifiers",
                "model training",
                "region-level label identification",
                "multiple bag-level labels",
                "JEC distance based kernels",
                "real image datasets MSRC",
                "Corel"
            ]
        },
        "id": 82,
        "cited_by": [
            {
                "year": "2017",
                "id": 48
            },
            {
                "year": "2017",
                "id": 49
            },
            {
                "year": "2013",
                "id": 372
            }
        ]
    },
    {
        "title": "Non-stationary correction of optical aberrations",
        "authors": [
            "Christian J. Schuler",
            "Michael Hirsch",
            "Stefan Harmeling",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "Taking a sharp photo at several megapixel resolution traditionally relies on high grade lenses. In this paper, we present an approach to alleviate image degradations caused by imperfect optics. We rely on a calibration step to encode the optical aberrations in a space-variant point spread function and obtain a corrected image by non-stationary deconvolution. By including the Bayer array in our image formation model, we can perform demosaicing as part of the deconvolution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126301",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 11,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Lenses",
                "Image color analysis",
                "Optical imaging",
                "Adaptive optics",
                "Optical distortion",
                "Convolution",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "aberrations",
                "calibration",
                "deconvolution",
                "image resolution",
                "lenses",
                "optical transfer function"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonstationary correction",
                "optical aberrations",
                "sharp photo",
                "megapixel resolution",
                "high grade lenses",
                "image degradations",
                "imperfect optics",
                "calibration step",
                "space-variant point spread function",
                "corrected image",
                "nonstationary deconvolution",
                "Bayer array",
                "image formation model",
                "demosaicing"
            ]
        },
        "id": 83,
        "cited_by": [
            {
                "year": "2017",
                "id": 342
            },
            {
                "year": "2015",
                "id": 68
            }
        ]
    },
    {
        "title": "Fast image-based localization using direct 2D-to-3D matching",
        "authors": [
            "Torsten Sattler",
            "Bastian Leibe",
            "Leif Kobbelt"
        ],
        "abstract": "Recently developed Structure from Motion (SfM) reconstruction approaches enable the creation of large scale 3D models of urban scenes. These compact scene representations can then be used for accurate image-based localization, creating the need for localization approaches that are able to efficiently handle such large amounts of data. An important bottleneck is the computation of 2D-to-3D correspondences required for pose estimation. Current stateof- the-art approaches use indirect matching techniques to accelerate this search. In this paper we demonstrate that direct 2D-to-3D matching methods have a considerable potential for improving registration performance. We derive a direct matching framework based on visual vocabulary quantization and a prioritized correspondence search. Through extensive experiments, we show that our framework efficiently handles large datasets and outperforms current state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126302",
        "reference_list": [
            {
                "year": "2009",
                "id": 78
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 117,
            "other": 71,
            "total": 188
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Visualization",
                "Cameras",
                "Solid modeling",
                "Registers",
                "Estimation",
                "Image retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "image reconstruction",
                "image representation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image-based localization",
                "2D-to-3D matching",
                "structure from motion reconstruction",
                "SfM reconstruction",
                "3D models",
                "compact scene representations"
            ]
        },
        "id": 84,
        "cited_by": [
            {
                "year": "2017",
                "id": 0
            },
            {
                "year": "2017",
                "id": 65
            },
            {
                "year": "2017",
                "id": 250
            },
            {
                "year": "2015",
                "id": 271
            },
            {
                "year": "2015",
                "id": 300
            },
            {
                "year": "2015",
                "id": 301
            },
            {
                "year": "2015",
                "id": 505
            },
            {
                "year": "2013",
                "id": 34
            },
            {
                "year": "2013",
                "id": 326
            }
        ]
    },
    {
        "title": "Multiview 3D warps",
        "authors": [
            "Alessio Del Bue",
            "Adrien Bartoli"
        ],
        "abstract": "Image registration and 3D reconstruction are fundamental computer vision and medical imaging problems. They are particularly challenging when the input data are images of a deforming body obtained by a single moving camera. We propose a new modelling framework, the multiview 3D warps. Existing models are twofold: they estimate inter-image warps which are often inconsistent between the different images and do not model the underlying 3D structure, or reconstruct just a sparse set of points. In contrast, our multiview 3D warps combine the advantages of both; they have an explicit 3D component and a set of 3D deformations combined with projection to 2D. They thus capture the dense deforming body's time-varying shape and camera pose. The advantages over the classical solutions are numerous: thanks to our feature-based estimation method for the multiview 3D warps, one can not only augment the original images but also retarget or clone the observed body's 3D deformations by changing the pose. Experimental results on simulated and real data are reported, confirming the advantages of our framework over existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126303",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Shape",
                "Cameras",
                "Solid modeling",
                "Optimization",
                "Measurement",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image registration",
                "pose estimation",
                "shape recognition",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview 3D warps",
                "image registration",
                "3D reconstruction",
                "computer vision",
                "medical imaging",
                "moving camera",
                "modelling framework",
                "inter-image warp",
                "3D component",
                "3D deformation",
                "dense deforming body",
                "time-varying shape",
                "camera pose",
                "feature-based estimation"
            ]
        },
        "id": 85,
        "cited_by": []
    },
    {
        "title": "Temporally coded flash illumination for motion deblurring",
        "authors": [
            "Scott McCloskey"
        ],
        "abstract": "We use temporally sequenced flash illumination to capture coded exposure images of fast-moving objects in low light environments. These coded flash images allow for accurate estimation of blur-free latent images in the presence of object motion. By distributing flashes over a window of time, we lessen eye safety concerns associated with powerful all-at-once flashes. We show how our flash-based coded exposure system has better robustness to increasing object velocity than shutter-based exposure coding, thereby obviating the need for pre-exposure velocity estimation. We also show that the quality of the estimated sharp image is robust to varying levels of ambient illumination. This and other benefits of our coded flash system are demonstrated with real images acquired using prototype hardware.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126304",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Ash",
                "Lighting",
                "Cameras",
                "Timing",
                "Image reconstruction",
                "Estimation",
                "Optical sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "lighting",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporally coded flash illumination",
                "motion deblurring",
                "temporally sequenced flash illumination",
                "coded exposure image",
                "blur-free latent image estimation",
                "object motion",
                "eye safety concern",
                "all-at-once flash",
                "flash-based coded exposure system",
                "ambient illumination"
            ]
        },
        "id": 86,
        "cited_by": []
    },
    {
        "title": "Multiplexed illumination for scene recovery in the presence of global illumination",
        "authors": [
            "Jinwei Gu",
            "Toshihiro Kobayashi",
            "Mohit Gupta",
            "Shree K. Nayar"
        ],
        "abstract": "Global illumination effects such as inter-reflections and subsurface scattering result in systematic, and often significant errors in scene recovery using active illumination. Recently, it was shown that the direct and global components could be separated efficiently for a scene illuminated with a single light source. In this paper, we study the problem of direct-global separation for multiple light sources. We derive a theoretical lower bound for the number of required images, and propose a multiplexed illumination scheme which achieves this lower bound. We analyze the signal-to-noise ratio (SNR) characteristics of the proposed illumination multiplexing method in the context of direct-global separation. We apply our method to several scene recovery techniques requiring multiple light sources, including shape from shading, structured light 3D scanning, photometric stereo, and reflectance estimation. Both simulation and experimental results show that the proposed method can accurately recover scene information with fewer images compared to sequentially separating direct-global components for each light source.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126305",
        "reference_list": [
            {
                "year": "2007",
                "id": 250
            },
            {
                "year": "2003",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 0,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Light sources",
                "Lighting",
                "Signal to noise ratio",
                "Frequency modulation",
                "Frequency division multiplexing"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "illumination multiplexing method",
                "scene recovery",
                "global illumination",
                "interreflection",
                "subsurface scattering",
                "active illumination",
                "direct-global separation",
                "multiple light sources",
                "signal-to-noise ratio",
                "SNR characteristic",
                "shape from shading",
                "structured light 3D scanning",
                "photometric stereo",
                "reflectance estimation"
            ]
        },
        "id": 87,
        "cited_by": [
            {
                "year": "2013",
                "id": 179
            },
            {
                "year": "2013",
                "id": 184
            }
        ]
    },
    {
        "title": "Inferring social relations from visual concepts",
        "authors": [
            "Lei Ding",
            "Alper Yilmaz"
        ],
        "abstract": "In this paper, we study the problem of social relational inference using visual concepts which serve as indicators of actors' social interactions. While social network analysis from videos has started to gain attention in the recent years, the existing work either uses proximity or co-occurrence statistics, or exploit a holistic model of the scene content where the relations are assumed to stay constant throughout the video. This work permits changing relations and argues that there exists a relationship between the visual concepts and the social relations among actors, which is a fundamentally new concept in computer vision. Specifically, we leverage the existing large-scale concept detectors to generate concept score vectors to represent the video content, and we further map them to grouping cues that are used to detect the social structure. In our framework, a probabilistic graphical model with temporal smoothing provides a means to analyze social relations among actors and detect communities. Experiments on Youtube videos and theatrical movies validate the proposed framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126306",
        "reference_list": [
            {
                "year": "2009",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 5,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Visualization",
                "Communities",
                "Social network services",
                "Feature extraction",
                "Semantics",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "social networking (online)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual concepts",
                "social relational inference",
                "actor social interaction",
                "social network analysis",
                "co-occurrence statistics",
                "computer vision",
                "large-scale concept detectors",
                "video content",
                "probabilistic graphical model",
                "temporal smoothing",
                "Youtube videos",
                "theatrical movies"
            ]
        },
        "id": 88,
        "cited_by": [
            {
                "year": "2015",
                "id": 405
            }
        ]
    },
    {
        "title": "Sparse dictionary-based representation and recognition of action attributes",
        "authors": [
            "Qiang Qiu",
            "Zhuolin Jiang",
            "Rama Chellappa"
        ],
        "abstract": "We present an approach for dictionary learning of action attributes via information maximization. We unify the class distribution and appearance information into an objective function for learning a sparse dictionary of action attributes. The objective function maximizes the mutual information between what has been learned and what remains to be learned in terms of appearance information and class distribution for each dictionary item. We propose a Gaussian Process (GP) model for sparse representation to optimize the dictionary objective function. The sparse coding property allows a kernel with a compact support in GP to realize a very efficient dictionary learning process. Hence we can describe an action video by a set of compact and discriminative action attributes. More importantly, we can recognize modeled action categories in a sparse feature space, which can be generalized to unseen and unmodeled action categories. Experimental results demonstrate the effectiveness of our approach in action recognition applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126307",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2009",
                "id": 56
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 11,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Humans",
                "Entropy",
                "Encoding",
                "Mutual information",
                "Histograms",
                "Probabilistic logic"
            ],
            "INSPEC: Controlled Indexing": [
                "dictionaries",
                "Gaussian processes",
                "image coding",
                "image representation",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparse dictionary-based representation",
                "sparse dictionary-based recognition",
                "action attributes",
                "dictionary learning process",
                "information maximization",
                "class distribution",
                "appearance information",
                "Gaussian process model",
                "sparse coding property",
                "modeled action categories",
                "unmodeled action categories",
                "sparse feature space"
            ]
        },
        "id": 89,
        "cited_by": [
            {
                "year": "2015",
                "id": 26
            },
            {
                "year": "2013",
                "id": 151
            }
        ]
    },
    {
        "title": "Robust unsupervised motion pattern inference from video and applications",
        "authors": [
            "Xuemei Zhao",
            "G\u00e9rard Medioni"
        ],
        "abstract": "We propose an unsupervised learning framework to infer motion patterns in videos and in turn use them to improve tracking of moving objects in sequences from static cameras. Based on tracklets, we use a manifold learning method Tensor Voting to infer the local geometric structures in (x, y) space, and embed tracklet points into (x, y, \u03b8) space, where \u03b8 represents motion direction. In this space, points automatically form intrinsic manifold structures, each of which corresponds to a motion pattern. To define each group, a novel robustmanifold grouping algorithm is proposed. Tensor Voting is performed to provide multiple geometric cues which formulate multiple similarity kernels between any pair of points, and a spectral clustering technique is used in this multiple kernel setting. The grouping algorithm achieves better performance than state-of-the-art methods in our applications. Extracted motion patterns can then be used as a prior to improve the performance of any object tracker. It is especially useful to reduce false alarms and ID switches. Experiments are performed on challenging real-world sequences, and a quantitative analysis of the results shows the framework effectively improves state-of-the-art tracker.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126308",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Manifolds",
                "Tensile stress",
                "Kernel",
                "Motion segmentation",
                "Robustness",
                "Eigenvalues and eigenfunctions"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image motion analysis",
                "inference mechanisms",
                "object tracking",
                "pattern recognition",
                "unsupervised learning",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised motion pattern inference",
                "video",
                "unsupervised learning framework",
                "motion patterns",
                "moving object tracking",
                "tensor voting",
                "geometric structures"
            ]
        },
        "id": 90,
        "cited_by": [
            {
                "year": "2015",
                "id": 350
            }
        ]
    },
    {
        "title": "Articulated part-based model for joint object detection and pose estimation",
        "authors": [
            "Min Sun",
            "Silvio Savarese"
        ],
        "abstract": "Despite recent successes, pose estimators are still somewhat fragile, and they frequently rely on a precise knowledge of the location of the object. Unfortunately, articulated objects are also very difficult to detect. Knowledge about the articulated nature of these objects, however, can substantially contribute to the task of finding them in an image. It is somewhat surprising, that these two tasks are usually treated entirely separately. In this paper, we propose an Articulated Part-based Model (APM) for jointly detecting objects and estimating their poses. APM recursively represents an object as a collection of parts at multiple levels of detail, from coarse-to-fine, where parts at every level are connected to a coarser level through a parent-child relationship (Fig. 1(b)-Horizontal). Parts are further grouped into part-types (e.g., left-facing head, long stretching arm, etc) so as to model appearance variations (Fig. 1(b)-Vertical). By having the ability to share appearance models of part types and by decomposing complex poses into parent-child pairwise relationships, APM strikes a good balance between model complexity and model richness. Extensive quantitative and qualitative experiment results on public datasets show that APM outperforms state-of-the-art methods. We also show results on PASCAL 2007 - cats and dogs - two highly challenging articulated object categories.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126309",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2005",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 63,
            "other": 29,
            "total": 92
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Estimation",
                "Head",
                "Deformable models",
                "Torso",
                "Feature extraction",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "articulated part-based model",
                "pose estimation",
                "object detection",
                "left-facing head",
                "long stretching arm",
                "model appearance variations",
                "share appearance models",
                "parent-child pairwise relationships",
                "model complexity",
                "model richness",
                "public datasets",
                "PASCAL 2007",
                "articulated object categories"
            ]
        },
        "id": 91,
        "cited_by": [
            {
                "year": "2017",
                "id": 319
            },
            {
                "year": "2015",
                "id": 175
            },
            {
                "year": "2015",
                "id": 220
            },
            {
                "year": "2015",
                "id": 278
            },
            {
                "year": "2015",
                "id": 374
            },
            {
                "year": "2013",
                "id": 90
            },
            {
                "year": "2013",
                "id": 302
            },
            {
                "year": "2013",
                "id": 314
            },
            {
                "year": "2013",
                "id": 425
            }
        ]
    },
    {
        "title": "Accurate 3D pose estimation from a single depth image",
        "authors": [
            "Mao Ye",
            "Xianwang Wang",
            "Ruigang Yang",
            "Liu Ren",
            "Marc Pollefeys"
        ],
        "abstract": "This paper presents a novel system to estimate body pose configuration from a single depth map. It combines both pose detection and pose refinement. The input depth map is matched with a set of pre-captured motion exemplars to generate a body configuration estimation, as well as semantic labeling of the input point cloud. The initial estimation is then refined by directly fitting the body configuration with the observation (e.g., the input depth). In addition to the new system architecture, our other contributions include modifying a point cloud smoothing technique to deal with very noisy input depth maps, a point cloud alignment and pose search algorithm that is view-independent and efficient. Experiments on a public dataset show that our approach achieves significantly higher accuracy than previous state-of-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126310",
        "reference_list": [
            {
                "year": "2009",
                "id": 21
            }
        ],
        "citation": {
            "ieee": 68,
            "other": 44,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Joints",
                "Shape",
                "Cameras",
                "Sensors",
                "Accuracy",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "object detection",
                "pose estimation",
                "smoothing methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D pose estimation",
                "single depth image",
                "body pose configuration estimation",
                "pose detection",
                "pose refinement",
                "input depth map",
                "input point cloud semantic labeling",
                "point cloud smoothing technique",
                "point cloud alignment",
                "pose search algorithm",
                "human motion modeling"
            ]
        },
        "id": 92,
        "cited_by": [
            {
                "year": "2017",
                "id": 363
            },
            {
                "year": "2015",
                "id": 344
            },
            {
                "year": "2013",
                "id": 137
            },
            {
                "year": "2013",
                "id": 306
            }
        ]
    },
    {
        "title": "Means in spaces of tree-like shapes",
        "authors": [
            "Aasa Feragen",
            "S\u00f8ren Hauberg",
            "Mads Nielsen",
            "Fran\u00e7ois Lauze"
        ],
        "abstract": "The mean is often the most important statistic of a dataset as it provides a single point that summarizes the entire set. While the mean is readily defined and computed in Euclidean spaces, no commonly accepted solutions are currently available in more complicated spaces, such as spaces of tree-structured data. In this paper we study the notion of means, both generally in Gromov's CAT(0)-spaces (metric spaces of non-positive curvature), but also specifically in the space of tree-like shapes. We prove local existence and uniqueness of means in such spaces and discuss three different algorithms for computing means. We make an experimental evaluation of the three algorithms through experiments on three different sets of data with tree-like structure: a synthetic dataset, a leaf morphology dataset from images, and a set of human airway subtrees from medical CT scans. This experimental study provides great insight into the behavior of the different methods and how they relate to each other. More importantly, it also provides mathematically well-founded, tractable and robust \u201caverage trees\u201d. This statistic is of utmost importance due to the ever-presence of tree-like structures in human anatomy, e.g., airways and vascularization systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126311",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 6,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Extraterrestrial measurements",
                "Complexity theory",
                "Geometry",
                "Manifolds",
                "Three dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "shape recognition",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tree-like shapes",
                "mean",
                "statistics",
                "Euclidean space",
                "tree-structured data",
                "CAT(0)-spaces",
                "metric space",
                "nonpositive curvature",
                "tree-like structure",
                "leaf morphology dataset",
                "human airway subtrees",
                "medical CT scan",
                "human anatomy"
            ]
        },
        "id": 93,
        "cited_by": []
    },
    {
        "title": "Probabilistic group-level motion analysis and scenario recognition",
        "authors": [
            "Ming-Ching Chang",
            "Nils Krahnstoever",
            "Weina Ge"
        ],
        "abstract": "This paper addresses the challenge of recognizing behavior of groups of individuals in unconstraint surveillance environments. As opposed to approaches that rely on agglomerative or decisive hierarchical clustering techniques, we propose to recognize group interactions without making hard decisions about the underlying group structure. Instead we use a probabilistic grouping strategy evaluated from the pairwise spatial-temporal tracking information. A path-based grouping scheme determines a soft segmentation of groups and produces a weighted connection graph where its edges express the probability of individuals belonging to a group. Without further segmenting this graph, we show how a large number of low- and high-level behavior recognition tasks can be performed. Our work builds on a mature multi-camera multi-target person tracking system that operates in real-time. We derive probabilistic models to analyze individual track motion as well as group interactions. We show that the soft grouping can combine with motion analysis elegantly to robustly detect and predict group-level activities. Experimental results demonstrate the efficacy of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126312",
        "reference_list": [
            {
                "year": "2003",
                "id": 98
            },
            {
                "year": "2009",
                "id": 33
            },
            {
                "year": "2009",
                "id": 178
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 20,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Probabilistic logic",
                "Tracking",
                "Legged locomotion",
                "Cognition",
                "Robustness",
                "History"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "group theory",
                "image motion analysis",
                "image recognition",
                "image segmentation",
                "pattern clustering",
                "probability",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic group-level motion analysis",
                "scenario recognition",
                "agglomerative technique",
                "decisive hierarchical clustering technique",
                "group interaction recognition",
                "probabilistic grouping strategy",
                "pairwise spatial-temporal tracking information",
                "path-based grouping scheme",
                "soft segmentation",
                "weighted connection graph",
                "low-level behavior recognition task",
                "high-level behavior recognition task",
                "multicamera multitarget person tracking system",
                "soft grouping"
            ]
        },
        "id": 94,
        "cited_by": []
    },
    {
        "title": "A linear subspace learning approach via sparse coding",
        "authors": [
            "Lei Zhang",
            "Pengfei Zhu",
            "Qinghua Hu",
            "David Zhang"
        ],
        "abstract": "Linear subspace learning (LSL) is a popular approach to image recognition and it aims to reveal the essential features of high dimensional data, e.g., facial images, in a lower dimensional space by linear projection. Most LSL methods compute directly the statistics of original training samples to learn the subspace. However, these methods do not effectively exploit the different contributions of different image components to image recognition. We propose a novel LSL approach by sparse coding and feature grouping. A dictionary is learned from the training dataset, and it is used to sparsely decompose the training samples. The decomposed image components are grouped into a more discriminative part (MDP) and a less discriminative part (LDP). An unsupervised criterion and a supervised criterion are then proposed to learn the desired subspace, where the MDP is preserved and the LDP is suppressed simultaneously. The experimental results on benchmark face image databases validated that the proposed methods outperform many state-of-the-art LSL schemes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126313",
        "reference_list": [
            {
                "year": "2011",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 12,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Dictionaries",
                "Image coding",
                "Face",
                "Principal component analysis",
                "Encoding",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image coding",
                "learning (artificial intelligence)",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear subspace learning approach",
                "sparse coding",
                "image recognition",
                "high dimensional data",
                "facial images",
                "lower dimensional space",
                "linear projection",
                "LSL methods",
                "statistics",
                "original training samples",
                "feature grouping",
                "dictionary",
                "training dataset",
                "decomposed image components",
                "MDP",
                "less discriminative part",
                "LDP",
                "unsupervised criterion",
                "benchmark face image databases",
                "state-of-the-art LSL schemes"
            ]
        },
        "id": 95,
        "cited_by": []
    },
    {
        "title": "Discovering object instances from scenes of Daily Living",
        "authors": [
            "Hongwen Kang",
            "Martial Hebert",
            "Takeo Kanade"
        ],
        "abstract": "We propose an approach to identify and segment objects from scenes that a person (or robot) encounters in Activities of Daily Living (ADL). Images collected in those cluttered scenes contain multiple objects. Each image provides only a partial, possibly very different view of each object. An object instance discovery program must be able to link pieces of visual information from multiple images and extract the consistent patterns. Most papers on unsupervised discovery of object models are concerned with object categories. In contrast, this paper aims at identifying and extracting regions corresponding to specific object instances, e.g., two different laptops in the laptop category. By focusing on specific instances, we enforce explicit constraints on geometric consistency (such as scale, orientation), and appearance consistency (such as color, texture and shape). Using multiple segmentations as the basic building block, our program processes a noisy \"soup\" of segments and extracts object models as groups of mutually consistent segments. Our approach was tested on three different types of image sets: two from indoor ADL environments and one from Flickr.com. The results demonstrate robustness of our program to severe clutter, occlusion, changes of viewpoint and interference from irrelevant images. Our approach achieves significant improvement over with two existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126314",
        "reference_list": [
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2009",
                "id": 59
            },
            {
                "year": "2007",
                "id": 67
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 0,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Image color analysis",
                "Robots",
                "Object recognition",
                "Robustness",
                "Portable computers"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer graphics",
                "feature extraction",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object instance discovery program",
                "object identification",
                "image segmentation",
                "activities-of-daily living",
                "object model unsupervised discovery",
                "object categories",
                "region identification",
                "region extraction",
                "geometric consistency",
                "appearance consistency",
                "indoor ADL environments",
                "Flickr.com"
            ]
        },
        "id": 96,
        "cited_by": []
    },
    {
        "title": "Close the loop: Joint blind image restoration and recognition with sparse representation prior",
        "authors": [
            "Haichao Zhang",
            "Jianchao Yang",
            "Yanning Zhang",
            "Nasser M. Nasrabadi",
            "Thomas S. Huang"
        ],
        "abstract": "Most previous visual recognition systems simply assume ideal inputs without real-world degradations, such as low resolution, motion blur and out-of-focus blur. In presence of such unknown degradations, the conventional approach first resorts to blind image restoration and then feeds the restored image into a classifier. Treating restoration and recognition separately, such a straightforward approach, however, suffers greatly from the defective output of the ill-posed blind image restoration. In this paper, we present a joint blind image restoration and recognition method based on the sparse representation prior to handle the challenging problem of face recognition from low-quality images, where the degradation model is realistic and totally unknown. The sparse representation prior states that the degraded input image, if correctly restored, will have a good sparse representation in terms of the training set, which indicates the identity of the test image. The proposed algorithm achieves simultaneous restoration and recognition by iteratively solving the blind image restoration in pursuit of the sparest representation for recognition. Based on such a sparse representation prior, we demonstrate that the image restoration task and the recognition task can benefit greatly from each other. Extensive experiments on face datasets under various degradations are carried out and the results of our joint model shows significant improvements over conventional methods of treating the two tasks independently.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126315",
        "reference_list": [
            {
                "year": "2007",
                "id": 76
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 6,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Image recognition",
                "Training",
                "Kernel",
                "Degradation",
                "Face",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "blind image restoration",
                "image recognition",
                "sparse representation prior",
                "visual recognition system",
                "low resolution image",
                "motion blur",
                "out-of-focus blur",
                "face recognition"
            ]
        },
        "id": 97,
        "cited_by": [
            {
                "year": "2017",
                "id": 26
            },
            {
                "year": "2017",
                "id": 501
            },
            {
                "year": "2013",
                "id": 311
            }
        ]
    },
    {
        "title": "Learning spatiotemporal graphs of human activities",
        "authors": [
            "William Brendel",
            "Sinisa Todorovic"
        ],
        "abstract": "Complex human activities occurring in videos can be defined in terms of temporal configurations of primitive actions. Prior work typically hand-picks the primitives, their total number, and temporal relations (e.g., allow only followed-by), and then only estimates their relative significance for activity recognition. We advance prior work by learning what activity parts and their spatiotemporal relations should be captured to represent the activity, and how relevant they are for enabling efficient inference in realistic videos. We represent videos by spatiotemporal graphs, where nodes correspond to multiscale video segments, and edges capture their hierarchical, temporal, and spatial relationships. Access to video segments is provided by our new, multiscale segmenter. Given a set of training spatiotemporal graphs, we learn their archetype graph, and pdf's associated with model nodes and edges. The model adaptively learns from data relevant video segments and their relations, addressing the \u201cwhat\u201d and \u201chow.\u201d Inference and learning are formulated within the same framework - that of a robust, least-squares optimization - which is invariant to arbitrary permutations of nodes in spatiotemporal graphs. The model is used for parsing new videos in terms of detecting and localizing relevant activity parts. We out-perform the state of the art on benchmark Olympic and UT human-interaction datasets, under a favorable complexity-vs.-accuracy trade-off.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126316",
        "reference_list": [
            {
                "year": "2007",
                "id": 87
            },
            {
                "year": "2007",
                "id": 55
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2009",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 85,
            "other": 52,
            "total": 137
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Electron tubes",
                "Spatiotemporal phenomena",
                "Hidden Markov models",
                "Training",
                "Optimization",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "graphs",
                "image recognition",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatiotemporal graphs",
                "complex human activities",
                "activity recognition",
                "spatiotemporal relations",
                "realistic videos",
                "multiscale video segment",
                "multiscale segmenter",
                "archetype graph"
            ]
        },
        "id": 98,
        "cited_by": [
            {
                "year": "2017",
                "id": 308
            },
            {
                "year": "2015",
                "id": 6
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2015",
                "id": 521
            },
            {
                "year": "2013",
                "id": 3
            },
            {
                "year": "2013",
                "id": 335
            },
            {
                "year": "2013",
                "id": 340
            },
            {
                "year": "2013",
                "id": 342
            },
            {
                "year": "2013",
                "id": 395
            },
            {
                "year": "2013",
                "id": 443
            }
        ]
    },
    {
        "title": "A chains model for localizing participants of group activities in videos",
        "authors": [
            "Mohamed R. Amer",
            "Sinisa Todorovic"
        ],
        "abstract": "Given a video, we would like to recognize group activities, localize video parts where these activities occur, and detect actors involved in them. This advances prior work that typically focuses only on video classification. We make a number of contributions. First, we specify a new, mid-level, video feature aimed at summarizing local visual cues into bags of the right detections (BORDs). BORDs seek to identify the right people who participate in a target group activity among many noisy people detections. Second, we formulate a new, generative, chains model of group activities. Inference of the chains model identifies a subset of BORDs in the video that belong to occurrences of the activity, and organizes them in an ensemble of temporal chains. The chains extend over, and thus localize, the time intervals occupied by the activity. We formulate a new MAP inference algorithm that iterates two steps: i) Warps the chains of BORDs in space and time to their expected locations, so the transformed BORDs can better summarize local visual cues; and ii) Maximizes the posterior probability of the chains. We outperform the state of the art on benchmark UT-Human Interaction and Collective Activities datasets, under reasonable running times.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126317",
        "reference_list": [
            {
                "year": "2007",
                "id": 55
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2009",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 14,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Feature extraction",
                "Histograms",
                "Humans",
                "Spatiotemporal phenomena",
                "Visualization",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "maximum likelihood estimation",
                "probability",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "chains model",
                "group activities",
                "video classification",
                "video feature",
                "local visual cues",
                "bags of the right detections",
                "temporal chains",
                "time interval",
                "MAP inference algorithm",
                "posterior probability",
                "UT-human interaction"
            ]
        },
        "id": 99,
        "cited_by": [
            {
                "year": "2015",
                "id": 362
            }
        ]
    },
    {
        "title": "Unsupervised metric learning by Self-Smoothing Operator",
        "authors": [
            "Jiayan Jiang",
            "Bo Wang",
            "Zhuowen Tu"
        ],
        "abstract": "In this paper, we propose a diffusion-based approach to improve an input similarity metric. The diffusion process propagates similarity mass along the intrinsic manifold of data points. Our approach results in a global similarity metric which differs from the query-specific one for ranking produced by label propagation [26]. Unlike diffusion maps [7], our approach directly improves a given similarity metric without introducing any extra distance notions. We call our approach Self-Smoothing Operator (SSO). To demonstrate its wide applicability, experiments are reported on image retrieval, clustering, classification, and segmentation tasks. In most cases, using SSO results in significant performance gains over the original similarity metrics, with also very evident advantage over diffusion maps.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126318",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 12,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Kernel",
                "Smoothing methods",
                "Manifolds",
                "Shape",
                "Diffusion processes",
                "MPEG 7 Standard"
            ],
            "INSPEC: Controlled Indexing": [
                "diffusion",
                "image classification",
                "image retrieval",
                "mathematical operators",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised metric learning",
                "self-smoothing operator",
                "diffusion-based approach",
                "input similarity metric",
                "diffusion process",
                "similarity mass",
                "intrinsic manifold",
                "data points",
                "global similarity metric",
                "label propagation",
                "diffusion maps",
                "distance notions",
                "SSO",
                "image retrieval",
                "image clustering",
                "image classification",
                "image segmentation"
            ]
        },
        "id": 100,
        "cited_by": [
            {
                "year": "2017",
                "id": 80
            },
            {
                "year": "2013",
                "id": 53
            }
        ]
    },
    {
        "title": "Kernel non-rigid structure from motion",
        "authors": [
            "Paulo F. U. Gotardo",
            "Aleix M. Martinez"
        ],
        "abstract": "Non-rigid structure from motion (NRSFM) is a difficult, underconstrained problem in computer vision. The standard approach in NRSFM constrains 3D shape deformation using a linear combination of K basis shapes; the solution is then obtained as the low-rank factorization of an input observation matrix. An important but overlooked problem with this approach is that non-linear deformations are often observed; these deformations lead to a weakened low-rank constraint due to the need to use additional basis shapes to linearly model points that move along curves. Here, we demonstrate how the kernel trick can be applied in standard NRSFM. As a result, we model complex, deformable 3D shapes as the outputs of a non-linear mapping whose inputs are points within a low-dimensional shape space. This approach is flexible and can use different kernels to build different non-linear models. Using the kernel trick, our model complements the low-rank constraint by capturing non-linear relationships in the shape coefficients of the linear model. The net effect can be seen as using non-linear dimensionality reduction to further compress the (shape) space of possible solutions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126319",
        "reference_list": [],
        "citation": {
            "ieee": 37,
            "other": 17,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "deformation",
                "image motion analysis",
                "matrix decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonrigid structure from motion",
                "NRSFM",
                "computer vision",
                "3D shape deformation",
                "K basis shapes",
                "low-rank factorization"
            ]
        },
        "id": 101,
        "cited_by": [
            {
                "year": "2017",
                "id": 409
            },
            {
                "year": "2015",
                "id": 84
            }
        ]
    },
    {
        "title": "Active scene recognition with vision and language",
        "authors": [
            "Xiaodong Yu",
            "Cornelia Ferm\u00fcller",
            "Ching Lik Teo",
            "Yezhou Yang",
            "Yiannis Aloimonos"
        ],
        "abstract": "This paper presents a novel approach to utilizing high level knowledge for the problem of scene recognition in an active vision framework, which we call active scene recognition. In traditional approaches, high level knowledge is used in the post-processing to combine the outputs of the object detectors to achieve better classification performance. In contrast, the proposed approach employs high level knowledge actively by implementing an interaction between a reasoning module and a sensory module (Figure 1). Following this paradigm, we implemented an active scene recognizer and evaluated it with a dataset of 20 scenes and 100+ objects. We also extended it to the analysis of dynamic scenes for activity recognition with attributes. Experiments demonstrate the effectiveness of the active paradigm in introducing attention and additional constraints into the sensing process.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126320",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cognition",
                "Training",
                "Detectors",
                "Equations",
                "Support vector machines",
                "Accuracy",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "inference mechanisms",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active scene recognition",
                "high level knowledge utilization",
                "object detectors",
                "classification performance",
                "reasoning module",
                "sensory module",
                "sensing process",
                "computer vision"
            ]
        },
        "id": 102,
        "cited_by": []
    },
    {
        "title": "Informative feature selection for object recognition via Sparse PCA",
        "authors": [
            "Nikhil Naikal",
            "Allen Y. Yang",
            "S. Shankar Sastry"
        ],
        "abstract": "Bag-of-words (BoW) methods are a popular class of object recognition methods that use image features (e.g., SIFT) to form visual dictionaries and subsequent histogram vectors to represent object images in the recognition process. The accuracy of the BoW classifiers, however, is often limited by the presence of uninformative features extracted from the background or irrelevant image segments. Most existing solutions to prune out uninformative features rely on enforcing pairwise epipolar geometry via an expensive structure-from-motion (SfM) procedure. Such solutions are known to break down easily when the camera transformation is large or when the features are extracted from low-resolution, low-quality images. In this paper, we propose a novel method to select informative object features using a more efficient algorithm called Sparse PCA. First, we show that using a large-scale multiple-view object database, informative features can be reliably identified from a highdimensional visual dictionary by applying Sparse PCA on the histograms of each object category. Our experiment shows that the new algorithm improves recognition accuracy compared to the traditional BoW methods and SfM methods. Second, we present a new solution to Sparse PCA as a semidefinite programming problem using the Augmented Lagrangian Method. The new solver outperforms the state of the art for estimating sparse principal vectors as a basis for a low-dimensional subspace model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126321",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 15,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis",
                "Visualization",
                "Feature extraction",
                "Object recognition",
                "Covariance matrix",
                "Training",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image retrieval",
                "mathematical programming",
                "object recognition",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "informative feature selection",
                "object recognition",
                "sparse PCA",
                "bag-of-words method",
                "BoW method",
                "image feature",
                "SIFT",
                "visual dictionary",
                "subsequent histogram vector",
                "feature extraction",
                "pairwise epipolar geometry",
                "expensive structure-from-motion procedure",
                "SfM procedure",
                "large-scale multiple-view object database",
                "semidefinite programming",
                "augmented Lagrangian method",
                "sparse principal vector",
                "low-dimensional subspace model"
            ]
        },
        "id": 103,
        "cited_by": [
            {
                "year": "2015",
                "id": 475
            }
        ]
    },
    {
        "title": "Simultaneous multi-body stereo and segmentation",
        "authors": [
            "Guofeng Zhang",
            "Jiaya Jia",
            "Hujun Bao"
        ],
        "abstract": "This paper presents a novel multi-body multi-view stereo method to simultaneously recover dense depth maps and perform segmentation with the input of a monocular image sequence. Unlike traditional multi-view stereo approaches that generally handle a single static scene or an object, we show that depth estimation and segmentation can be jointly modeled and be globally solved in an energy minimization framework for ubiquitous scenes containing multiple independently moving rigid objects. Our major contribution includes a new multi-body stereo model, which integrates the color, geometry, and layer constraints for spatio-temporal depth recovery and automatic object segmentation. A two-pass optimization scheme is proposed to progressively update the estimates. Our method is applied to a variety of challenging examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126322",
        "reference_list": [
            {
                "year": "2007",
                "id": 143
            },
            {
                "year": "2007",
                "id": 124
            },
            {
                "year": "2001",
                "id": 70
            },
            {
                "year": "2005",
                "id": 171
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Three dimensional displays",
                "Labeling",
                "Cameras",
                "Geometry",
                "Stereo vision",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image sequences",
                "optimisation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multibody stereo image",
                "image segmentation",
                "multiview stereo method",
                "dense depth maps",
                "monocular image sequence",
                "optimization"
            ]
        },
        "id": 104,
        "cited_by": []
    },
    {
        "title": "Multi-label visual classification with label exclusive context",
        "authors": [
            "Xiangyu Chen",
            "Xiao-Tong Yuan",
            "Qiang Chen",
            "Shuicheng Yan",
            "Tat-Seng Chua"
        ],
        "abstract": "We introduce in this paper a novel approach to multi-label image classification which incorporates a new type of context - label exclusive context - with linear representation and classification. Given a set of exclusive label groups that describe the negative relationship among class labels, our method, namely LELR for Label Exclusive Linear Representation, enforces repulsive assignment of the labels from each group to a query image. The problem can be formulated as an exclusive Lasso (eLasso) model with group overlaps and affine transformation. Since existing eLasso solvers are not directly applicable to solving such an variant of eLasso in our setting, we propose a Nesterov's smoothing approximation algorithm for efficient optimization. Extensive comparing experiments on the challenging real-world visual classification benchmarks demonstrate the effectiveness of incorporating label exclusive context into visual classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126323",
        "reference_list": [
            {
                "year": "2009",
                "id": 29
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 11,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Context",
                "Visualization",
                "Kernel",
                "Approximation methods",
                "Training",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image classification",
                "image representation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multilabel visual classification",
                "label exclusive context",
                "multilabel image classification",
                "linear classification",
                "label exclusive linear representation",
                "exclusive lasso model",
                "affine transformation",
                "Nesterov smoothing approximation algorithm",
                "optimization efficiency",
                "visual classification benchmark"
            ]
        },
        "id": 105,
        "cited_by": [
            {
                "year": "2015",
                "id": 129
            }
        ]
    },
    {
        "title": "Efficient algorithm for low-rank matrix factorization with missing components and performance comparison of latest algorithms",
        "authors": [
            "Takayuki Okatani",
            "Takahiro Yoshida",
            "Koichiro Deguchi"
        ],
        "abstract": "This paper examines numerical algorithms for factorization of a low-rank matrix with missing components. We first propose a new method that incorporates a damping factor into the Wiberg method to solve the problem. The new method is characterized by the way it constrains the ambiguity of the matrix factorization, which helps improve both the global convergence ability and the local convergence speed. We then present experimental comparisons with the latest methods used to solve the problem. No comprehensive comparison of the methods that have been proposed recently has yet been reported in literature. In our experiments, we prioritize the assessment of the global convergence performance of each method, that is, how often and how fast the method can reach the global optimum starting from random initial values. Our conclusion is that top performance is achieved by a group of methods based on Newton-family minimization with damping factor that reduce the problem by eliminating either of the two factored matrices. Our method, which belongs to this group, consistently shows a 100% global convergence rate for different types of affine structure from motion data with a very high population of missing components.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126324",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 12,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Convergence",
                "Damping",
                "Equations",
                "MATLAB",
                "Newton method",
                "Dinosaurs",
                "Sparse matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "affine transforms",
                "convergence",
                "damping",
                "image processing",
                "matrix decomposition",
                "minimisation",
                "Newton method",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "low-rank matrix factorization",
                "numerical algorithms",
                "damping factor",
                "Wiberg method",
                "global convergence ability",
                "local convergence speed",
                "global convergence performance",
                "random initial values",
                "Newton-family minimization",
                "factored matrices",
                "global convergence rate",
                "affine structure",
                "motion data",
                "image processing"
            ]
        },
        "id": 106,
        "cited_by": [
            {
                "year": "2017",
                "id": 527
            },
            {
                "year": "2013",
                "id": 166
            },
            {
                "year": "2013",
                "id": 310
            }
        ]
    },
    {
        "title": "Simultaneous correspondence and non-rigid 3D reconstruction of the coronary tree from single X-ray images",
        "authors": [
            "Eduard Serradell",
            "Adriana Romero",
            "Rub\u00e9n Leta",
            "Carlo Gatta",
            "Francesc Moreno-Noguer"
        ],
        "abstract": "We present a novel approach to simultaneously reconstruct the 3D structure of a non-rigid coronary tree and estimate point correspondences between an input X-ray image and a reference 3D shape. At the core of our approach lies an optimization scheme that iteratively fits a generative 3D model of increasing complexity and guides the matching process. As a result, and in contrast to existing approaches that assume rigidity or quasi-rigidity of the structure, our method is able to retrieve large non-linear deformations even when the input data is corrupted by the presence of noise and partial occlusions. We extensively evaluate our approach under synthetic and real data and demonstrate a remarkable improvement compared to state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126325",
        "reference_list": [
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2003",
                "id": 173
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 9,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "X-ray imaging",
                "Feature extraction",
                "Shape",
                "Solid modeling",
                "Training data",
                "Noise"
            ],
            "INSPEC: Controlled Indexing": [
                "blood vessels",
                "estimation theory",
                "image reconstruction",
                "medical image processing",
                "solid modelling",
                "trees (mathematics)",
                "X-ray imaging"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "non-rigid 3D reconstruction",
                "coronary tree",
                "single X-ray images",
                "point correspondence estimation",
                "input X-ray image",
                "reference 3D shape",
                "optimization scheme",
                "generative 3D model",
                "matching process",
                "quasi-rigidity",
                "nonlinear deformations",
                "partial occlusions",
                "synthetic data",
                "real data"
            ]
        },
        "id": 107,
        "cited_by": []
    },
    {
        "title": "Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes",
        "authors": [
            "Stefan Hinterstoisser",
            "Stefan Holzer",
            "Cedric Cagniart",
            "Slobodan Ilic",
            "Kurt Konolige",
            "Nassir Navab",
            "Vincent Lepetit"
        ],
        "abstract": "We present a method for detecting 3D objects using multi-modalities. While it is generic, we demonstrate it on the combination of an image and a dense depth map which give complementary object information. It works in real-time, under heavy clutter, does not require a time consuming training stage, and can handle untextured objects. It is based on an efficient representation of templates that capture the different modalities, and we show in many experiments on commodity hardware that our approach significantly outperforms state-of-the-art methods on single modalities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126326",
        "reference_list": [
            {
                "year": "2007",
                "id": 252
            }
        ],
        "citation": {
            "ieee": 85,
            "other": 55,
            "total": 140
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "object detection",
                "real-time systems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multimodal templates",
                "real-time detection",
                "texture-less objects",
                "cluttered scenes",
                "3D object detection",
                "multimodalities",
                "dense depth map",
                "complementary object information"
            ]
        },
        "id": 108,
        "cited_by": [
            {
                "year": "2017",
                "id": 13
            },
            {
                "year": "2017",
                "id": 159
            }
        ]
    },
    {
        "title": "Slow feature analysis and decorrelation filtering for separating correlated sources",
        "authors": [
            "H\u00e0 Quang Minh",
            "Laurenz Wiskott"
        ],
        "abstract": "We generalize the method of Slow Feature Analysis for vector-valued functions of multivariables and apply it to the problem of blind source separation, in particular image separation. For the linear case, exact mathematical analysis is given, which shows in particular that the sources are perfectly separated by SFA if and only if they and their first order derivatives are uncorrelated. When the sources are correlated, we apply the following technique called decorrelation filtering: use a linear filter to decorrelate the sources and their derivatives, then apply the separating matrix obtained on the filtered sources to the original sources. We show that if the filtered sources are perfectly separated by this matrix, then so are the original sources. We show how to numerically obtain such a decorrelation filter by solving a nonlinear optimization problem. This technique can also be applied to other linear separation methods, whose output signals are decorrelated, such as ICA.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126327",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Decorrelation",
                "Eigenvalues and eigenfunctions",
                "Optimization",
                "Blind source separation",
                "Correlation",
                "Color",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "blind source separation",
                "correlation methods",
                "image processing",
                "mathematical analysis",
                "matrix algebra",
                "nonlinear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "slow feature analysis",
                "decorrelation filtering",
                "correlated source separation",
                "vector-valued functions",
                "blind source separation",
                "image separation",
                "mathematical analysis",
                "linear filter",
                "separating matrix",
                "nonlinear optimization problem",
                "linear separation method",
                "ICA"
            ]
        },
        "id": 109,
        "cited_by": [
            {
                "year": "2013",
                "id": 354
            }
        ]
    },
    {
        "title": "From images to scenes: Compressing an image cluster into a single scene model for place recognition",
        "authors": [
            "Edward Johns",
            "Guang-Zhong Yang"
        ],
        "abstract": "The recognition of a place depicted in an image typically adopts methods from image retrieval in large-scale databases. First, a query image is described as a \u201cbag-of-features\u201d and compared to every image in the database. Second, the most similar images are passed to a geometric verification stage. However, this is an inefficient approach when considering that some database images may be almost identical, and many image features may not repeatedly occur. We address this issue by clustering similar database images to represent distinct scenes, and tracking local features that are consistently detected to form a set of real-world landmarks. Query images are then matched to landmarks rather than features, and a probabilistic model of landmark properties is learned from the cluster to appropriately verify or reject putative feature matches. We present novelties in both a bag-of-features retrieval and geometric verification stage based on this concept. Results on a database of 200K images of popular tourist destinations show improvements in both recognition performance and efficiency compared to traditional image retrieval methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126328",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 10,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Vectors",
                "Dictionaries",
                "Image retrieval",
                "Probability",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "data compression",
                "geometry",
                "image coding",
                "image retrieval",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image compression",
                "image cluster",
                "place recognition",
                "image retrieval",
                "large-scale databases",
                "query image",
                "geometric verification stage"
            ]
        },
        "id": 110,
        "cited_by": []
    },
    {
        "title": "2D-3D fusion for layer decomposition of urban facades",
        "authors": [
            "Yangyan Li",
            "Qian Zheng",
            "Andrei Sharf",
            "Daniel Cohen-Or",
            "Baoquan Chen",
            "Niloy J. Mitra"
        ],
        "abstract": "We present a method for fusing two acquisition modes, 2D photographs and 3D LiDAR scans, for depth-layer decomposition of urban facades. The two modes have complementary characteristics: point cloud scans are coherent and inherently 3D, but are often sparse, noisy, and incomplete; photographs, on the other hand, are of high resolution, easy to acquire, and dense, but view-dependent and inherently 2D, lacking critical depth information. In this paper we use photographs to enhance the acquired LiDAR data. Our key observation is that with an initial registration of the 2D and 3D datasets we can decompose the input photographs into rectified depth layers. We decompose the input photographs into rectangular planar fragments and diffuse depth information from the corresponding 3D scan onto the fragments by solving a multi-label assignment problem. Our layer decomposition enables accurate repetition detection in each planar layer, using which we propagate geometry, remove outliers and enhance the 3D scan. Finally, the algorithm produces an enhanced, layered, textured model. We evaluate our algorithm on complex multi-planar building facades, where direct autocorrelation methods for repetition detection fail. We demonstrate how 2D photographs help improve the 3D scans by exploiting data redundancy, and transferring high level structural information to (plausibly) complete large missing regions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126329",
        "reference_list": [
            {
                "year": "2009",
                "id": 10
            },
            {
                "year": "2007",
                "id": 94
            },
            {
                "year": "2009",
                "id": 241
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 18,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Buildings",
                "Image edge detection",
                "Solid modeling",
                "Image segmentation",
                "Laser radar",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "data visualisation",
                "optical radar",
                "photography",
                "structural engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "2D-3D fusion",
                "depth-layer decomposition",
                "urban facade",
                "acquisition mode",
                "2D photograph",
                "3D LiDAR scan",
                "point cloud scan",
                "rectangular planar fragment",
                "multilabel assignment problem",
                "multiplanar building facade",
                "direct autocorrelation method",
                "data redundancy"
            ]
        },
        "id": 111,
        "cited_by": [
            {
                "year": "2015",
                "id": 233
            }
        ]
    },
    {
        "title": "Recursive MDL via graph cuts: Application to segmentation",
        "authors": [
            "Lena Gorelick",
            "Andrew Delong",
            "Olga Veksler",
            "Yuri Boykov"
        ],
        "abstract": "We propose a novel patch-based image representation that is useful because it (1) inherently detects regions with repetitive structure at multiple scales and (2) yields a parameterless hierarchical segmentation. We describe an image by breaking it into coherent regions where each region is well-described (easily reconstructed) by repeatedly instantiating a patch using a set of simple transformations. In other words, a good segment is one that has sufficient repetition of some pattern, and a patch is useful if it contains a pattern that is repeated in the image. Our criterion is naturally expressed by the well-established minimum description length (MDL) principle. MDL prefers spatially coherent regions with consistent appearance and avoids parameter tuning. We minimize the description length (in bits) of the image by encoding it with patches. Because a patch is itself an image, we measure its description length by applying the same idea recursively: encode a patch by breaking it into regions described by yet simpler patches. The resulting hierarchy of inter-dependent patches naturally leads to a hierarchical segmentation. We minimize description length over our class of image representations (all patch hierarchies / partitions). We formulate this problem as a recursive multi-label energy. Existing optimization techniques are either inapplicable or get stuck in poor local minima. We propose a new hierarchical fusion (HF) algorithm for energies containing a hierarchy of 'label costs'. Our algorithm is a contribution in itself and should be useful for this new and difficult class of energies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126330",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Labeling",
                "Encoding",
                "Image color analysis",
                "Complexity theory",
                "Image coding",
                "Image representation"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image coding",
                "image fusion",
                "image representation",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "recursive minimum description length",
                "graph cuts",
                "patch-based image representation",
                "region detection",
                "parameterless hierarchical segmentation",
                "description length minimisation",
                "image encoding",
                "patch encoding",
                "recursive multilabel energy",
                "hierarchical fusion algorithm"
            ]
        },
        "id": 112,
        "cited_by": []
    },
    {
        "title": "What characterizes a shadow boundary under the sun and sky?",
        "authors": [
            "Xiang Huang",
            "Gang Hua",
            "Jack Tumblin",
            "Lance Williams"
        ],
        "abstract": "Despite decades of study, robust shadow detection remains difficult, especially within a single color image. We describe a new approach to detect shadow boundaries in images of outdoor scenes lit only by the sun and sky. The method first extracts visual features of candidate edges that are motivated by physical models of illumination and occluders. We feed these features into a Support Vector Machine (SVM) that was trained to discriminate between most-likely shadow-edge candidates and less-likely ones. Finally, we connect edges to help reject non-shadow edge candidates, and to encourage closed, connected shadow boundaries. On benchmark shadow-edge data sets from Lalonde et al. and Zhu et al., our method showed substantial improvements when compared to other recent shadow-detection methods based on statistical learning.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126331",
        "reference_list": [
            {
                "year": "2009",
                "id": 192
            },
            {
                "year": "2005",
                "id": 34
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 3,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Image edge detection",
                "Sun",
                "Image color analysis",
                "Visualization",
                "Support vector machines",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "feature extraction",
                "image colour analysis",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shadow boundary",
                "sun",
                "sky",
                "robust shadow detection",
                "color image",
                "visual feature extraction",
                "illumination",
                "occluders",
                "support vector machine",
                "SVM",
                "statistical learning"
            ]
        },
        "id": 113,
        "cited_by": [
            {
                "year": "2017",
                "id": 473
            },
            {
                "year": "2015",
                "id": 378
            }
        ]
    },
    {
        "title": "Learning a mixture of sparse distance metrics for classification and dimensionality reduction",
        "authors": [
            "Yi Hong",
            "Quannan Li",
            "Jiayan Jiang",
            "Zhuowen Tu"
        ],
        "abstract": "This paper extends the neighborhood components analysis method (NCA) to learning a mixture of sparse distance metrics for classification and dimensionality reduction. We emphasize two important properties in the recent learning literature, locality and sparsity, and (1) pursue a set of local distance metrics by maximizing a conditional likelihood of observed data; and (2) add \u2113 1 -norm of eigenvalues of the distance metric to favor low rank matrices of fewer parameters. Experimental results on standard UCI machine learning datasets, face recognition datasets, and image categorization datasets demonstrate the feasibility of our approach for both distance metric learning and dimensionality reduction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126332",
        "reference_list": [
            {
                "year": "2009",
                "id": 268
            },
            {
                "year": "2007",
                "id": 20
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Eigenvalues and eigenfunctions",
                "Machine learning",
                "Accuracy",
                "Support vector machines",
                "Training data",
                "Data visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "face recognition",
                "image classification",
                "sparse matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparse distance metrics",
                "classification",
                "dimensionality reduction",
                "neighborhood components analysis method",
                "conditional likelihood",
                "eigenvalues",
                "standard UCI machine learning dataset",
                "face recognition dataset",
                "image categorization dataset",
                "distance metric learning"
            ]
        },
        "id": 114,
        "cited_by": []
    },
    {
        "title": "Fusing generic objectness and visual saliency for salient object detection",
        "authors": [
            "Kai-Yueh Chang",
            "Tyng-Luh Liu",
            "Hwann-Tzong Chen",
            "Shang-Hong Lai"
        ],
        "abstract": "We present a novel computational model to explore the relatedness of objectness and saliency, each of which plays an important role in the study of visual attention. The proposed framework conceptually integrates these two concepts via constructing a graphical model to account for their relationships, and concurrently improves their estimation by iteratively optimizing a novel energy function realizing the model. Specifically, the energy function comprises the objectness, the saliency, and the interaction energy, respectively corresponding to explain their individual regularities and the mutual effects. Minimizing the energy by fixing one or the other would elegantly transform the model into solving the problem of objectness or saliency estimation, while the useful information from the other concept can be utilized through the interaction term. Experimental results on two benchmark datasets demonstrate that the proposed model can simultaneously yield a saliency map of better quality and a more meaningful objectness output for salient object detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126333",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2009",
                "id": 287
            },
            {
                "year": "2009",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 18,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Object detection",
                "Detectors",
                "Databases",
                "Computational modeling",
                "Image edge detection",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image fusion",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fusing generic objectness",
                "visual saliency",
                "salient object detection",
                "visual attention"
            ]
        },
        "id": 115,
        "cited_by": [
            {
                "year": "2017",
                "id": 189
            },
            {
                "year": "2017",
                "id": 477
            },
            {
                "year": "2017",
                "id": 573
            },
            {
                "year": "2017",
                "id": 594
            },
            {
                "year": "2015",
                "id": 24
            },
            {
                "year": "2015",
                "id": 45
            },
            {
                "year": "2013",
                "id": 190
            },
            {
                "year": "2013",
                "id": 207
            },
            {
                "year": "2013",
                "id": 246
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2013",
                "id": 371
            },
            {
                "year": "2013",
                "id": 415
            }
        ]
    },
    {
        "title": "Trajectory reconstruction from non-overlapping surveillance cameras with relative depth ordering constraints",
        "authors": [
            "Branislav Micusik"
        ],
        "abstract": "We present a method for reconstructing a trajectory of an object moving in front of non-overlapping fully or partially calibrated cameras. The non-overlapping setup turns that problem ill-posed as no point correspondences can be established which are necessary for the well known point triangulation. The proposed solution instead builds on the assumption of trajectory smoothness and depth ordering prior information. We propose a novel formulation with a consistent minimization criterion and a way to utilize the depth ordering prior reflected by the size change of a bounding box associated to an image point being tracked. Reconstructing trajectory minimizing the trajectory smoothness, its re-projection error and employing the depth priors is casted as the Second Order Cone Program yielding a global optimum. The new formulation together with the proposed depth prior significantly improves the trajectory reconstruction in sense of accuracy and topology, and speeds up the solver. Synthetic and real experiments validate the feasibility of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126334",
        "reference_list": [
            {
                "year": "2005",
                "id": 128
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 3,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Trajectory",
                "Three dimensional displays",
                "Image reconstruction",
                "Vectors",
                "Calibration",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "motion estimation",
                "object detection",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "trajectory reconstruction",
                "nonoverlapping surveillance cameras",
                "relative depth ordering constraints",
                "point triangulation",
                "consistent minimization criterion",
                "image point",
                "second order cone program"
            ]
        },
        "id": 116,
        "cited_by": []
    },
    {
        "title": "Tasting families of features for image classification",
        "authors": [
            "Charles Dubout",
            "Fran\u00e7ois Fleuret"
        ],
        "abstract": "Using multiple families of image features is a very efficient strategy to improve performance in object detection or recognition. However, such a strategy induces multiple challenges for machine learning methods, both from a computational and a statistical perspective. The main contribution of this paper is a novel feature sampling procedure dubbed \"Tasting\" to improve the efficiency of Boosting in such a context. Instead of sampling features in a uniform manner, Tasting continuously estimates the expected loss reduction for each family from a limited set of features sampled prior to the learning, and biases the sampling accordingly. We evaluate the performance of this procedure with tens of families of features on four image classification and object detection data-sets. We show that Tasting, which does not require the tuning of any meta-parameter, outperforms systematically variants of uniform sampling and state-of- the-art approaches based on bandit strategies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126335",
        "reference_list": [
            {
                "year": "2009",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object detection",
                "sampling methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image classification",
                "object detection",
                "object recognition",
                "machine learning",
                "statistical perspective",
                "feature sampling procedure",
                "boosting",
                "Tasting"
            ]
        },
        "id": 117,
        "cited_by": []
    },
    {
        "title": "Fully automatic pose-invariant face recognition via 3D pose normalization",
        "authors": [
            "Akshay Asthana",
            "Tim K. Marks",
            "Michael J. Jones",
            "Kinh H. Tieu",
            "MV Rohith"
        ],
        "abstract": "An ideal approach to the problem of pose-invariant face recognition would handle continuous pose variations, would not be database specific, and would achieve high accuracy without any manual intervention. Most of the existing approaches fail to match one or more of these goals. In this paper, we present a fully automatic system for pose-invariant face recognition that not only meets these requirements but also outperforms other comparable methods. We propose a 3D pose normalization method that is completely automatic and leverages the accurate 2D facial feature points found by the system. The current system can handle 3D pose variation up to \u00b145\u00b0 in yaw and \u00b130\u00b0 in pitch angles. Recognition experiments were conducted on the USF 3D, Multi-PIE, CMU-PIE, FERET, and FacePix databases. Our system not only shows excellent generalization by achieving high accuracy on all 5 databases but also outperforms other methods convincingly.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126336",
        "reference_list": [],
        "citation": {
            "ieee": 81,
            "other": 48,
            "total": 129
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Estimation",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "pose estimation",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic pose-invariant face recognition",
                "continuous pose variations",
                "manual intervention",
                "fully automatic system",
                "3D pose normalization method",
                "2D facial feature points",
                "3D pose variation",
                "yaw",
                "pitch angles",
                "USF 3D databases",
                "multiPIE databases",
                "CMU-PIE databases",
                "FERET databases",
                "FacePix databases"
            ]
        },
        "id": 118,
        "cited_by": [
            {
                "year": "2017",
                "id": 587
            },
            {
                "year": "2015",
                "id": 218
            },
            {
                "year": "2015",
                "id": 428
            },
            {
                "year": "2013",
                "id": 14
            },
            {
                "year": "2013",
                "id": 301
            }
        ]
    },
    {
        "title": "Exploiting the Manhattan-world assumption for extrinsic self-calibration of multi-modal sensor networks",
        "authors": [
            "Marcel Br\u00fcckner",
            "Joachim Denzler"
        ],
        "abstract": "Many new applications are enabled by combining a multi-camera system with a Time-of-Flight (ToF) camera, which is able to simultaneously record intensity and depth images. Classical approaches for self-calibration of a multi-camera system fail to calibrate such a system due to the very different image modalities. In addition, the typical environments of multi-camera systems are man-made and consist primary of only low textured objects. However, at the same time they satisfy the Manhattan-world assumption. We formulate the multi-modal sensor network calibration as a Maximum a Posteriori (MAP) problem and solve it by minimizing the corresponding energy function. First we estimate two separate 3D reconstructions of the environment: one using the pan-tilt unit mounted ToF camera and one using the multi-camera system. We exploit the Manhattan-world assumption and estimate multiple initial calibration hypotheses by registering the three dominant orientations of planes. These hypotheses are used as prior knowledge of a subsequent MAP estimation aiming to align edges that are parallel to these dominant directions. To our knowledge, this is the first self-calibration approach that is able to calibrate a ToF camera with a multi-camera system. Quantitative experiments on real data demonstrate the high accuracy of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126337",
        "reference_list": [
            {
                "year": "2009",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three dimensional displays",
                "Calibration",
                "Image edge detection",
                "Estimation",
                "Surface reconstruction",
                "Charge coupled devices"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "image reconstruction",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Manhattan-world assumption",
                "extrinsic self-calibration",
                "multimodal sensor networks",
                "multicamera system",
                "time-of-flight camera",
                "ToF camera",
                "depth images",
                "image modalities",
                "maximum a posteriori problem",
                "3D reconstructions",
                "MAP estimation"
            ]
        },
        "id": 119,
        "cited_by": []
    },
    {
        "title": "Fast articulated motion tracking using a sums of Gaussians body model",
        "authors": [
            "Carsten Stoll",
            "Nils Hasler",
            "Juergen Gall",
            "Hans-Peter Seidel",
            "Christian Theobalt"
        ],
        "abstract": "We present an approach for modeling the human body by Sums of spatial Gaussians (SoG), allowing us to perform fast and high-quality markerless motion capture from multi-view video sequences. The SoG model is equipped with a color model to represent the shape and appearance of the human and can be reconstructed from a sparse set of images. Similar to the human body, we also represent the image domain as SoG that models color consistent image blobs. Based on the SoG models of the image and the human body, we introduce a novel continuous and differentiable model-to-image similarity measure that can be used to estimate the skeletal motion of a human at 5-15 frames per second even for many camera views. In our experiments, we show that our method, which does not rely on silhouettes or training data, offers an good balance between accuracy and computational cost.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126338",
        "reference_list": [],
        "citation": {
            "ieee": 54,
            "other": 33,
            "total": 87
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Three dimensional displays",
                "Solid modeling",
                "Humans",
                "Joints",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image colour analysis",
                "image motion analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion tracking",
                "Gaussians body model",
                "sums of spatial Gaussians",
                "markerless motion capture",
                "multiview video sequences",
                "SoG model",
                "color model",
                "image blobs",
                "model-to-image similarity measure",
                "skeletal motion"
            ]
        },
        "id": 120,
        "cited_by": [
            {
                "year": "2017",
                "id": 94
            },
            {
                "year": "2015",
                "id": 85
            },
            {
                "year": "2015",
                "id": 372
            },
            {
                "year": "2013",
                "id": 137
            },
            {
                "year": "2013",
                "id": 306
            }
        ]
    },
    {
        "title": "Learning equivariant structured output SVM regressors",
        "authors": [
            "Andrea Vedaldi",
            "Matthew Blaschko",
            "Andrew Zisserman"
        ],
        "abstract": "Equivariance and invariance are often desired properties of a computer vision system. However, currently available strategies generally rely on virtual sampling, leaving open the question of how many samples are necessary, on the use of invariant feature representations, which can mistakenly discard information relevant to the vision task, or on the use of latent variable models, which result in non-convex training and expensive inference at test time. We propose here a generalization of structured output SVM regressors that can incorporate equivariance and invariance into a convex training procedure, enabling the incorporation of large families of transformations, while maintaining optimality and tractability. Importantly, test time inference does not require the estimation of latent variables, resulting in highly efficient objective functions. This results in a natural formulation for treating equivariance and invariance that is easily implemented as an adaptation of off-the-shelf optimization software, obviating the need for ad hoc sampling strategies. Theoretical results relating to vicinal risk, and experiments on challenging aerial car and pedestrian detection tasks show the effectiveness of the proposed solution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126339",
        "reference_list": [
            {
                "year": "2007",
                "id": 201
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 10,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Kernel",
                "Detectors",
                "Training",
                "Object detection",
                "Feature extraction",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "regression analysis",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "equivariant structured output SVM regressor",
                "computer vision system",
                "virtual sampling",
                "invariant feature representation",
                "nonconvex training",
                "expensive inference",
                "convex training procedure",
                "optimality",
                "tractability",
                "test time inference",
                "equivariance",
                "invariance",
                "off-the-shelf optimization software",
                "ad hoc sampling strategy",
                "aerial car",
                "pedestrian detection task"
            ]
        },
        "id": 121,
        "cited_by": []
    },
    {
        "title": "Robust object pose estimation via statistical manifold modeling",
        "authors": [
            "Liang Mei",
            "Jingen Liu",
            "Alfred Hero",
            "Silvio Savarese"
        ],
        "abstract": "We propose a novel statistical manifold modeling approach that is capable of classifying poses of object categories from video sequences by simultaneously minimizing the intra-class variability and maximizing inter-pose distance. Following the intuition that an object part based representation and a suitable part selection process may help achieve our purpose, we formulate the part selection problem from a statistical manifold modeling perspective and treat part selection as adjusting the manifold of the object (parameterized by pose) by means of the manifold \u201calignment\u201d and \u201cexpansion\u201d operations. We show that manifold alignment and expansion are equivalent to minimizing the intra-class distance given a pose while increasing the inter-pose distance given an object instance respectively. We formulate and solve this (otherwise intractable) part selection problem as a combinatorial optimization problem using graph analysis techniques. Quantitative and qualitative experimental analysis validates our theoretical claims.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126340",
        "reference_list": [
            {
                "year": "2009",
                "id": 121
            },
            {
                "year": "2009",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 9,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Trajectory",
                "Optimization",
                "Video sequences",
                "Estimation",
                "Joints",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image classification",
                "image sequences",
                "object detection",
                "optimisation",
                "pose estimation",
                "statistical analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust object pose estimation",
                "pose classification",
                "object category",
                "video sequences",
                "intra-class variability",
                "inter-pose distance",
                "part selection process",
                "part selection problem",
                "statistical manifold modeling perspective",
                "manifold alignment",
                "expansion operations",
                "manifold expansion",
                "combinatorial optimization problem",
                "graph analysis techniques",
                "quantitative experimental analysis",
                "qualitative experimental analysis",
                "intraclass distance"
            ]
        },
        "id": 122,
        "cited_by": []
    },
    {
        "title": "Optimizing polynomial solvers for minimal geometry problems",
        "authors": [
            "Oleg Naroditsky",
            "Kostas Daniilidis"
        ],
        "abstract": "In recent years polynomial solvers based on algebraic geometry techniques, and specifically the action matrix method, have become popular for solving minimal problems in computer vision. In this paper we develop a new method for reducing the computational time and improving numerical stability of algorithms using this method. To achieve this, we propose and prove a set of algebraic conditions which allow us to reduce the size of the elimination template (polynomial coefficient matrix), which leads to faster LU or QR decomposition. Our technique is generic and has potential to improve performance of many solvers that use the action matrix method. We demonstrate the approach on specific examples, including an image stitching algorithm where computation time is halved and single precision arithmetic can be used.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126341",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Matrix decomposition",
                "Polynomials",
                "Numerical stability",
                "Eigenvalues and eigenfunctions",
                "Optimization",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "geometry",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "polynomial solvers",
                "minimal geometry problems",
                "algebraic geometry techniques",
                "action matrix method",
                "computer vision",
                "computational time",
                "numerical stability",
                "image stitching"
            ]
        },
        "id": 123,
        "cited_by": [
            {
                "year": "2017",
                "id": 241
            }
        ]
    },
    {
        "title": "From contours to 3D object detection and pose estimation",
        "authors": [
            "Nadia Payet",
            "Sinisa Todorovic"
        ],
        "abstract": "This paper addresses view-invariant object detection and pose estimation from a single image. While recent work focuses on object-centered representations of point-based object features, we revisit the viewer-centered framework, and use image contours as basic features. Given training examples of arbitrary views of an object, we learn a sparse object model in terms of a few view-dependent shape templates. The shape templates are jointly used for detecting object occurrences and estimating their 3D poses in a new image. Instrumental to this is our new mid-level feature, called bag of boundaries (BOB), aimed at lifting from individual edges toward their more informative summaries for identifying object boundaries amidst the background clutter. In inference, BOBs are placed on deformable grids both in the image and the shape templates, and then matched. This is formulated as a convex optimization problem that accommodates invariance to non-rigid, locally affine shape deformations. Evaluation on benchmark datasets demonstrates our competitive results relative to the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126342",
        "reference_list": [
            {
                "year": "2009",
                "id": 172
            },
            {
                "year": "2007",
                "id": 146
            },
            {
                "year": "2009",
                "id": 27
            },
            {
                "year": "2007",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 21,
            "total": 61
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Training",
                "Three dimensional displays",
                "Cameras",
                "Histograms",
                "Image edge detection",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image representation",
                "object detection",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object detection",
                "pose estimation",
                "view-invariant object detection",
                "object-centered representation",
                "point-based object feature",
                "viewer-centered framework",
                "image contours",
                "sparse object model",
                "view-dependent shape templates",
                "object occurrence detection",
                "bag of boundaries",
                "informative summary",
                "object boundary identification",
                "background clutter",
                "deformable grids",
                "convex optimization problem",
                "shape deformation"
            ]
        },
        "id": 124,
        "cited_by": [
            {
                "year": "2015",
                "id": 265
            },
            {
                "year": "2015",
                "id": 490
            }
        ]
    },
    {
        "title": "Semantic contours from inverse detectors",
        "authors": [
            "Bharath Hariharan",
            "Pablo Arbel\u00e1ez",
            "Lubomir Bourdev",
            "Subhransu Maji",
            "Jitendra Malik"
        ],
        "abstract": "We study the challenging problem of localizing and classifying category-specific object contours in real world images. For this purpose, we present a simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours. We also provide a principled way of combining information from different part detectors and across categories. In order to study the problem and evaluate quantitatively our approach, we present a dataset of semantic exterior boundaries on more than 20, 000 object instances belonging to 20 categories, using the images from the VOC2011 PASCAL challenge [7].",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126343",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2005",
                "id": 159
            },
            {
                "year": "2007",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 189,
            "other": 73,
            "total": 262
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Semantics",
                "Vectors",
                "Feature extraction",
                "Support vector machines",
                "Head",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic contour",
                "inverse detector",
                "category-specific object contour",
                "real world image",
                "generic object detector",
                "bottom-up contour"
            ]
        },
        "id": 125,
        "cited_by": [
            {
                "year": "2017",
                "id": 22
            },
            {
                "year": "2017",
                "id": 79
            },
            {
                "year": "2017",
                "id": 87
            },
            {
                "year": "2017",
                "id": 286
            },
            {
                "year": "2017",
                "id": 368
            },
            {
                "year": "2017",
                "id": 372
            },
            {
                "year": "2017",
                "id": 375
            },
            {
                "year": "2017",
                "id": 437
            },
            {
                "year": "2017",
                "id": 446
            },
            {
                "year": "2017",
                "id": 475
            },
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2017",
                "id": 529
            },
            {
                "year": "2015",
                "id": 56
            },
            {
                "year": "2015",
                "id": 153
            },
            {
                "year": "2015",
                "id": 169
            },
            {
                "year": "2015",
                "id": 170
            },
            {
                "year": "2015",
                "id": 172
            },
            {
                "year": "2015",
                "id": 182
            },
            {
                "year": "2015",
                "id": 194
            },
            {
                "year": "2015",
                "id": 200
            },
            {
                "year": "2015",
                "id": 288
            }
        ]
    },
    {
        "title": "Domain adaptation for object recognition: An unsupervised approach",
        "authors": [
            "Raghuraman Gopalan",
            "Ruonan Li",
            "Rama Chellappa"
        ],
        "abstract": "Adapting the classifier trained on a source domain to recognize instances from a new target domain is an important problem that is receiving recent attention. In this paper, we present one of the first studies on unsupervised domain adaptation in the context of object recognition, where we have labeled data only from the source domain (and therefore do not have correspondences between object categories across domains). Motivated by incremental learning, we create intermediate representations of data between the two domains by viewing the generative subspaces (of same dimension) created from these domains as points on the Grassmann manifold, and sampling points along the geodesic between them to obtain subspaces that provide a meaningful description of the underlying domain shift. We then obtain the projections of labeled source domain data onto these subspaces, from which a discriminative classifier is learnt to classify projected data from the target domain. We discuss extensions of our approach for semi-supervised adaptation, and for cases with multiple source and target domains, and report competitive results on standard datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126344",
        "reference_list": [],
        "citation": {
            "ieee": 235,
            "other": 118,
            "total": 353
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Object recognition",
                "Vectors",
                "Feature extraction",
                "Measurement",
                "Data models",
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image sampling",
                "object recognition",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "unsupervised approach",
                "target domain",
                "unsupervised domain adaptation",
                "object category",
                "incremental learning",
                "data representations",
                "Grassmann manifold",
                "sampling points",
                "underlying domain shift",
                "labeled source domain",
                "discriminative classifier",
                "semisupervised adaptation"
            ]
        },
        "id": 126,
        "cited_by": [
            {
                "year": "2017",
                "id": 62
            },
            {
                "year": "2017",
                "id": 213
            },
            {
                "year": "2017",
                "id": 377
            },
            {
                "year": "2017",
                "id": 599
            },
            {
                "year": "2015",
                "id": 118
            },
            {
                "year": "2015",
                "id": 273
            },
            {
                "year": "2015",
                "id": 373
            },
            {
                "year": "2015",
                "id": 404
            },
            {
                "year": "2015",
                "id": 429
            },
            {
                "year": "2015",
                "id": 468
            },
            {
                "year": "2015",
                "id": 474
            },
            {
                "year": "2013",
                "id": 95
            },
            {
                "year": "2013",
                "id": 98
            },
            {
                "year": "2013",
                "id": 111
            },
            {
                "year": "2013",
                "id": 320
            },
            {
                "year": "2013",
                "id": 325
            },
            {
                "year": "2013",
                "id": 369
            },
            {
                "year": "2013",
                "id": 389
            },
            {
                "year": "2013",
                "id": 400
            },
            {
                "year": "2013",
                "id": 428
            },
            {
                "year": "2013",
                "id": 445
            }
        ]
    },
    {
        "title": "Action recognition using rank-1 approximation of Joint Self-Similarity Volume",
        "authors": [
            "Chuan Sun",
            "Imran Junejo",
            "Hassan Foroosh"
        ],
        "abstract": "In this paper, we make three main contributions in the area of action recognition: (i) We introduce the concept of Joint Self-Similarity Volume (Joint SSV) for modeling dynamical systems, and show that by using a new optimized rank-1 tensor approximation of Joint SSV one can obtain compact low-dimensional descriptors that very accurately preserve the dynamics of the original system, e.g. an action video sequence; (ii) The descriptor vectors derived from the optimized rank-1 approximation make it possible to recognize actions without explicitly aligning the action sequences of varying speed of execution or different frame rates; (iii) The method is generic and can be applied using different low-level features such as silhouettes, histogram of oriented gradients, etc. Hence, it does not necessarily require explicit tracking of features in the space-time volume. Our experimental results on three public datasets demonstrate that our method produces remarkably good results and outperforms all baseline methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126345",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Joints",
                "Vectors",
                "Feature extraction",
                "Tensile stress",
                "Approximation algorithms",
                "Least squares approximation"
            ],
            "INSPEC: Controlled Indexing": [
                "fractals",
                "image recognition",
                "image sequences",
                "tensors",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action recognition",
                "rank-1 approximation",
                "joint self-similarity volume",
                "joint SSV",
                "tensor approximation",
                "action video sequence",
                "space-time volume"
            ]
        },
        "id": 127,
        "cited_by": []
    },
    {
        "title": "Positive definite dictionary learning for region covariances",
        "authors": [
            "Ravishankar Sivalingam",
            "Daniel Boley",
            "Vassilios Morellas",
            "Nikolaos Papanikolopoulos"
        ],
        "abstract": "Sparse models have proven to be extremely successful in image processing and computer vision, and most efforts have been focused on sparse representation of vectors. The success of sparse modeling and the popularity of region covariances have inspired the development of sparse coding approaches for positive definite matrices. While in earlier work [1], the dictionary was pre-determined, it is clearly advantageous to learn a concise dictionary adaptively from the data at hand. In this paper, we propose a novel approach for dictionary learning over positive definite matrices. The dictionary is learned by alternating minimization between the sparse coding and dictionary update stages, and two different atom update methods are described. The online versions of the dictionary update techniques are also outlined. Experimental results demonstrate that the proposed learning methods yield better dictionaries for positive definite sparse coding. The learned dictionaries are applied to texture and face data, leading to improved classification accuracy and strong detection performance, respectively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126346",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 6,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Encoding",
                "Training",
                "Learning systems",
                "Sparse matrices",
                "Vectors",
                "Symmetric matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "covariance matrices",
                "image representation",
                "sparse matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "positive definite dictionary learning",
                "region covariances",
                "image processing",
                "computer vision",
                "sparse representation",
                "sparse modeling",
                "positive definite matrices",
                "concise dictionary",
                "dictionary update technique",
                "positive definite sparse coding",
                "face data"
            ]
        },
        "id": 128,
        "cited_by": []
    },
    {
        "title": "A graph cut algorithm for higher-order Markov Random Fields",
        "authors": [
            "Alexander Fix",
            "Aritanan Gruber",
            "Endre Boros",
            "Ramin Zabih"
        ],
        "abstract": "Higher-order Markov Random Fields, which can capture important properties of natural images, have become increasingly important in computer vision. While graph cuts work well for first-order MRF's, until recently they have rarely been effective for higher-order MRF's. Ishikawa's graph cut technique [8, 9] shows great promise for many higher-order MRF's. His method transforms an arbitrary higher-order MRF with binary labels into a first-order one with the same minima. If all the terms are submodular the exact solution can be easily found; otherwise, pseudo-boolean optimization techniques can produce an optimal labeling for a subset of the variables. We present a new transformation with better performance than [8, 9], both theoretically and experimentally. While [8, 9] transforms each higher-order term independently, we transform a group of terms at once. For n binary variables, each of which appears in terms with k other variables, at worst we produce n non-submodular terms, while [8, 9] produces O(nk). We identify a local completeness property that makes our method perform even better, and show that under certain assumptions several important vision problems (including common variants of fusion moves) have this property. Running on the same field of experts dataset used in [8, 9] we optimally label significantly more variables (96% versus 80%) and converge more rapidly to a lower energy. Preliminary experiments suggest that some other higher-order MRF's used in stereo [20] and segmentation [1] are also locally complete and would thus benefit from our work.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126347",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 17,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Optimization",
                "Educational institutions",
                "Computer vision",
                "Transforms",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph cut algorithm",
                "higher-order Markov random fields",
                "natural images",
                "computer vision",
                "first-order MRF",
                "graph cut technique",
                "exact solution",
                "pseudo-Boolean optimization techniques",
                "optimal labeling",
                "local completeness property",
                "fusion moves"
            ]
        },
        "id": 129,
        "cited_by": [
            {
                "year": "2017",
                "id": 273
            },
            {
                "year": "2017",
                "id": 445
            },
            {
                "year": "2013",
                "id": 387
            }
        ]
    },
    {
        "title": "Salient object detection by composition",
        "authors": [
            "Jie Feng",
            "Yichen Wei",
            "Litian Tao",
            "Chao Zhang",
            "Jian Sun"
        ],
        "abstract": "Conventional saliency analysis methods measure the saliency of individual pixels. The resulting saliency map inevitably loses information in the original image and finding salient objects in it is difficult. We propose to detect salient objects by directly measuring the saliency of an image window in the original image and adopt the well established sliding window based object detection paradigm. We present a simple definition for window saliency, i.e., the cost of composing the window using the remaining parts of the image. The definition uses the entire image as the context and agrees with human intuition. It no longer relies on idealistic assumptions usually used before (e.g., \"back- ground is homogenous\") and generalizes well to complex objects and backgrounds in real world images. To realize the definition, we illustrate how to incorporate different cues such as appearance, position, and size. Based on a segment-based representation, the window composition cost function can be efficiently evaluated by a greedy optimization algorithm. Extensive evaluation on challenging object detection datasets verifies better efficacy and efficiency of the proposed method comparing to the state-of-the-art, making it a good pre-processing tool for subsequent applications. Moreover, we hope to stimulate further work towards the challenging yet important problem of generic salient object detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126348",
        "reference_list": [
            {
                "year": "2009",
                "id": 287
            },
            {
                "year": "2009",
                "id": 281
            },
            {
                "year": "2009",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 11,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Object detection",
                "Detectors",
                "Optimization",
                "Context",
                "Humans",
                "Complexity theory"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "salient object detection",
                "saliency analysis method",
                "saliency map",
                "sliding window based object detection",
                "window saliency",
                "segment-based representation",
                "window composition cost function",
                "greedy optimization algorithm"
            ]
        },
        "id": 130,
        "cited_by": [
            {
                "year": "2017",
                "id": 573
            },
            {
                "year": "2015",
                "id": 6
            },
            {
                "year": "2013",
                "id": 47
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2013",
                "id": 415
            }
        ]
    },
    {
        "title": "Human activity prediction: Early recognition of ongoing activities from streaming videos",
        "authors": [
            "M. S. Ryoo"
        ],
        "abstract": "In this paper, we present a novel approach of human activity prediction. Human activity prediction is a probabilistic process of inferring ongoing activities from videos only containing onsets (i.e. the beginning part) of the activities. The goal is to enable early recognition of unfinished activities as opposed to the after-the-fact classification of completed activities. Activity prediction methodologies are particularly necessary for surveillance systems which are required to prevent crimes and dangerous activities from occurring. We probabilistically formulate the activity prediction problem, and introduce new methodologies designed for the prediction. We represent an activity as an integral histogram of spatio-temporal features, efficiently modeling how feature distributions change over time. The new recognition methodology named dynamic bag-of-words is developed, which considers sequential nature of human activities while maintaining advantages of the bag-of-words to handle noisy observations. Our experiments confirm that our approach reliably recognizes ongoing activities from streaming videos with a high accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126349",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2009",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 112,
            "other": 97,
            "total": 209
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Histograms",
                "Humans",
                "Feature extraction",
                "Computational modeling",
                "Probabilistic logic",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "video signal processing",
                "video streaming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human activity prediction",
                "video streaming",
                "after-the-fact classification",
                "dynamic bag-of-words",
                "activity prediction methodologies"
            ]
        },
        "id": 131,
        "cited_by": [
            {
                "year": "2017",
                "id": 5
            },
            {
                "year": "2017",
                "id": 29
            },
            {
                "year": "2017",
                "id": 122
            },
            {
                "year": "2017",
                "id": 308
            },
            {
                "year": "2017",
                "id": 383
            },
            {
                "year": "2017",
                "id": 389
            },
            {
                "year": "2017",
                "id": 606
            },
            {
                "year": "2015",
                "id": 356
            },
            {
                "year": "2015",
                "id": 502
            },
            {
                "year": "2015",
                "id": 521
            },
            {
                "year": "2013",
                "id": 277
            }
        ]
    },
    {
        "title": "Dynamic and hierarchical multi-structure geometric model fitting",
        "authors": [
            "Hoi Sim Wong",
            "Tat-Jun Chin",
            "Jin Yu",
            "David Suter"
        ],
        "abstract": "The ability to generate good model hypotheses is instrumental to accurate and robust geometric model fitting. We present a novel dynamic hypothesis generation algorithm for robust fitting of multiple structures. Underpinning our method is a fast guided sampling scheme enabled by analysing correlation of preferences induced by data and hypothesis residuals. Our method progressively accumulates evidence in the search space, and uses the information to dynamically (1) identify outliers, (2) filter unpromising hypotheses, and (3) bias the sampling for active discovery of multiple structures in the data-All achieved without sacrificing the speed associated with sampling-based methods. Our algorithm yields a disproportionately higher number of good hypotheses among the sampling outcomes, i.e., most hypotheses correspond to the genuine structures in the data. This directly supports a novel hierarchical model fitting algorithm that elicits the underlying stratified manner in which the structures are organized, allowing more meaningful results than traditional \u201cflat\u201d multi-structure fitting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126350",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 18,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Heuristic algorithms",
                "Data models",
                "Computational modeling",
                "Algorithm design and analysis",
                "Sampling methods",
                "Filtering",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "curve fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical multistructure geometric model fitting",
                "dynamic hypothesis generation algorithm",
                "multistructure data"
            ]
        },
        "id": 132,
        "cited_by": [
            {
                "year": "2015",
                "id": 323
            }
        ]
    },
    {
        "title": "Learning a category independent object detection cascade",
        "authors": [
            "Esa Rahtu",
            "Juho Kannala",
            "Matthew Blaschko"
        ],
        "abstract": "Cascades are a popular framework to speed up object detection systems. Here we focus on the first layers of a category independent object detection cascade in which we sample a large number of windows from an objectness prior, and then discriminatively learn to filter these candidate windows by an order of magnitude. We make a number of contributions to cascade design that substantially improve over the state of the art: (i) our novel objectness prior gives much higher recall than competing methods, (ii) we propose objectness features that give high performance with very low computational cost, and (iii) we make use of a structured output ranking approach to learn highly effective, but inexpensive linear feature combinations by directly optimizing cascade performance. Thorough evaluation on the PASCAL VOC data set shows consistent improvement over the current state of the art, and over alternative discriminative learning strategies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126351",
        "reference_list": [
            {
                "year": "2001",
                "id": 199
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 60,
            "other": 29,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Image edge detection",
                "Object detection",
                "Vectors",
                "Bismuth",
                "Training",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "category independent object detection cascade",
                "structured output ranking approach",
                "linear feature combination",
                "PASCAL"
            ]
        },
        "id": 133,
        "cited_by": [
            {
                "year": "2015",
                "id": 31
            },
            {
                "year": "2015",
                "id": 128
            },
            {
                "year": "2015",
                "id": 225
            },
            {
                "year": "2015",
                "id": 225
            },
            {
                "year": "2015",
                "id": 276
            },
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2013",
                "id": 47
            },
            {
                "year": "2013",
                "id": 316
            }
        ]
    },
    {
        "title": "Face recognition based on non-corresponding region matching",
        "authors": [
            "Annan Li",
            "Shiguang Shan",
            "Xilin Chen",
            "Wen Gao"
        ],
        "abstract": "In previous works of face recognition, similarity between faces is measured by comparing corresponding face regions. That is to say, matching eyes with eyes and mouths with mouths etc.. In this paper, we propose that face can be also recognized by matching non-corresponding facial regions. In another word face can be recognized by matching eyes with mouths, for example. Specifically, the problem we study in this paper can be formulated as how to measure the possibility whether two non-corresponding face regions belong to the same face. We propose that the possibility can be measured via canonical correlation analysis. Experimental results show that it is feasible to recognize face via non-corresponding region matching. The proposed method provides an alternative and more flexible way to recognize faces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126352",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Testing",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "noncorresponding region matching",
                "face region",
                "noncorresponding facial region",
                "word face",
                "eyes matching",
                "canonical correlation analysis"
            ]
        },
        "id": 134,
        "cited_by": [
            {
                "year": "2013",
                "id": 260
            }
        ]
    },
    {
        "title": "Building large urban environments from unstructured point data",
        "authors": [
            "Florent Lafarge",
            "Cl\u00e9ment Mallet"
        ],
        "abstract": "We present a robust method for modeling cities from unstructured point data. Our algorithm provides a more complete description than existing approaches by reconstructing simultaneously buildings, trees and topologically complex grounds. Buildings are modeled by an original approach which guarantees a high generalization level while having semantized and compact representations. Geometric 3D-primitives such as planes, cylinders, spheres or cones describe regular roof sections, and are combined with mesh-patches that represent irregular roof components. The various urban components interact through a non-convex energy minimization problem in which they are propagated under arrangement constraints over a planimetric map. We experimentally validate the approach on complex urban structures and large urban scenes of millions of points.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126353",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2009",
                "id": 277
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 14,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Buildings",
                "Shape",
                "Adaptation models",
                "Clutter",
                "Minimization",
                "Vegetation",
                "Cities and towns"
            ],
            "INSPEC: Controlled Indexing": [
                "cartography",
                "concave programming",
                "geographic information systems",
                "image reconstruction",
                "mesh generation",
                "solid modelling",
                "topology",
                "town and country planning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "urban environments",
                "unstructured point data",
                "robust method",
                "modeling city",
                "buildings",
                "trees",
                "topologically complex grounds",
                "high generalization level",
                "compact representations",
                "geometric 3D-primitives",
                "mesh-patches",
                "irregular roof components",
                "urban components",
                "nonconvex energy minimization problem",
                "arrangement constraints",
                "planimetric map",
                "urban structures",
                "urban scenes"
            ]
        },
        "id": 135,
        "cited_by": [
            {
                "year": "2015",
                "id": 239
            }
        ]
    },
    {
        "title": "What an image reveals about material reflectance",
        "authors": [
            "Manmohan Chandraker",
            "Ravi Ramamoorthi"
        ],
        "abstract": "We derive precise conditions under which material reflectance properties may be estimated from a single image of a homogeneous curved surface (canonically a sphere), lit by a directional source. Based on the observation that light is reflected along certain (a priori unknown) preferred directions such as the half-angle, we propose a semiparametric BRDF abstraction that lies between purely parametric and purely data-driven models. Formulating BRDF estimation as a particular type of semiparametric regression, both the preferred directions and the form of BRDF variation along them can be estimated from data. Our approach has significant theoretical, algorithmic and empirical benefits, lends insights into material behavior and enables novel applications. While it is well-known that fitting multi-lobe BRDFs may be ill-posed under certain conditions, prior to this work, precise results for the well-posedness of BRDF estimation had remained elusive. Since our BRDF representation is derived from physical intuition, but relies on data, we avoid pitfalls of both parametric (low generalizability) and non-parametric regression (low interpretability, curse of dimensionality). Finally, we discuss several applications such as single-image relighting, light source estimation and physically meaningful BRDF editing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126354",
        "reference_list": [
            {
                "year": "2009",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 6,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Materials",
                "Light sources",
                "Brain models",
                "Vectors",
                "Parametric statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image processing",
                "material reflectance",
                "homogeneous curved surface",
                "semiparametric BRDF abstraction",
                "semiparametric regression"
            ]
        },
        "id": 136,
        "cited_by": [
            {
                "year": "2017",
                "id": 2
            },
            {
                "year": "2015",
                "id": 397
            }
        ]
    },
    {
        "title": "Predicting occupation via human clothing and contexts",
        "authors": [
            "Zheng Song",
            "Meng Wang",
            "Xian-sheng Hua",
            "Shuicheng Yan"
        ],
        "abstract": "Predicting human occupations in photos has great application potentials in intelligent services and systems. However, using traditional classification methods cannot reliably distinguish different occupations due to the complex relations between occupations and the low-level image features. In this paper, we investigate the human occupation prediction problem by modeling the appearances of human clothing as well as surrounding context. The human clothing, regarding its complex details and variant appearances, is described via part-based modeling on the automatically aligned patches of human body parts. The image patches are represented with semantic-level patterns such as clothes and haircut styles using methods based on sparse coding towards informative and noise-tolerant capacities. This description of human clothing is proved to be more effective than traditional methods. Different kinds of surrounding context are also investigated as a complementarity of human clothing features in the cases that the background information is available. Experiments are conducted on a well labeled image database that contains more than 5; 000 images from 20 representative occupation categories. The preliminary study shows the human occupation is reasonably predictable using the proposed clothing features and possible context.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126355",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2009",
                "id": 119
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 16,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Context",
                "Clothing",
                "Feature extraction",
                "Image reconstruction",
                "Image color analysis",
                "Head"
            ],
            "INSPEC: Controlled Indexing": [
                "clothing",
                "feature extraction",
                "image classification",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human clothing",
                "intelligent service system",
                "classification method",
                "low-level image feature",
                "human occupation prediction",
                "image patches",
                "semantic-level pattern",
                "sparse coding",
                "representative occupation category"
            ]
        },
        "id": 137,
        "cited_by": [
            {
                "year": "2017",
                "id": 40
            },
            {
                "year": "2015",
                "id": 118
            },
            {
                "year": "2015",
                "id": 373
            },
            {
                "year": "2013",
                "id": 439
            },
            {
                "year": "2013",
                "id": 453
            }
        ]
    },
    {
        "title": "A data-driven approach for real-time full body pose reconstruction from a depth camera",
        "authors": [
            "Andreas Baak",
            "Meinard M\u00fcller",
            "Gaurav Bharaj",
            "Hans-Peter Seidel",
            "Christian Theobalt"
        ],
        "abstract": "In recent years, depth cameras have become a widely available sensor type that captures depth images at real-time frame rates. Even though recent approaches have shown that 3D pose estimation from monocular 2.5D depth images has become feasible, there are still challenging problems due to strong noise in the depth data and self-occlusions in the motions being captured. In this paper, we present an efficient and robust pose estimation framework for tracking full-body motions from a single depth image stream. Following a data-driven hybrid strategy that combines local optimization with global retrieval techniques, we contribute several technical improvements that lead to speed-ups of an order of magnitude compared to previous approaches. In particular, we introduce a variant of Dijkstra's algorithm to efficiently extract pose features from the depth data and describe a novel late-fusion scheme based on an efficiently computable sparse Hausdorff distance to combine local and global pose estimates. Our experiments show that the combination of these techniques facilitates real-time tracking with stable results even for fast and complex motions, making it applicable to a wide range of inter-active scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126356",
        "reference_list": [
            {
                "year": "2009",
                "id": 183
            },
            {
                "year": "2005",
                "id": 46
            },
            {
                "year": "2009",
                "id": 177
            },
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 76,
            "other": 43,
            "total": 119
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Optimization",
                "Estimation",
                "Tracking",
                "Torso",
                "Cameras",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image motion analysis",
                "image reconstruction",
                "object tracking",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "data-driven approach",
                "real-time full body pose reconstruction",
                "depth camera",
                "real-time frame rates",
                "3D pose estimation",
                "monocular 2.5D depth images",
                "depth data",
                "self-occlusions",
                "full-body motion tracking",
                "single depth image stream",
                "data-driven hybrid strategy",
                "global retrieval techniques",
                "Dijkstra's algorithm",
                "pose features",
                "sparse Hausdorff distance",
                "local pose estimates",
                "global pose estimates",
                "real-time tracking"
            ]
        },
        "id": 138,
        "cited_by": [
            {
                "year": "2017",
                "id": 374
            },
            {
                "year": "2015",
                "id": 372
            },
            {
                "year": "2013",
                "id": 137
            },
            {
                "year": "2013",
                "id": 306
            },
            {
                "year": "2013",
                "id": 402
            }
        ]
    },
    {},
    {
        "title": "Shading-based dynamic shape refinement from multi-view video under general illumination",
        "authors": [
            "Chenglei Wu",
            "Kiran Varanasi",
            "Yebin Liu",
            "Hans-Peter Seidel",
            "Christian Theobalt"
        ],
        "abstract": "We present an approach to add true fine-scale spatio-temporal shape detail to dynamic scene geometry captured from multi-view video footage. Our approach exploits shading information to recover the millimeter-scale surface structure, but in contrast to related approaches succeeds under general unconstrained lighting conditions. Our method starts off from a set of multi-view video frames and an initial series of reconstructed coarse 3D meshes that lack any surface detail. In a spatio-temporal maximum a posteriori probability (MAP) inference framework, our approach first estimates the incident illumination and the spatially-varying albedo map on the mesh surface for every time instant. Thereafter, albedo and illumination are used to estimate the true geometric detail visible in the images and add it to the coarse reconstructions. The MAP framework uses weak temporal priors on lighting, albedo and geometry which improve reconstruction quality yet allow for temporal variations in the data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126358",
        "reference_list": [
            {
                "year": "2007",
                "id": 100
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 30,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Shape",
                "Geometry",
                "Image reconstruction",
                "Surface reconstruction",
                "Three dimensional displays",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer graphics",
                "image reconstruction",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shading-based dynamic shape refinement",
                "true fine-scale spatio-temporal shape",
                "dynamic scene geometry",
                "multiview video footage",
                "shading information",
                "millimeter-scale surface structure",
                "multiview video frames",
                "reconstructed coarse 3D meshes",
                "inference framework",
                "incident illumination",
                "mesh surface",
                "coarse reconstruction",
                "reconstruction quality"
            ]
        },
        "id": 140,
        "cited_by": [
            {
                "year": "2017",
                "id": 328
            },
            {
                "year": "2017",
                "id": 330
            },
            {
                "year": "2015",
                "id": 85
            }
        ]
    },
    {
        "title": "Modeling temporal coherence for optical flow",
        "authors": [
            "Sebastian Volz",
            "Andr\u00e9s Bruhn",
            "Levi Valgaerts",
            "Henning Zimmer"
        ],
        "abstract": "Despite the fact that temporal coherence is undeniably one of the key aspects when processing video data, this concept has hardly been exploited in recent optical flow methods. In this paper, we will present a novel parametrization for multi-frame optical flow computation that naturally enables us to embed the assumption of a temporally coherent spatial flow structure, as well as the assumption that the optical flow is smooth along motion trajectories. While the first assumption is realized by expanding spatial regularization over multiple frames, the second assumption is imposed by two novel first- and second-order trajectorial smoothness terms. With respect to the latter, we investigate an adaptive decision scheme that makes a local (per pixel) or global (per sequence) selection of the most appropriate model possible. Experiments show the clear superiority of our approach when compared to existing strategies for imposing temporal coherence. Moreover, we demonstrate the state-of-the-art performance of our method by achieving Top 3 results at the widely used Middlebury benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126359",
        "reference_list": [
            {
                "year": "2005",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 33,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Trajectory",
                "Computational modeling",
                "Optical imaging",
                "Coherence",
                "Estimation",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporal coherence modeling",
                "video data processing",
                "optical flow methods",
                "multiframe optical flow computation",
                "temporally coherent spatial flow structure",
                "motion trajectory",
                "spatial regularization",
                "trajectorial smoothness terms",
                "adaptive decision scheme",
                "local selection",
                "global selection",
                "state-of-the-art performance",
                "Middlebury benchmark"
            ]
        },
        "id": 141,
        "cited_by": []
    },
    {
        "title": "Variational stereo in dynamic illumination",
        "authors": [
            "Yohay Swirski",
            "Yoav Y. Schechner",
            "Tal Nir"
        ],
        "abstract": "Temporal irradiance variations are useful for finding dense stereo correspondences. These variations can be created artificially using structured light. They also occur naturally underwater. We introduce a variational optimization formulation for finding a dense stereo correspondence field. It is based on multi-frame optical flow, adapted to stereo. The formulation uses a sequence of stereo frames, and yields dense and robust results. The inherent aperture problem of optical flow is resolved using a temporal sequence of stereo frame-pairs. The results are achieved even without considering epi-polar geometry. The method has the ability to handle dynamic stereo underwater, in harsh conditions of flickering illumination. The method is demonstrated experimentally both outdoors and indoors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126360",
        "reference_list": [
            {
                "year": "2009",
                "id": 26
            },
            {
                "year": "2009",
                "id": 296
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical imaging",
                "Noise measurement",
                "Computer vision",
                "Brightness",
                "Integrated optics",
                "Stereo vision",
                "Image motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational stereo",
                "dynamic illumination",
                "irradiance variation",
                "variational optimization formulation",
                "dense stereo correspondence field",
                "multiframe optical flow",
                "stereo frames",
                "inherent aperture problem",
                "temporal sequence",
                "stereo frame-pairs",
                "epi-polar geometry",
                "dynamic stereo underwater",
                "flickering illumination"
            ]
        },
        "id": 142,
        "cited_by": []
    },
    {
        "title": "Discovering favorite views of popular places with iconoid shift",
        "authors": [
            "Tobias Weyand",
            "Bastian Leibe"
        ],
        "abstract": "In this paper, we propose a novel algorithm for automatic landmark building discovery in large, unstructured image collections. In contrast to other approaches which aim at a hard clustering, we regard the task as a mode estimation problem. Our algorithm searches for local attractors in the image distribution that have a maximal mutual homography overlap with the images in their neighborhood. Those attractors correspond to central, iconic views of single objects or buildings, which we efficiently extract using a medoid shift search with a novel distance measure. We propose efficient algorithms for performing this search. Most importantly, our approach performs only an efficient local exploration of the matching graph that makes it applicable for large-scale analysis of photo collections. We show experimental results validating our approach on a dataset of 500k images of the inner city of Paris.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126361",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2009",
                "id": 78
            },
            {
                "year": "2009",
                "id": 269
            },
            {
                "year": "2007",
                "id": 24
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 10,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Buildings",
                "Clustering algorithms",
                "Visualization",
                "Image retrieval",
                "Three dimensional displays",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "cartography",
                "computer vision",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "iconoid shift",
                "automatic landmark building discovery",
                "unstructured image collection",
                "mode estimation problem",
                "image distribution",
                "maximal mutual homography overlap",
                "medoid shift search",
                "distance measure"
            ]
        },
        "id": 143,
        "cited_by": [
            {
                "year": "2015",
                "id": 167
            },
            {
                "year": "2015",
                "id": 237
            },
            {
                "year": "2013",
                "id": 434
            }
        ]
    },
    {
        "title": "Spatiotemporal oriented energies for spacetime stereo",
        "authors": [
            "Mikhail Sizintsev",
            "Richard P. Wildes"
        ],
        "abstract": "This paper presents a novel approach to recovering temporally coherent estimates of 3D structure of a dynamic scene from a sequence of binocular stereo images. The approach is based on matching spatiotemporal orientation distributions between left and right temporal image streams, which encapsulates both local spatial and temporal structure for disparity estimation. By capturing spatial and temporal structure in this unified fashion, both sources of information combine to yield disparity estimates that are naturally temporal coherent, while helping to resolve matches that might be ambiguous when either source is considered alone. Further, by allowing subsets of the orientation measurements to support different disparity estimates, an approach to recovering multilayer disparity from spacetime stereo is realized. The approach has been implemented with real-time performance on commodity GPUs. Empirical evaluation shows that the approach yields qualitatively and quantitatively superior disparity estimates in comparison to various alternative approaches, including the ability to provide accurate multilayer estimates in the presence of (semi)transparent and specular surfaces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126362",
        "reference_list": [
            {
                "year": "2007",
                "id": 173
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatiotemporal phenomena",
                "Three dimensional displays",
                "Stereo vision",
                "Nonhomogeneous media",
                "Estimation",
                "Position measurement",
                "Energy measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "real-time systems",
                "spatiotemporal phenomena",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatiotemporal oriented energy",
                "spacetime stereo",
                "temporally coherent estimates",
                "3D structure",
                "dynamic scene",
                "binocular stereo images",
                "spatiotemporal orientation distributions",
                "temporal image streams",
                "local spatial struture",
                "temporal structure",
                "disparity estimation",
                "disparity estimates",
                "orientation measurements",
                "multilayer disparity",
                "real-time performance",
                "commodity GPU",
                "semitransparent surfaces",
                "specular surfaces"
            ]
        },
        "id": 144,
        "cited_by": []
    },
    {
        "title": "StereoCut: Consistent interactive object selection in stereo image pairs",
        "authors": [
            "Brian L. Price",
            "Scott Cohen"
        ],
        "abstract": "Methods of interacting with stereo image pairs are important for handling the increasing amount of stereoscopic 3D data now being produced. In this paper, we introduce a framework for interactively selecting objects in two stereo images simultaneously using graph cut. A key contribution of our method is the use of stereo correspondence probability distributions to govern the strength of the connection between the two images. This allows information from arbitrary stereo matching algorithms to be utilized by our method. We show how to enforce consistency in these distributions to improve the results. For comparisons, we introduce a new dataset of stereo images and ground truth selections. We evaluate different correspondence distributions and show that our method is effective in selecting objects from stereo pairs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126363",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2009",
                "id": 99
            },
            {
                "year": "2003",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 11,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Probability density function",
                "Image segmentation",
                "Stereo image processing",
                "Computational modeling",
                "Three dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "statistical distributions",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "StereoCut",
                "consistent interactive object selection",
                "stereo image pairs",
                "stereoscopic 3D data",
                "graph cut",
                "stereo correspondence probability distribution",
                "arbitrary stereo matching algorithm"
            ]
        },
        "id": 145,
        "cited_by": [
            {
                "year": "2015",
                "id": 192
            }
        ]
    },
    {
        "title": "Graph mode-based contextual kernels for robust SVM tracking",
        "authors": [
            "Xi Li",
            "Anthony Dick",
            "Hanzi Wang",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "abstract": "Visual tracking has been typically solved as a binary classification problem. Most existing trackers only consider the pairwise interactions between samples, and thereby ignore the higher-order contextual interactions, which may lead to the sensitivity to complicated factors such as noises, outliers, background clutters and so on. In this paper, we propose a visual tracker based on support vector machines (SVMs), for which a novel graph mode-based contextual kernel is designed to effectively capture the higher-order contextual information from samples. To do so, we first create a visual graph whose similarity matrix is determined by a baseline visual kernel. Second, a set of high-order contexts are discovered in the visual graph. The problem of discovering these high-order contexts is solved by seeking modes of the visual graph. Each graph mode corresponds to a vertex community termed as a high-order context. Third, we construct a contextual kernel that effectively captures the interaction information between the high-order contexts. Finally, this contextual kernel is embedded into SVMs for robust tracking. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126364",
        "reference_list": [
            {
                "year": "2007",
                "id": 73
            },
            {
                "year": "2007",
                "id": 111
            },
            {
                "year": "2007",
                "id": 115
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 8,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Visualization",
                "Support vector machines",
                "Context",
                "Target tracking",
                "Videos",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "matrix algebra",
                "object tracking",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph mode-based contextual kernel",
                "robust SVM tracking",
                "visual tracking",
                "binary classification problem",
                "pairwise interaction",
                "support vector machine",
                "visual graph",
                "similarity matrix",
                "vertex community"
            ]
        },
        "id": 146,
        "cited_by": [
            {
                "year": "2013",
                "id": 195
            },
            {
                "year": "2013",
                "id": 347
            }
        ]
    },
    {
        "title": "Gaussian process regression flow for analysis of motion trajectories",
        "authors": [
            "Kihwan Kim",
            "Dongryeol Lee",
            "Irfan Essa"
        ],
        "abstract": "Recognition of motions and activities of objects in videos requires effective representations for analysis and matching of motion trajectories. In this paper, we introduce a new representation specifically aimed at matching motion trajectories. We model a trajectory as a continuous dense flow field from a sparse set of vector sequences using Gaussian Process Regression. Furthermore, we introduce a random sampling strategy for learning stable classes of motions from limited data. Our representation allows for incrementally predicting possible paths and detecting anomalous events from online trajectories. This representation also supports matching of complex motions with acceleration changes and pauses or stops within a trajectory. We use the proposed approach for classifying and predicting motion trajectories in traffic monitoring domains and test on several data sets. We show that our approach works well on various types of complete and incomplete trajectories from a variety of video data sets with different frame rates.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126365",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 2,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Vectors",
                "Gaussian processes",
                "Videos",
                "Training",
                "Tracking",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image matching",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Gaussian process regression flow",
                "motion trajectory matching",
                "motion recognition",
                "continuous dense flow field",
                "vector sequences",
                "random sampling strategy",
                "anomalous event detection",
                "online trajectory",
                "traffic monitoring domains",
                "video data sets"
            ]
        },
        "id": 147,
        "cited_by": [
            {
                "year": "2015",
                "id": 483
            },
            {
                "year": "2013",
                "id": 20
            }
        ]
    },
    {
        "title": "Cluster-based color space optimizations",
        "authors": [
            "Cheryl Lau",
            "Wolfgang Heidrich",
            "Rafal Mantiuk"
        ],
        "abstract": "Transformations between different color spaces and gamuts are ubiquitous operations performed on images. Often, these transformations involve information loss, for example when mapping from color to grayscale for printing, from multispectral or multiprimary data to tristimulus spaces, or from one color gamut to another. In all these applications, there exists a straightforward \u201cnatural\u201d mapping from the source space to the target space, but the mapping is not bijective, resulting in information loss due to metamerism and similar effects. We propose a cluster-based approach for optimizing the transformation for individual images in a way that preserves as much of the information as possible from the source space while staying as faithful as possible to the natural mapping. Our approach can be applied to a host of color transformation problems including color to gray, gamut mapping, conversion of multispectral and multiprimary data to tristimulus colors, and image optimization for color deficient viewers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126366",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 9,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Optimization",
                "Vectors",
                "Color",
                "Image edge detection",
                "Gray-scale",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image enhancement",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cluster-based color space optimization",
                "color spaces",
                "ubiquitous operation",
                "grayscale",
                "multispectral data",
                "multiprimary data",
                "tristimulus spaces",
                "color gamut mapping",
                "source space",
                "color transformation problem",
                "tristimulus colors",
                "image optimization",
                "color deficient viewers"
            ]
        },
        "id": 148,
        "cited_by": []
    },
    {
        "title": "Refractive shape from light field distortion",
        "authors": [
            "Gordon Wetzstein",
            "David Roodnick",
            "Wolfgang Heidrich",
            "Ramesh Raskar"
        ],
        "abstract": "Acquiring transparent, refractive objects is challenging as these kinds of objects can only be observed by analyzing the distortion of reference background patterns. We present a new, single image approach to reconstructing thin transparent surfaces, such as thin solids or surfaces of fluids. Our method is based on observing the distortion of light field background illumination. Light field probes have the potential to encode up to four dimensions in varying colors and intensities: spatial and angular variation on the probe surface; commonly employed reference patterns are only two-dimensional by coding either position or angle on the probe. We show that the additional information can be used to reconstruct refractive surface normals and a sparse set of control points from a single photograph.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126367",
        "reference_list": [
            {
                "year": "2007",
                "id": 100
            },
            {
                "year": "2005",
                "id": 137
            },
            {
                "year": "2005",
                "id": 189
            },
            {
                "year": "2005",
                "id": 205
            },
            {
                "year": "2007",
                "id": 43
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 12,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Surface reconstruction",
                "Probes",
                "Image color analysis",
                "Lighting",
                "Lenses"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "light refraction",
                "photographic process"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "refractive shape",
                "light field distortion",
                "refractive objects",
                "background pattern",
                "thin transparent surface",
                "light field background illumination",
                "light field probes",
                "angular variation",
                "probe surface",
                "reference patterns",
                "refractive surface"
            ]
        },
        "id": 149,
        "cited_by": [
            {
                "year": "2015",
                "id": 377
            },
            {
                "year": "2015",
                "id": 383
            },
            {
                "year": "2015",
                "id": 384
            }
        ]
    },
    {
        "title": "A geometric solver for calibrated stereo egomotion",
        "authors": [
            "Enrique Dunn",
            "Brian Clipp",
            "Jan-Michael Frahm"
        ],
        "abstract": "This paper introduces a novel geometrical solution for the pose estimation of a stereo camera system as commonly used in robotics, where the camera system balances between coverage and overlap. The proposed approach considers a set of features observed, respectively, in four, three and two views. In contrast to most algebraic solutions our constraints are geometrically meaningful. Initially, we use a four view feature to restrict our translation vector to lie on the surface of a sphere while setting orientation as a function of translation up to a single rotational degree of freedom. Next, we use a three view feature to restrict the translation vector to lie on a circle on the sphere, while completely defining orientation as a function of translation. Finally, we use a two view feature to determine the translation vector lying on the intersection of the circle and one of the generator lines of a doubly ruled quadric. We show how for this final step, the problem can be reduced to the intersection of two coplanar circles. We also analyze the degenerate configurations of the proposed solver and perform an experimental evaluation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126368",
        "reference_list": [
            {
                "year": "2009",
                "id": 221
            },
            {
                "year": "2001",
                "id": 117
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three dimensional displays",
                "Vectors",
                "Stereo vision",
                "Robot vision systems",
                "Estimation",
                "Generators"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "feature extraction",
                "image motion analysis",
                "pose estimation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric solver",
                "calibrated stereo egomotion",
                "pose estimation",
                "stereo camera system",
                "four view feature",
                "three view feature",
                "translation vector"
            ]
        },
        "id": 150,
        "cited_by": []
    },
    {
        "title": "Tracking by Sampling Trackers",
        "authors": [
            "Junseok Kwon",
            "Kyoung Mu Lee"
        ],
        "abstract": "We propose a novel tracking framework called visual tracker sampler that tracks a target robustly by searching for the appropriate trackers in each frame. Since the real-world tracking environment varies severely over time, the trackers should be adapted or newly constructed depending on the current situation. To do this, our method obtains several samples of not only the states of the target but also the trackers themselves during the sampling process. The trackers are efficiently sampled using the Markov Chain Monte Carlo method from the predefined tracker space by proposing new appearance models, motion models, state representation types, and observation types, which are the basic important components of visual trackers. Then, the sampled trackers run in parallel and interact with each other while covering various target variations efficiently. The experiment demonstrates that our method tracks targets accurately and robustly in the real-world tracking environments and outperforms the state-of-the-art tracking methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126369",
        "reference_list": [
            {
                "year": "2005",
                "id": 194
            }
        ],
        "citation": {
            "ieee": 45,
            "other": 33,
            "total": 78
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "Markov processes",
                "Monte Carlo methods",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tracking framework",
                "visual tracker sampler",
                "target tracking",
                "sampling process",
                "Markov Chain Monte Carlo method",
                "appearance models",
                "motion models",
                "state representation types",
                "observation types"
            ]
        },
        "id": 151,
        "cited_by": [
            {
                "year": "2015",
                "id": 335
            },
            {
                "year": "2015",
                "id": 345
            },
            {
                "year": "2013",
                "id": 195
            },
            {
                "year": "2013",
                "id": 347
            }
        ]
    },
    {
        "title": "Simultaneous localization, mapping and deblurring",
        "authors": [
            "Hee Seok Lee",
            "Junghyun Kwon",
            "Kyoung Mu Lee"
        ],
        "abstract": "Handling motion blur is one of important issues in visual SLAM. For a fast-moving camera, motion blur is an unavoidable effect and it can degrade the results of localization and reconstruction severely. In this paper, we present a unified algorithm to handle motion blur for visual SLAM, including the blur-robust data association method and the fast deblurring method. In our framework, camera motion and 3-D point structures are reconstructed by SLAM, and the information from SLAM makes the estimation of motion blur quite easy and effective. Reversely, estimating motion blur enables robust data association and drift-free localization of SLAM with blurred images. The blurred images are recovered by fast deconvolution using SLAM data, and more features are extracted and registered to the map so that the SLAM procedure can be continued even with the blurred images. In this way, visual SLAM and deblurring are solved simultaneously, and improve each other's results significantly.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126370",
        "reference_list": [
            {
                "year": "2007",
                "id": 275
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 1,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Cameras",
                "Simultaneous localization and mapping",
                "Visualization",
                "Image reconstruction",
                "Feature extraction",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "motion estimation",
                "SLAM (robots)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "simultaneous localization",
                "mapping",
                "visual SLAM",
                "fast-moving camera",
                "motion blur handling",
                "blur-robust data association method",
                "fast deblurring method",
                "camera motion",
                "3D point structures",
                "motion blur estimation",
                "drift-free localization",
                "fast deconvolution",
                "feature extraction"
            ]
        },
        "id": 152,
        "cited_by": []
    },
    {
        "title": "Stereo reconstruction using high order likelihood",
        "authors": [
            "Ho Yub Jung",
            "Kyoung Mu Lee",
            "Sang Uk Lee"
        ],
        "abstract": "Under the popular Bayesian approach, a stereo problem can be formulated by defining likelihood and prior. Likelihoods are often associated with unary terms and priors are defined by pair-wise or higher order cliques in Markov random field (MRF). In this paper, we propose to use high order likelihood model in stereo. Numerous conventional patch based matching methods such as normalized cross correlation, Laplacian of Gaussian, or census filters are designed under the naive assumption that all the pixels of a patch have the same disparities. However, patch-wise cost can be formulated as higher order cliques for MRF so that the matching cost is a function of image patch's disparities. A patch obtained from the projected image by a disparity map should provide a better match without the blurring effect around disparity discontinuities. Among patch-wise high order matching costs, the census filter approach can be easily reduced to pair-wise cliques. The experimental results on census filter-based high order likelihood demonstrate the advantages of high order likelihood over independent identically distributed unary model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126371",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Image edge detection",
                "Stereo vision",
                "Optimization",
                "Correlation",
                "Mathematical model",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image reconstruction",
                "maximum likelihood estimation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo reconstruction",
                "high order likelihood model",
                "Markov random field",
                "patch based matching method",
                "normalized cross correlation method",
                "Laplacian-of-Gaussian method",
                "census filters method",
                "patch-wise cost",
                "matching cost",
                "disparity map"
            ]
        },
        "id": 153,
        "cited_by": [
            {
                "year": "2015",
                "id": 196
            }
        ]
    },
    {
        "title": "Dynamic texture classification using dynamic fractal analysis",
        "authors": [
            "Yong Xu",
            "Yuhui Quan",
            "Haibin Ling",
            "Hui Ji"
        ],
        "abstract": "In this paper, we developed a novel tool called dynamic fractal analysis for dynamic texture (DT) classification, which not only provides a rich description of DT but also has strong robustness to environmental changes. The resulting dynamic fractal spectrum (DFS) for DT sequences consists of two components: One is the volumetric dynamic fractal spectrum component (V-DFS) that captures the stochastic self-similarities of DT sequences as 3D volume datasets; the other is the multi-slice dynamic fractal spectrum component (S-DFS) that encodes fractal structures of DT sequences on 2D slices along different views of the 3D volume. Various types of measures of DT sequences are collected in our approach to analyze DT sequences from different perspectives. The experimental evaluation is conducted on three widely used benchmark datasets. In all the experiments, our method demonstrated excellent performance in comparison with state-of-the-art approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126372",
        "reference_list": [
            {
                "year": "2003",
                "id": 161
            },
            {
                "year": "2007",
                "id": 254
            },
            {
                "year": "2007",
                "id": 37
            },
            {
                "year": "2001",
                "id": 214
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 22,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Fractals",
                "Three dimensional displays",
                "Vectors",
                "Dynamics",
                "Stochastic processes",
                "Video sequences",
                "Brightness"
            ],
            "INSPEC: Controlled Indexing": [
                "fractals",
                "image classification",
                "image texture",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic texture classification",
                "dynamic fractal analysis",
                "volumetric dynamic fractal spectrum component",
                "V-DFS",
                "stochastic self-similarity",
                "3D volume dataset",
                "multislice dynamic fractal spectrum component",
                "S-DFS",
                "fractal structure",
                "2D slices"
            ]
        },
        "id": 154,
        "cited_by": [
            {
                "year": "2015",
                "id": 8
            }
        ]
    },
    {
        "title": "A joint learning framework for attribute models and object descriptions",
        "authors": [
            "Dhruv Mahajan",
            "Sundararajan Sellamanickam",
            "Vinod Nair"
        ],
        "abstract": "We present a new approach to learning attribute-based descriptions of objects. Unlike earlier works, we do not assume that the descriptions are hand-labeled. Instead, our approach jointly learns both the attribute classifiers and the descriptions from data. By incorporating class information into the attribute classifier learning, we get an attribute-level representation that generalizes well to both unseen examples of known classes and unseen classes. We consider two different settings, one with unlabeled images available for learning, and another without. The former corresponds to a novel transductive setting where the unlabeled images can come from new classes. Results from Animals with Attributes and a-Yahoo, a-Pascal benchmark datasets show that the learned representations give similar or even better accuracy than the hand-labeled descriptions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126373",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 17,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Training",
                "Optimization",
                "Semantics",
                "Supervised learning",
                "Dolphins"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint learning framework",
                "object description",
                "learning attribute-based description",
                "class information",
                "attribute classifier learning",
                "attribute-level representation",
                "unlabeled image",
                "transductive learning"
            ]
        },
        "id": 155,
        "cited_by": [
            {
                "year": "2017",
                "id": 443
            },
            {
                "year": "2015",
                "id": 211
            },
            {
                "year": "2015",
                "id": 465
            },
            {
                "year": "2013",
                "id": 264
            }
        ]
    },
    {
        "title": "Data-driven crowd analysis in videos",
        "authors": [
            "Mikel Rodriguez",
            "Josef Sivic",
            "Ivan Laptev",
            "Jean-Yves Audibert"
        ],
        "abstract": "In this work we present a new crowd analysis algorithm powered by behavior priors that are learned on a large database of crowd videos gathered from the Internet. The algorithm works by first learning a set of crowd behavior priors off-line. During testing, crowd patches are matched to the database and behavior priors are transferred. We adhere to the insight that despite the fact that the entire space of possible crowd behaviors is infinite, the space of distinguishable crowd motion patterns may not be all that large. For many individuals in a crowd, we are able to find analogous crowd patches in our database which contain similar patterns of behavior that can effectively act as priors to constrain the difficult task of tracking an individual in a crowd. Our algorithm is data-driven and, unlike some crowd characterization methods, does not require us to have seen the test video beforehand. It performs like state-of-the-art methods for tracking people having common crowd behaviors and outperforms the methods when the tracked individual behaves in an unusual way.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126374",
        "reference_list": [
            {
                "year": "2009",
                "id": 178
            },
            {
                "year": "2007",
                "id": 97
            },
            {
                "year": "2011",
                "id": 308
            },
            {
                "year": "2003",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 65,
            "other": 31,
            "total": 96
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Tracking",
                "Databases",
                "Testing",
                "Vectors",
                "Analytical models",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "behavioural sciences computing",
                "pattern recognition",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "data-driven crowd analysis",
                "crowd analysis algorithm",
                "large database",
                "crowd videos",
                "Internet",
                "crowd patches",
                "distinguishable crowd motion patterns",
                "data-driven algorithm",
                "crowd characterization methods",
                "common crowd behaviors"
            ]
        },
        "id": 156,
        "cited_by": [
            {
                "year": "2011",
                "id": 308
            }
        ]
    },
    {
        "title": "Outdoor human motion capture using inverse kinematics and von mises-fisher sampling",
        "authors": [
            "Gerard Pons-Moll",
            "Andreas Baak",
            "Juergen Gall",
            "Laura Leal-Taix\u00e9",
            "Meinard M\u00fcller",
            "Hans-Peter Seidel",
            "Bodo Rosenhahn"
        ],
        "abstract": "Human motion capturing (HMC) from multiview image sequences is an extremely difficult problem due to depth and orientation ambiguities and the high dimensionality of the state space. In this paper, we introduce a novel hybrid HMC system that combines video input with sparse inertial sensor input. Employing an annealing particle-based optimization scheme, our idea is to use orientation cues derived from the inertial input to sample particles from the manifold of valid poses. Then, visual cues derived from the video input are used to weight these particles and to iteratively derive the final pose. As our main contribution, we propose an efficient sampling procedure where the particles are derived analytically using inverse kinematics on the orientation cues. Additionally, we introduce a novel sensor noise model to account for uncertainties based on the von Mises-Fisher distribution. Doing so, orientation constraints are naturally fulfilled and the number of needed particles can be kept very small. More generally, our method can be used to sample poses that fulfill arbitrary orientation or positional kinematic constraints. In the experiments, we show that our system can track even highly dynamic motions in an outdoor environment with changing illumination, background clutter, and shadows.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126375",
        "reference_list": [
            {
                "year": "2009",
                "id": 183
            },
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 11,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Kinematics",
                "Bones",
                "Noise",
                "Joints",
                "Tracking",
                "Optimization",
                "Manifolds"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sampling",
                "image sensors",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "outdoor human motion capture",
                "inverse kinematics",
                "von Mises-Fisher sampling",
                "multiview image sequence",
                "hybrid human motion capture system",
                "sparse inertial sensor input",
                "annealing particle-based optimization scheme",
                "orientation cues",
                "visual cues",
                "sensor noise model",
                "von Mises-Fisher distribution",
                "positional kinematic constraints"
            ]
        },
        "id": 157,
        "cited_by": [
            {
                "year": "2017",
                "id": 415
            },
            {
                "year": "2013",
                "id": 402
            }
        ]
    },
    {
        "title": "Panoramic stereo video textures",
        "authors": [
            "Vincent Couture",
            "Michael S. Langer",
            "S\u00e9bastien Roy"
        ],
        "abstract": "A panoramic stereo (or omnistereo) pair of images provides depth information from stereo up to 360 degrees around a central observer. Because omnistereo lenses or mirrors do not yet exist, synthesizing omnistereo images requires multiple stereo camera positions and baseline orientations. Recent omnistereo methods stitch together many small field of view images called slits which are captured by one or two cameras following a circular motion. However, these methods produce omnistereo images for static scenes only. The situation is much more challenging for dynamic scenes since stitching needs to occur over both space and time and should synchronize the motion between left and right views as much as possible. This paper presents the first ever method for synthesizing panoramic stereo video textures. The method uses full frames rather than slits and uses blending across seams rather than smoothing or matching based on graph cuts. The method produces loopable panoramic stereo videos that can be displayed up to 360 degrees around a viewer.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126376",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 5,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Stereo vision",
                "Synchronization",
                "Calibration",
                "Visualization",
                "Image edge detection",
                "Smoothing methods"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image texture",
                "stereo image processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "panoramic stereo video texture synthesis",
                "omnistereo images",
                "stereo camera positions",
                "baseline orientations",
                "slits",
                "circular motion",
                "dynamic scenes",
                "full frames",
                "blending",
                "graph cuts"
            ]
        },
        "id": 158,
        "cited_by": []
    },
    {
        "title": "Centralized sparse representation for image restoration",
        "authors": [
            "Weisheng Dong",
            "Lei Zhang",
            "Guangming Shi"
        ],
        "abstract": "This paper proposes a novel sparse representation model called centralized sparse representation (CSR) for image restoration tasks. In order for faithful image reconstruction, it is expected that the sparse coding coefficients of the degraded image should be as close as possible to those of the unknown original image with the given dictionary. However, since the available data are the degraded (noisy, blurred and/or down-sampled) versions of the original image, the sparse coding coefficients are often not accurate enough if only the local sparsity of the image is considered, as in many existing sparse representation models. To make the sparse coding more accurate, a centralized sparsity constraint is introduced by exploiting the nonlocal image statistics. The local sparsity and the nonlocal sparsity constraints are unified into a variational framework for optimization. Extensive experiments on image restoration validated that our CSR model achieves convincing improvement over previous state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126377",
        "reference_list": [],
        "citation": {
            "ieee": 72,
            "other": 53,
            "total": 125
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Image restoration",
                "Encoding",
                "Kernel",
                "Image coding",
                "Vectors",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "image representation",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "centralized sparse representation model",
                "image restoration",
                "image reconstruction",
                "sparse coding",
                "centralized sparsity constraint",
                "nonlocal image statistics"
            ]
        },
        "id": 159,
        "cited_by": [
            {
                "year": "2017",
                "id": 266
            },
            {
                "year": "2015",
                "id": 26
            },
            {
                "year": "2015",
                "id": 38
            },
            {
                "year": "2015",
                "id": 41
            },
            {
                "year": "2015",
                "id": 50
            },
            {
                "year": "2015",
                "id": 396
            }
        ]
    },
    {
        "title": "Introducing total curvature for image processing",
        "authors": [
            "Bastian Goldluecke",
            "Daniel Cremers"
        ],
        "abstract": "We introduce the novel continuous regularizer total curvature (TC) for images u: \u03a9 \u2192 \u211d. It is defined as the Menger-Melnikov curvature of the Radon measure |Du|, which can be understood as a measure theoretic formulation of curvature mathematically related to mean curvature. The functional is not convex, therefore we define a convex relaxation which yields a close approximation. Similar to the total variation, the relaxation can be written as the support functional of a convex set, which means that there are stable and efficient minimization algorithms available when it is used as a regularizer in image processing problems. Our current implementation can handle general inverse problems, inpainting and segmentation. We demonstrate in experiments and comparisons how the regularizer performs in practice.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126378",
        "reference_list": [
            {
                "year": "2007",
                "id": 132
            },
            {
                "year": "2009",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 4,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "TV",
                "Minimization",
                "Approximation methods",
                "Integral equations",
                "Noise reduction",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image processing",
                "minimisation",
                "Radon transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "total curvature",
                "image processing",
                "Menger-Melnikov curvature",
                "Radon measure",
                "convex relaxation",
                "convex set",
                "minimization algorithm"
            ]
        },
        "id": 160,
        "cited_by": [
            {
                "year": "2013",
                "id": 291
            }
        ]
    },
    {
        "title": "Viewpoint-aware object detection and pose estimation",
        "authors": [
            "Daniel Glasner",
            "Meirav Galun",
            "Sharon Alpert",
            "Ronen Basri",
            "Gregory Shakhnarovich"
        ],
        "abstract": "We describe an approach to category-level detection and viewpoint estimation for rigid 3D objects from single 2D images. In contrast to many existing methods, we directly integrate 3D reasoning with an appearance-based voting architecture. Our method relies on a nonparametric representation of a joint distribution of shape and appearance of the object class. Our voting method employs a novel parametrization of joint detection and viewpoint hypothesis space, allowing efficient accumulation of evidence. We combine this with a re-scoring and refinement mechanism, using an ensemble of view-specific Support Vector Machines. We evaluate the performance of our approach in detection and pose estimation of cars on a number of benchmark datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126379",
        "reference_list": [
            {
                "year": "2009",
                "id": 172
            },
            {
                "year": "2007",
                "id": 146
            },
            {
                "year": "2009",
                "id": 27
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 44,
            "other": 12,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Solid modeling",
                "Estimation",
                "Support vector machines",
                "Training",
                "Feature extraction",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "pose estimation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "viewpoint-aware object detection",
                "pose estimation",
                "category-level detection",
                "viewpoint estimation",
                "rigid 3D objects",
                "single 2D images",
                "3D reasoning",
                "appearance-based voting architecture",
                "nonparametric representation",
                "joint distribution",
                "voting method",
                "joint detection parametrization",
                "viewpoint hypothesis space",
                "re-scoring mechanism",
                "refinement mechanism",
                "view-specific support vector machines"
            ]
        },
        "id": 161,
        "cited_by": [
            {
                "year": "2017",
                "id": 548
            },
            {
                "year": "2015",
                "id": 103
            },
            {
                "year": "2015",
                "id": 115
            },
            {
                "year": "2015",
                "id": 320
            },
            {
                "year": "2013",
                "id": 44
            },
            {
                "year": "2013",
                "id": 94
            },
            {
                "year": "2013",
                "id": 217
            },
            {
                "year": "2013",
                "id": 319
            }
        ]
    },
    {
        "title": "Video Primal Sketch: A generic middle-level representation of video",
        "authors": [
            "Zhi Han",
            "Zongben Xu",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper presents a middle-level video representation named Video Primal Sketch (VPS), which integrates two regimes of models: i) sparse coding model using static or moving primitives to explicitly represent moving corners, lines, feature points, etc., ii) FRAME/MRF model with spatio-temporal filters to implicitly represent textured motion, such as water and fire, by matching feature statistics, i.e. histograms. This paper makes three contributions: i) learning a dictionary of video primitives as parametric generative model; ii) studying the Spatio-Temporal FRAME (ST-FRAME) model for modeling and synthesizing textured motion; and iii) developing a parsimonious hybrid model for generic video representation. VPS selects the proper representation automatically and is compatible with high-level action representations. In the experiments, we synthesize a series of dynamic textures, reconstruct real videos and show varying VPS over the change of densities causing by the scale transition in videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126380",
        "reference_list": [
            {
                "year": "2009",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Histograms",
                "Image reconstruction",
                "Dictionaries",
                "Encoding",
                "Dynamics",
                "Bismuth"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image motion analysis",
                "image texture",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video primal sketch",
                "generic middle-level video representation",
                "sparse coding model",
                "FRAME-MRF model",
                "spatio-temporal filters",
                "static primitives",
                "moving primitives",
                "feature statistics matching",
                "video primitives dictionary learning",
                "parametric generative model",
                "spatio-temporal FRAME model",
                "textured motion synthesis",
                "parsimonious hybrid model"
            ]
        },
        "id": 162,
        "cited_by": []
    },
    {
        "title": "3D scene flow estimation with a rigid motion prior",
        "authors": [
            "Christoph Vogel",
            "Konrad Schindler",
            "Stefan Roth"
        ],
        "abstract": "We present an approach to 3D scene flow estimation, which exploits that in realistic scenarios image motion is frequently dominated by observer motion and independent, but rigid object motion. We cast the dense estimation of both scene structure and 3D motion from sequences of two or more views as a single energy minimization problem. We show that agnostic smoothness priors, such as the popular total variation, are biased against motion discontinuities in viewing direction. Instead, we propose to regularize by encouraging local rigidity of the 3D scene. We derive a local rigidity constraint of the 3D scene flow and define a smoothness term that penalizes deviations from that constraint, thus favoring solutions that consist largely of rigidly moving parts. Our experiments show that the new rigid motion prior reduces the 3D flow error by 42% compared to standard TV regularization with the same data term.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126381",
        "reference_list": [
            {
                "year": "2007",
                "id": 160
            },
            {
                "year": "2009",
                "id": 213
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 18,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Cameras",
                "Optical imaging",
                "Estimation",
                "Adaptive optics",
                "Brightness",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D scene flow estimation",
                "rigid motion prior",
                "image motion",
                "observer motion",
                "object motion",
                "view sequence",
                "single energy minimization problem",
                "agnostic smoothness prior",
                "smoothness term"
            ]
        },
        "id": 163,
        "cited_by": [
            {
                "year": "2017",
                "id": 271
            },
            {
                "year": "2015",
                "id": 493
            },
            {
                "year": "2013",
                "id": 171
            }
        ]
    },
    {
        "title": "RECON: Scale-adaptive robust estimation via Residual Consensus",
        "authors": [
            "Rahul Raguram",
            "Jan-Michael Frahm"
        ],
        "abstract": "In this paper, we present a novel, threshold-free robust estimation framework capable of efficiently fitting models to contaminated data. While RANSAC and its many variants have emerged as popular tools for robust estimation, their performance is largely dependent on the availability of a reasonable prior estimate of the inlier threshold. In this work, we aim to remove this threshold dependency. We build on the observation that models generated from uncontaminated minimal subsets are \u201cconsistent\u201d in terms of the behavior of their residuals, while contaminated models exhibit uncorrelated behavior. By leveraging this observation, we then develop a very simple, yet effective algorithm that does not require apriori knowledge of either the scale of the noise, or the fraction of uncontaminated points. The resulting estimator, RECON (REsidual CONsensus), is capable of elegantly adapting to the contamination level of the data, and shows excellent performance even at low inlier ratios and high noise levels. We demonstrate the efficiency of our framework on a variety of challenging estimation problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126382",
        "reference_list": [
            {
                "year": "2009",
                "id": 52
            },
            {
                "year": "2009",
                "id": 282
            },
            {
                "year": "2003",
                "id": 27
            },
            {
                "year": "2009",
                "id": 267
            },
            {
                "year": "2009",
                "id": 269
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 7,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Data models",
                "Estimation",
                "Robustness",
                "Silicon",
                "Noise",
                "Pollution measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "estimation theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "RECON",
                "scale-adaptive robust estimation",
                "residual consensus",
                "threshold-free robust estimation",
                "RANSAC",
                "contamination level",
                "noise level"
            ]
        },
        "id": 164,
        "cited_by": [
            {
                "year": "2015",
                "id": 254
            }
        ]
    },
    {
        "title": "Scene recognition and weakly supervised object localization with deformable part-based models",
        "authors": [
            "Megha Pandey",
            "Svetlana Lazebnik"
        ],
        "abstract": "Weakly supervised discovery of common visual structure in highly variable, cluttered images is a key problem in recognition. We address this problem using deformable part-based models (DPM's) with latent SVM training [6]. These models have been introduced for fully supervised training of object detectors, but we demonstrate that they are also capable of more open-ended learning of latent structure for such tasks as scene recognition and weakly supervised object localization. For scene recognition, DPM's can capture recurring visual elements and salient objects; in combination with standard global image features, they obtain state-of-the-art results on the MIT 67-category indoor scene dataset. For weakly supervised object localization, optimization over latent DPM parameters can discover the spatial extent of objects in cluttered training images without ground-truth bounding boxes. The resulting method outperforms a recent state-of-the-art weakly supervised object localization approach on the PASCAL-07 dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126383",
        "reference_list": [],
        "citation": {
            "ieee": 158,
            "other": 79,
            "total": 237
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene recognition",
                "weakly supervised object localization",
                "deformable part-based model",
                "weakly supervised discovery",
                "latent SVM training",
                "object detector",
                "open-ended learning",
                "latent structure",
                "recurring visual element",
                "salient object",
                "indoor scene dataset",
                "PASCAL-07 dataset"
            ]
        },
        "id": 165,
        "cited_by": [
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2015",
                "id": 6
            },
            {
                "year": "2015",
                "id": 139
            },
            {
                "year": "2015",
                "id": 143
            },
            {
                "year": "2015",
                "id": 159
            },
            {
                "year": "2015",
                "id": 277
            },
            {
                "year": "2013",
                "id": 38
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2013",
                "id": 104
            },
            {
                "year": "2013",
                "id": 324
            },
            {
                "year": "2013",
                "id": 327
            },
            {
                "year": "2013",
                "id": 372
            },
            {
                "year": "2013",
                "id": 424
            }
        ]
    },
    {
        "title": "Pushing the limits of digital imaging using structured illumination",
        "authors": [
            "Prasanna Rangarajan",
            "Indranil Sinharoy",
            "Panos Papamichalis",
            "Marc P. Christensen"
        ],
        "abstract": "The present work describes an active stereo apparatus that can not only recover scene geometry but also resolve spatial detail beyond the camera optical cutoff. The apparatus is comprised of a camera and a projector whose center-of-perspective is located in the camera pupil plane 1 . The scene is illuminated with warped sinusoidal patterns as opposed to periodic or coded patterns. The findings reported in this work can help design imaging systems that feature improved optical resolution and 3D acquisition capabilities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126384",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Spatial resolution",
                "Demodulation",
                "Conferences",
                "Phase measurement",
                "Cameras",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "lighting",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "digital imaging",
                "structured illumination",
                "active stereo apparatus",
                "projector center-of-perspective",
                "warped sinusoidal pattern",
                "optical resolution capability",
                "3D acquisition capability",
                "scene geometry recovery"
            ]
        },
        "id": 166,
        "cited_by": []
    },
    {
        "title": "Superpixel tracking",
        "authors": [
            "Shu Wang",
            "Huchuan Lu",
            "Fan Yang",
            "Ming-Hsuan Yang"
        ],
        "abstract": "While numerous algorithms have been proposed for object tracking with demonstrated success, it remains a challenging problem for a tracker to handle large change in scale, motion, shape deformation with occlusion. One of the main reasons is the lack of effective image representation to account for appearance variation. Most trackers use high-level appearance structure or low-level cues for representing and matching target objects. In this paper, we propose a tracking method from the perspective of mid-level vision with structural information captured in superpixels. We present a discriminative appearance model based on superpixels, thereby facilitating a tracker to distinguish the target and the background with mid-level cues. The tracking task is then formulated by computing a target-background confidence map, and obtaining the best candidate by maximum a posterior estimate. Experimental results demonstrate that our tracker is able to handle heavy occlusion and recover from drifts. In conjunction with online update, the proposed algorithm is shown to perform favorably against existing methods for object tracking.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126385",
        "reference_list": [
            {
                "year": "2003",
                "id": 46
            },
            {
                "year": "2003",
                "id": 1
            }
        ],
        "citation": {
            "ieee": 65,
            "other": 58,
            "total": 123
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Computational modeling",
                "Training",
                "Visualization",
                "Feature extraction",
                "Adaptation models"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image representation",
                "maximum likelihood estimation",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "superpixel tracking",
                "object tracking",
                "shape deformation",
                "image representation",
                "appearance variation",
                "high-level appearance structure",
                "low-level cues",
                "target object representation",
                "target object matching",
                "discriminative appearance model",
                "target-background confidence map",
                "maximum a posterior estimation"
            ]
        },
        "id": 167,
        "cited_by": [
            {
                "year": "2015",
                "id": 341
            },
            {
                "year": "2013",
                "id": 48
            },
            {
                "year": "2013",
                "id": 138
            },
            {
                "year": "2013",
                "id": 279
            },
            {
                "year": "2013",
                "id": 309
            },
            {
                "year": "2013",
                "id": 347
            }
        ]
    },
    {
        "title": "Human action recognition by learning bases of action attributes and parts",
        "authors": [
            "Bangpeng Yao",
            "Xiaoye Jiang",
            "Aditya Khosla",
            "Andy Lai Lin",
            "Leonidas Guibas",
            "Li Fei-Fei"
        ],
        "abstract": "In this work, we propose to use attributes and parts for recognizing human actions in still images. We define action attributes as the verbs that describe the properties of human actions, while the parts of actions are objects and poselets that are closely related to the actions. We jointly model the attributes and parts by learning a set of sparse bases that are shown to carry much semantic meaning. Then, the attributes and parts of an action image can be reconstructed from sparse coefficients with respect to the learned bases. This dual sparsity provides theoretical guarantee of our bases learning and feature reconstruction approach. On the PASCAL action dataset and a new \u201cStanford 40 Actions\u201d dataset, we show that our method extracts meaningful high-order interactions between attributes and parts in human actions while achieving state-of-the-art classification performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126386",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2009",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 127,
            "other": 88,
            "total": 215
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Detectors",
                "Image reconstruction",
                "Vectors",
                "Feature extraction",
                "Noise",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image reconstruction",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human action recognition",
                "action attributes",
                "action parts",
                "still images",
                "sparse bases learning",
                "sparse coefficients",
                "action image reconstruction",
                "feature reconstruction approach",
                "PASCAL action dataset",
                "Stanford 40 actions dataset"
            ]
        },
        "id": 168,
        "cited_by": [
            {
                "year": "2017",
                "id": 47
            },
            {
                "year": "2017",
                "id": 61
            },
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2017",
                "id": 224
            },
            {
                "year": "2017",
                "id": 357
            },
            {
                "year": "2017",
                "id": 438
            },
            {
                "year": "2017",
                "id": 450
            },
            {
                "year": "2017",
                "id": 521
            },
            {
                "year": "2017",
                "id": 544
            },
            {
                "year": "2015",
                "id": 113
            },
            {
                "year": "2013",
                "id": 264
            },
            {
                "year": "2013",
                "id": 282
            },
            {
                "year": "2013",
                "id": 424
            }
        ]
    },
    {
        "title": "Single-shot high dynamic range imaging with conventional camera hardware",
        "authors": [
            "Keigo Hirakawa",
            "Paul M. Simon"
        ],
        "abstract": "A combination of photographic filter placed over the lens and the color filter array on image sensor induces differences in red, green, and blue channel sensitivities. Spectrally selective single-shot HDR (S4HDR) imaging treats this as an exposure bracketing. Optimally exposed regions of low dynamic range red/green/blue color components are merged in a principled manner to yield a single HDR color image. Though not expected to yield results superior to the traditional time multiplexing counterparts, the single-shot HDR solution we propose is a robust alternative that can be realized with conventional camera hardware.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126387",
        "reference_list": [
            {
                "year": "2003",
                "id": 153
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 3,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Sensors",
                "Cameras",
                "Image sensors",
                "Color",
                "Multiplexing"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image colour analysis",
                "image sensors",
                "optical filters",
                "photography"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single-shot high dynamic range imaging",
                "conventional camera hardware",
                "photographic filter",
                "lens",
                "color filter array",
                "image sensor",
                "spectrally selective single-shot HDR",
                "S4HDR imaging",
                "exposure bracketing",
                "low dynamic range",
                "single HDR color image"
            ]
        },
        "id": 169,
        "cited_by": []
    },
    {
        "title": "Unwrapping low-rank textures on generalized cylindrical surfaces",
        "authors": [
            "Zhengdong Zhang",
            "Xiao Liang",
            "Yi Ma"
        ],
        "abstract": "In this paper, we show how to reconstruct both 3D shape and 2D texture of a class of surfaces from a single perspective image. We consider the so-called the generalized cylindrical surfaces that are wrapped with low-rank textures. They can be used to model most curved building facades in urban areas or deformed book pages scanned for text recognition. Our method leverages on the recent new techniques for low-rank matrix recovery and sparse error correction and it generalizes existing techniques from planar surfaces to a much larger class of important 3D surfaces. As we will show with extensive simulations and experiments, the proposed algorithm can precisely rectify deformation of textures caused by both perspective projection and surface shape. It works for a wide range of symmetric or regular textures that are ubiquitous in images of urban environments, objects, or texts, and it is very robust to sparse occlusion, noise, and saturation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126388",
        "reference_list": [
            {
                "year": "2007",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 16,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface texture",
                "Cameras",
                "Three dimensional displays",
                "Shape",
                "Surface reconstruction",
                "Jacobian matrices",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "error correction",
                "image reconstruction",
                "image texture",
                "matrix algebra",
                "shape recognition",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "low-rank texture unwrapping",
                "generalized cylindrical surfaces",
                "3D shape reconstruction",
                "2D texture reconstruction",
                "curved building facades",
                "deformed book pages",
                "text recognition",
                "low-rank matrix recovery",
                "sparse error correction",
                "texture deformation rectification",
                "computer vision"
            ]
        },
        "id": 170,
        "cited_by": []
    },
    {
        "title": "Fast template matching in non-linear tone-mapped images",
        "authors": [
            "Yacov Hel-Or",
            "Hagit Hel-Or",
            "Eyal David"
        ],
        "abstract": "We propose a fast pattern matching scheme termed Matching by Tone Mapping (MTM) which allows matching under non-linear tone mappings. We show that, when tone mapping is approximated by a piecewise constant function, a fast computational scheme is possible requiring computational time similar to the fast implementation of Normalized Cross Correlation (NCC). In fact, the MTM measure can be viewed as a generalization of the NCC for non-linear mappings and actually reduces to NCC when mappings are restricted to be linear. The MTM is shown to be invariant to non-linear tone mappings, and is empirically shown to be highly discriminative and robust to noise.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126389",
        "reference_list": [
            {
                "year": "2003",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 5,
            "total": 20
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast template matching",
                "nonlinear tone-mapped image",
                "pattern matching scheme",
                "matching by tone mapping",
                "nonlinear tone mapping",
                "tone mapping approximation",
                "piecewise constant function",
                "nonlinear mapping"
            ]
        },
        "id": 171,
        "cited_by": []
    },
    {
        "title": "An adaptive coupled-layer visual model for robust visual tracking",
        "authors": [
            "Luka \u010cehovin",
            "Matej Kristan",
            "Ales\u0161 Leonardis"
        ],
        "abstract": "This paper addresses the problem of tracking objects which undergo rapid and significant appearance changes. We propose a novel coupled-layer visual model that combines the target's global and local appearance. The local layer in this model is a set of local patches that geometrically constrain the changes in the target's appearance. This layer probabilistically adapts to the target's geometric deformation, while its structure is updated by removing and adding the local patches. The addition of the patches is constrained by the global layer that probabilistically models target's global visual properties such as color, shape and apparent local motion. The global visual properties are updated during tracking using the stable patches from the local layer. By this coupled constraint paradigm between the adaptation of the global and the local layer, we achieve a more robust tracking through significant appearance changes. Indeed, the experimental results on challenging sequences confirm that our tracker outperforms the related state-of-the-art trackers by having smaller failure rate as well as better accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126390",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 26,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Target tracking",
                "Adaptation models",
                "Computational modeling",
                "Shape",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image motion analysis",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive coupled-layer visual model",
                "robust visual tracking",
                "object tracking",
                "geometric deformation",
                "global visual properties",
                "stable patches"
            ]
        },
        "id": 172,
        "cited_by": [
            {
                "year": "2013",
                "id": 254
            },
            {
                "year": "2013",
                "id": 364
            }
        ]
    },
    {
        "title": "Extracting foreground masks towards object recognition",
        "authors": [
            "Amir Rosenfeld",
            "Daphna Weinshall"
        ],
        "abstract": "Effective segmentation prior to recognition has been shown to improve recognition performance. However, most segmentation algorithms adopt methods which are not explicitly linked to the goal of object recognition. Here we solve a related but slightly different problem in order to assist object recognition more directly - the extraction of a foreground mask, which identifies the locations of objects in the image. We propose a novel foreground/background segmentation algorithm that attempts to segment the interesting objects from the rest of the image, while maximizing an objective function which is tightly related to object recognition. We do this in a manner which requires no class-specific knowledge of object categories, using a probabilistic formulation which is derived from manually segmented images. The model includes a geometric prior and an appearance prior, whose parameters are learnt on the fly from images that are similar to the query image. We use graph-cut based energy minimization to enforce spatial coherence on the model's output. The method is tested on the challenging VOC09 and VOC10 segmentation datasets, achieving excellent results in providing a foreground mask. We also provide comparisons to the recent segmentation method of [7].",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126391",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 12,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Training",
                "Visualization",
                "Object recognition",
                "Approximation methods",
                "Kernel",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "graph theory",
                "image segmentation",
                "minimisation",
                "object recognition",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "foreground mask extraction",
                "object recognition",
                "foreground segmentation",
                "background segmentation",
                "probabilistic formulation",
                "geometric prior",
                "appearance prior",
                "graph-cut based energy minimization",
                "VOC09 segmentation dataset",
                "VOC10 segmentation dataset"
            ]
        },
        "id": 173,
        "cited_by": [
            {
                "year": "2013",
                "id": 106
            },
            {
                "year": "2013",
                "id": 415
            }
        ]
    },
    {
        "title": "Efficient parallel message computation for MAP inference",
        "authors": [
            "Stavros Alchatzidis",
            "Aristeidis Sotiras",
            "Nikos Paragios"
        ],
        "abstract": "First order Markov Random Fields (MRFs) have become a predominant tool in Computer Vision over the past decade. Such a success was mostly due to the development of efficient optimization algorithms both in terms of speed as well as in terms of optimality properties. Message passing algorithms are among the most popular methods due to their good performance for a wide range of pairwise potential functions (PPFs). Their main bottleneck is computational complexity. In this paper, we revisit message computation as a distance transformation using a more formal setting than [8] to generalize it to arbitrary PPFs. The method is based on [20] yielding accurate results for a specific class of PPFs and in most other cases a close approximation. The proposed algorithm is parallel and thus enables us to fully take advantage of the computational power of parallel processing architectures. The proposed scheme coupled with an efficient belief propagation algorithm [8] and implemented on a massively parallel coprocessor provides results as accurate as state of the art inference methods, though is in general one order of magnitude faster in terms of speed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126392",
        "reference_list": [
            {
                "year": "2007",
                "id": 51
            },
            {
                "year": "2003",
                "id": 118
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Labeling",
                "Belief propagation",
                "Approximation algorithms",
                "Optimization",
                "Approximation methods",
                "Graphics processing unit"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "coprocessors",
                "inference mechanisms",
                "Markov processes",
                "message passing",
                "parallel processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parallel message computation",
                "MAP inference",
                "Markov random fields",
                "computer vision",
                "optimization algorithms",
                "message passing algorithms",
                "pairwise potential functions",
                "computational complexity",
                "parallel processing architectures",
                "belief propagation algorithm",
                "parallel coprocessor"
            ]
        },
        "id": 174,
        "cited_by": []
    },
    {
        "title": "Superpixels via pseudo-Boolean optimization",
        "authors": [
            "Yuhang Zhang",
            "Richard Hartley",
            "John Mashford",
            "Stewart Burn"
        ],
        "abstract": "We propose an algorithm for creating superpixels. The major step in our algorithm is simply minimizing two pseudo-Boolean functions. The processing time of our algorithm on images of moderate size is only half a second. Experiments on a benchmark dataset show that our method produces superpixels of comparable quality with existing algorithms. Last but not least, the speed of our algorithm is independent of the number of superpixels, which is usually the bottle-neck for the traditional algorithms of superpixel creation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126393",
        "reference_list": [
            {
                "year": "2009",
                "id": 263
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 10,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Strips",
                "Image color analysis",
                "Smoothing methods",
                "Image segmentation",
                "Image edge detection",
                "Shape",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pseudo-Boolean optimization",
                "pseudo-Boolean function minimisation",
                "superpixel creation",
                "superpixel segmentation"
            ]
        },
        "id": 175,
        "cited_by": []
    },
    {
        "title": "Annotator rationales for visual recognition",
        "authors": [
            "Jeff Donahue",
            "Kristen Grauman"
        ],
        "abstract": "Traditional supervised visual learning simply asks annotators \u201cwhat\u201d label an image should have. We propose an approach for image classification problems requiring subjective judgment that also asks \u201cwhy\u201d, and uses that information to enrich the learned model. We develop two forms of visual annotator rationales: in the first, the annotator highlights the spatial region of interest he found most influential to the label selected, and in the second, he comments on the visual attributes that were most important. For either case, we show how to map the response to synthetic contrast examples, and then exploit an existing large-margin learning technique to refine the decision boundary accordingly. Results on multiple scene categorization and human attractiveness tasks show the promise of our approach, which can more accurately learn complex categories with the explanations behind the label choices.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126394",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2003",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 14,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Humans",
                "Training",
                "Support vector machines",
                "Face",
                "Natural language processing",
                "Vocabulary"
            ],
            "INSPEC: Controlled Indexing": [
                "category theory",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "annotator rationales",
                "visual recognition",
                "supervised visual learning",
                "image label",
                "image classification",
                "decision boundary",
                "scene categorization"
            ]
        },
        "id": 176,
        "cited_by": [
            {
                "year": "2015",
                "id": 118
            },
            {
                "year": "2013",
                "id": 102
            }
        ]
    },
    {
        "title": "Actively selecting annotations among objects and attributes",
        "authors": [
            "Adriana Kovashka",
            "Sudheendra Vijayanarasimhan",
            "Kristen Grauman"
        ],
        "abstract": "We present an active learning approach to choose image annotation requests among both object category labels and the objects' attribute labels. The goal is to solicit those labels that will best use human effort when training a multi-class object recognition model. In contrast to previous work in active visual category learning, our approach directly exploits the dependencies between human-nameable visual attributes and the objects they describe, shifting its requests in either label space accordingly. We adopt a discriminative latent model that captures object-attribute and attribute-attribute relationships, and then define a suitable entropy reduction selection criterion to predict the influence a new label might have throughout those connections. On three challenging datasets, we demonstrate that the method can more successfully accelerate object learning relative to both passive learning and traditional active learning approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126395",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2009",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 15,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Entropy",
                "Humans",
                "Labeling",
                "Computational modeling",
                "Object recognition",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "entropy",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image annotation request",
                "object category label",
                "multiclass object recognition model",
                "active visual category learning",
                "human-nameable visual attributes",
                "discriminative latent model",
                "object attribute",
                "entropy reduction selection criterion",
                "object learning",
                "passive learning",
                "active learning"
            ]
        },
        "id": 177,
        "cited_by": [
            {
                "year": "2015",
                "id": 304
            },
            {
                "year": "2015",
                "id": 316
            },
            {
                "year": "2013",
                "id": 26
            },
            {
                "year": "2013",
                "id": 37
            },
            {
                "year": "2013",
                "id": 150
            },
            {
                "year": "2013",
                "id": 228
            },
            {
                "year": "2013",
                "id": 264
            }
        ]
    },
    {
        "title": "Decoupling photometry and geometry in dense variational camera calibration",
        "authors": [
            "Mathieu Aubry",
            "Kalin Kolev",
            "Bastian Goldluecke",
            "Daniel Cremers"
        ],
        "abstract": "We introduce a spatially dense variational approach to estimate the calibration of multiple cameras in the context of 3D reconstruction. We propose a relaxation scheme which allows to transform the original photometric error into a geometric one, thereby decoupling the problems of dense matching and camera calibration. In both quantitative and qualitative experiments, we demonstrate that the proposed decoupling scheme allows for robust and accurate estimation of camera parameters. In particular, the presented dense camera calibration formulation leads to substantial improvements both in the reconstructed 3D geometry and in the super-resolution texture estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126396",
        "reference_list": [
            {
                "year": "2009",
                "id": 215
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Three dimensional displays",
                "Estimation",
                "Geometry",
                "Computational modeling",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "geometry",
                "image matching",
                "image reconstruction",
                "image resolution",
                "image texture",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "decoupling photometry",
                "dense variational camera calibration",
                "3D reconstruction",
                "photometric error",
                "dense matching",
                "3D geometry",
                "super-resolution texture estimation"
            ]
        },
        "id": 178,
        "cited_by": []
    },
    {
        "title": "Action recognition in videos acquired by a moving camera using motion decomposition of Lagrangian particle trajectories",
        "authors": [
            "Shandong Wu",
            "Omar Oreifej",
            "Mubarak Shah"
        ],
        "abstract": "Recognition of human actions in a video acquired by a moving camera typically requires standard preprocessing steps such as motion compensation, moving object detection and object tracking. The errors from the motion compensation step propagate to the object detection stage, resulting in miss-detections, which further complicates the tracking stage, resulting in cluttered and incorrect tracks. Therefore, action recognition from a moving camera is considered very challenging. In this paper, we propose a novel approach which does not follow the standard steps, and accordingly avoids the aforementioned difficulties. Our approach is based on Lagrangian particle trajectories which are a set of dense trajectories obtained by advecting optical flow over time, thus capturing the ensemble motions of a scene. This is done in frames of unaligned video, and no object detection is required. In order to handle the moving camera, we propose a novel approach based on low rank optimization, where we decompose the trajectories into their camera-induced and object-induced components. Having obtained the relevant object motion trajectories, we compute a compact set of chaotic invariant features which captures the characteristics of the trajectories. Consequently, a SVM is employed to learn and recognize the human actions using the computed motion features. We performed intensive experiments on multiple benchmark datasets and two new aerial datasets called ARG and APHill, and obtained promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126397",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2007",
                "id": 206
            },
            {
                "year": "2009",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 53,
            "other": 36,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "chaos",
                "image recognition",
                "image sequences",
                "motion compensation",
                "object detection",
                "support vector machines",
                "tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video",
                "motion decomposition",
                "Lagrangian particle trajectory",
                "human action recognition",
                "moving camera",
                "motion compensation",
                "moving object detection",
                "object tracking",
                "tracking stage",
                "optical flow",
                "ensemble motion capturing",
                "low rank optimization",
                "trajectory decomposition",
                "camera-induced component",
                "object-induced component",
                "chaotic invariant features",
                "SVM",
                "aerial datasets",
                "ARG",
                "APHill"
            ]
        },
        "id": 179,
        "cited_by": [
            {
                "year": "2013",
                "id": 443
            }
        ]
    },
    {
        "title": "The truth about cats and dogs",
        "authors": [
            "Omkar M Parkhi",
            "Andrea Vedaldi",
            "C. V. Jawahar",
            "Andrew Zisserman"
        ],
        "abstract": "Template-based object detectors such as the deformable parts model of Felzenszwalb et al. [11] achieve state-of-the-art performance for a variety of object categories, but are still outperformed by simpler bag-of-words models for highly flexible objects such as cats and dogs. In these cases we propose to use the template-based model to detect a distinctive part for the class, followed by detecting the rest of the object via segmentation on image specific information learnt from that part. This approach is motivated by two observations: (i) many object classes contain distinctive parts that can be detected very reliably by template-based detectors, whilst the entire object cannot; (ii) many classes (e.g. animals) have fairly homogeneous coloring and texture that can be used to segment the object once a sample is provided in an image. We show quantitatively that our method substantially outperforms whole-body template-based detectors for these highly deformable object categories, and indeed achieves accuracy comparable to the state-of-the-art on the PASCAL VOC competition, which includes other models such as bag-of-words.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126398",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 20,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Image color analysis",
                "Head",
                "Cats",
                "Image edge detection",
                "Image segmentation",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image segmentation",
                "image texture",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "template-based object detector",
                "deformable parts model",
                "bag-of-words model",
                "highly flexible object",
                "image segmentation",
                "coloring",
                "texture"
            ]
        },
        "id": 180,
        "cited_by": [
            {
                "year": "2017",
                "id": 547
            },
            {
                "year": "2015",
                "id": 175
            },
            {
                "year": "2015",
                "id": 278
            },
            {
                "year": "2015",
                "id": 278
            },
            {
                "year": "2013",
                "id": 38
            },
            {
                "year": "2013",
                "id": 90
            },
            {
                "year": "2013",
                "id": 370
            }
        ]
    },
    {
        "title": "Multi-class semi-supervised SVMs with Positiveness Exclusive Regularization",
        "authors": [
            "Xiaobai Liu",
            "Xiaotong Yuan",
            "Shuicheng Yan",
            "Hai Jin"
        ],
        "abstract": "In this work, we address the problem of multi-class classification problem in semi-supervised setting. A regularized multi-task learning approach is presented to train multiple binary-class Semi-Supervised Support Vector Machines (S3VMs) using the one-vs-rest strategy within a joint framework. A novel type of regularization, namely Positiveness Exclusive Regularization (PER), is introduced to induce the following prior: if an unlabeled sample receives significant positive response from one of the classifiers, it is less likely for this sample to receive positive responses from the other classifiers. That is, we expect an exclusive relationship among different S3VMs for evaluating the same unlabeled sample. We propose to use an \u2113 1,2 -norm regularizer as an implementation of PER. The objective of our approach is to minimize an empirical risk regularized by a PER term and a manifold regularization term. An efficient Nesterov-type smoothing approximation based method is developed for optimization. Evaluations with comparisons are conducted on several benchmarks for visual classification to demonstrate the advantages of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126399",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Approximation methods",
                "Training",
                "Optimization",
                "Smoothing methods",
                "Joints",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "classification",
                "multiprogramming",
                "optimisation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiclass semisupervised SVM",
                "positiveness exclusive regularization",
                "multiclass classification",
                "regularized multitask learning",
                "multiple binary-class semi-supervised support vector machines",
                "S3VM",
                "manifold regularization term",
                "Nesterov-type smoothing approximation",
                "optimization",
                "visual classification"
            ]
        },
        "id": 181,
        "cited_by": [
            {
                "year": "2013",
                "id": 258
            }
        ]
    },
    {
        "title": "Discriminative high order SVD: Adaptive tensor subspace selection for image classification, clustering, and retrieval",
        "authors": [
            "Dijun Luo",
            "Heng Huang",
            "Chris Ding"
        ],
        "abstract": "Tensor based dimensionality reduction has recently attracted attention from computer vision and pattern recognition communities for both feature extraction and data compression. As an unsupervised method, High-Order Singular Value Decomposition (HOSVD) searches for low-rank subspaces such that the low-rank approximation error is minimized. In this paper, we propose a new unsupervised high-order tensor decomposition approach which employs the strength of discriminative analysis and K-means clustering to adaptively select subspaces that improve the clustering, classification, and retrieval capabilities of HOSVD. We provide both theoretical analysis to guarantee that our new method generates more discriminative subspaces and empirical studies on several public computer vision data sets to show the consistent improvement of our method over existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126400",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Clustering algorithms",
                "Accuracy",
                "Principal component analysis",
                "Computer vision",
                "Measurement",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "data compression",
                "feature extraction",
                "image classification",
                "image retrieval",
                "pattern clustering",
                "singular value decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative high order SVD",
                "adaptive tensor subspace selection",
                "image classification",
                "image clustering",
                "image retrieval",
                "tensor based dimensionality reduction",
                "computer vision",
                "pattern recognition",
                "feature extraction",
                "data compression",
                "high-order singular value decomposition",
                "low-rank subspaces",
                "low-rank approximation error",
                "unsupervised high-order tensor decomposition approach",
                "K-means clustering"
            ]
        },
        "id": 182,
        "cited_by": []
    },
    {
        "title": "Recognising spontaneous facial micro-expressions",
        "authors": [
            "Tomas Pfister",
            "Xiaobai Li",
            "Guoying Zhao",
            "Matti Pietik\u00e4inen"
        ],
        "abstract": "Facial micro-expressions are rapid involuntary facial expressions which reveal suppressed affect. To the best knowledge of the authors, there is no previous work that successfully recognises spontaneous facial micro-expressions. In this paper we show how a temporal interpolation model together with the first comprehensive spontaneous micro-expression corpus enable us to accurately recognise these very short expressions. We designed an induced emotion suppression experiment to collect the new corpus using a high-speed camera. The system is the first to recognise spontaneous facial micro-expressions and achieves very promising results that compare favourably with the human micro-expression detection accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126401",
        "reference_list": [
            {
                "year": "2005",
                "id": 158
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 68,
            "other": 44,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Face recognition",
                "Kernel",
                "Feature extraction",
                "Interpolation",
                "Training",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "emotion recognition",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spontaneous facial microexpression recognition",
                "temporal interpolation model",
                "spontaneous microexpression corpus",
                "emotion suppression experiment",
                "human microexpression detection"
            ]
        },
        "id": 183,
        "cited_by": [
            {
                "year": "2015",
                "id": 404
            }
        ]
    },
    {
        "title": "End-to-end scene text recognition",
        "authors": [
            "Kai Wang",
            "Boris Babenko",
            "Serge Belongie"
        ],
        "abstract": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126402",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            }
        ],
        "citation": {
            "ieee": 76,
            "other": 55,
            "total": 131
        },
        "keywords": {
            "IEEE Keywords": [
                "Pipelines",
                "Training",
                "Text recognition",
                "Optical character recognition software",
                "Image recognition",
                "Object recognition",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "object recognition",
                "optical character recognition",
                "text detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "end-to-end scene text recognition",
                "word detection",
                "natural images",
                "computer vision community",
                "text detection",
                "image word recognition",
                "OCR engine",
                "object recognition",
                "domain-specific methods"
            ]
        },
        "id": 184,
        "cited_by": [
            {
                "year": "2017",
                "id": 519
            },
            {
                "year": "2017",
                "id": 533
            },
            {
                "year": "2017",
                "id": 550
            },
            {
                "year": "2015",
                "id": 134
            },
            {
                "year": "2015",
                "id": 138
            },
            {
                "year": "2015",
                "id": 519
            },
            {
                "year": "2013",
                "id": 12
            },
            {
                "year": "2013",
                "id": 70
            },
            {
                "year": "2013",
                "id": 97
            },
            {
                "year": "2013",
                "id": 379
            }
        ]
    },
    {
        "title": "Spatial pyramid co-occurrence for image classification",
        "authors": [
            "Yi Yang",
            "Shawn Newsam"
        ],
        "abstract": "We describe a novel image representation termed spatial pyramid co-occurrence which characterizes both the photometric and geometric aspects of an image. Specifically, the co-occurrences of visual words are computed with respect to spatial predicates over a hierarchical spatial partitioning of an image. The representation captures both the absolute and relative spatial arrangement of the words and, through the choice and combination of the predicates, can characterize a variety of spatial relationships. Our representation is motivated by the analysis of overhead imagery such as from satellites or aircraft. This imagery generally does not have an absolute reference frame and thus the relative spatial arrangement of the image elements often becomes the key discriminating feature. We validate this hypothesis using a challenging ground truth image dataset of 21 land-use classes manually extracted from high-resolution aerial imagery. Our approach is shown to result in higher classification rates than a non-spatial bagof- visual-words approach as well as a popular approach for characterizing the absolute spatial arrangement of visual words, the spatial pyramid representation of Lazebnik et al. [7]. While our primary objective is analyzing overhead imagery, we demonstrate that our approach achieves state-of-the-art performance on the Graz-01 object class dataset and performs competitively on the 15 Scene dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126403",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2007",
                "id": 20
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 105,
            "other": 41,
            "total": 146
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Kernel",
                "Histograms",
                "Dictionaries",
                "Spatial resolution",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image representation",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatial pyramid co-occurrence",
                "image classification",
                "image representation",
                "photometric aspects",
                "geometric aspects",
                "visual words",
                "hierarchical spatial partitioning",
                "word spatial arrangement",
                "spatial relationships",
                "truth image dataset",
                "land-use classes",
                "high-resolution aerial imagery",
                "nonspatial bag-of- visual-words approach",
                "spatial pyramid representation",
                "Graz-01 object class dataset"
            ]
        },
        "id": 185,
        "cited_by": [
            {
                "year": "2015",
                "id": 5
            },
            {
                "year": "2015",
                "id": 209
            }
        ]
    },
    {
        "title": "Learning parameterized histogram kernels on the simplex manifold for image and action classification",
        "authors": [
            "Vitaly Ablavsky",
            "Stan Sclaroff"
        ],
        "abstract": "State-of-the-art image and action classification systems often employ vocabulary-based representations. The classification accuracy achieved with such vocabulary-based representations depends significantly on the chosen histogram-distance. In particular, when the decision function is a support-vector-machine (SVM), the classification accuracy depends on the chosen histogram kernel. In this paper we focus on smoothly-parameterized kernels in the space of histograms, such as, but not limited to, kernels that are derived from smoothly-parameterized histogram-distance functions. We learn parameters of histogram kernels so that the SVM accuracy is improved. This is accomplished by simultaneously maximizing the SVM's geometric margin and minimizing an estimate of its generalization error. We validate our approach on a previously-published two-class synthetic dataset and three real-world multi-class datasets: Oxford5K, KTH, and UCF. On these datasets our approach yields results that compare favorably to or exceed the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126404",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Support vector machines",
                "Histograms",
                "Measurement",
                "Accuracy",
                "Training",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parameterized histogram kernel learning",
                "simplex manifold",
                "image classification",
                "action classification",
                "vocabulary-based representations",
                "UCF",
                "KTH",
                "Oxford5K",
                "generalization error estimate minimization",
                "SVM geometric margin maximization",
                "smoothly-parameterized histogram-distance functions",
                "support-vector-machine",
                "histogram-distance"
            ]
        },
        "id": 186,
        "cited_by": [
            {
                "year": "2013",
                "id": 332
            }
        ]
    },
    {
        "title": "Exemplar extraction using spatio-temporal hierarchical agglomerative clustering for face recognition in video",
        "authors": [
            "John See",
            "Chikkannan Eswaran"
        ],
        "abstract": "Many recent works have attempted to improve object recognition by exploiting temporal dynamics, an intrinsic property of video sequences. In this paper, a new spatio-temporal hierarchical agglomerative clustering (STHAC) method is proposed for automatic extraction of face exemplars for face recognition in video sequences. Two variants of STHAC are presented - a global variety that unifies spatial and temporal distances between points, and a local variety that introduces perturbation of distances based on a local spatio-temporal neighborhood criterion. Faces that are nearest to the cluster means are chosen as exemplars for the testing stage, where subjects in the test video sequences are recognized using a probabilistic-based classifier. Extensive evaluation on a face video database demonstrates the effectiveness of our proposed method, and the significance of incorporating temporal information for exemplar extraction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126405",
        "reference_list": [
            {
                "year": "2005",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Face recognition",
                "Training",
                "Video sequences",
                "Manifolds",
                "Feature extraction",
                "Probabilistic logic"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image sequences",
                "pattern clustering",
                "probability",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic face exemplar extraction",
                "spatio-temporal hierarchical agglomerative clustering",
                "face recognition",
                "object recognition",
                "temporal dynamics",
                "test video sequences",
                "STHAC",
                "local spatio-temporal neighborhood criterion",
                "cluster means",
                "probabilistic-based classifier",
                "face video database"
            ]
        },
        "id": 187,
        "cited_by": []
    },
    {
        "title": "Modeling spatial layout with fisher vectors for image categorization",
        "authors": [
            "Josip Krapac",
            "Jakob Verbeek",
            "Fr\u00e9d\u00e9ric Jurie"
        ],
        "abstract": "We introduce an extension of bag-of-words image representations to encode spatial layout. Using the Fisher kernel framework we derive a representation that encodes the spatial mean and the variance of image regions associated with visual words. We extend this representation by using a Gaussian mixture model to encode spatial layout, and show that this model is related to a soft-assign version of the spatial pyramid representation. We also combine our representation of spatial layout with the use of Fisher kernels to encode the appearance of local features. Through an extensive experimental evaluation, we show that our representation yields state-of-the-art image categorization results, while being more compact than spatial pyramid representations. In particular, using Fisher kernels to encode both appearance and spatial layout results in an image representation that is computationally efficient, compact, and yields excellent performance while using linear classifiers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126406",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2007",
                "id": 67
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2009",
                "id": 253
            }
        ],
        "citation": {
            "ieee": 44,
            "other": 53,
            "total": 97
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Vectors",
                "Computational modeling",
                "Layout",
                "Vocabulary",
                "Kernel",
                "Image representation"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image classification",
                "image coding",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatial layout modeling",
                "Fisher vectors",
                "image categorization",
                "bag-of-words image representation",
                "spatial layout encoding",
                "Fisher kernel framework",
                "Gaussian mixture model",
                "soft-assign version",
                "spatial pyramid representation",
                "spatial layout representation",
                "Fisher kernels",
                "linear classifier"
            ]
        },
        "id": 188,
        "cited_by": [
            {
                "year": "2015",
                "id": 115
            },
            {
                "year": "2013",
                "id": 226
            }
        ]
    },
    {
        "title": "Shape-constrained Gaussian process regression for facial-point-based head-pose normalization",
        "authors": [
            "Ognjen Rudovic",
            "Maja Pantic"
        ],
        "abstract": "Given the facial points extracted from an image of a face in an arbitrary pose, the goal of facial-point-based head-pose normalization is to obtain the corresponding facial points in a predefined pose (e.g., frontal). This involves inference of complex and high-dimensional mappings due to the large number of the facial points employed, and due to differences in head-pose and facial expression. Most regression-based approaches for learning such mappings focus on modeling correlations only between the inputs (i.e., the facial points in a non-frontal pose) and the outputs (i.e., the facial points in the frontal pose), but not within the inputs and the outputs of the model. This makes these models prone to errors due to noise and outliers in test data, often resulting in anatomically impossible facial configurations formed by their predictions. To address this, we propose Shape-constrained Gaussian Process (SC-GP) regression for facial-point-based head-pose normalization. Specifically, a deformable face-shape model is used to learn a face-shape prior, which is placed on both the input and the output of GP regression in order to constrain the model predictions to anatomically feasible facial configurations. Our extensive experiments on both synthetic and real image data show that the proposed approach generalizes well across poses and handles successfully noise and outliers in test data. In addition, the proposed model outperforms previously proposed approaches to facial-point-based head-pose normalization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126407",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 2,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Deformable models",
                "Computational modeling",
                "Training",
                "Data models",
                "Three dimensional displays",
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "Gaussian processes",
                "pose estimation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape-constrained Gaussian process regression",
                "facial-point-based head-pose normalization",
                "facial point extraction",
                "deformable face-shape model",
                "arbitrary pose",
                "high-dimensional mappings"
            ]
        },
        "id": 189,
        "cited_by": []
    },
    {
        "title": "Spectral learning of latent semantics for action recognition",
        "authors": [
            "Zhiwu Lu",
            "Yuxin Peng",
            "Horace H.S. Ip"
        ],
        "abstract": "This paper proposes novel spectral methods for learning latent semantics (i.e. high-level features) from a large vocabulary of abundant mid-level features (i.e. visual keywords), which can help to bridge the semantic gap in the challenging task of action recognition. To discover the manifold structure hidden among mid-level features, we develop spectral embedding approaches based on graphs and hypergraphs, without the need to tune any parameter for graph construction which is a key step of manifold learning. In particular, the traditional graphs are constructed by linear reconstruction with sparse coding. In the new embedding space, we learn high-level latent semantics automatically from abundant mid-level features through spectral clustering. The learnt latent semantics can be readily used for action recognition with SVM by defining a histogram intersection kernel. Different from the traditional latent semantic analysis based on topic models, our two spectral methods for semantic learning can discover the manifold structure hidden among mid-level features, which results in compact but discriminative high-level features. The experimental results on two standard action datasets have shown the superior performance of our spectral methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126408",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Feature extraction",
                "Manifolds",
                "Encoding",
                "Vocabulary",
                "Support vector machines",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image motion analysis",
                "learning (artificial intelligence)",
                "object recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spectral learning",
                "latent semantics",
                "action recognition",
                "high-level feature",
                "visual keyword",
                "spectral embedding approach",
                "hypergraph",
                "manifold learning",
                "sparse coding",
                "spectral clustering",
                "SVM",
                "histogram intersection kernel",
                "semantic learning"
            ]
        },
        "id": 190,
        "cited_by": []
    },
    {
        "title": "Scale and object aware image retargeting for thumbnail browsing",
        "authors": [
            "Jin Sun",
            "Haibin Ling"
        ],
        "abstract": "Many image retargeting algorithms, despite aesthetically carving images smaller, pay limited attention to image browsing tasks where tiny thumbnails are presented. When applying traditional retargeting methods for generating thumbnails, several important issues frequently arise, including thumbnail scales, object completeness and local structure smoothness. To address these issues, we propose a novel image retargeting algorithm, Scale and Object Aware Retargeting (SOAR), which has four components: (1) a scale dependent saliency map to integrate size information of thumbnails, (2) objectness (Alexe et al. 2010) for preserving object completeness, (3) a cyclic seam carving algorithm to guide continuous retarget warping, and (4) a thin-plate-spline (TPS) retarget warping algorithm that champions local structure smoothness. The effectiveness of the proposed algorithm is evaluated both quantitatively and qualitatively. The quantitative evaluation is conducted through an image browsing user study to measure the effectiveness of different thumbnail generating algorithms, followed by the ANOVA analysis. The qualitative study is performed on the RetargetMe benchmark dataset. In both studies, SOAR generates very promising performance, in comparison with state-of-the-art retargeting algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126409",
        "reference_list": [
            {
                "year": "2009",
                "id": 19
            },
            {
                "year": "2007",
                "id": 170
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 11,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Visualization",
                "Algorithm design and analysis",
                "Pollution measurement",
                "Accuracy",
                "Image segmentation",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale and object aware image retargeting",
                "thumbnail browsing",
                "image browsing tasks",
                "thumbnail scales",
                "object completeness preservation",
                "local structure smoothness",
                "scale dependent saliency map",
                "objectness",
                "cyclic seam carving algorithm",
                "thin-plate-spline retarget warping algorithm",
                "ANOVA analysis",
                "RetargetMe benchmark dataset"
            ]
        },
        "id": 191,
        "cited_by": [
            {
                "year": "2017",
                "id": 21
            },
            {
                "year": "2017",
                "id": 22
            },
            {
                "year": "2013",
                "id": 246
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2013",
                "id": 415
            }
        ]
    },
    {
        "title": "Learning component-level sparse representation using histogram information for image classification",
        "authors": [
            "Chen-Kuo Chiang",
            "Chih-Hsueh Duan",
            "Shang-Hong Lai",
            "Shih-Fu Chang"
        ],
        "abstract": "A novel component-level dictionary learning framework which exploits image group characteristics within sparse coding is introduced in this work. Unlike previous methods, which select the dictionaries that best reconstruct the data, we present an energy minimization formulation that jointly optimizes the learning of both sparse dictionary and component level importance within one unified framework to give a discriminative representation for image groups. The importance measures how well each feature component represents the image group property with the dictionary by using histogram information. Then, dictionaries are updated iteratively to reduce the influence of unimportant components, thus refining the sparse representation for each image group. In the end, by keeping the top K important components, a compact representation is derived for the sparse coding dictionary. Experimental results on several public datasets are shown to demonstrate the superior performance of the proposed algorithm compared to the-state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126410",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Image reconstruction",
                "Training",
                "Encoding",
                "Vectors",
                "Histograms",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image coding",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "component-level sparse representation learning",
                "histogram information",
                "image classification",
                "component-level dictionary learning",
                "energy minimization formulation",
                "sparse coding dictionary"
            ]
        },
        "id": 192,
        "cited_by": [
            {
                "year": "2013",
                "id": 151
            }
        ]
    },
    {
        "title": "Locally rigid globally non-rigid surface registration",
        "authors": [
            "Kent Fujiwara",
            "Ko Nishino",
            "Jun Takamatsu",
            "Bo Zheng",
            "Katsushi Ikeuchi"
        ],
        "abstract": "We present a novel non-rigid surface registration method that achieves high accuracy and matches characteristic features without manual intervention. The key insight is to consider the entire shape as a collection of local structures that individually undergo rigid transformations to collectively deform the global structure. We realize this locally rigid but globally non-rigid surface registration with a newly derived dual-grid Free-form Deformation (FFD) framework. We first represent the source and target shapes with their signed distance fields (SDF). We then superimpose a sampling grid onto a conventional FFD grid that is dual to the control points. Each control point is then iteratively translated by a rigid transformation that minimizes the difference between two SDFs within the corresponding sampling region. The translated control points then interpolate the embedding space within the FFD grid and determine the overall deformation. The experimental results clearly demonstrate that our method is capable of overcoming the difficulty of preserving and matching local features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126411",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Aerospace electronics",
                "Accuracy",
                "Vectors",
                "Registers",
                "Three dimensional displays",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonrigid surface registration method",
                "characteristic features matching",
                "local structures collection",
                "global structure deformation",
                "dual-grid free-form deformation framework",
                "signed distance fields",
                "FFD grid"
            ]
        },
        "id": 193,
        "cited_by": []
    },
    {
        "title": "Who Blocks Who: Simultaneous clothing segmentation for grouping images",
        "authors": [
            "Nan Wang",
            "Haizhou Ai"
        ],
        "abstract": "Clothing is one of the most informative cues of human appearance. In this paper, we propose a novel multi-person clothing segmentation algorithm for highly occluded images. The key idea is combining blocking models to address the person-wise occlusions. In contrary to the traditional layered model that tries to solve the full layer ranking problem, the proposed blocking model partitions the problem into a series of pair-wise ones and then determines the local blocking relationship based on individual and contextual information. Thus, it is capable of dealing with cases with a large number of people. Additionally, we propose a layout model formulated as Markov Network which incorporates the blocking relationship to pursue an approximately optimal clothing layout for group people. Experiments demonstrated on a group images dataset show the effectiveness of our algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126412",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 12,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Clothing",
                "Shape",
                "Layout",
                "Face",
                "Image segmentation",
                "Decision trees",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "clothing",
                "computer graphics",
                "image segmentation",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "simultaneous clothing segmentation",
                "multiperson clothing segmentation algorithm",
                "occluded images",
                "person-wise occlusion",
                "blocking model partition",
                "contextual information",
                "layout model",
                "Markov network",
                "blocking relationship",
                "optimal clothing layout",
                "group people",
                "group image dataset"
            ]
        },
        "id": 194,
        "cited_by": [
            {
                "year": "2015",
                "id": 154
            },
            {
                "year": "2015",
                "id": 373
            },
            {
                "year": "2013",
                "id": 439
            }
        ]
    },
    {
        "title": "Describing people: A poselet-based approach to attribute classification",
        "authors": [
            "Lubomir Bourdev",
            "Subhransu Maji",
            "Jitendra Malik"
        ],
        "abstract": "We propose a method for recognizing attributes, such as the gender, hair style and types of clothes of people under large variation in viewpoint, pose, articulation and occlusion typical of personal photo album images. Robust attribute classifiers under such conditions must be invariant to pose, but inferring the pose in itself is a challenging problem. We use a part-based approach based on poselets. Our parts implicitly decompose the aspect (the pose and viewpoint). We train attribute classifiers for each such aspect and we combine them together in a discriminative model. We propose a new dataset of 8000 people with annotated attributes. Our method performs very well on this dataset, significantly outperforming a baseline built on the spatial pyramid match kernel method. On gender recognition we outperform a commercial face recognition system.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126413",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2009",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 113,
            "other": 62,
            "total": 175
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Training",
                "Skin",
                "Hair",
                "Feature extraction",
                "Vectors",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image recognition",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "poselet-based approach",
                "attribute classification",
                "personal photo album image",
                "discriminative model",
                "annotated attribute",
                "spatial pyramid match kernel method",
                "gender recognition",
                "face recognition",
                "attribute recognition",
                "hair style recognition",
                "clothing type recognition"
            ]
        },
        "id": 195,
        "cited_by": [
            {
                "year": "2017",
                "id": 182
            },
            {
                "year": "2015",
                "id": 110
            },
            {
                "year": "2015",
                "id": 118
            },
            {
                "year": "2015",
                "id": 161
            },
            {
                "year": "2015",
                "id": 220
            },
            {
                "year": "2015",
                "id": 264
            },
            {
                "year": "2015",
                "id": 416
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2013",
                "id": 264
            },
            {
                "year": "2013",
                "id": 439
            },
            {
                "year": "2013",
                "id": 453
            }
        ]
    },
    {
        "title": "Learning occlusion with likelihoods for visual tracking",
        "authors": [
            "Suha Kwak",
            "Woonhyun Nam",
            "Bohyung Han",
            "Joon Hee Han"
        ],
        "abstract": "We propose a novel algorithm to detect occlusion for visual tracking through learning with observation likelihoods. In our technique, target is divided into regular grid cells and the state of occlusion is determined for each cell using a classifier. Each cell in the target is associated with many small patches, and the patch likelihoods observed during tracking construct a feature vector, which is used for classification. Since the occlusion is learned with patch likelihoods instead of patches themselves, the classifier is universally applicable to any videos or objects for occlusion reasoning. Our occlusion detection algorithm has decent performance in accuracy, which is sufficient to improve tracking performance significantly. The proposed algorithm can be combined with many generic tracking methods, and we adopt L 1 minimization tracker to test the performance of our framework. The advantage of our algorithm is supported by quantitative and qualitative evaluation, and successful tracking and occlusion reasoning results are illustrated in many challenging video sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126414",
        "reference_list": [
            {
                "year": "2009",
                "id": 196
            },
            {
                "year": "2005",
                "id": 194
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 5,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Cognition",
                "Vectors",
                "Training",
                "Minimization",
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "computer vision",
                "image classification",
                "image sequences",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "occlusion learning",
                "visual tracking likelihood",
                "observation likelihood learning",
                "regular grid cells",
                "classifier",
                "feature vector",
                "patch likelihoods",
                "occlusion reasoning",
                "occlusion detection algorithm",
                "L1 minimization tracker",
                "video sequences"
            ]
        },
        "id": 196,
        "cited_by": [
            {
                "year": "2015",
                "id": 341
            },
            {
                "year": "2013",
                "id": 286
            },
            {
                "year": "2013",
                "id": 420
            }
        ]
    },
    {
        "title": "Unsupervised metric learning for face identification in TV video",
        "authors": [
            "Ramazan Gokberk Cinbis",
            "Jakob Verbeek",
            "Cordelia Schmid"
        ],
        "abstract": "The goal of face identification is to decide whether two faces depict the same person or not. This paper addresses the identification problem for face-tracks that are automatically collected from uncontrolled TV video data. Face-track identification is an important component in systems that automatically label characters in TV series or movies based on subtitles and/or scripts: it enables effective transfer of the sparse text-based supervision to other faces. We show that, without manually labeling any examples, metric learning can be effectively used to address this problem. This is possible by using pairs of faces within a track as positive examples, while negative training examples can be generated from pairs of face tracks of different people that appear together in a video frame. In this manner we can learn a cast-specific metric, adapted to the people appearing in a particular video, without using any supervision. Identification performance can be further improved using semi-supervised learning where we also include labels for some of the face tracks. We show that our cast-specific metrics not only improve identification, but also recognition and clustering.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126415",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2009",
                "id": 135
            }
        ],
        "citation": {
            "ieee": 50,
            "other": 17,
            "total": 67
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Measurement",
                "Training",
                "Face recognition",
                "TV",
                "Feature extraction",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)",
                "pattern clustering",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "usupervised metric learning",
                "face identification",
                "identification problem",
                "face-tracks",
                "uncontrolled TV video data",
                "face-track identification",
                "TV series",
                "movies",
                "sparse text-based supervision",
                "negative training examples",
                "cast-specific metric",
                "semi-supervised learning",
                "image clustering"
            ]
        },
        "id": 197,
        "cited_by": [
            {
                "year": "2017",
                "id": 554
            },
            {
                "year": "2013",
                "id": 332
            },
            {
                "year": "2013",
                "id": 356
            }
        ]
    },
    {
        "title": "A revisit to cost aggregation in stereo matching: How far can we reduce its computational redundancy?",
        "authors": [
            "Dongbo Min",
            "Jiangbo Lu",
            "Minh N. Do"
        ],
        "abstract": "This paper presents a novel method for performing an efficient cost aggregation in stereo matching. The cost aggregation problem is re-formulated with a perspective of a histogram, and it gives us a potential to reduce the complexity of the cost aggregation significantly. Different from the previous methods which have tried to reduce the complexity in terms of the size of an image and a matching window, our approach focuses on reducing the computational redundancy which exists among the search range, caused by a repeated filtering for all disparity hypotheses. Moreover, we also reduce the complexity of the window-based filtering through an efficient sampling scheme inside the matching window. The trade-off between accuracy and complexity is extensively investigated into parameters used in the proposed method. Experimental results show that the proposed method provides high-quality disparity maps with low complexity. This work provides new insights into complexity-constrained stereo matching algorithm design.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126416",
        "reference_list": [
            {
                "year": "2007",
                "id": 66
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 12,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Logic gates",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "filtering theory",
                "image matching",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cost aggregation",
                "computational redundancy",
                "window matching",
                "repeated filtering",
                "disparity hypothesis",
                "window-based filtering",
                "disparity maps",
                "complexity-constrained stereo matching algorithm design"
            ]
        },
        "id": 198,
        "cited_by": [
            {
                "year": "2013",
                "id": 6
            }
        ]
    },
    {
        "title": "Scan rectification for structured light range sensors with rolling shutters",
        "authors": [
            "Erik Ringaby",
            "Per-Erik Forss\u00e9n"
        ],
        "abstract": "Structured light range sensors, such as the Microsoft Kinect, have recently become popular as perception devices for computer vision and robotic systems. These sensors use CMOS imaging chips with electronic rolling shutters (ERS). When using such a sensor on a moving platform, both the image, and the depth map, will exhibit geometric distortions. We introduce an algorithm that can suppress such distortions, by rectifying the 3D point clouds from the range sensor. This is done by first estimating the time continuous 3D camera trajectory, and then transforming the 3D points to where they would have been, if the camera had been stationary. To ensure that image and range data are synchronous, the camera trajectory is computed from KLT tracks on the structured-light frames, after suppressing the structured-light pattern. We evaluate our rectification, by measuring angles between the visible sides of a cube, before and after rectification. We also measure how much better the 3D point clouds can be aligned after rectification. The obtained improvement is also related to the actual rotational velocity, measured using a MEMS gyroscope.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126417",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three dimensional displays",
                "Solid modeling",
                "Simultaneous localization and mapping",
                "Spline",
                "Trajectory"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "CMOS image sensors",
                "computational geometry",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scan rectification",
                "structured light range sensor",
                "Microsoft Kinect",
                "CMOS imaging chip",
                "electronic rolling shutter",
                "ERS",
                "geometric distortion",
                "3D point cloud",
                "time continuous 3D camera trajectory",
                "rotational velocity",
                "MEMS gyroscope"
            ]
        },
        "id": 199,
        "cited_by": [
            {
                "year": "2015",
                "id": 252
            },
            {
                "year": "2013",
                "id": 251
            }
        ]
    },
    {
        "title": "Object segmentation in video: A hierarchical variational approach for turning point trajectories into dense regions",
        "authors": [
            "Peter Ochs",
            "Thomas Brox"
        ],
        "abstract": "Point trajectories have emerged as a powerful means to obtain high quality and fully unsupervised segmentation of objects in video shots. They can exploit the long term motion difference between objects, but they tend to be sparse due to computational reasons and the difficulty in estimating motion in homogeneous areas. In this paper we introduce a variational method to obtain dense segmentations from such sparse trajectory clusters. Information is propagated with a hierarchical, nonlinear diffusion process that runs in the continuous domain but takes superpixels into account. We show that this process raises the density from 3% to 100% and even increases the average precision of labels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126418",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2009",
                "id": 106
            },
            {
                "year": "2009",
                "id": 82
            },
            {
                "year": "2009",
                "id": 99
            },
            {
                "year": "2009",
                "id": 156
            },
            {
                "year": "2007",
                "id": 88
            }
        ],
        "citation": {
            "ieee": 70,
            "other": 33,
            "total": 103
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Mathematical model",
                "Motion segmentation",
                "Diffusion processes",
                "Image color analysis",
                "Equations",
                "Adaptive optics"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "motion estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object segmentation",
                "hierarchical variational approach",
                "point trajectories turning",
                "dense regions",
                "video shots",
                "motion estimation",
                "dense segmentations",
                "sparse trajectory clusters",
                "hierarchical nonlinear diffusion process"
            ]
        },
        "id": 200,
        "cited_by": [
            {
                "year": "2017",
                "id": 123
            },
            {
                "year": "2017",
                "id": 379
            },
            {
                "year": "2017",
                "id": 445
            },
            {
                "year": "2017",
                "id": 470
            },
            {
                "year": "2015",
                "id": 354
            },
            {
                "year": "2015",
                "id": 360
            },
            {
                "year": "2015",
                "id": 364
            },
            {
                "year": "2015",
                "id": 365
            },
            {
                "year": "2013",
                "id": 278
            },
            {
                "year": "2013",
                "id": 440
            }
        ]
    },
    {
        "title": "Gradient-based learning of higher-order image features",
        "authors": [
            "Roland Memisevic"
        ],
        "abstract": "Recent work on unsupervised feature learning has shown that learning on polynomial expansions of input patches, such as on pair-wise products of pixel intensities, can improve the performance of feature learners and extend their applicability to spatio-temporal problems, such as human action recognition or learning of image transformations. Learning of such higher order features, however, has been much more difficult than standard dictionary learning, because of the high dimensionality and because standard learning criteria are not applicable. Here, we show how one can cast the problem of learning higher-order features as the problem of learning a parametric family of manifolds. This allows us to apply a variant of a de-noising autoencoder network to learn higher-order features using simple gradient based optimization. Our experiments show that the approach can outperform existing higher-order models, while training and inference are exact, fast, and simple.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126419",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 10,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Manifolds",
                "Training",
                "Noise reduction",
                "Encoding",
                "Vectors",
                "Decoding"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gradient-based learning",
                "higher-order image features",
                "unsupervised feature learning",
                "polynomial expansions",
                "pair-wise products",
                "pixel intensities",
                "feature learners",
                "spatio-temporal problems",
                "human action recognition",
                "image transformations",
                "higher order features",
                "dictionary learning",
                "higher-order feature learning",
                "denoising autoencoder network",
                "higher-order features",
                "gradient based optimization",
                "training",
                "inference"
            ]
        },
        "id": 201,
        "cited_by": []
    },
    {
        "title": "Scale space for central catadioptric systems: Towards a generic camera feature extractor",
        "authors": [
            "Luis Puig",
            "J. J. Guerrero"
        ],
        "abstract": "In this paper we propose a new approach to compute the scale space of any omnidirectional image acquired with a central catadioptric system. When these cameras are central they are explained using the sphere camera model, which unifies in a single model, conventional, paracatadioptric and hypercatadioptric systems. Scale space is essential in the detection and matching of interest points, in particular scale invariant points based on Laplacian of Gaussians, like the well known SIFT. We combine the sphere camera model and the partial differential equations framework on manifolds, to compute the Laplace-Beltrami (LB) operator which is a second order differential operator required to perform the Gaussian smoothing on catadioptric images. We perform experiments with synthetic and real images to validate the generalization of our approach to any central catadioptric system.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126420",
        "reference_list": [
            {
                "year": "2007",
                "id": 54
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 9,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Cameras",
                "Mirrors",
                "Computational modeling",
                "Mathematical model",
                "Equations",
                "Smoothing methods"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Gaussian processes",
                "image matching",
                "smoothing methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale space",
                "central catadioptric system",
                "generic camera feature extractor",
                "omnidirectional image",
                "sphere camera model",
                "paracatadioptric system",
                "hypercatadioptric system",
                "interest point detection",
                "interest point matching",
                "scale invariant points",
                "Laplacian",
                "Gaussians",
                "partial differential equations framework",
                "Laplace-Beltrami operator",
                "second order differential operator",
                "Gaussian smoothing"
            ]
        },
        "id": 202,
        "cited_by": []
    },
    {
        "title": "Coherency Sensitive Hashing",
        "authors": [
            "Simon Korman",
            "Shai Avidan"
        ],
        "abstract": "Coherency Sensitive Hashing (CSH) extends Locality Sensitivity Hashing (LSH) and PatchMatch to quickly find matching patches between two images. LSH relies on hashing, which maps similar patches to the same bin, in order to find matching patches. PatchMatch, on the other hand, relies on the observation that images are coherent, to propagate good matches to their neighbors, in the image plane. It uses random patch assignment to seed the initial matching. CSH relies on hashing to seed the initial patch matching and on image coherence to propagate good matches. In addition, hashing lets it propagate information between patches with similar appearance (i.e., map to the same bin). This way, information is propagated much faster because it can use similarity in appearance space or neighborhood in the image plane. As a result, CSH is at least three to four times faster than PatchMatch and more accurate, especially in textured regions, where reconstruction artifacts are most noticeable to the human eye. We verified CSH on a new, large scale, data set of 133 image pairs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126421",
        "reference_list": [],
        "citation": {
            "ieee": 71,
            "other": 33,
            "total": 104
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Indexing",
                "Approximation algorithms",
                "Artificial neural networks",
                "Vectors",
                "Error analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coherency sensitive hashing",
                "locality sensitivity hashing",
                "matching patches",
                "PatchMatch",
                "random patch assignment",
                "initial patch matching",
                "image coherence"
            ]
        },
        "id": 203,
        "cited_by": [
            {
                "year": "2015",
                "id": 36
            },
            {
                "year": "2015",
                "id": 448
            },
            {
                "year": "2013",
                "id": 11
            },
            {
                "year": "2013",
                "id": 96
            },
            {
                "year": "2013",
                "id": 286
            }
        ]
    },
    {
        "title": "Latent Low-Rank Representation for subspace segmentation and feature extraction",
        "authors": [
            "Guangcan Liu",
            "Shuicheng Yan"
        ],
        "abstract": "Low-Rank Representation (LRR) [16, 17] is an effective method for exploring the multiple subspace structures of data. Usually, the observed data matrix itself is chosen as the dictionary, which is a key aspect of LRR. However, such a strategy may depress the performance, especially when the observations are insufficient and/or grossly corrupted. In this paper we therefore propose to construct the dictionary by using both observed and unobserved, hidden data. We show that the effects of the hidden data can be approximately recovered by solving a nuclear norm minimization problem, which is convex and can be solved efficiently. The formulation of the proposed method, called Latent Low-Rank Representation (LatLRR), seamlessly integrates subspace segmentation and feature extraction into a unified framework, and thus provides us with a solution for both subspace segmentation and feature extraction. As a subspace segmentation algorithm, LatLRR is an enhanced version of LRR and outperforms the state-of-the-art algorithms. Being an unsupervised feature extraction algorithm, LatLRR is able to robustly extract salient features from corrupted data, and thus can work much better than the benchmark that utilizes the original data vectors as features for classification. Compared to dimension reduction based methods, LatLRR is more robust to noise.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126422",
        "reference_list": [
            {
                "year": "2005",
                "id": 158
            },
            {
                "year": "2009",
                "id": 86
            }
        ],
        "citation": {
            "ieee": 144,
            "other": 66,
            "total": 210
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Robustness",
                "Motion segmentation",
                "Noise",
                "Vectors",
                "Dictionaries",
                "Strontium"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "data encapsulation",
                "data structures",
                "feature extraction",
                "image segmentation",
                "matrix algebra",
                "minimisation",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "latent low-rank representation",
                "multiple subspace data structures",
                "observed data matrix",
                "dictionary",
                "hidden data",
                "nuclear norm minimization problem",
                "convex problem",
                "LatLRR",
                "subspace segmentation algorithm",
                "state-of-the-art algorithms",
                "unsupervised feature extraction algorithm",
                "salient features",
                "corrupted data",
                "data vectors",
                "dimension reduction based methods"
            ]
        },
        "id": 204,
        "cited_by": [
            {
                "year": "2013",
                "id": 28
            },
            {
                "year": "2013",
                "id": 167
            },
            {
                "year": "2013",
                "id": 386
            },
            {
                "year": "2013",
                "id": 442
            }
        ]
    },
    {
        "title": "High quality depth map upsampling for 3D-TOF cameras",
        "authors": [
            "Jaesik Park",
            "Hyeongwoo Kim",
            "Yu-Wing Tai",
            "Michael S. Brown",
            "Inso Kweon"
        ],
        "abstract": "This paper describes an application framework to perform high quality upsampling on depth maps captured from a low-resolution and noisy 3D time-of-flight (3D-ToF) camera that has been coupled with a high-resolution RGB camera. Our framework is inspired by recent work that uses nonlocal means filtering to regularize depth maps in order to maintain fine detail and structure. Our framework extends this regularization with an additional edge weighting scheme based on several image features based on the additional high-resolution RGB input. Quantitative and qualitative results show that our method outperforms existing approaches for 3D-ToF upsampling. We describe the complete process for this system, including device calibration, scene warping for input alignment, and even how the results can be further processed using simple user markup.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126423",
        "reference_list": [],
        "citation": {
            "ieee": 169,
            "other": 79,
            "total": 248
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image color analysis",
                "Image edge detection",
                "Image resolution",
                "Three dimensional displays",
                "Image segmentation",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image sampling",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high quality depth map upsampling",
                "3D-TOF cameras",
                "3D time-of-flight camera",
                "high-resolution RGB camera",
                "edge weighting scheme",
                "device calibration",
                "scene warping"
            ]
        },
        "id": 205,
        "cited_by": [
            {
                "year": "2015",
                "id": 19
            },
            {
                "year": "2015",
                "id": 39
            },
            {
                "year": "2015",
                "id": 57
            },
            {
                "year": "2015",
                "id": 93
            },
            {
                "year": "2015",
                "id": 380
            },
            {
                "year": "2015",
                "id": 447
            },
            {
                "year": "2013",
                "id": 11
            },
            {
                "year": "2013",
                "id": 123
            },
            {
                "year": "2013",
                "id": 192
            }
        ]
    },
    {
        "title": "Complementary hashing for approximate nearest neighbor search",
        "authors": [
            "Hao Xu",
            "Jingdong Wang",
            "Zhu Li",
            "Gang Zeng",
            "Shipeng Li",
            "Nenghai Yu"
        ],
        "abstract": "Recently, hashing based Approximate Nearest Neighbor (ANN) techniques have been attracting lots of attention in computer vision. The data-dependent hashing methods, e.g., Spectral Hashing, expects better performance than the data-blind counterparts, e.g., Locality Sensitive Hashing (LSH). However, most data-dependent hashing methods only employ a single hash table. When higher recall is desired, they have to retrieve exponentially growing number of hash buckets around the bucket containing the query, which may drag down the precision rapidly. In this paper, we propose a so-called complementary hashing approach, which is able to balance the precision and recall in a more effective way. The key idea is to employ multiple complementary hash tables, which are learned sequentially in a boosting manner, so that, given a query, its true nearest neighbors missed from the active bucket of one hash table are more likely to be found in the active bucket of the next hash table. Compared with LSH that also can exploit multiple hash tables, our approach is more effective to find true NNs, thanks to the complementarity property of the hash tables from our approach. Experimental results on large scale ANN search show that the proposed method significantly improves the performance and outperforms the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126424",
        "reference_list": [
            {
                "year": "2009",
                "id": 274
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 3,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial neural networks",
                "Boosting",
                "Covariance matrix",
                "Sparse matrices",
                "Databases",
                "Redundancy",
                "Eigenvalues and eigenfunctions"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "complementary hashing",
                "nearest neighbor search",
                "approximate nearest neighbor",
                "computer vision",
                "data-dependent hashing method",
                "spectral hashing",
                "locality sensitive hashing",
                "hash table",
                "boosting manner"
            ]
        },
        "id": 206,
        "cited_by": [
            {
                "year": "2015",
                "id": 123
            },
            {
                "year": "2013",
                "id": 32
            },
            {
                "year": "2013",
                "id": 265
            },
            {
                "year": "2013",
                "id": 283
            },
            {
                "year": "2013",
                "id": 441
            }
        ]
    },
    {
        "title": "Compact correlation coding for visual object categorization",
        "authors": [
            "Nobuyuki Morioka",
            "Shin'ichi Satoh"
        ],
        "abstract": "Spatial relationships between local features are thought to play a vital role in representing object categories. However, learning a compact set of higher-order spatial features based on visual words, e.g., doublets and triplets, remains a challenging problem as possible combinations of visual words grow exponentially. While the local pairwise codebook achieves a compact codebook of pairs of spatially close local features without feature selection, its formulation is not scale invariant and is only suitable for densely sampled local features. In contrast, the proximity distribution kernel is a scale-invariant and robust representation capturing rich spatial proximity information between local features, but its representation grows quadratically in the number of visual words. Inspired by the two abovementioned techniques, this paper presents the compact correlation coding that combines the strengths of the two. Our method achieves a compact representation that is scaleinvariant and robust against object deformation. In addition, we adopt sparse coding instead of k-means clustering during the codebook construction to increase the discriminative power of our method. We systematically evaluate our method against both the local pairwise codebook and proximity distribution kernel on several challenging object categorization datasets to show performance improvements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126425",
        "reference_list": [
            {
                "year": "2005",
                "id": 107
            },
            {
                "year": "2007",
                "id": 20
            },
            {
                "year": "2007",
                "id": 67
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 6,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Encoding",
                "Kernel",
                "Joints",
                "Image coding",
                "Correlation",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "correlation methods",
                "image coding",
                "image representation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "compact correlation coding",
                "visual object categorization",
                "object category representation",
                "visual words",
                "local pairwise codebook",
                "compact codebook",
                "proximity distribution kernel",
                "scale-invariant representation",
                "spatial proximity information",
                "object deformation",
                "k-means clustering",
                "codebook construction",
                "sparse coding",
                "computer vision",
                "image representation"
            ]
        },
        "id": 207,
        "cited_by": []
    },
    {
        "title": "Face recognition via local sparse coding",
        "authors": [
            "Ilias Theodorakopoulos",
            "Ioannis Rigas",
            "George Economou",
            "Spiros Fotopoulos"
        ],
        "abstract": "In this paper the face recognition problem is addressed in a part-based sparse approach through the comparison of respective facial regions between different images. To this purpose, a sparse coding procedure is applied to non-overlapping patches derived from frontal-face images, in order to extract local facial information. An adequate measure is introduced, incorporating the resulted sparse representation along with the Hamming distance, in order to express pairwise similarities between faces. Finally, a simple Nearest Neighbor classifier is employed to determine the identity of each facial image. In addition, a new criterion is presented for the rejection of outliers. The emerged face recognition scheme is evaluated using publicly available facial image databases, and the results are compared with those of other well-established recognition methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126426",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Training",
                "Face recognition",
                "Databases",
                "Image recognition",
                "Vectors",
                "Face"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "local sparse coding",
                "part-based sparse approach",
                "nonoverlapping patches",
                "frontal-face images",
                "local facial information",
                "sparse representation",
                "Hamming distance",
                "nearest neighbor classifier"
            ]
        },
        "id": 208,
        "cited_by": []
    },
    {
        "title": "Speeded-up, relaxed spatial matching",
        "authors": [
            "Giorgos Tolias",
            "Yannis Avrithis"
        ],
        "abstract": "A wide range of properties and assumptions determine the most appropriate spatial matching model for an application, e.g. recognition, detection, registration, or large scale image retrieval. Most notably, these include discriminative power, geometric invariance, rigidity constraints, mapping constraints, assumptions made on the underlying features or descriptors and, of course, computational complexity. Having image retrieval in mind, we present a very simple model inspired by Hough voting in the transformation space, where votes arise from single feature correspondences. A relaxed matching process allows for multiple matching surfaces or non-rigid objects under one-to-one mapping, yet is linear in the number of correspondences. We apply it to geometry re-ranking in a search engine, yielding superior performance with the same space requirements but a dramatic speed-up compared to the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126427",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 16,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Shape",
                "Geometry",
                "Computational modeling",
                "Image edge detection",
                "Image retrieval",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "Hough transforms",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatial matching model",
                "discriminative power",
                "geometric invariance",
                "rigidity constraints",
                "mapping constraints",
                "computational complexity",
                "image retrieval",
                "Hough voting",
                "transformation space",
                "feature correspondences",
                "multiple matching surfaces",
                "one-to-one mapping",
                "geometry re-ranking",
                "search engine"
            ]
        },
        "id": 209,
        "cited_by": [
            {
                "year": "2013",
                "id": 174
            }
        ]
    },
    {
        "title": "Active clustering of document fragments using information derived from both images and catalogs",
        "authors": [
            "Lior Wolf",
            "Lior Litwak",
            "Nachum Dershowitz",
            "Roni Shweka",
            "Yaacov Choueka"
        ],
        "abstract": "Many significant historical corpora contain leaves that are mixed up and no longer bound in their original state as multi-page documents. The reconstruction of old manuscripts from a mix of disjoint leaves can therefore be of paramount importance to historians and literary scholars. Previously, it was shown that visual similarity provides meaningful pair-wise similarities between handwritten leaves. Here, we go a step further and suggest a semiautomatic clustering tool that helps reconstruct the original documents. The proposed solution is based on a graphical model that makes inferences based on catalog information provided for each leaf as well as on the pairwise similarities of handwriting. Several novel active clustering techniques are explored, and the solution is applied to a significant part of the Cairo Genizah, where the problem of joining leaves remains unsolved even after a century of extensive study by hundreds of scholars.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126428",
        "reference_list": [
            {
                "year": "2007",
                "id": 51
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 4,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Catalogs",
                "Computational modeling",
                "Graphical models",
                "Complexity theory",
                "Humans",
                "Data models",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "history",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active clustering",
                "document fragments",
                "image information",
                "historical corpora",
                "multipage documents",
                "semiautomatic clustering",
                "original document clustering",
                "graphical model",
                "catalog information",
                "handwriting pairwise similarities"
            ]
        },
        "id": 210,
        "cited_by": []
    },
    {
        "title": "Decision tree fields",
        "authors": [
            "Sebastian Nowozin",
            "Carsten Rother",
            "Shai Bagon",
            "Toby Sharp",
            "Bangpeng Yao",
            "Pushmeet Kohli"
        ],
        "abstract": "This paper introduces a new formulation for discrete image labeling tasks, the Decision Tree Field (DTF), that combines and generalizes random forests and conditional random fields (CRF) which have been widely used in computer vision. In a typical CRF model the unary potentials are derived from sophisticated random forest or boosting based classifiers, however, the pairwise potentials are assumed to (1) have a simple parametric form with a pre-specified and fixed dependence on the image data, and (2) to be defined on the basis of a small and fixed neighborhood. In contrast, in DTF, local interactions between multiple variables are determined by means of decision trees evaluated on the image data, allowing the interactions to be adapted to the image content. This results in powerful graphical models which are able to represent complex label structure. Our key technical contribution is to show that the DTF model can be trained efficiently and jointly using a convex approximate likelihood function, enabling us to learn over a million free model parameters. We show experimentally that for applications which have a rich and complex label structure, our model achieves excellent results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126429",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2005",
                "id": 72
            },
            {
                "year": "2007",
                "id": 140
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 17,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Decision trees",
                "Training",
                "Labeling",
                "Computational modeling",
                "Data models",
                "Graphical models",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "decision trees"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "decision tree fields",
                "discrete image labeling tasks",
                "random forests",
                "conditional random fields",
                "computer vision",
                "boosting based classifiers",
                "graphical models",
                "convex approximate likelihood function"
            ]
        },
        "id": 211,
        "cited_by": [
            {
                "year": "2015",
                "id": 201
            },
            {
                "year": "2013",
                "id": 230
            },
            {
                "year": "2013",
                "id": 382
            }
        ]
    },
    {
        "title": "Exploring regularized feature selection for person specific face verification",
        "authors": [
            "Yixiong Liang",
            "Shenghui Liao",
            "Lei Wang",
            "Beiji Zou"
        ],
        "abstract": "In this paper, we explore the regularized feature selection method for person specific face verification in unconstrained environments. We reformulate the generalization of the single-task sparsity-enforced feature selection method to multi-task cases as a simultaneous sparse approximation problem. We also investigate two feature selection strategies in the multi-task generalization based on the positive and negative feature correlation assumptions across different persons. Simultaneous orthogonal matching pursuit (SOMP) is adopted and modified to solve the corresponding optimization problems. We further proposed a named simultaneous subspace pursuit (SSP) methods which generalize the subspace pursuit method to solve the corresponding optimization problems. The performance of different feature selection strategies and different solvers for face verification are compared on the challenging LFW face database. Our experimental results show that 1) the selected subsets based on positive correlation assumption are more effective than those based on the negative correlation assumption; 2) the OMP-based solvers outperform SP-based solvers in terms of feature selection and 3) the regularized methods with OMP-based solvers can outperform state-of-the-art feature selection methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126430",
        "reference_list": [
            {
                "year": "2001",
                "id": 142
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Approximation methods",
                "Relaxation methods",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "face recognition",
                "feature extraction",
                "image matching",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "regularized feature selection method",
                "person specific face verification",
                "simultaneous sparse approximation problem",
                "multitask generalization",
                "feature correlation assumption",
                "simultaneous orthogonal matching pursuit",
                "simultaneous subspace pursuit method",
                "optimization problem"
            ]
        },
        "id": 212,
        "cited_by": []
    },
    {
        "title": "Stereo time-of-flight",
        "authors": [
            "Victor Casta\u00f1eda",
            "Diana Mateus",
            "Nassir Navab"
        ],
        "abstract": "This paper describes a novel method to acquire depth images using a pair of ToF (Time of Flight) cameras. As opposed to approaches that filter, calibrate or do 3D reconstructions posterior to the image acquisition, we propose to combine the measurements of the two cameras at the acquisition level. To do so, we define a three-stages procedure, during which we actively modify the infrared lighting of the scene: first, the two cameras emit an infrared signal one after the other (stages 1 and 2), and then, simultaneously (stage 3). Assuming the scene is static during the three stages, we gather the depth measurements obtained with both cameras and define a cost function to optimize the two depth images. A quantitative evaluation of the performance of the proposed method for different objects and stereo configurations is provided based on a simulation of the ToF cameras. Results on real images are also presented. Both in simulation and real images the stereo-ToF acquisition produces more accurate depth measurements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126431",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 8,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial intelligence",
                "Lighting",
                "Charge coupled devices"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo time-of-flight",
                "depth image",
                "ToF camera",
                "image acquisition",
                "infrared lighting",
                "cost function",
                "stereo-ToF acquisition"
            ]
        },
        "id": 213,
        "cited_by": []
    },
    {
        "title": "Variational recursive joint estimation of dense scene structure and camera motion from monocular high speed traffic sequences",
        "authors": [
            "Florian Becker",
            "Frank Lenzen",
            "J\u00f6rg H. Kappes",
            "Christoph Schn\u00f6rr"
        ],
        "abstract": "We present an approach to jointly estimating camera motion and dense scene structure in terms of depth maps from monocular image sequences in driver-assistance scenarios. For two consecutive frames of a sequence taken with a single fast moving camera, the approach combines numerical estimation of egomotion on the Euclidean manifold of motion parameters with variational regularization of dense depth map estimation. Embedding this online joint estimator into a recursive framework achieves a pronounced spatio-temporal filtering effect and robustness. We report the evaluation of thousands of images taken from a car moving at speed up to 100 km/h. The results compare favorably with two alternative settings that require more input data: stereo based scene reconstruction and camera motion estimation in batch mode using multiple frames. The employed benchmark dataset is publicly available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126432",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 4,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Estimation",
                "Approximation methods",
                "Image sequences",
                "Image reconstruction",
                "Stereo vision",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "traffic engineering computing",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational recursive joint estimation",
                "dense scene structure",
                "camera motion",
                "monocular high speed traffic sequences",
                "monocular image sequences",
                "driver-assistance scenarios",
                "numerical estimation",
                "egomotion",
                "Euclidean manifold",
                "motion parameters",
                "variational regularization",
                "dense depth map estimation"
            ]
        },
        "id": 214,
        "cited_by": []
    },
    {
        "title": "Incremental on-line semi-supervised learning for segmenting the left ventricle of the heart from ultrasound data",
        "authors": [
            "Gustavo Carneiro",
            "Jacinto C. Nascimento"
        ],
        "abstract": "Recently, there has been an increasing interest in the investigation of statistical pattern recognition models for the fully automatic segmentation of the left ventricle (LV) of the heart from ultrasound data. The main vulnerability of these models resides in the need of large manually annotated training sets for the parameter estimation procedure. The issue is that these training sets need to be annotated by clinicians, which makes this training set acquisition process quite expensive. Therefore, reducing the dependence on large training sets is important for a more extensive exploration of statistical models in the LV segmentation problem. In this paper, we present a novel incremental on-line semi-supervised learning model that reduces the need of large training sets for estimating the parameters of statistical models. Compared to other semi-supervised techniques, our method yields an on-line incremental re-training and segmentation instead of the off-line incremental re-training and segmentation more commonly found in the literature. Another innovation of our approach is that we use a statistical model based on deep learning architectures, which are easily adapted to this on-line incremental learning framework. We show that our fully automatic LV segmentation method achieves state-of-the-art accuracy with training sets containing less than twenty annotated images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126433",
        "reference_list": [
            {
                "year": "2003",
                "id": 83
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical ultrasonics",
                "cardiology",
                "image segmentation",
                "learning (artificial intelligence)",
                "medical image processing",
                "parameter estimation",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "incremental online semisupervised learning model",
                "heart left ventricle segmentation",
                "ultrasound data",
                "statistical pattern recognition models",
                "parameter estimation procedure",
                "large training sets",
                "online incremental retraining",
                "deep learning architectures"
            ]
        },
        "id": 215,
        "cited_by": []
    },
    {
        "title": "Linear stereo matching",
        "authors": [
            "Leonardo De-Maeztu",
            "Stefano Mattoccia",
            "Arantxa Villanueva",
            "Rafael Cabeza"
        ],
        "abstract": "Recent local stereo matching algorithms based on an adaptive-weight strategy achieve accuracy similar to global approaches. One of the major problems of these algorithms is that they are computationally expensive and this complexity increases proportionally to the window size. This paper proposes a novel cost aggregation step with complexity independent of the window size (i.e. O(1)) that outperforms state-of-the-art O(1) methods. Moreover, compared to other O(1) approaches, our method does not rely on integral histograms enabling aggregation using colour images instead of grayscale ones. Finally, to improve the results of the proposed algorithm a disparity refinement pipeline is also proposed. The overall algorithm produces results comparable to those of state-of-the-art stereo matching algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126434",
        "reference_list": [],
        "citation": {
            "ieee": 47,
            "other": 19,
            "total": 66
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Gray-scale",
                "Stereo vision",
                "Adaptation models",
                "Accuracy",
                "Histograms",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image colour analysis",
                "image matching",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear stereo matching",
                "adaptive-weight strategy",
                "cost aggregation",
                "colour images",
                "disparity refinement pipeline"
            ]
        },
        "id": 216,
        "cited_by": []
    },
    {
        "title": "Feature seeding for action recognition",
        "authors": [
            "Pyry Matikainen",
            "Rahul Sukthankar",
            "Martial Hebert"
        ],
        "abstract": "Progress in action recognition has been in large part due to advances in the features that drive learning-based methods. However, the relative sparsity of training data and the risk of overfitting have made it difficult to directly search for good features. In this paper we suggest using synthetic data to search for robust features that can more easily take advantage of limited data, rather than using the synthetic data directly as a substitute for real data. We demonstrate that the features discovered by our selection method, which we call seeding, improve performance on an action classification task on real data, even though the synthetic data from which the features are seeded differs significantly from the real data, both in terms of appearance and the set of action classes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126435",
        "reference_list": [
            {
                "year": "2009",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Trajectory",
                "Humans",
                "Histograms",
                "Feature extraction",
                "Support vector machines",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature seeding",
                "action recognition",
                "learning-based method",
                "synthetic data",
                "selection method",
                "action classification task"
            ]
        },
        "id": 217,
        "cited_by": [
            {
                "year": "2015",
                "id": 337
            }
        ]
    },
    {
        "title": "The medial feature detector: Stable regions from image boundaries",
        "authors": [
            "Yannis Avrithis",
            "Konstantinos Rapantzikos"
        ],
        "abstract": "We present a local feature detector that is able to detect regions of arbitrary scale and shape, without scale space construction. We compute a weighted distance map on image gradient, using our exact linear-time algorithm, a variant of group marching for Euclidean space. We find the weighted medial axis by extending residues, typically used in Voronoi skeletons. We decompose the medial axis into a graph representing image structure in terms of peaks and saddle points. A duality property enables reconstruction of regions using the same marching method. We greedily group regions taking both contrast and shape into account. On the way, we select regions according to our shape fragmentation factor, favoring those well enclosed by boundaries-even incomplete. We achieve state of the art performance in matching and retrieval experiments with reduced memory and computational requirements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126436",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Feature extraction",
                "Image edge detection",
                "Transforms",
                "Euclidean distance",
                "Detectors",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "feature extraction",
                "graph theory",
                "image matching",
                "image reconstruction",
                "image representation",
                "image retrieval",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "medial feature detector",
                "image boundaries",
                "local feature detector",
                "weighted distance map",
                "image gradient",
                "linear-time algorithm",
                "Euclidean space",
                "weighted medial axis",
                "group marching",
                "Voronoi skeletons",
                "graph image structure representation",
                "saddle points",
                "peak points",
                "duality property",
                "shape fragmentation factor",
                "matching experiments",
                "retrieval experiments"
            ]
        },
        "id": 218,
        "cited_by": [
            {
                "year": "2017",
                "id": 285
            }
        ]
    },
    {
        "title": "Towards accurate and efficient representation of image irradiance of convex-Lambertian objects under unknown near lighting",
        "authors": [
            "Shireen Y. Elhabian",
            "Ham Rara",
            "Aly A. Farag"
        ],
        "abstract": "Surface irradiance signals are turned into outgoing radiance through the surface reflectance function, which can be significantly perturbed by the illumination conditions. Due to their low-frequency nature, irradiance signals can be represented using low-order basis functions, where spherical harmonics (SH) have been extensively used to provide such basis. When capturing image irradiance from a single viewpoint, the visible part of the object's surface constructs the upper hemisphere of the surface normals where the SH are no longer orthonormal. This reduced domain paves the way for even lower-dimensional approximation since full spherical representation is not needed. While harmonic basis are known to be optimal under distant light, light coming from near-by objects and indoor environments are common near light scenarios; it is essential to relax distant light assumption. Considering light source(s) distributed uniformly over the upper hemisphere, we propose the use of hemispherical harmonics (HSH) to model image irradiance of convex Lambertian objects perceived from single viewpoint under unknown near illumination. We prove analytically, and experimentally validated, that the Lambertian kernel has a more compact harmonic expansion in the hemispherical domain when compared to its spherical counterpart. We illustrate that HSH provide an efficient and accurate low-dimensional representation of image irradiance of Lambertian objects under near lighting conditions in contrast to SH.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126437",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Harmonic analysis",
                "Lighting",
                "Light sources",
                "Function approximation",
                "Kernel",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image representation",
                "lighting",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image irradiance representation",
                "convex-Lambertian objects",
                "near lighting",
                "surface irradiance signal",
                "illumination condition",
                "spherical harmonics",
                "low-order basis function",
                "lower-dimensional approximation",
                "hemispherical harmonics",
                "Lambertian kernel"
            ]
        },
        "id": 219,
        "cited_by": []
    },
    {
        "title": "Spatio-temporal clustering of probabilistic region trajectories",
        "authors": [
            "Fabio Galasso",
            "Masahiro Iwasaki",
            "Kunio Nobori",
            "Roberto Cipolla"
        ],
        "abstract": "We propose a novel model for the spatio-temporal clustering of trajectories based on motion, which applies to challenging street-view video sequences of pedestrians captured by a mobile camera. A key contribution of our work is the introduction of novel probabilistic region trajectories, motivated by the non-repeatability of segmentation of frames in a video sequence. Hierarchical image segments are obtained by using a state-of-the-art hierarchical segmentation algorithm, and connected from adjacent frames in a directed acyclic graph. The region trajectories and measures of confidence are extracted from this graph using a dynamic programming-based optimisation. Our second main contribution is a Bayesian framework with a twofold goal: to learn the optimal, in a maximum likelihood sense, Random Forests classifier of motion patterns based on video features, and construct a unique graph from region trajectories of different frames, lengths and hierarchical levels. Finally, we demonstrate the use of Isomap for effective spatio-temporal clustering of the region trajectories of pedestrians. We support our claims with experimental results on new and existing challenging video sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126438",
        "reference_list": [
            {
                "year": "2009",
                "id": 110
            },
            {
                "year": "2009",
                "id": 211
            },
            {
                "year": "2009",
                "id": 188
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 7,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Image segmentation",
                "Video sequences",
                "Probabilistic logic",
                "Motion segmentation",
                "Radio frequency",
                "Clustering algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "directed graphs",
                "dynamic programming",
                "feature extraction",
                "image motion analysis",
                "image segmentation",
                "maximum likelihood estimation",
                "pattern clustering",
                "pedestrians",
                "random processes",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatio-temporal clustering",
                "probabilistic region trajectory",
                "street-view video sequence",
                "pedestrian",
                "mobile camera",
                "hierarchical segmentation algorithm",
                "directed acyclic graph",
                "dynamic programming-based optimisation",
                "Bayesian framework",
                "maximum likelihood algorithm",
                "random forests classifier",
                "motion pattern",
                "video feature",
                "Isomap"
            ]
        },
        "id": 220,
        "cited_by": [
            {
                "year": "2013",
                "id": 440
            }
        ]
    },
    {
        "title": "Face reconstruction in the wild",
        "authors": [
            "Ira Kemelmacher-Shlizerman",
            "Steven M. Seitz"
        ],
        "abstract": "We address the problem of reconstructing 3D face models from large unstructured photo collections, e.g., obtained by Google image search or from personal photo collections in iPhoto. This problem is extremely challenging due to the high degree of variability in pose, illumination, facial expression, non-rigid changes in face shape and reflectance over time and occlusions. In light of this extreme variability, no single reconstruction can be consistent with all of the images. Instead, we define as the goal of reconstruction to recover a model that is locally consistent with the image set. I.e., each local region of the model is consistent with a large set of photos, resulting in a model that captures the dominant trends in the input data for different parts of the face. Our approach leverages multi-image shading, but unlike traditional photometric stereo approaches, allows for changes in viewpoint and shape. We optimize over pose, shape, and lighting in an iterative approach that seeks to minimize the rank of the transformed images. This approach produces high quality shape models for a wide range of celebrities from photos available on the Internet.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126439",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            }
        ],
        "citation": {
            "ieee": 46,
            "other": 18,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Image reconstruction",
                "Lighting",
                "Three dimensional displays",
                "Surface reconstruction",
                "Approximation methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face reconstruction",
                "3D face models",
                "large unstructured photo collections",
                "facial expression",
                "face shape",
                "multiimage shading",
                "photometric stereo approach",
                "iterative approach"
            ]
        },
        "id": 221,
        "cited_by": [
            {
                "year": "2017",
                "id": 107
            },
            {
                "year": "2017",
                "id": 391
            },
            {
                "year": "2015",
                "id": 441
            },
            {
                "year": "2013",
                "id": 127
            },
            {
                "year": "2013",
                "id": 406
            },
            {
                "year": "2013",
                "id": 450
            }
        ]
    },
    {
        "title": "Modeling image similarity by Gaussian mixture models and the Signature Quadratic Form Distance",
        "authors": [
            "Christian Beecks",
            "Anca Maria Ivanescu",
            "Steffen Kirchhoff",
            "Thomas Seidl"
        ],
        "abstract": "Modeling image similarity for browsing and searching in voluminous image databases is a challenging task of nearly all content-based image retrieval systems. One promising way of defining image similarity consists in applying distance-based similarity measures on compact image representations. Beyond feature histograms and feature signatures, more general feature representations are mixture models of which the Gaussian mixture model is the most prominent one. This feature representation can be compared by employing approximations of the Kullback-Leibler Divergence. Although several of those approximations have been successfully applied to model image similarity, their applicability to mixture models based on high-dimensional feature descriptors is questionable. In this paper, we thus introduce the Signature Quadratic Form Distance to measure the distance between two Gaussian mixture models of high-dimensional feature descriptors. We show the analytical computation of the proposed Gaussian Quadratic Form Distance and evaluate its retrieval performance by making use of different benchmark image databases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126440",
        "reference_list": [
            {
                "year": "2003",
                "id": 64
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2001",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 10,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Feature extraction",
                "Adaptation models",
                "Image databases",
                "Approximation methods",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "feature extraction",
                "image matching",
                "image representation",
                "image retrieval",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image similarity",
                "gaussian mixture models",
                "signature quadratic form distance",
                "image databases",
                "content-based image retrieval systems",
                "distance-based similarity measures",
                "image representations",
                "feature histograms",
                "feature signatures",
                "Kullback-Leibler divergence"
            ]
        },
        "id": 222,
        "cited_by": [
            {
                "year": "2013",
                "id": 210
            }
        ]
    },
    {
        "title": "Diagonal preconditioning for first order primal-dual algorithms in convex optimization",
        "authors": [
            "Thomas Pock",
            "Antonin Chambolle"
        ],
        "abstract": "In this paper we study preconditioning techniques for the first-order primal-dual algorithm proposed in [5]. In particular, we propose simple and easy to compute diagonal preconditioners for which convergence of the algorithm is guaranteed without the need to compute any step size parameters. As a by-product, we show that for a certain instance of the preconditioning, the proposed algorithm is equivalent to the old and widely unknown alternating step method for monotropic programming [7]. We show numerical results on general linear programming problems and a few standard computer vision problems. In all examples, the preconditioned algorithm significantly outperforms the algorithm of [5].",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126441",
        "reference_list": [
            {
                "year": "2009",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 60,
            "other": 86,
            "total": 146
        },
        "keywords": {
            "IEEE Keywords": [
                "Convergence",
                "IP networks",
                "Symmetric matrices",
                "Algorithm design and analysis",
                "Partitioning algorithms",
                "Computer vision",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "image denoising",
                "linear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "diagonal preconditioning technique",
                "first order primal-dual algorithm",
                "convex optimization",
                "monotropic programming",
                "linear programming",
                "computer vision"
            ]
        },
        "id": 223,
        "cited_by": [
            {
                "year": "2015",
                "id": 75
            },
            {
                "year": "2013",
                "id": 123
            }
        ]
    },
    {
        "title": "Blurring-invariant Riemannian metrics for comparing signals and images",
        "authors": [
            "Zhengwu Zhang",
            "Eric Klassen",
            "Anuj Srivastava",
            "Pavan Turaga",
            "Rama Chellappa"
        ],
        "abstract": "We propose a novel Riemannian framework for comparing signals and images in a manner that is invariant to their levels of blur. This framework uses a log-Fourier representation of signals/images in which the set of all possible Gaussian blurs of a signal, i.e. its orbits under semigroup action of Gaussian blur functions, is a straight line. Using a set of Riemannian metrics under which the group actions are by isometries, the orbits are compared via distances between orbits. We demonstrate this framework using a number of experimental results involving 1D signals and 2D images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126442",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Space vehicles",
                "Orbits",
                "Fourier transforms",
                "Polynomials",
                "Vectors",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "blurring-invariant Riemannian metrics",
                "log-Fourier representation",
                "signal representation",
                "image representation",
                "Gaussian blur function"
            ]
        },
        "id": 224,
        "cited_by": []
    },
    {
        "title": "A selective spatio-temporal interest point detector for human action recognition in complex scenes",
        "authors": [
            "Bhaskar Chakraborty",
            "Michael B. Holte",
            "Thomas B. Moeslund",
            "Jordi Gonz\u00e0lez",
            "F. Xavier Roca"
        ],
        "abstract": "Recent progress in the field of human action recognition points towards the use of Spatio-Temporal Interest Points (STIPs) for local descriptor-based recognition strategies. In this paper we present a new approach for STIP detection by applying surround suppression combined with local and temporal constraints. Our method is significantly different from existing STIP detectors and improves the performance by detecting more repeatable, stable and distinctive STIPs for human actors, while suppressing unwanted background STIPs. For action representation we use a bag-of-visual words (BoV) model of local N-jet features to build a vocabulary of visual-words. To this end, we introduce a novel vocabulary building strategy by combining spatial pyramid and vocabulary compression techniques, resulting in improved performance and efficiency. Action class specific Support Vector Machine (SVM) classifiers are trained for categorization of human actions. A comprehensive set of experiments on existing benchmark datasets, and more challenging datasets of complex scenes, validate our approach and show state-of-the-art performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126443",
        "reference_list": [
            {
                "year": "2009",
                "id": 118
            },
            {
                "year": "2009",
                "id": 248
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2007",
                "id": 84
            },
            {
                "year": "2009",
                "id": 262
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 10,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Vocabulary",
                "Feature extraction",
                "Detectors",
                "YouTube",
                "Humans",
                "Support vector machines",
                "Buildings"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object recognition",
                "support vector machines",
                "vocabulary"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "selective spatio-temporal interest point detector",
                "human action recognition",
                "complex scenes",
                "local descriptor-based recognition strategies",
                "STIP detection",
                "surround suppression",
                "local constraints",
                "temporal constraints",
                "bag-of-visual words model",
                "local N-jet features",
                "visual-words vocabulary",
                "vocabulary building strategy",
                "spatial pyramid technique",
                "vocabulary compression techniques",
                "support vector machine classifier"
            ]
        },
        "id": 225,
        "cited_by": []
    },
    {
        "title": "Assessing the aesthetic quality of photographs using generic image descriptors",
        "authors": [
            "Luca Marchesotti",
            "Florent Perronnin",
            "Diane Larlus",
            "Gabriela Csurka"
        ],
        "abstract": "In this paper, we automatically assess the aesthetic properties of images. In the past, this problem has been addressed by hand-crafting features which would correlate with best photographic practices (e.g. \u201cDoes this image respect the rule of thirds?\u201d) or with photographic techniques (e.g. \u201cIs this image a macro?\u201d). We depart from this line of research and propose to use generic image descriptors to assess aesthetic quality. We experimentally show that the descriptors we use, which aggregate statistics computed from low-level local features, implicitly encode the aesthetic properties explicitly used by state-of-the-art methods and outperform them by a significant margin.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126444",
        "reference_list": [
            {
                "year": "2009",
                "id": 30
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 89,
            "other": 79,
            "total": 168
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Visualization",
                "Semantics",
                "Feature extraction",
                "Accuracy",
                "Histograms",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "photography",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photograph aesthetic quality assessment",
                "generic image descriptors",
                "hand-crafting features",
                "photographic techniques",
                "statistic aggregation",
                "low-level local features",
                "image quality assessment",
                "binary classification problem"
            ]
        },
        "id": 226,
        "cited_by": [
            {
                "year": "2017",
                "id": 66
            },
            {
                "year": "2017",
                "id": 230
            },
            {
                "year": "2017",
                "id": 370
            }
        ]
    },
    {
        "title": "A graph-matching kernel for object categorization",
        "authors": [
            "Olivier Duchenne",
            "Armand Joulin",
            "Jean Ponce"
        ],
        "abstract": "This paper addresses the problem of category-level image classification. The underlying image model is a graph whose nodes correspond to a dense set of regions, and edges reflect the underlying grid structure of the image and act as springs to guarantee the geometric consistency of nearby regions during matching. A fast approximate algorithm for matching the graphs associated with two images is presented. This algorithm is used to construct a kernel appropriate for SVM-based image classification, and experiments with the Caltech 101, Caltech 256, and Scenes datasets demonstrate performance that matches or exceeds the state of the art for methods using a single type of features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126445",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2003",
                "id": 35
            },
            {
                "year": "2009",
                "id": 55
            }
        ],
        "citation": {
            "ieee": 78,
            "other": 44,
            "total": 122
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Approximation algorithms",
                "Support vector machines",
                "Image retrieval",
                "Optimization",
                "Vectors",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "graph theory",
                "image classification",
                "image matching",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph-matching kernel",
                "object categorization",
                "category-level image classification",
                "image model",
                "grid structure",
                "geometric consistency",
                "fast approximate algorithm",
                "SVM-based image classification"
            ]
        },
        "id": 227,
        "cited_by": [
            {
                "year": "2015",
                "id": 22
            },
            {
                "year": "2013",
                "id": 3
            },
            {
                "year": "2013",
                "id": 35
            },
            {
                "year": "2013",
                "id": 105
            },
            {
                "year": "2013",
                "id": 165
            },
            {
                "year": "2013",
                "id": 424
            }
        ]
    },
    {
        "title": "Learning specific-class segmentation from diverse data",
        "authors": [
            "M. Pawan Kumar",
            "Haithem Turki",
            "Dan Preston",
            "Daphne Koller"
        ],
        "abstract": "We consider the task of learning the parameters of a segmentation model that assigns a specific semantic class to each pixel of a given image. The main problem we face is the lack of fully supervised data. We address this issue by developing a principled framework for learning the parameters of a specific-class segmentation model using diverse data. More precisely, we propose a latent structural support vector machine formulation, where the latent variables model any missing information in the human annotation. Of particular interest to us are three types of annotations: (i) images segmented using generic foreground or background classes; (ii) images with bounding boxes specified for objects; and (iii) images labeled to indicate the presence of a class. Using large, publicly available datasets we show that our approach is able to exploit the information present in different annotations to improve the accuracy of a state-of-the art region-based model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126446",
        "reference_list": [
            {
                "year": "2007",
                "id": 125
            },
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2009",
                "id": 35
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 11,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Labeling",
                "Computational modeling",
                "Training",
                "Inference algorithms",
                "Semantics",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "learning (artificial intelligence)",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "diverse data",
                "semantic class",
                "supervised data",
                "specific-class segmentation model",
                "latent structural support vector machine formulation",
                "latent variables model",
                "human annotation",
                "image segmentation",
                "generic foreground",
                "background classes",
                "publicly available datasets",
                "state-of-the art region-based model"
            ]
        },
        "id": 228,
        "cited_by": [
            {
                "year": "2015",
                "id": 194
            }
        ]
    },
    {
        "title": "Image representation by active curves",
        "authors": [
            "Wenze Hu",
            "Ying Nian Wu",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper proposes a sparse image representation using deformable templates of simple geometric structures that are commonly observed in images of natural scenes. These deformable templates include active curve templates and active corner templates. An active curve template is a composition of Gabor wavelet elements placed with equal spacing on a straight line segment or a circular arc segment of constant curvature, where each Gabor wavelet element is allowed to locally shift its location and orientation, so that the original line and arc segment of the active curve template can be deformed to fit the observed image. An active corner or angle template is a composition of two active curve templates that share a common end point, and the active curve templates are allowed to vary their overall lengths and curvatures, so that the original corner template can deform to match the observed image. This paper then proposes a hierarchical computational architecture of summax maps that pursues a sparse representation of an image by selecting a small number of active curve and corner templates from a dictionary of all such templates. Experiments show that the proposed method is capable of finding sparse representations of natural images. It is also shown that object templates can be learned by selecting and composing active curve and corner templates.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126447",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Matching pursuit algorithms",
                "Indexes",
                "Prototypes",
                "Computational modeling",
                "Image edge detection",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "curve fitting",
                "image representation",
                "wavelet transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image representation",
                "Gabor wavelet elements",
                "active curve template",
                "straight line segment",
                "circular arc segment",
                "angle template",
                "summax maps"
            ]
        },
        "id": 229,
        "cited_by": []
    },
    {
        "title": "Geometrically consistent stereo seam carving",
        "authors": [
            "Tali Basha",
            "Yael Moses",
            "Shai Avidan"
        ],
        "abstract": "Image retargeting algorithms attempt to adapt the image content to the screen without distorting the important objects in the scene. Existing methods address retargeting of a single image. In this paper we propose a novel method for retargeting a pair of stereo images. Naively retargeting each image independently will distort the geometric structure and make it impossible to perceive the 3D structure of the scene. We show how to extend a single image seam carving to work on a pair of images. Our method minimizes the visual distortion in each of the images as well as the depth distortion. A key property of the proposed method is that it takes into account the visibility relations between pixels in the image pair (occluded and occluding pixels). As a result, our method guarantees, as we formally prove, that the retargeted pair is geometrically consistent with a feasible 3D scene, similar to the original one. Hence, the retargeted stereo pair can be viewed on a stereoscopic display or processed by any computer vision algorithm. We demonstrate our method on a number of challenging indoor and outdoor stereo images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126448",
        "reference_list": [
            {
                "year": "2007",
                "id": 160
            },
            {
                "year": "2007",
                "id": 170
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 15,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Geometry",
                "Couplings",
                "Green products",
                "Computer vision",
                "Stereo image processing"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer graphics",
                "computer vision",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometrically consistent stereo seam carving",
                "image retargeting algorithm",
                "image content",
                "geometric structure",
                "image seam carving",
                "visual distortion",
                "depth distortion",
                "3D scene",
                "retargeted stereo pair",
                "stereoscopic display",
                "computer vision algorithm",
                "indoor stereo image",
                "outdoor stereo image"
            ]
        },
        "id": 230,
        "cited_by": []
    },
    {
        "title": "The NBNN kernel",
        "authors": [
            "T. Tuytelaars",
            "M. Fritz",
            "K. Saenko",
            "T. Darrell"
        ],
        "abstract": "Naive Bayes Nearest Neighbor (NBNN) has recently been proposed as a powerful, non-parametric approach for object classification, that manages to achieve remarkably good results thanks to the avoidance of a vector quantization step and the use of image-to-class comparisons, yielding good generalization. In this paper, we introduce a kernelized version of NBNN. This way, we can learn the classifier in a discriminative setting. Moreover, it then becomes straightforward to combine it with other kernels. In particular, we show that our NBNN kernel is complementary to standard bag-of-features based kernels, focussing on local generalization as opposed to global image composition. By combining them, we achieve state-of-the-art results on Caltech101 and 15 Scenes datasets. As a side contribution, we also investigate how to speed up the NBNN computations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126449",
        "reference_list": [
            {
                "year": "2007",
                "id": 225
            },
            {
                "year": "2007",
                "id": 36
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2003",
                "id": 35
            },
            {
                "year": "2009",
                "id": 253
            }
        ],
        "citation": {
            "ieee": 41,
            "other": 26,
            "total": 67
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Training",
                "Accuracy",
                "Feature extraction",
                "Support vector machines",
                "Algorithm design and analysis",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "NBNN kernel",
                "naive Bayes nearest neighbor",
                "nonparametric approach",
                "object classification",
                "vector quantization",
                "image-to-class comparison",
                "local generalization",
                "global image composition"
            ]
        },
        "id": 231,
        "cited_by": [
            {
                "year": "2013",
                "id": 111
            }
        ]
    },
    {
        "title": "Strong supervision from weak annotation: Interactive training of deformable part models",
        "authors": [
            "Steve Branson",
            "Pietro Perona",
            "Serge Belongie"
        ],
        "abstract": "We propose a framework for large scale learning and annotation of structured models. The system interleaves interactive labeling (where the current model is used to semi-automate the labeling of a new example) and online learning (where a newly labeled example is used to update the current model parameters). This framework is scalable to large datasets and complex image models and is shown to have excellent theoretical and practical properties in terms of train time, optimality guarantees, and bounds on the amount of annotation effort per image. We apply this framework to part-based detection, and introduce a novel algorithm for interactive labeling of deformable part models. The labeling tool updates and displays in real-time the maximum likelihood location of all parts as the user clicks and drags the location of one or more parts. We demonstrate that the system can be used to efficiently and robustly train part and pose detectors on the CUB Birds-200-a challenging dataset of birds in unconstrained pose and environment.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126450",
        "reference_list": [
            {
                "year": "2009",
                "id": 29
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 15,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Training",
                "Labeling",
                "Deformable models",
                "Dynamic programming",
                "Heuristic algorithms",
                "Mice"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "maximum likelihood estimation",
                "object detection",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interactive training",
                "deformable part model",
                "large scale learning",
                "structured model annotation",
                "interactive labeling",
                "online learning",
                "image annotation",
                "part-based detection",
                "tool updates labeling",
                "maximum likelihood location",
                "pose detector",
                "CUB Birds-200"
            ]
        },
        "id": 232,
        "cited_by": [
            {
                "year": "2015",
                "id": 281
            },
            {
                "year": "2013",
                "id": 90
            },
            {
                "year": "2013",
                "id": 213
            },
            {
                "year": "2013",
                "id": 217
            },
            {
                "year": "2013",
                "id": 314
            }
        ]
    },
    {
        "title": "Linear time offline tracking and lower envelope algorithms",
        "authors": [
            "Steve Gu",
            "Ying Zheng",
            "Carlo Tomasi"
        ],
        "abstract": "Offline tracking of visual objects is particularly helpful in the presence of significant occlusions, when a frame-by-frame, causal tracker is likely to lose sight of the target. In addition, the trajectories found by offline tracking are typically smoother and more stable because of the global optimization this approach entails. In contrast with previous work, we show that this global optimization can be performed in O(MNT) time for T frames of video at M \u00d7 N resolution, with the help of the generalized distance transform developed by Felzenszwalb and Huttenlocher [13]. Recognizing the importance of this distance transform, we extend the computation to a more general lower envelope algorithm in certain heterogeneous l 1 -distance metric spaces. The generalized lower envelope algorithm is of complexity O(MN(M+N)) and is useful for a more challenging offline tracking problem. Experiments show that trajectories found by offline tracking are superior to those computed by online tracking methods, and are computed at 100 frames per second.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126451",
        "reference_list": [
            {
                "year": "2005",
                "id": 92
            },
            {
                "year": "2003",
                "id": 188
            },
            {
                "year": "2007",
                "id": 110
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Transforms",
                "Trajectory",
                "Optimization",
                "Tracking",
                "Complexity theory",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "object tracking",
                "optimisation",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear time offline tracking",
                "lower envelope algorithms",
                "offline visual object tracking",
                "occlusions",
                "global optimization",
                "distance transform"
            ]
        },
        "id": 233,
        "cited_by": [
            {
                "year": "2013",
                "id": 286
            }
        ]
    },
    {
        "title": "Robust and efficient parametric face alignment",
        "authors": [
            "Georgios Tzimiropoulos",
            "Stefanos Zafeiriou",
            "Maja Pantic"
        ],
        "abstract": "We propose a correlation-based approach to parametric object alignment particularly suitable for face analysis applications which require efficiency and robustness against occlusions and illumination changes. Our algorithm registers two images by iteratively maximizing their correlation coefficient using gradient ascent. We compute this correlation coefficient from complex gradients which capture the orientation of image structures rather than pixel intensities. The maximization of this gradient correlation coefficient results in an algorithm which is as computationally efficient as \u2113 2 norm-based algorithms, can be extended within the inverse compositional framework (without the need for Hessian recomputation) and is robust to outliers. To the best of our knowledge, no other algorithm has been proposed so far having all three features. We show the robustness of our algorithm for the problem of face alignment in the presence of occlusions and non-uniform illumination changes. The code that reproduces the results of our paper can be found at http://ibug.doc.ic.ac.uk/resources.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126452",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 13,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Robustness",
                "Correlation",
                "Vectors",
                "Cost function",
                "Lighting",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "correlation methods",
                "face recognition",
                "gradient methods",
                "image resolution",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parametric face alignment",
                "correlation-based approach",
                "parametric object alignment",
                "face analysis",
                "occlusions",
                "illumination changes",
                "correlation coefficient",
                "gradient ascent",
                "image structures",
                "pixel intensities",
                "\u21132 norm-based algorithms"
            ]
        },
        "id": 234,
        "cited_by": []
    },
    {
        "title": "Aerial 3D reconstruction with line-constrained dynamic programming",
        "authors": [
            "Huei-Hung Liao",
            "Yuping Lin",
            "Gerard Medioni"
        ],
        "abstract": "Aerial imagery of an urban environment is often characterized by significant occlusions, sharp edges, and textureless regions, leading to poor 3D reconstruction using conventional multi-view stereo methods. In this paper, we propose a novel approach to 3D reconstruction of urban areas from a set of uncalibrated aerial images. A very general structural prior is assumed that urban scenes consist mostly of planar surfaces oriented either in a horizontal or an arbitrary vertical orientation. In addition, most structural edges associated with such surfaces are also horizontal or vertical. These two assumptions provide powerful constraints on the underlying 3D geometry. The main contribution of this paper is to translate the two constraints on 3D structure into intra-image-column and inter-image-column constraints, respectively, and to formulate the dense reconstruction as a 2-pass Dynamic Programming problem, which is solved in complete parallel on a GPU. The result is an accurate cloud of 3D dense points of the underlying urban scene. Our algorithm completes the reconstruction of 1M points with 160 available discrete height levels in under a hundred seconds. Results on multiple datasets show that we are capable of preserving a high level of structural detail and visual quality.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126453",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2009",
                "id": 10
            },
            {
                "year": "2009",
                "id": 241
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Image reconstruction",
                "Feature extraction",
                "Cameras",
                "Surface reconstruction",
                "Dynamic programming",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "geophysical image processing",
                "graphics processing units",
                "image reconstruction",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "aerial 3D reconstruction",
                "line-constrained dynamic programming",
                "urban environment aerial imagery",
                "multiview stereo methods",
                "structural edges",
                "3D geometry",
                "intra-image-column constraint",
                "interimage-column constraints",
                "2-pass dynamic programming problem",
                "GPU",
                "3D dense points"
            ]
        },
        "id": 235,
        "cited_by": []
    },
    {
        "title": "Multiclass transfer learning from unconstrained priors",
        "authors": [
            "Luo Jie",
            "Tatiana Tommasi",
            "Barbara Caputo"
        ],
        "abstract": "The vast majority of transfer learning methods proposed in the visual recognition domain over the last years addresses the problem of object category detection, assuming a strong control over the priors from which transfer is done. This is a strict condition, as it concretely limits the use of this type of approach in several settings: for instance, it does not allow in general to use off-the-shelf models as priors. Moreover, the lack of a multiclass formulation for most of the existing transfer learning algorithms prevents using them for object categorization problems, where their use might be beneficial, especially when the number of categories grows and it becomes harder to get enough annotated data for training standard learning methods. This paper presents a multiclass transfer learning algorithm that allows to take advantage of priors built over different features and with different learning methods than the one used for learning the new task. We use the priors as experts, and transfer their outputs to the new incoming samples as additional information. We cast the learning problem within the Multi Kernel Learning framework. The resulting formulation solves efficiently a joint optimization problem that determines from where and how much to transfer, with a principled multiclass formulation. Extensive experiments illustrate the value of this approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126454",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 15,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Variable speed drives",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "category theory",
                "learning (artificial intelligence)",
                "object detection",
                "object recognition",
                "optimisation",
                "visual perception"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unconstrained priors",
                "transfer learning methods",
                "visual recognition domain",
                "object category detection",
                "off-the-shelf models",
                "transfer learning algorithms",
                "object categorization problems",
                "annotated data",
                "standard learning methods",
                "multiclass transfer learning algorithm",
                "multikernel learning framework",
                "joint optimization problem",
                "principled multiclass formulation"
            ]
        },
        "id": 236,
        "cited_by": [
            {
                "year": "2015",
                "id": 304
            },
            {
                "year": "2013",
                "id": 274
            }
        ]
    },
    {
        "title": "Level-set person segmentation and tracking with multi-region appearance models and top-down shape information",
        "authors": [
            "Esther Horbert",
            "Konstantinos Rematas",
            "Bastian Leibe"
        ],
        "abstract": "In this paper, we address the problem of segmentation-based tracking of multiple articulated persons. We propose two improvements to current level-set tracking formulations. The first is a localized appearance model that uses additional level-sets in order to enforce a hierarchical subdivision of the object shape into multiple connected regions with distinct appearance models. The second is a novel mechanism to include detailed object shape information in the form of a per-pixel figure/ground probability map obtained from an object detection process. Both contributions are seamlessly integrated into the level-set framework. Together, they considerably improve the accuracy of the tracked segmentations. We experimentally evaluate our proposed approach on two challenging sequences and demonstrate its good performance in practice.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126455",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 10,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Level set",
                "Adaptation models",
                "Target tracking",
                "Color",
                "Robustness",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "object detection",
                "object tracking",
                "set theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "level-set person segmentation",
                "level-set person tracking",
                "multiregion appearance model",
                "top-down shape information",
                "multiple articulated person",
                "localized appearance model",
                "object shape information",
                "figure probability map",
                "ground probability map",
                "object detection process"
            ]
        },
        "id": 237,
        "cited_by": []
    },
    {
        "title": "Segmentation as selective search for object recognition",
        "authors": [
            "Koen E. A. van de Sande",
            "Jasper R. R. Uijlings",
            "Theo Gevers",
            "Arnold W. M. Smeulders"
        ],
        "abstract": "For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7% of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5% for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126456",
        "reference_list": [
            {
                "year": "2009",
                "id": 30
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 145,
            "other": 107,
            "total": 252
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image color analysis",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "object recognition",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "segmentation",
                "selective search",
                "object recognition",
                "exhaustive search",
                "bag-of-words method"
            ]
        },
        "id": 238,
        "cited_by": [
            {
                "year": "2017",
                "id": 204
            },
            {
                "year": "2017",
                "id": 307
            },
            {
                "year": "2015",
                "id": 111
            },
            {
                "year": "2015",
                "id": 126
            },
            {
                "year": "2015",
                "id": 128
            },
            {
                "year": "2015",
                "id": 211
            },
            {
                "year": "2015",
                "id": 225
            },
            {
                "year": "2015",
                "id": 286
            },
            {
                "year": "2015",
                "id": 287
            },
            {
                "year": "2015",
                "id": 318
            },
            {
                "year": "2015",
                "id": 322
            },
            {
                "year": "2015",
                "id": 352
            },
            {
                "year": "2015",
                "id": 373
            },
            {
                "year": "2015",
                "id": 375
            },
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2013",
                "id": 47
            },
            {
                "year": "2013",
                "id": 257
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2013",
                "id": 425
            }
        ]
    },
    {
        "title": "Multiscale, curvature-based shape representation for surfaces",
        "authors": [
            "Ruirui Jiang",
            "Xianfeng Gu"
        ],
        "abstract": "This paper presents a multiscale, curvature-based shape representation technique for general genus zero closed surfaces. The method is invariant under rotation, translation, scaling, or general isometric deformations; it is robust to noise and preserves intrinsic symmetry. The method is a direct generalization of the Curvature Scale Space (CSS) shape descriptor for planar curves. In our method, the Riemannian metric of the surface is deformed under Ricci flow, such that the Gaussian curvature evolves according to a heat diffusion process. Eventually the surface becomes a sphere with constant positive curvature everywhere. The evolution of zero curvature curves on the surface is utilized as the shape descriptor. Our experimental results on a 3D geometric database with about 80 shapes demonstrate the efficiency and efficacy of the method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126457",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Measurement",
                "Three dimensional displays",
                "Surface treatment",
                "Noise",
                "Heating",
                "Cascading style sheets"
            ],
            "INSPEC: Controlled Indexing": [
                "curve fitting",
                "image representation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curvature-based shape representation",
                "general genus zero closed surface",
                "general isometric deformation",
                "intrinsic symmetry",
                "curvature scale space",
                "shape descriptor",
                "planar curves",
                "Riemannian metric",
                "Ricci flow",
                "Gaussian curvature",
                "heat diffusion process",
                "constant positive curvature",
                "curvature curves",
                "3D geometric database"
            ]
        },
        "id": 239,
        "cited_by": []
    },
    {
        "title": "Unstructured light scanning to overcome interreflections",
        "authors": [
            "Vincent Couture",
            "Nicolas Martin",
            "S\u00e9bastien Roy"
        ],
        "abstract": "Reconstruction from structured light can be greatly affected by interreflections between surfaces in the scene. This paper introduces band-pass white noise patterns designed specifically to reduce interreflections, and still be robust to standard challenges in scanning systems such as scene depth discontinuities, defocus and low camera-projector pixel ratio. While this approach uses unstructured light patterns that increase the number of required projected images, it is up to our knowledge the first method that is able to recover scene disparities in the presence of both scene discontinuities and interreflections. Furthermore, the method does not require calibration (geometric nor photometric) or post-processing such as dynamic programming or phase unwrapping. We show results for a challenging scene and compare them to correspondences obtained with the well-known Gray code and Phase-shift methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126458",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 6,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Lighting",
                "Calibration",
                "Robustness",
                "Band pass filters",
                "Noise",
                "Reflective binary codes"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unstructured light scanning",
                "interreflections",
                "band-pass white noise patterns",
                "scanning systems",
                "scene depth discontinuities",
                "low camera-projector pixel ratio",
                "unstructured light patterns",
                "scene discontinuities",
                "gray code",
                "phase-shift method"
            ]
        },
        "id": 240,
        "cited_by": [
            {
                "year": "2013",
                "id": 184
            }
        ]
    },
    {
        "title": "Similarity invariant classification of events by KL divergence minimization",
        "authors": [
            "Salman Khokhar",
            "Imran Saleemi",
            "Mubarak Shah"
        ],
        "abstract": "This paper proposes a novel method for recognition and classification of events represented by Mixture distributions of location and flow. The main idea is to classify observed events into semantically meaningful groups even when motion is observed from distinct viewpoints. Events in the proposed framework are modeled as motion patterns, which are represented by mixtures of multivariate Gaussians, and are obtained by hierarchical clustering of optical flow in the four dimensional space (x, y, u, v). Such motion patterns observed from varying viewpoints, and in distinct locations or datasets, can be compared using different families of divergences between statistical distributions, given that a transformation between the views is known. One of the major contributions of this paper is to compare and match two motion pattern mixture distributions by estimating the similarity transformation between them, that minimizes their Kullback-Leibler (KL) divergence. The KL divergence between Gaussian mixtures is approximated by Monte Carlo sampling, and the minimization is accomplished by employing an iterative nonlinear least squares estimation method, which bears close resemblance to the Iterative Closest Point (ICP) algorithm. We present a robust framework for matching of high-dimensional, sampled point sets representing statistical distributions, by defining similarity measures between them, for global energy minimization. The proposed approach is tested for classification of events observed across several datasets, captured from both static and moving cameras, involving real world pedestrian as well as vehicular motion. Encouraging results are obtained which demonstrate the feasibility and validity of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126459",
        "reference_list": [
            {
                "year": "2009",
                "id": 214
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image classification",
                "iterative methods",
                "least squares approximations",
                "Monte Carlo methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "invariant classification",
                "KL divergence minimization",
                "mixture distributions",
                "distinct viewpoints",
                "multivariate Gaussian mixtures",
                "Kullback-Leibler",
                "Monte Carlo sampling",
                "iterative nonlinear least squares estimation method",
                "iterative closest point",
                "ICP",
                "statistical distributions"
            ]
        },
        "id": 241,
        "cited_by": []
    },
    {
        "title": "Dense one-shot 3D reconstruction by detecting continuous regions with parallel line projection",
        "authors": [
            "Ryusuke Sagawa",
            "Hiroshi Kawasaki",
            "Shota Kiyota",
            "Ryo Furukawa"
        ],
        "abstract": "3D scanning of moving objects has many applications, for example, marker-less motion capture, analysis on fluid dynamics, object explosion and so on. One of the approach to acquire accurate shape is a projector-camera system, especially the methods that reconstructs a shape by using a single image with static pattern is suitable for capturing fast moving object. In this paper, we propose a method that uses a grid pattern consisting of sets of parallel lines. The pattern is spatially encoded by a periodic color pattern. While informations are sparse in the camera image, the proposed method extracts the dense (pixel-wise) phase informations from the sparse pattern. As the result, continuous regions in the camera images can be extracted by analyzing the phase. Since there remain one DOF for each region, we propose the linear solution to eliminate the DOF by using geometric informations of the devices, i.e. epipolar constraint. In addition, solution space is finite because projected pattern consists of parallel lines with same intervals, the linear equation can be efficiently solved by integer least square method. In this paper, the formulations for both single and multiple projectors are presented. We evaluated the accuracy of correspondences and showed the comparison with respect to the number of projectors by simulation. Finally, the dense 3D reconstruction of moving objects are presented in the experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126460",
        "reference_list": [
            {
                "year": "2001",
                "id": 153
            },
            {
                "year": "2009",
                "id": 228
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 8,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image colour analysis",
                "image motion analysis",
                "image reconstruction",
                "least squares approximations",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense one-shot 3D reconstruction",
                "continuous region detection",
                "parallel line projection",
                "moving object scanning",
                "grid pattern",
                "periodic color pattern",
                "camera image extraction",
                "epipolar constraint",
                "linear equation",
                "integer least square method",
                "single projector",
                "multiple projector",
                "moving object reconstruction"
            ]
        },
        "id": 242,
        "cited_by": [
            {
                "year": "2017",
                "id": 12
            },
            {
                "year": "2017",
                "id": 487
            },
            {
                "year": "2015",
                "id": 398
            }
        ]
    },
    {
        "title": "Fourier Active Appearance Models",
        "authors": [
            "Rajitha Navarathna",
            "Sridha Sridharan",
            "Simon Lucey"
        ],
        "abstract": "Gaining invariance to camera and illumination variations has been a well investigated topic in Active Appearance Model (AAM) fitting literature. The major problem lies in the inability of the appearance parameters of the AAM to generalize to unseen conditions. An attractive approach for gaining invariance is to fit an AAM to a multiple filter response (e.g. Gabor) representation of the input image. Naively applying this concept with a traditional AAM is computationally prohibitive, especially as the number of filter responses increase. In this paper, we present a computationally efficient AAM fitting algorithm based on the Lucas-Kanade (LK) algorithm posed in the Fourier domain that affords invariance to both expression and illumination. We refer to this as a Fourier AAM (FAAM), and show that this method gives substantial improvement in person specific AAM fitting performance over traditional AAM fitting methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126461",
        "reference_list": [
            {
                "year": "2009",
                "id": 289
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Shape",
                "Robustness",
                "Jacobian matrices",
                "Vectors",
                "Lighting",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "Fourier transforms",
                "Gabor filters",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Fourier active appearance models",
                "camera",
                "illumination variations",
                "input image multiple filter response representation",
                "AAM fitting algorithm",
                "Lucas-Kanade algorithm"
            ]
        },
        "id": 243,
        "cited_by": []
    },
    {
        "title": "Source constrained clustering",
        "authors": [
            "Ekaterina Taralova",
            "Fernando De la Torre",
            "Martial Hebert"
        ],
        "abstract": "We consider the problem of quantizing data generated from disparate sources, e.g. subjects performing actions with different styles, movies with particular genre bias, various conditions in which images of objects are taken, etc. These are scenarios where unsupervised clustering produces inadequate codebooks because algorithms like K-means tend to cluster samples based on data biases (e.g. cluster subjects), rather than cluster similar samples across sources (e.g. cluster actions). We propose a new quantization technique, Source Constrained Clustering (SCC), which extends the K-means algorithm by enforcing clusters to group samples from multiple sources. We evaluate the method in the context of activity recognition from videos in an unconstrained environment. Experiments on several tasks and features show that using source information improves classification performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126462",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 8,
            "total": 15
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "pattern clustering",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "source constrained clustering",
                "unsupervised clustering",
                "codebooks",
                "quantization technique",
                "K-means algorithm",
                "activity recognition"
            ]
        },
        "id": 244,
        "cited_by": [
            {
                "year": "2017",
                "id": 304
            }
        ]
    },
    {
        "title": "Contour Code: Robust and efficient multispectral palmprint encoding for human recognition",
        "authors": [
            "Zohaib Khan",
            "Ajmal Mian",
            "Yiqun Hu"
        ],
        "abstract": "We propose `Contour Code', a novel representation and binary hash table encoding for multispectral palmprint recognition. We first present a reliable technique for the extraction of a region of interest (ROI) from palm images acquired with non-contact sensors. The Contour Code representation is then derived from the Nonsubsampled Contourlet Transform. A uniscale pyramidal filter is convolved with the ROI followed by the application of a directional filter bank. The dominant directional subband establishes the orientation at each pixel and the index corresponding to this subband is encoded in the Contour Code representation. Unlike existing representations which extract orientation features directly from the palm images, the Contour Code uses a two stage filtering to extract robust orientation features. The Contour Code is binarized into an efficient hash table structure that only requires indexing and summation operations for simultaneous one-to-many matching with an embedded score level fusion of multiple bands. We quantitatively evaluate the accuracy of the ROI extraction by comparison with a manually produced ground truth. Multispectral palmprint verification results on the PolyU and CASIA databases show that the Contour Code achieves an EER reduction upto 50%, compared to state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126463",
        "reference_list": [],
        "citation": {
            "ieee": 29,
            "other": 12,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Databases",
                "Veins",
                "Robustness",
                "Encoding",
                "Vectors",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "palmprint recognition",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contour code representation",
                "multispectral palmprint encoding",
                "human recognition",
                "binary hash table encoding",
                "multispectral palmprint recognition",
                "palm image",
                "noncontact sensors",
                "nonsubsampled contourlet transform",
                "uniscale pyramidal filter",
                "directional filter bank",
                "feature extraction",
                "hash table structure",
                "one-to-many matching",
                "embedded score level fusion",
                "multispectral palmprint verification"
            ]
        },
        "id": 245,
        "cited_by": []
    },
    {
        "title": "Discriminative multi-manifold analysis for face recognition from a single training sample per person",
        "authors": [
            "Jiwen Lu",
            "Yap-Peng Tan",
            "Gang Wang"
        ],
        "abstract": "Conventional appearance-based face recognition methods usually assume there are multiple samples per person (MSPP) available during the training phase for discriminative feature extraction. In many practical face recognition applications such as law enhancement, e-passport and ID card identification, this assumption, however, may not hold as there is only a single sample per person (SSPP) enrolled or recorded in these systems. Many popular face recognition methods fail to work well in this scenario because there are not enough samples for discriminant learning. To address this problem, we propose in this paper a novel discriminative multi-manifold analysis (DMMA) method by learning discriminative features from image patches. First, we partition each enrolled image into several non-overlapping patches to form an image set for each sample per person. Then, we formulate the SSPP face recognition as a manifold-manifold matching problem and learn multiple DMMA feature spaces to maximize the manifold margins of different persons. Lastly, we propose a reconstruction-based manifold-manifold distance to identify the unlabeled subjects. Experimental results on three widely used face databases are presented to demonstrate the efficacy of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126464",
        "reference_list": [
            {
                "year": "2007",
                "id": 16
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 14,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Feature extraction",
                "Training",
                "Face recognition",
                "Face",
                "Eigenvalues and eigenfunctions",
                "Nose"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image matching",
                "image reconstruction",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative multimanifold analysis",
                "appearance-based face recognition",
                "discriminative feature extraction",
                "single sample per person",
                "discriminant learning",
                "discriminative feature learning",
                "image patch",
                "manifold-manifold matching problem",
                "reconstruction-based manifold-manifold distance"
            ]
        },
        "id": 246,
        "cited_by": []
    },
    {
        "title": "Home 3D body scans from noisy image and range data",
        "authors": [
            "Alexander Weiss",
            "David Hirshberg",
            "Michael J. Black"
        ],
        "abstract": "The 3D shape of the human body is useful for applications in fitness, games and apparel. Accurate body scanners, however, are expensive, limiting the availability of 3D body models. We present a method for human shape reconstruction from noisy monocular image and range data using a single inexpensive commodity sensor. The approach combines low-resolution image silhouettes with coarse range data to estimate a parametric model of the body. Accurate 3D shape estimates are obtained by combining multiple monocular views of a person moving in front of the sensor. To cope with varying body pose, we use a SCAPE body model which factors 3D body shape and pose variations. This enables the estimation of a single consistent shape while allowing pose to vary. Additionally, we describe a novel method to minimize the distance between the projected 3D body contour and the image silhouette that uses analytic derivatives of the objective function. We propose a simple method to estimate standard body measurements from the recovered SCAPE model and show that the accuracy of our method is competitive with commercial body scanning systems costing orders of magnitude more.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126465",
        "reference_list": [
            {
                "year": "2009",
                "id": 177
            }
        ],
        "citation": {
            "ieee": 67,
            "other": 72,
            "total": 139
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three dimensional displays",
                "Solid modeling",
                "Cameras",
                "Computational modeling",
                "Optimization",
                "Calibration"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image resolution",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "home 3D body scanning",
                "noisy image",
                "range data",
                "fitness",
                "games",
                "apparel",
                "3D body model",
                "human shape reconstruction",
                "noisy monocular image",
                "commodity sensor",
                "low-resolution image silhouettes",
                "parametric model estimation",
                "3D shape estimation",
                "monocular views",
                "SCAPE body model",
                "3D body contour",
                "body measurement"
            ]
        },
        "id": 247,
        "cited_by": [
            {
                "year": "2015",
                "id": 177
            },
            {
                "year": "2015",
                "id": 243
            },
            {
                "year": "2015",
                "id": 256
            },
            {
                "year": "2013",
                "id": 137
            },
            {
                "year": "2013",
                "id": 194
            }
        ]
    },
    {
        "title": "Random ensemble metrics for object recognition",
        "authors": [
            "Tatsuo Kozakaya",
            "Satoshi Ito",
            "Susumu Kubota"
        ],
        "abstract": "This paper presents a novel and generic approach for metric learning, random ensemble metrics (REMetric). To improve generalization performance, we introduce the concept of ensemble learning to the metric learning scheme. Unlike previous methods, our method does not optimize the global objective function for the whole training data. It learns multiple discriminative projection vectors obtained from linear support vector machines (SVM) using randomly subsampled training data. The final metric matrix is then obtained by integrating these vectors. As a result of using SVM, the learned metric has an excellent scalability for the dimensionality of features. Therefore, it does not require any prior dimensionality reduction techniques such as PCA. Moreover, our method allows us to unify dimensionality reduction and metric learning by controlling the number of the projection vectors. We demonstrate through experiments, that our method can avoid overfitting even though a relatively small number of training data is provided. The experiments are performed with three different datasets; the Viewpoint Invariant Pedestrian Recognition (VIPeR) dataset, the Labeled Face in the Wild (LFW) dataset and the Oxford 102 category flower dataset. The results show that our method achieves equivalent or superior performance compared to existing state-of-the-art metric learning methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126466",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 4,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "pedestrians",
                "random processes",
                "support vector machines",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "metric learning",
                "random ensemble metrics",
                "generalization performance",
                "ensemble learning",
                "global objective function",
                "multiple discriminative projection vectors",
                "linear support vector machine",
                "randomly subsampled training data",
                "scalability",
                "dimensionality reduction",
                "viewpoint invariant pedestrian recognition dataset",
                "labeled face in the wild dataset",
                "Oxford 102 category flower dataset",
                "LFW dataset",
                "VIPeR dataset",
                "object recognition"
            ]
        },
        "id": 248,
        "cited_by": [
            {
                "year": "2013",
                "id": 258
            }
        ]
    },
    {
        "title": "Basis constrained 3D scene flow on a dynamic proxy",
        "authors": [
            "Neil Birkbeck",
            "Dana Cobza\u015f",
            "Martin J\u00e4gersand"
        ],
        "abstract": "Existing scene flow approaches mainly focus on two-frame stereo-pair configurations and reconstruct an image-based representation of scene flow. Instead, we propose a variational formulation of scene flow relative to a coarse proxy geometry, which is better suited for many views. Furthermore, a linear basis is used to represent temporal surface flow, allowing for longer-range temporal correspondence with fewer variables. Our formulation takes known proxy motion into account (e.g, if the proxy is a tracked human subject), which enables 3D trajectory reconstruction when only a single view is available. Additionally, through the appropriate proxy and basis, our framework generalizes existing approaches for scene flow, optic-flow, and two-frame stereo. We illustrate results on real-data for both static and moving proxy surfaces over several frames.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126467",
        "reference_list": [
            {
                "year": "2005",
                "id": 96
            },
            {
                "year": "2009",
                "id": 215
            },
            {
                "year": "2007",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Image reconstruction",
                "Surface reconstruction",
                "Cameras",
                "Optical imaging",
                "Bones",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image representation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "basis constrained 3D scene flow",
                "dynamic proxy",
                "two-frame stereo-pair configuration",
                "image-based representation",
                "variational formulation",
                "coarse proxy geometry",
                "temporal surface flow",
                "proxy motion",
                "3D trajectory reconstruction"
            ]
        },
        "id": 249,
        "cited_by": []
    },
    {
        "title": "Active geodesics: Region-based active contour segmentation with a global edge-based constraint",
        "authors": [
            "Vikram Appia",
            "Anthony Yezzi"
        ],
        "abstract": "We present an active geodesic contour model in which we constrain the evolving active contour to be a geodesic with respect to a weighted edge-based energy through its entire evolution rather than just at its final state (as in the traditional geodesic active contour models). Since the contour is always a geodesic throughout the evolution, we automatically get local optimality with respect to an edge fitting criterion. This enables us to construct a purely region-based energy minimization model without having to devise arbitrary weights in the combination of our energy function to balance edge-based terms with the region-based terms. We show that this novel approach of combining edge information as the geodesic constraint in optimizing a purely region-based energy yields a new class of active contours which exhibit both local and global behaviors that are naturally responsive to intuitive types of user interaction. We also show the relationship of this new class of globally constrained active contours with traditional minimal path methods, which seek global minimizers of purely edge-based energies without incorporating region-based criteria. Finally, we present some numerical examples to illustrate the benefits of this approach over traditional active contour models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126468",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Electric shock",
                "Image segmentation",
                "Active contours",
                "Mathematical model",
                "Level set",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "curve fitting",
                "differential geometry",
                "edge detection",
                "image segmentation",
                "medical image processing",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "region-based active contour segmentation",
                "global edge-based constraint",
                "active geodesic contour model",
                "edge fitting criterion",
                "region-based energy minimization model",
                "region-based energy optimization",
                "user interaction",
                "globally constrained active contours",
                "global minimizer"
            ]
        },
        "id": 250,
        "cited_by": []
    },
    {
        "title": "Localized principal component analysis based curve evolution: A divide and conquer approach",
        "authors": [
            "Vikram Appia",
            "Balaji Ganapathy",
            "Anthony Yezzi",
            "Tracy Faber"
        ],
        "abstract": "We propose a novel localized principal component analysis (PCA) based curve evolution approach which evolves the segmenting curve semi-locally within various target regions (divisions) in an image and then combines these locally accurate segmentation curves to obtain a global segmentation. The training data for our approach consists of training shapes and associated auxiliary (target) masks. The masks indicate the various regions of the shape exhibiting highly correlated variations locally which may be rather independent of the variations in the distant parts of the global shape. Thus, in a sense, we are clustering the variations exhibited in the training data set. We then use a parametric model to implicitly represent each localized segmentation curve as a combination of the local shape priors obtained by representing the training shapes and the masks as a collection of signed distance functions. We also propose a parametric model to combine the locally evolved segmentation curves into a single hybrid (global) segmentation. Finally, we combine the evolution of these semi-local and global parameters to minimize an objective energy function. The resulting algorithm thus provides a globally accurate solution, which retains the local variations in shape. We present some results to illustrate how our approach performs better than the traditional approach with fully global PCA.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126469",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Level set",
                "Image segmentation",
                "Principal component analysis",
                "Training",
                "Training data",
                "Manuals"
            ],
            "INSPEC: Controlled Indexing": [
                "divide and conquer methods",
                "image segmentation",
                "pattern clustering",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "localized principal component analysis",
                "divide and conquer approach",
                "curve evolution approach",
                "global segmentation curve",
                "training shape representation",
                "auxiliary masks",
                "variation clustering",
                "parametric model",
                "signed distance functions",
                "objective energy function minimization"
            ]
        },
        "id": 251,
        "cited_by": []
    },
    {
        "title": "Material-specific user colour profiles from imaging spectroscopy data",
        "authors": [
            "Lin Gu",
            "Cong Phuoc Huynh",
            "Antonio Robles-Kelly",
            "Jun Zhou"
        ],
        "abstract": "In this paper, we present a method which permits the creation of user colour preferences for object materials and lights in the scene making use of imaging spectroscopy data. To do this, we build upon the heterogeneous nature of the scene by imposing consistency over object materials so as to allow for small compositional variations across objects in the image. Once the consistency has been imposed, we aim at maximising the quality of the images under consideration based upon user input. This provides the flexibility necessary to utilise user profiles for the automatic processing of real world imagery while avoiding undesirable effects encountered when colour images are produced. We provide results on real-world imagery and illustrate how the method can be used to produce material-specific colours based upon user input.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126470",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Materials",
                "Image color analysis",
                "Libraries",
                "Equations",
                "Mathematical model",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "user interfaces"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "material-specific user colour profile",
                "imaging spectroscopy data",
                "user colour preference",
                "image quality"
            ]
        },
        "id": 252,
        "cited_by": []
    },
    {
        "title": "Key-segments for video object segmentation",
        "authors": [
            "Yong Jae Lee",
            "Jaechul Kim",
            "Kristen Grauman"
        ],
        "abstract": "We present an approach to discover and segment foreground object(s) in video. Given an unannotated video sequence, the method first identifies object-like regions in any frame according to both static and dynamic cues. We then compute a series of binary partitions among those candidate \u201ckey-segments\u201d to discover hypothesis groups with persistent appearance and motion. Finally, using each ranked hypothesis in turn, we estimate a pixel-level object labeling across all frames, where (a) the foreground likelihood depends on both the hypothesis's appearance as well as a novel localization prior based on partial shape matching, and (b) the background likelihood depends on cues pulled from the key-segments' (possibly diverse) surroundings observed across the sequence. Compared to existing methods, our approach automatically focuses on the persistent foreground regions of interest while resisting oversegmentation. We apply our method to challenging benchmark videos, and show competitive or better results than the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126471",
        "reference_list": [
            {
                "year": "2009",
                "id": 106
            },
            {
                "year": "2009",
                "id": 196
            },
            {
                "year": "2009",
                "id": 99
            },
            {
                "year": "2009",
                "id": 186
            }
        ],
        "citation": {
            "ieee": 164,
            "other": 67,
            "total": 231
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Motion segmentation",
                "Image segmentation",
                "Object segmentation",
                "Image color analysis",
                "Proposals",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image segmentation",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video object segmentation",
                "foreground object",
                "unannotated video sequence",
                "binary partitions",
                "ranked hypothesis",
                "pixel-level object labeling",
                "partial shape matching",
                "background likelihood",
                "oversegmentation"
            ]
        },
        "id": 253,
        "cited_by": [
            {
                "year": "2017",
                "id": 30
            },
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2017",
                "id": 175
            },
            {
                "year": "2017",
                "id": 228
            },
            {
                "year": "2017",
                "id": 379
            },
            {
                "year": "2017",
                "id": 470
            },
            {
                "year": "2017",
                "id": 534
            },
            {
                "year": "2015",
                "id": 184
            },
            {
                "year": "2015",
                "id": 360
            },
            {
                "year": "2015",
                "id": 486
            },
            {
                "year": "2015",
                "id": 492
            },
            {
                "year": "2015",
                "id": 500
            },
            {
                "year": "2013",
                "id": 20
            },
            {
                "year": "2013",
                "id": 196
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2013",
                "id": 279
            },
            {
                "year": "2013",
                "id": 342
            }
        ]
    },
    {
        "title": "Discriminative figure-centric models for joint action localization and recognition",
        "authors": [
            "Tian Lan",
            "Yang Wang",
            "Greg Mori"
        ],
        "abstract": "In this paper we develop an algorithm for action recognition and localization in videos. The algorithm uses a figure-centric visual word representation. Different from previous approaches it does not require reliable human detection and tracking as input. Instead, the person location is treated as a latent variable that is inferred simultaneously with action recognition. A spatial model for an action is learned in a discriminative fashion under a figure-centric representation. Temporal smoothness over video sequences is also enforced. We present results on the UCF-Sports dataset, verifying the effectiveness of our model in situations where detection and tracking of individuals is challenging.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126472",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2009",
                "id": 204
            },
            {
                "year": "2009",
                "id": 62
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 6,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Loss measurement",
                "Humans",
                "Detectors",
                "Vectors",
                "Shape",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "object recognition",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative figure-centric model",
                "action localization",
                "action recognition",
                "figure-centric visual word representation",
                "temporal smoothness",
                "video sequences",
                "UCF-Sports dataset"
            ]
        },
        "id": 254,
        "cited_by": [
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 462
            },
            {
                "year": "2017",
                "id": 466
            },
            {
                "year": "2017",
                "id": 552
            },
            {
                "year": "2017",
                "id": 610
            },
            {
                "year": "2015",
                "id": 353
            },
            {
                "year": "2015",
                "id": 366
            },
            {
                "year": "2015",
                "id": 368
            },
            {
                "year": "2015",
                "id": 403
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2015",
                "id": 512
            },
            {
                "year": "2013",
                "id": 342
            },
            {
                "year": "2013",
                "id": 395
            }
        ]
    },
    {
        "title": "Building a better probabilistic model of images by factorization",
        "authors": [
            "Benjamin J. Culpepper",
            "Jascha Sohl-Dickstein",
            "Bruno A. Olshausen"
        ],
        "abstract": "We describe a directed bilinear model that learns higher-order groupings among features of natural images. The model represents images in terms of two sets of latent variables: one set of variables represents which feature groups are active, while the other specifies the relative activity within groups. Such a factorized representation is beneficial because it is stable in response to small variations in the placement of features while still preserving information about relative spatial relationships. When trained on MNIST digits, the resulting representation provides state of the art performance in classification using a simple classifier. When trained on natural images, the model learns to group features according to proximity in position, orientation, and scale. The model achieves high log-likelihood (-94 nats), surpassing the current state of the art for natural images achievable with an mcRBM model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126473",
        "reference_list": [
            {
                "year": "2003",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Mathematical model",
                "Training",
                "Data models",
                "Computational modeling",
                "Kernel",
                "Accuracy",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "directed bilinear model",
                "higher order grouping",
                "natural image classification",
                "latent variables",
                "factorized representation",
                "relative spatial relationships",
                "MNIST digits",
                "log likelihood",
                "probabilistic model"
            ]
        },
        "id": 255,
        "cited_by": []
    },
    {
        "title": "Adaptive deconvolutional networks for mid and high level feature learning",
        "authors": [
            "Matthew D. Zeiler",
            "Graham W. Taylor",
            "Rob Fergus"
        ],
        "abstract": "We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126474",
        "reference_list": [
            {
                "year": "2009",
                "id": 276
            }
        ],
        "citation": {
            "ieee": 188,
            "other": 110,
            "total": 298
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Switches",
                "Computational modeling",
                "Adaptation models",
                "Mathematical model",
                "Training",
                "Deconvolution"
            ],
            "INSPEC: Controlled Indexing": [
                "deconvolution",
                "feature extraction",
                "image classification",
                "image representation",
                "inference mechanisms",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive deconvolutional networks",
                "mid level feature learning",
                "high level feature learning",
                "hierarchical model",
                "image decompositions",
                "convolutional sparse coding",
                "max pooling",
                "natural images",
                "low-level edges",
                "mid-level edge junctions",
                "high-level object parts",
                "complete objects",
                "inference scheme",
                "Caltech-101 datasets",
                "Caltech-256 datasets",
                "classifier",
                "feature extraction"
            ]
        },
        "id": 256,
        "cited_by": [
            {
                "year": "2017",
                "id": 27
            },
            {
                "year": "2017",
                "id": 88
            },
            {
                "year": "2017",
                "id": 187
            },
            {
                "year": "2017",
                "id": 214
            },
            {
                "year": "2017",
                "id": 449
            },
            {
                "year": "2015",
                "id": 169
            },
            {
                "year": "2015",
                "id": 203
            },
            {
                "year": "2015",
                "id": 306
            },
            {
                "year": "2015",
                "id": 307
            },
            {
                "year": "2015",
                "id": 329
            },
            {
                "year": "2015",
                "id": 370
            },
            {
                "year": "2013",
                "id": 14
            },
            {
                "year": "2013",
                "id": 173
            },
            {
                "year": "2013",
                "id": 256
            }
        ]
    },
    {
        "title": "Detailed reconstruction of 3D plant root shape",
        "authors": [
            "Ying Zheng",
            "Steve Gu",
            "Herbert Edelsbrunner",
            "Carlo Tomasi",
            "Philip Benfey"
        ],
        "abstract": "We study the 3D reconstruction of plant roots from multiple 2D images. To meet the challenge caused by the delicate nature of thin branches, we make three innovations to cope with the sensitivity to image quality and calibration. First, we model the background as a harmonic function to improve the segmentation of the root in each 2D image. Second, we develop the concept of the regularized visual hull which reduces the effect of jittering and refraction by ensuring consistency with one 2D image. Third, we guarantee connectedness through adjustments to the 3D reconstruction that minimize global error. Our software is part of a biological phenotype/genotype study of agricultural root systems. It has been tested on more than 40 plant roots and results are promising in terms of reconstruction quality and efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126475",
        "reference_list": [
            {
                "year": "2001",
                "id": 52
            },
            {
                "year": "2009",
                "id": 103
            },
            {
                "year": "2003",
                "id": 171
            },
            {
                "year": "2007",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 9,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Three dimensional displays",
                "Image reconstruction",
                "Shape",
                "Harmonic analysis",
                "Optimization",
                "Electronic mail"
            ],
            "INSPEC: Controlled Indexing": [
                "biology computing",
                "calibration",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "detailed reconstruction",
                "3D plant root shape",
                "3D reconstruction",
                "2D images",
                "image quality",
                "calibration",
                "harmonic function",
                "root segmentation",
                "regularized visual hull",
                "jittering",
                "refraction",
                "global error",
                "biological phenotype study",
                "biological genotype study",
                "agricultural root systems",
                "reconstruction quality",
                "reconstruction efficiency"
            ]
        },
        "id": 257,
        "cited_by": []
    },
    {
        "title": "Automated corpus callosum extraction via Laplace-Beltrami nodal parcellation and intrinsic geodesic curvature flows on surfaces",
        "authors": [
            "Rongjie Lai",
            "Yonggang Shi",
            "Nancy Sicotte",
            "Arthur W. Toga"
        ],
        "abstract": "Corpus callosum (CC) is an important structure in human brain anatomy. In this work, we propose a fully automated and robust approach to extract corpus callosum from T1-weighted structural MR images. The novelty of our method is composed of two key steps. In the first step, we find an initial guess for the curve representation of CC by using the zero level set of the first nontrivial Laplace-Beltrami (LB) eigenfunction on the white matter surface. In the second step, the initial curve is deformed toward the final solution with a geodesic curvature flow on the white matter surface. For numerical solution of the geodesic curvature flow on surfaces, we represent the contour implicitly on a triangular mesh and develop efficient numerical schemes based on finite element method. Because our method depends only on the intrinsic geometry of the white matter surface, it is robust to orientation differences of the brain across population. In our experiments, we validate the proposed algorithm on 32 brains from a clinical study of multiple sclerosis disease and demonstrate that the accuracy of our results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126476",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Eigenvalues and eigenfunctions",
                "Robustness",
                "Geometry",
                "Sparse matrices",
                "Image segmentation",
                "Multiple sclerosis",
                "Symmetric matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain",
                "feature extraction",
                "image representation",
                "medical image processing",
                "mesh generation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automated corpus callosum extraction",
                "Laplace-Beltrami nodal parcellation",
                "intrinsic geodesic curvature",
                "human brain anatomy",
                "T1-weighted structural MR images",
                "curve representation",
                "zero level set",
                "nontrivial Laplace-Beltrami eigenfunction",
                "geodesic curvature flow",
                "numerical solution",
                "contour implicitly representation",
                "triangular mesh",
                "finite element method",
                "intrinsic geometry",
                "white matter surface",
                "sclerosis disease"
            ]
        },
        "id": 258,
        "cited_by": []
    },
    {
        "title": "Linear dependency modeling for feature fusion",
        "authors": [
            "Andy J H Ma",
            "Pong C Yuen"
        ],
        "abstract": "This paper addresses the independent assumption issue in fusion process. In the last decade, dependency modeling techniques were developed under a specific distribution of classifiers. This paper proposes a new framework to model the dependency between features without any assumption on feature/classifier distribution. In this paper, we prove that feature dependency can be modeled by a linear combination of the posterior probabilities under some mild assumptions. Based on the linear combination property, two methods, namely Linear Classifier Dependency Modeling (LCDM) and Linear Feature Dependency Modeling (LFDM), are derived and developed for dependency modeling in classifier level and feature level, respectively. The optimal models for LCDM and LFDM are learned by maximizing the margin between the genuine and imposter posterior probabilities. Both synthetic data and real datasets are used for experiments. Experimental results show that LFDM outperforms all existing combination methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126477",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Computational modeling",
                "Training",
                "Optimization",
                "Feature extraction",
                "Joints",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image fusion",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature fusion",
                "classifier distribution",
                "posterior probability",
                "linear combination property",
                "linear classifier dependency modeling",
                "linear feature dependency modeling",
                "LCDM model",
                "LFDM model"
            ]
        },
        "id": 259,
        "cited_by": []
    },
    {
        "title": "Text-based image retrieval using progressive multi-instance learning",
        "authors": [
            "Wen Li",
            "Lixin Duan",
            "Dong Xu",
            "Ivor Wai-Hung Tsang"
        ],
        "abstract": "Relevant and irrelevant images collected from the Web (e.g., Flickr.com) have been employed as loosely labeled training data for image categorization and retrieval. In this work, we propose a new approach to learn a robust classifier for text-based image retrieval (TBIR) using relevant and irrelevant training web images, in which we explicitly handle noise in the loose labels of training images. Specifically, we first partition the relevant and irrelevant training web images into clusters. By treating each cluster as a \u201cbag\u201d and the images in each bag as \u201cinstances\u201d, we formulate this task as a multi-instance learning problem with constrained positive bags, in which each positive bag contains at least a portion of positive instances. We present a new algorithm called MIL-CPB to effectively exploit such constraints on positive bags and predict the labels of test instances (images). Observing that the constraints on positive bags may not always be satisfied in our application, we additionally propose a progressive scheme (referred to as Progressive MIL-CPB, or PMIL-CPB) to further improve the retrieval performance, in which we iteratively partition the top-ranked training web images from the current MIL-CPB classifier to construct more confident positive \u201cbags\u201d and then add these new \u201cbags\u201d as training data to learn the subsequent MIL-CPB classifiers. Comprehensive experiments on two challenging real-world web image data sets demonstrate the effectiveness of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126478",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 15,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image retrieval",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "text-based image retrieval",
                "progressive multiinstance learning",
                "robust classifier",
                "Web image",
                "constrained positive bags",
                "positive instance",
                "MIL-CPB algorithm",
                "image categorization"
            ]
        },
        "id": 260,
        "cited_by": [
            {
                "year": "2017",
                "id": 76
            }
        ]
    },
    {
        "title": "Handling label noise in video classification via multiple instance learning",
        "authors": [
            "Thomas Leung",
            "Yang Song",
            "John Zhang"
        ],
        "abstract": "In many classification tasks, the use of expert-labeled data for training is often prohibitively expensive. The use of weakly-labeled data is an attractive solution but raises the problem of label noise. Multiple instance learning, whereby training samples are \u201cbagged\u201d instead of treated as singletons, offers a possible approach to mitigating the effects of label noise. In this paper, we propose the use of MILBoost [28] in a large-scale video taxonomic classification system comprised of hundreds of binary classifiers to handle noisy training data. We test on data with both artificial and real-world noise and compare against the state-of-the-art classifiers based on AdaBoost. We also explore the effects of different bag sizes on different levels of noise on the final classifier performance. Experiments show that when training classifiers with noisy data, MILBoost provides an improvement in performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126479",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2009",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 8,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise",
                "Training",
                "Feature extraction",
                "Noise level",
                "Taxonomy",
                "Training data",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "label noise",
                "video classification",
                "multiple instance learning",
                "expert-labeled data",
                "MILBoost",
                "large-scale video taxonomic classification system",
                "binary classifiers",
                "noisy training data"
            ]
        },
        "id": 261,
        "cited_by": [
            {
                "year": "2013",
                "id": 91
            }
        ]
    },
    {
        "title": "Physically-based motion models for 3D tracking: A convex formulation",
        "authors": [
            "Mathieu Salzmann",
            "Raquel Urtasun"
        ],
        "abstract": "In this paper, we propose a physically-based dynamical model for tracking. Our model relies on Newton's second law of motion, which governs any real-world dynamical system. As a consequence, it can be generally applied to very different tracking problems. Furthermore, since the equations describing Newton's second law are simple linear equalities, they can be incorporated in any tracking framework at very little cost. Leveraging this lets us introduce a convex formulation of 3D tracking from monocular images. We demonstrate the strengths of our approach on various types of motion, such as billiards and acrobatics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126480",
        "reference_list": [
            {
                "year": "2009",
                "id": 307
            },
            {
                "year": "2001",
                "id": 148
            },
            {
                "year": "2007",
                "id": 276
            },
            {
                "year": "2005",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 13,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Tracking",
                "Markov processes",
                "Computational modeling",
                "Mathematical model",
                "Humans",
                "Gravity"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image motion analysis",
                "linear matrix inequalities",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "physically-based motion model",
                "3D tracking framework",
                "convex formulation",
                "Newton second law of motion",
                "real-world dynamical system",
                "linear equalities",
                "monocular images"
            ]
        },
        "id": 262,
        "cited_by": [
            {
                "year": "2015",
                "id": 84
            }
        ]
    },
    {
        "title": "Discriminative learning of relaxed hierarchy for large-scale visual recognition",
        "authors": [
            "Tianshi Gao",
            "Daphne Koller"
        ],
        "abstract": "In the real visual world, the number of categories a classifier needs to discriminate is on the order of hundreds or thousands. For example, the SUN dataset [24] contains 899 scene categories and ImageNet [6] has 15,589 synsets. Designing a multiclass classifier that is both accurate and fast at test time is an extremely important problem in both machine learning and computer vision communities. To achieve a good trade-off between accuracy and speed, we adopt the relaxed hierarchy structure from [15], where a set of binary classifiers are organized in a tree or DAG (directed acyclic graph) structure. At each node, classes are colored into positive and negative groups which are separated by a binary classifier while a subset of confusing classes is ignored. We color the classes and learn the induced binary classifier simultaneously using a unified and principled max-margin optimization. We provide an analysis on generalization error to justify our design. Our method has been tested on both Caltech-256 (object recognition) [9] and the SUN dataset (scene classification) [24], and shows significant improvement over existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126481",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 3,
            "total": 17
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "directed graphs",
                "image classification",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative learning",
                "large-scale visual recognition",
                "SUN dataset",
                "ImageNet",
                "multiclass classifier",
                "machine learning",
                "computer vision communities",
                "relaxed hierarchy structure",
                "binary classifiers",
                "directed acyclic graph structure",
                "max-margin optimization",
                "generalization error analysis",
                "Caltech-256 dataset",
                "object recognition",
                "scene classification"
            ]
        },
        "id": 263,
        "cited_by": [
            {
                "year": "2015",
                "id": 305
            },
            {
                "year": "2013",
                "id": 33
            },
            {
                "year": "2013",
                "id": 259
            }
        ]
    },
    {
        "title": "Realtime multibody visual SLAM with a smoothly moving monocular camera",
        "authors": [
            "Abhijit Kundu",
            "K Madhava Krishna",
            "C. V. Jawahar"
        ],
        "abstract": "This paper presents a realtime, incremental multibody visual SLAM system that allows choosing between full 3D reconstruction or simply tracking of the moving objects. Motion reconstruction of dynamic points or objects from a monocular camera is considered very hard due to well known problems of observability. We attempt to solve the problem with a Bearing only Tracking (BOT) and by integrating multiple cues to avoid observability issues. The BOT is accomplished through a particle filter, and by integrating multiple cues from the reconstruction pipeline. With the help of these cues, many real world scenarios which are considered unobservable with a monocular camera is solved to reasonable accuracy. This enables building of a unified dynamic 3D map of scenes involving multiple moving objects. Tracking and reconstruction is preceded by motion segmentation and detection which makes use of efficient geometric constraints to avoid difficult degenerate motions, where objects move in the epipolar plane. Results reported on multiple challenging real world image sequences verify the efficacy of the proposed framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126482",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 7,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Tracking",
                "Simultaneous localization and mapping",
                "Three dimensional displays",
                "Visualization",
                "Image reconstruction",
                "Motion segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "image segmentation",
                "image sequences",
                "motion estimation",
                "object tracking",
                "observability",
                "particle filtering (numerical methods)",
                "robot vision",
                "SLAM (robots)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "realtime multibody visual SLAM system",
                "smoothly moving monocular camera",
                "incremental multibody visual SLAM system",
                "3D reconstruction",
                "moving object tracking",
                "motion reconstruction",
                "observability problem",
                "bearing only tracking",
                "particle filter",
                "unified dynamic 3D map",
                "geometric constraints",
                "epipolar plane",
                "real world image sequences",
                "motion segmentation",
                "motion detection"
            ]
        },
        "id": 264,
        "cited_by": []
    },
    {
        "title": "Full DOF tracking of a hand interacting with an object by modeling occlusions and physical constraints",
        "authors": [
            "Iason Oikonomidis",
            "Nikolaos Kyriazis",
            "Antonis A. Argyros"
        ],
        "abstract": "Due to occlusions, the estimation of the full pose of a human hand interacting with an object is much more challenging than pose recovery of a hand observed in isolation. In this work we formulate an optimization problem whose solution is the 26-DOF hand pose together with the pose and model parameters of the manipulated object. Optimization seeks for the joint hand-object model that (a) best explains the incompleteness of observations resulting from occlusions due to hand-object interaction and (b) is physically plausible in the sense that the hand does not share the same physical space with the object. The proposed method is the first that solves efficiently the continuous, full-DOF, joint hand-object tracking problem based solely on markerless multicamera input. Additionally, it is the first to demonstrate how hand-object interaction can be exploited as a context that facilitates hand pose estimation, instead of being considered as a complicating factor. Extensive quantitative and qualitative experiments with simulated and real world image sequences as well as a comparative evaluation with a state-of-the-art method for pose estimation of isolated hands, support the above findings.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126483",
        "reference_list": [
            {
                "year": "2009",
                "id": 189
            },
            {
                "year": "2001",
                "id": 51
            }
        ],
        "citation": {
            "ieee": 64,
            "other": 55,
            "total": 119
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "object tracking",
                "optimisation",
                "pose estimation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "full DOF tracking",
                "hand-object interaction",
                "occlusions modeling",
                "physical constraints modeling",
                "optimization",
                "hand-object tracking",
                "hand pose estimation"
            ]
        },
        "id": 265,
        "cited_by": [
            {
                "year": "2017",
                "id": 121
            },
            {
                "year": "2017",
                "id": 329
            },
            {
                "year": "2015",
                "id": 81
            },
            {
                "year": "2015",
                "id": 91
            },
            {
                "year": "2015",
                "id": 344
            },
            {
                "year": "2015",
                "id": 370
            },
            {
                "year": "2013",
                "id": 306
            },
            {
                "year": "2013",
                "id": 402
            }
        ]
    },
    {
        "title": "Convex multi-region probabilistic segmentation with shape prior in the isometric log-ratio transformation space",
        "authors": [
            "Shawn Andrews",
            "Chris McIntosh",
            "Ghassan Hamarneh"
        ],
        "abstract": "Image segmentation is often performed via the minimization of an energy function over a domain of possible segmentations. The effectiveness and applicability of such methods depends greatly on the properties of the energy function and its domain, and on what information can be encoded by it. Here we propose an energy function that achieves several important goals. Specifically, our energy function is convex and incorporates shape prior information while simultaneously generating a probabilistic segmentation for multiple regions. Our energy function represents multi-region probabilistic segmentations as elements of a vector space using the isometric log-ratio (ILR) transformation. To our knowledge, these four goals (convex, with shape priors, multi-region, and probabilistic) do not exist together in any other method, and this is the first time ILR is used in an image segmentation method. We provide examples demonstrating the usefulness of these features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126484",
        "reference_list": [
            {
                "year": "2009",
                "id": 36
            },
            {
                "year": "2009",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image segmentation",
                "Probabilistic logic",
                "Vectors",
                "Training data",
                "Principal component analysis",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convex multiregion probabilistic segmentation",
                "isometric log-ratio transformation space",
                "image segmentation",
                "energy function"
            ]
        },
        "id": 266,
        "cited_by": []
    },
    {
        "title": "A dimensionality result for multiple homography matrices",
        "authors": [
            "Wojciech Chojnacki",
            "Anton van den Hengel"
        ],
        "abstract": "It is shown that the set of all I-element collections of interdependent homography matrices describing homographies induced by I planes in the 3D scene between two views has dimension 4I + 7. This improves on an earlier result which gave an upper bound for the dimension in question, and solves a long-standing open problem. The significance of the present result lies in that it is critical to the identification of the full set of constraints to which collections of interdependent homography matrices are subject, which in turn is critical to the design of constrained optimisation techniques for estimating such collections from image data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126485",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Transmission line matrix methods",
                "Vectors",
                "Arrays",
                "Polynomials",
                "Matrix converters",
                "Three dimensional displays",
                "Manifolds"
            ],
            "INSPEC: Controlled Indexing": [
                "matrix algebra",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "homography matrix",
                "I-element collection",
                "3D scene",
                "constrained optimisation technique"
            ]
        },
        "id": 267,
        "cited_by": []
    },
    {
        "title": "Image segmentation by figure-ground composition into maximal cliques",
        "authors": [
            "Adrian Ion",
            "Joao Carreira",
            "Cristian Sminchisescu"
        ],
        "abstract": "We propose a mid-level statistical model for image segmentation that composes multiple figure-ground hypotheses (FG) obtained by applying constraints at different locations and scales, into larger interpretations (tilings) of the entire image. Inference is cast as optimization over sets of maximal cliques sampled from a graph connecting all non-overlapping figure-ground segment hypotheses. Potential functions over cliques combine unary, Gestalt-based figure qualities, and pairwise compatibilities among spatially neighboring segments, constrained by T-junctions and the boundary interface statistics of real scenes. Learning the model parameters is based on maximum likelihood, alternating between sampling image tilings and optimizing their potential function parameters. State of the art results are reported on the Berkeley and Stanford segmentation datasets, as well as VOC2009, where a 28% improvement was achieved.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126486",
        "reference_list": [
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2007",
                "id": 144
            },
            {
                "year": "2009",
                "id": 1
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2003",
                "id": 1
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 12,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Computational modeling",
                "Junctions",
                "Image edge detection",
                "Optimization",
                "Approximation methods",
                "Complexity theory"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image sampling",
                "image segmentation",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "figure-ground composition",
                "maximal cliques",
                "mid-level statistical model",
                "figure-ground hypothesis",
                "inference",
                "graph",
                "cliques combine unary",
                "Gestalt-based figure quality",
                "pairwise compatibility",
                "T-junctions",
                "boundary interface statistics",
                "maximum likelihood",
                "image tiling sampling",
                "Berkeley segmentation datasets",
                "Stanford segmentation datasets",
                "VOC2009"
            ]
        },
        "id": 268,
        "cited_by": [
            {
                "year": "2015",
                "id": 352
            }
        ]
    },
    {
        "title": "HEAT: Iterative relevance feedback with one million images",
        "authors": [
            "Nicolae Suditu",
            "Fran\u00e7ois Fleuret"
        ],
        "abstract": "It has been shown repeatedly that iterative relevance feedback is a very efficient solution for content-based image retrieval. However, no existing system scales gracefully to hundreds of thousands or millions of images. We present a new approach dubbed Hierarchical and Expandable Adaptive Trace (HEAT) to tackle this problem. Our approach modulates on-the-fly the resolution of the interactive search in different parts of the image collection, by relying on a hierarchical organization of the images computed off-line. Internally, the strategy is to maintain an accurate approximation of the probabilities of relevance of the individual images while fixing an upper bound on the re- quired computation. Our system is compared on the ImageNet database to the state-of-the-art approach it extends, by conducting user evaluations on a sub-collection of 33,000 images. Its scalability is then demonstrated by conducting similar evaluations on 1,000,000 images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126487",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 4,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Approximation methods",
                "Image resolution",
                "Organizations",
                "Bayesian methods",
                "Approximation algorithms",
                "Computational modeling",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "image retrieval",
                "relevance feedback"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "HEAT",
                "iterative relevance feedback",
                "one million images",
                "content-based image retrieval",
                "hierarchical and expandable adaptive trace",
                "interactive search",
                "image collection",
                "upper bound",
                "ImageNet database"
            ]
        },
        "id": 269,
        "cited_by": []
    },
    {
        "title": "Dense disparity maps from sparse disparity measurements",
        "authors": [
            "Simon Hawe",
            "Martin Kleinsteuber",
            "Klaus Diepold"
        ],
        "abstract": "In this work we propose a method for estimating disparity maps from very few measurements. Based on the theory of Compressive Sensing, our algorithm accurately reconstructs disparity maps only using about 5% of the entire map. We propose a conjugate subgradient method for the arising optimization problem that is applicable to large scale systems and recovers the disparity map efficiently. Experiments are provided that show the effectiveness of the proposed approach and robust behavior under noisy conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126488",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 9,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Vectors",
                "Wavelet transforms",
                "Compressed sensing",
                "Equations",
                "Coherence"
            ],
            "INSPEC: Controlled Indexing": [
                "conjugate gradient methods",
                "image reconstruction",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense disparity map estimation",
                "sparse disparity measurements",
                "compressive sensing theory",
                "disparity map reconstruction",
                "conjugate subgradient method",
                "optimization problem"
            ]
        },
        "id": 270,
        "cited_by": []
    },
    {
        "title": "Geometrically consistent elastic matching of 3D shapes: A linear programming solution",
        "authors": [
            "Thomas Windheuser",
            "Ulrich Schlickewei",
            "Frank R. Schmidt",
            "Daniel Cremers"
        ],
        "abstract": "We propose a novel method for computing a geometrically consistent and spatially dense matching between two 3D shapes. Rather than mapping points to points we match infinitesimal surface patches while preserving the geometric structures. In this spirit we consider matchings as diffeomorphisms between the objects' surfaces which are by definition geometrically consistent. Based on the observation that such diffeomorphisms can be represented as closed and continuous surfaces in the product space of the two shapes we are led to a minimal surface problem in this product space. The proposed discrete formulation describes the search space with linear constraints. Computationally, our approach leads to a binary linear program whose relaxed version can be solved efficiently in a globally optimal manner. As cost function for matching, we consider a thin shell energy, measuring the physical energy necessary to deform one shape into the other. Experimental results demonstrate that the proposed LP relaxation allows to compute highquality matchings which reliably put into correspondence articulated 3D shapes. Moreover a quantitative evaluation shows improvements over existing works.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126489",
        "reference_list": [
            {
                "year": "2007",
                "id": 178
            },
            {
                "year": "2009",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 16,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Image edge detection",
                "Three dimensional displays",
                "Tensile stress",
                "Vectors",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "deformation",
                "linear programming",
                "search problems",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometrically consistent elastic matching",
                "3D shape mapping",
                "spatially dense matching",
                "infinitesimal surface patches",
                "object surface",
                "diffeomorphism",
                "continuous surface",
                "closed surface",
                "product space",
                "discrete formulation",
                "search space",
                "linear constraint",
                "linear programming",
                "cost function",
                "thin shell energy",
                "shape deformation"
            ]
        },
        "id": 271,
        "cited_by": [
            {
                "year": "2015",
                "id": 227
            },
            {
                "year": "2013",
                "id": 145
            },
            {
                "year": "2013",
                "id": 419
            }
        ]
    },
    {
        "title": "Object detection and segmentation from joint embedding of parts and pixels",
        "authors": [
            "Michael Maire",
            "Stella X. Yu",
            "Pietro Perona"
        ],
        "abstract": "We present a new framework in which image segmentation, figure/ground organization, and object detection all appear as the result of solving a single grouping problem. This framework serves as a perceptual organization stage that integrates information from low-level image cues with that of high-level part detectors. Pixels and parts each appear as nodes in a graph whose edges encode both affinity and ordering relationships. We derive a generalized eigen-problem from this graph and read off an interpretation of the image from the solution eigenvectors. Combining an off-the-shelf top-down part-based person detector with our low-level cues and grouping formulation, we demonstrate improvements to object detection and segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126490",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2009",
                "id": 254
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 9,
            "total": 27
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object detection",
                "object segmentation",
                "image segmentation",
                "figure/ground organization",
                "single grouping problem",
                "perceptual organization",
                "low-level image cues",
                "high-level part detectors",
                "ordering relationships",
                "generalized eigen-problem",
                "eigenvectors"
            ]
        },
        "id": 272,
        "cited_by": [
            {
                "year": "2015",
                "id": 175
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2013",
                "id": 272
            },
            {
                "year": "2013",
                "id": 288
            }
        ]
    },
    {
        "title": "Correspondence free registration through a point-to-model distance minimization",
        "authors": [
            "Mohammad Rouhani",
            "Angel D. Sappa"
        ],
        "abstract": "This paper presents a novel formulation, which derives in a smooth minimization problem, to tackle the rigid registration between a given point set and a model set. Unlike most of the existing works, which are based on minimizing a point-wise correspondence term, we propose to describe the model set by means of an implicit representation. It allows a new definition of the registration error, which works beyond the point level representation. Moreover, it could be used in a gradient-based optimization framework. The proposed approach consists of two stages. Firstly, a novel formulation is proposed that relates the registration parameters with the distance between the model and data set. Secondly, the registration parameters are obtained by means of the Levengberg-Marquardt algorithm. Experimental results and comparisons with state of the art show the validity of the proposed framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126491",
        "reference_list": [
            {
                "year": "2009",
                "id": 171
            },
            {
                "year": "2005",
                "id": 163
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Computational modeling",
                "IP networks",
                "Polynomials",
                "Vectors",
                "Approximation methods",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image registration",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "correspondence free registration",
                "point-to-model distance minimization",
                "smooth minimization problem",
                "point set",
                "model set",
                "point-wise correspondence term minimization",
                "implicit representation",
                "point level representation",
                "gradient-based optimization framework",
                "Levenberg-Marquardt algorithm"
            ]
        },
        "id": 273,
        "cited_by": []
    },
    {
        "title": "High quality image reconstruction from RAW and JPEG image pair",
        "authors": [
            "Lu Yuan",
            "Jian Sun"
        ],
        "abstract": "A camera RAW file contains minimally processed data from the image sensor. The contents of the RAW file include more information, and potentially higher quality, than the commonly used JPEG file. But the RAW file is typically several times larger than the JPEG file (taking fewer images, slower quick shooting) and lacks the standard file format (not ready-to-use, prolonging the image workflow). These drawbacks limit its applications. In this paper, we suggest a new \u201chybrid\u201d image capture mode: a high-res JPEG file and a low-res RAW file as alternative of the original RAW file. Most RAW users can be benefited from such a combination. To address this problem, we provide an effective approach to reconstruct a high quality image by combining the advantages of two kinds of files. We formulate this reconstruction process as a global optimization problem by enforcing two constraints: reconstruction constraint and detail consistency constraint. The final recovered image is smaller than the full-res RAW file, enables faster quick shooting, and has both richer information (e.g., color space, dynamic range, lossless 14 bits data) and higher resolution. In practice, the functionality of capturing such a \u201chybrid\u201d image pair in one-shot has been supported in some existing digital cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126492",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 4,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Transform coding",
                "Image reconstruction",
                "Histograms",
                "Image color analysis",
                "Spatial resolution",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image coding",
                "image reconstruction",
                "image resolution",
                "image sensors",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image reconstruction",
                "RAW image pair",
                "JPEG image pair",
                "camera RAW file",
                "image sensor",
                "standard file format",
                "image workflow",
                "hybrid image capture mode",
                "high-res JPEG file",
                "low-res RAW file",
                "reconstruction process",
                "global optimization problem",
                "reconstruction constraint",
                "consistency constraint",
                "recovered image",
                "image resolution",
                "hybrid image pair",
                "digital cameras"
            ]
        },
        "id": 274,
        "cited_by": []
    },
    {
        "title": "iGroup: Weakly supervised image and video grouping",
        "authors": [
            "Andrew Gilbert",
            "Richard Bowden"
        ],
        "abstract": "We present a generic, efficient and iterative algorithm for interactively clustering classes of images and videos. The approach moves away from the use of large hand labelled training datasets, instead allowing the user to find natural groups of similar content based upon a handful of \u201cseed\u201d examples. Two efficient data mining tools originally developed for text analysis; min-Hash and APriori are used and extended to achieve both speed and scalability on large image and video datasets. Inspired by the Bag-of-Words (BoW) architecture, the idea of an image signature is introduced as a simple descriptor on which nearest neighbour classification can be performed. The image signature is then dynamically expanded to identify common features amongst samples of the same class. The iterative approach uses APriori to identify common and distinctive elements of a small set of labelled true and false positive signatures. These elements are then accentuated in the signature to increase similarity between examples and \u201cpull\u201d positive classes together. By repeating this process, the accuracy of similarity increases dramatically despite only a few training examples, only 10% of the labelled groundtruth is needed, compared to other approaches. It is tested on two image datasets including the caltech101 [9] dataset and on three state-of-the-art action recognition datasets. On the YouTube [18] video dataset the accuracy increases from 72% to 97% using only 44 labelled examples from a dataset of over 1200 videos. The approach is both scalable and efficient, with an iteration on the full YouTube dataset taking around 1 minute on a standard desktop machine.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126493",
        "reference_list": [
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2007",
                "id": 67
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Histograms",
                "Visualization",
                "YouTube",
                "Association rules",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "image classification",
                "iterative methods",
                "pattern clustering",
                "text analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "iGroup",
                "weakly supervised image",
                "video grouping",
                "iterative algorithm",
                "image clustering",
                "video clustering",
                "data mining tool",
                "text analysis",
                "min-hash",
                "APriori",
                "bag-of-words architecture",
                "image signature",
                "nearest neighbour classification",
                "YouTube video dataset"
            ]
        },
        "id": 275,
        "cited_by": []
    },
    {
        "title": "Generalized background subtraction based on hybrid inference by belief propagation and Bayesian filtering",
        "authors": [
            "Suha Kwak",
            "Taegyu Lim",
            "Woonhyun Nam",
            "Bohyung Han",
            "Joon Hee Han"
        ],
        "abstract": "We propose a novel background subtraction algorithm for the videos captured by a moving camera. In our technique, foreground and background appearance models in each frame are constructed and propagated sequentially by Bayesian filtering. We estimate the posterior of appearance, which is computed by the product of the image likelihood in the current frame and the prior appearance propagated from the previous frame. The motion, which transfers the previous appearance models to the current frame, is estimated by nonparametric belief propagation; the initial motion field is obtained by optical flow and noisy and incomplete motions are corrected effectively through the inference procedure. Our framework is represented by a graphical model, where the sequential inference of motion and appearance is performed by the combination of belief propagation and Bayesian filtering. We compare our algorithm with the existing state-of-the-art technique and evaluate its performance quantitatively and qualitatively in several challenging videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126494",
        "reference_list": [
            {
                "year": "2003",
                "id": 8
            },
            {
                "year": "2009",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 7,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Estimation",
                "Predictive models",
                "Bayesian methods",
                "Random variables",
                "Videos",
                "Motion estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "image motion analysis",
                "image processing",
                "image representation",
                "image sequences",
                "maximum likelihood estimation",
                "video cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hybrid inference",
                "sequential motion inference",
                "graphical model representation",
                "optical flow",
                "nonparametric belief propagation",
                "image likelihood",
                "moving camera",
                "background subtraction algorithm",
                "Bayesian filtering"
            ]
        },
        "id": 276,
        "cited_by": [
            {
                "year": "2017",
                "id": 539
            },
            {
                "year": "2015",
                "id": 354
            },
            {
                "year": "2013",
                "id": 103
            },
            {
                "year": "2013",
                "id": 196
            }
        ]
    },
    {
        "title": "Color photometric stereo for multicolored surfaces",
        "authors": [
            "Robert Anderson",
            "Bj\u00f6rn Stenger",
            "Roberto Cipolla"
        ],
        "abstract": "We present a multispectral photometric stereo method for capturing geometry of deforming surfaces. A novel photometric calibration technique allows calibration of scenes containing multiple piecewise constant chromaticities. This method estimates per-pixel photometric properties, then uses a RANSAC-based approach to estimate the dominant chromaticities in the scene. A likelihood term is developed linking surface normal, image intensity and photometric properties, which allows estimating the number of chromaticities present in a scene to be framed as a model estimation problem. The Bayesian Information Criterion is applied to automatically estimate the number of chromaticities present during calibration. A two-camera stereo system provides low resolution geometry, allowing the likelihood term to be used in segmenting new images into regions of constant chromaticity. This segmentation is carried out in a Markov Random Field framework and allows the correct photometric properties to be used at each pixel to estimate a dense normal map. Results are shown on several challenging real-world sequences, demonstrating state-of-the-art results using only two cameras and three light sources. Quantitative evaluation is provided against synthetic ground truth data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126495",
        "reference_list": [
            {
                "year": "2007",
                "id": 100
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 5,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Calibration",
                "Image segmentation",
                "Geometry",
                "Stereo image processing",
                "Lighting",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "calibration",
                "cameras",
                "computational geometry",
                "image colour analysis",
                "image sequences",
                "Markov processes",
                "photometry",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color photometric stereo",
                "multicolored surfaces",
                "multispectral photometric stereo method",
                "deforming surface geometry",
                "photometric calibration technique",
                "RANSAC based approach",
                "dominant chromaticities",
                "Bayesian information criterion",
                "two camera stereo system",
                "low resolution geometry",
                "Markov random field framework",
                "real world sequences"
            ]
        },
        "id": 277,
        "cited_by": [
            {
                "year": "2015",
                "id": 94
            }
        ]
    },
    {
        "title": "Structured class-labels in random forests for semantic image labelling",
        "authors": [
            "Peter Kontschieder",
            "Samuel Rota Bul\u00f2",
            "Horst Bischof",
            "Marcello Pelillo"
        ],
        "abstract": "In this paper we propose a simple and effective way to integrate structural information in random forests for semantic image labelling. By structural information we refer to the inherently available, topological distribution of object classes in a given image. Different object class labels will not be randomly distributed over an image but usually form coherently labelled regions. In this work we provide a way to incorporate this topological information in the popular random forest framework for performing low-level, unary classification. Our paper has several contributions: First, we show how random forests can be augmented with structured label information. In the second part, we introduce a novel data splitting function that exploits the joint distributions observed in the structured label space for learning typical label transitions between object classes. Finally, we provide two possibilities for integrating the structured output predictions into concise, semantic labellings. In our experiments on the challenging MSRC and CamVid databases, we compare our method to standard random forest and conditional random field classification results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126496",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2009",
                "id": 64
            },
            {
                "year": "2007",
                "id": 145
            }
        ],
        "citation": {
            "ieee": 62,
            "other": 28,
            "total": 90
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Training",
                "Vegetation",
                "Training data",
                "Joints",
                "Semantics",
                "Decision trees"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "conditional random field classification",
                "standard random forest",
                "CamVid databases",
                "MSRC databases",
                "label transition learning",
                "joint distributions",
                "data splitting function",
                "structured label information",
                "unary classification",
                "structural information",
                "semantic image labelling",
                "structured class-labels"
            ]
        },
        "id": 278,
        "cited_by": [
            {
                "year": "2017",
                "id": 573
            },
            {
                "year": "2015",
                "id": 35
            },
            {
                "year": "2015",
                "id": 54
            },
            {
                "year": "2015",
                "id": 183
            },
            {
                "year": "2015",
                "id": 250
            },
            {
                "year": "2013",
                "id": 24
            },
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2013",
                "id": 164
            },
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2013",
                "id": 230
            }
        ]
    },
    {
        "title": "Delta-Dual Hierarchical Dirichlet Processes: A pragmatic abnormal behaviour detector",
        "authors": [
            "Tom S.F. Haines",
            "Tao Xiang"
        ],
        "abstract": "In the security domain a key problem is identifying rare behaviours of interest. Training examples for these behaviours may or may not exist, and if they do exist there will be few examples, quite probably one. We present a novel weakly supervised algorithm that can detect behaviours that either have never before been seen or for which there are few examples. Global context is modelled, allowing the detection of abnormal behaviours that in isolation appear normal. Pragmatic aspects are considered, such that no parameter tuning is required and real time performance is achieved.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126497",
        "reference_list": [
            {
                "year": "2009",
                "id": 149
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 2,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Mathematical model",
                "Context modeling",
                "Context",
                "Equations",
                "Bayesian methods",
                "Training",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "behavioural sciences computing",
                "learning (artificial intelligence)",
                "security",
                "stochastic processes",
                "video signal processing",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "delta-dual hierarchical Dirichlet process",
                "abnormal behaviour detector",
                "security",
                "weakly supervised algorithm",
                "automated video surveillance"
            ]
        },
        "id": 279,
        "cited_by": []
    },
    {
        "title": "Content-based photo quality assessment",
        "authors": [
            "Wei Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Automatically assessing photo quality from the perspective of visual aesthetics is of great interest in high-level vision research and has drawn much attention in recent years. In this paper, we propose content-based photo quality assessment using regional and global features. Under this framework, subject areas, which draw the most attentions of human eyes, are first extracted. Then regional features extracted from subject areas and the background regions are combined with global features to assess the photo quality. Since professional photographers may adopt different photographic techniques and may have different aesthetic criteria in mind when taking different types of photos (e.g. landscape versus portrait), we propose to segment regions and extract visual features in different ways according to the categorization of photo content. Therefore we divide the photos into seven categories based on their content and develop a set of new subject area extraction methods and new visual features, which are specially designed for different categories. This argument is supported by extensive experimental comparisons of existing photo quality assessment approaches as well as our new regional and global features over different categories of photos. Our new features significantly outperform the state-of-the-art methods. Another contribution of this work is to construct a large and diversified benchmark database for the research of photo quality assessment. It includes 17, 613 photos with manually labeled ground truth.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126498",
        "reference_list": [
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2007",
                "id": 203
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 20,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Image color analysis",
                "Humans",
                "Lighting",
                "Brightness",
                "Layout",
                "Quality assessment"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "content-based photo quality assessment",
                "visual aesthetics",
                "high-level vision research",
                "regional feature extraction",
                "global features",
                "background regions",
                "region segmentation",
                "visual feature extraction",
                "subject area extraction methods"
            ]
        },
        "id": 280,
        "cited_by": [
            {
                "year": "2017",
                "id": 66
            }
        ]
    },
    {
        "title": "Center-surround divergence of feature statistics for salient object detection",
        "authors": [
            "Dominik A. Klein",
            "Simone Frintrop"
        ],
        "abstract": "In this paper, we introduce a new method to detect salient objects in images. The approach is based on the standard structure of cognitive visual attention models, but realizes the computation of saliency in each feature dimension in an information-theoretic way. The method allows a consistent computation of all feature channels and a well-founded fusion of these channels to a saliency map. Our framework enables the computation of arbitrarily scaled features and local center-surround pairs in an efficient manner. We show that our approach outperforms eight state-of-the-art saliency detectors in terms of precision and recall.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126499",
        "reference_list": [
            {
                "year": "2007",
                "id": 12
            },
            {
                "year": "2009",
                "id": 287
            }
        ],
        "citation": {
            "ieee": 96,
            "other": 66,
            "total": 162
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Visualization",
                "Computational modeling",
                "Entropy",
                "Databases",
                "Humans",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "information theory",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "center-surround divergence",
                "feature statistics",
                "salient object detection",
                "cognitive visual attention models",
                "information theory",
                "saliency map"
            ]
        },
        "id": 281,
        "cited_by": [
            {
                "year": "2017",
                "id": 22
            },
            {
                "year": "2017",
                "id": 109
            },
            {
                "year": "2017",
                "id": 426
            },
            {
                "year": "2017",
                "id": 573
            },
            {
                "year": "2017",
                "id": 573
            },
            {
                "year": "2013",
                "id": 190
            },
            {
                "year": "2013",
                "id": 219
            },
            {
                "year": "2013",
                "id": 415
            }
        ]
    },
    {
        "title": "Latent structured models for human pose estimation",
        "authors": [
            "Catalin Ionescu",
            "Fuxin Li",
            "Cristian Sminchisescu"
        ],
        "abstract": "We present an approach for automatic 3D human pose reconstruction from monocular images, based on a discriminative formulation with latent segmentation inputs. We advanced the field of structured prediction and human pose reconstruction on several fronts. First, by working with a pool of figure-ground segment hypotheses, the prediction problem is formulated in terms of combined learning and inference over segment hypotheses and 3D human articular configurations. Beside constructing tractable formulations for the combined segment selection and pose estimation problem, we propose new augmented kernels that can better encode complex dependencies between output variables. Furthermore, we provide primal linear re-formulations based on Fourier kernel approximations, in order to scale-up the non-linear latent structured prediction methodology. The proposed models are shown to be competitive in the HumanEva benchmark and are also illustrated in a clip collected from a Hollywood movie, where the model can infer human poses from monocular images captured in complex environments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126500",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2009",
                "id": 148
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 21,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image segmentation",
                "Three dimensional displays",
                "Humans",
                "Estimation",
                "Training",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image reconstruction",
                "image segmentation",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "pose estimation",
                "prediction theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "latent structured model",
                "human pose estimation",
                "automatic 3D human pose reconstruction",
                "monocular images",
                "discriminative formulation",
                "latent segmentation inputs",
                "figure-ground segment hypothesis",
                "prediction problem",
                "learning",
                "inference",
                "3D human articular configuration",
                "tractable formulation",
                "combined segment selection",
                "augmented kernel",
                "complex dependency encoding",
                "primal linear reformulation",
                "Fourier kernel approximation",
                "nonlinear latent structured prediction methodology",
                "HumanEva benchmark",
                "Hollywood movie"
            ]
        },
        "id": 282,
        "cited_by": [
            {
                "year": "2017",
                "id": 415
            },
            {
                "year": "2015",
                "id": 72
            },
            {
                "year": "2015",
                "id": 315
            },
            {
                "year": "2015",
                "id": 317
            }
        ]
    },
    {
        "title": "Manhattan scene understanding using monocular, stereo, and 3D features",
        "authors": [
            "Alex Flint",
            "David Murray",
            "Ian Reid"
        ],
        "abstract": "This paper addresses scene understanding in the context of a moving camera, integrating semantic reasoning ideas from monocular vision with 3D information available through structure-from-motion. We combine geometric and photometric cues in a Bayesian framework, building on recent successes leveraging the indoor Manhattan assumption in monocular vision. We focus on indoor environments and show how to extract key boundaries while ignoring clutter and decorations. To achieve this we present a graphical model that relates photometric cues learned from labeled data, stereo photo-consistency across multiple views, and depth cues derived from structure-from-motion point clouds. We show how to solve MAP inference using dynamic programming, allowing exact, global inference in ~100 ms (in addition to feature computation of under one second) without using specialized hardware. Experiments show our system out-performing the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126501",
        "reference_list": [],
        "citation": {
            "ieee": 37,
            "other": 16,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Neodymium"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "computer vision",
                "dynamic programming",
                "feature extraction",
                "image motion analysis",
                "inference mechanisms",
                "natural scenes",
                "photometry",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Manhattan scene understanding",
                "monocular features",
                "stereo features",
                "3D features",
                "semantic reasoning",
                "monocular vision",
                "3D information",
                "Bayesian framework",
                "indoor Manhattan assumption",
                "key boundary extraction",
                "graphical model",
                "photometric cues",
                "geometric cues",
                "stereo photoconsistency",
                "structure-from-motion point clouds",
                "MAP inference",
                "dynamic programming",
                "global inference"
            ]
        },
        "id": 283,
        "cited_by": [
            {
                "year": "2013",
                "id": 4
            },
            {
                "year": "2013",
                "id": 61
            },
            {
                "year": "2013",
                "id": 158
            },
            {
                "year": "2013",
                "id": 382
            }
        ]
    },
    {
        "title": "A convex framework for image segmentation with moment constraints",
        "authors": [
            "Maria Klodt",
            "Daniel Cremers"
        ],
        "abstract": "Convex relaxation techniques have become a popular approach to image segmentation as they allow to compute solutions independent of initialization to a variety of image segmentation problems. In this paper, we will show that shape priors in terms of moment constraints can be imposed within the convex optimization framework, since they give rise to convex constraints. In particular, the lower-order moments correspond to the overall volume, the centroid, and the variance or covariance of the shape and can be easily imposed in interactive segmentation methods. Respective constraints can be imposed as hard constraints or soft constraints. Quantitative segmentation studies on a variety of images demonstrate that the user can easily impose such constraints with a few mouse clicks, giving rise to substantial improvements of the resulting segmentation, and reducing the average segmentation error from 12% to 0:35%. GPU-based computation times of around 1 second allow for interactive segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126502",
        "reference_list": [
            {
                "year": "2007",
                "id": 200
            },
            {
                "year": "2009",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 13,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Optimization",
                "Mice",
                "Image reconstruction",
                "Convex functions",
                "Level measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "graphics processing units",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "moment constraints",
                "convex relaxation techniques",
                "convex optimization framework",
                "convex constraints",
                "centroid",
                "shape covariance",
                "interactive segmentation methods",
                "GPU-based computation times"
            ]
        },
        "id": 284,
        "cited_by": [
            {
                "year": "2013",
                "id": 290
            }
        ]
    },
    {
        "title": "On the repeatability of the local reference frame for partial shape matching",
        "authors": [
            "Alioscia Petrelli",
            "Luigi Di Stefano"
        ],
        "abstract": "We investigate on local reference frames (LRF) deployed with 3D descriptors to achieve invariance to objects' pose. We address the task of matching together partial views of surfaces and propose an experimental study on a large corpus of real data which allows for clearly ranking existing LRF proposals based on their repeatability. Then, drawing inspiration from analysis of the experimental findings, we formulate a new proposal which, in particular, peculiarly includes a procedure aimed at estimating a repeatable LRF also at border features, which is very important when matching partial views of surfaces. Experiments show that the new proposal neatly outperforms existing methods in terms of repeatability, is computationally very efficient and provide relevant benefits in practical applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126503",
        "reference_list": [
            {
                "year": "2001",
                "id": 139
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 25,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Vectors",
                "Indexes",
                "Shape",
                "Proposals",
                "Noise",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local reference frame",
                "partial shape matching",
                "3D descriptors"
            ]
        },
        "id": 285,
        "cited_by": [
            {
                "year": "2015",
                "id": 177
            }
        ]
    },
    {
        "title": "Tabula rasa: Model transfer for object category detection",
        "authors": [
            "Yusuf Aytar",
            "Andrew Zisserman"
        ],
        "abstract": "Our objective is transfer training of a discriminatively trained object category detector, in order to reduce the number of training images required. To this end we propose three transfer learning formulations where a template learnt previously for other categories is used to regularize the training of a new category. All the formulations result in convex optimization problems. Experiments (on PASCAL VOC) demonstrate significant performance gains by transfer learning from one class to another (e.g. motorbike to bicycle), including one-shot learning, specialization from class to a subordinate class (e.g. from quadruped to horse) and transfer using multiple components. In the case of multiple training samples it is shown that a detection performance approaching that of the state of the art can be achieved with substantially fewer training samples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126504",
        "reference_list": [
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2009",
                "id": 47
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2007",
                "id": 224
            }
        ],
        "citation": {
            "ieee": 87,
            "other": 49,
            "total": 136
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Support vector machines",
                "Detectors",
                "Bicycles",
                "Vectors",
                "Motorcycles",
                "Adaptation models"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Tabula Rasa",
                "model transfer",
                "object category detection",
                "training images",
                "transfer learning formulations",
                "convex optimization problems",
                "one-shot learning"
            ]
        },
        "id": 286,
        "cited_by": [
            {
                "year": "2017",
                "id": 78
            },
            {
                "year": "2017",
                "id": 141
            },
            {
                "year": "2017",
                "id": 356
            },
            {
                "year": "2017",
                "id": 374
            },
            {
                "year": "2017",
                "id": 377
            },
            {
                "year": "2017",
                "id": 516
            },
            {
                "year": "2017",
                "id": 599
            },
            {
                "year": "2015",
                "id": 428
            },
            {
                "year": "2015",
                "id": 454
            },
            {
                "year": "2013",
                "id": 173
            },
            {
                "year": "2013",
                "id": 428
            }
        ]
    },
    {
        "title": "A robust pipeline for rapid feature-based pre-alignment of dense range scans",
        "authors": [
            "Francesco Bonarrigo",
            "Alberto Signoroni",
            "Riccardo Leonardi"
        ],
        "abstract": "Aiming at reaching an interactive and simplified usage of high-resolution 3D acquisition systems, this paper presents a fast and automated technique for pre-alignment of dense range images. Starting from a multi-scale feature point extraction and description, a processing chain composed by feature matching and correspondence searching, ranking grouping and skimming is performed to select the most reliable correspondences over which the correct alignment is estimated. Pre-alignment is obtained in few seconds per million point images on a off-the-shelf PC architecture. The experimental setup aimed to demonstrate the system behavior with respect to a set of concurrent requirements and the obtained performance are significant in the perspective of a fast, robust and unconstrained 3D object reconstruction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126505",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Feature extraction",
                "Pipelines",
                "Robustness",
                "Vectors",
                "Accuracy",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image reconstruction",
                "image resolution",
                "pipeline processing",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust pipeline",
                "rapid feature-based dense range scan image prealignment",
                "high-resolution 3D acquisition system",
                "feature matching",
                "grouping ranking",
                "off-the-shelf PC architecture",
                "3D object reconstruction",
                "multiscale feature point extraction",
                "multiscale feature point description",
                "feature searching"
            ]
        },
        "id": 287,
        "cited_by": []
    },
    {
        "title": "Unsupervised and semi-supervised learning via \u21131-norm graph",
        "authors": [
            "Feiping Nie",
            "Hua Wang",
            "Heng Huang",
            "Chris Ding"
        ],
        "abstract": "In this paper, we propose a novel \u2113 1 -norm graph model to perform unsupervised and semi-supervised learning methods. Instead of minimizing the \u2113 2 -norm of spectral embedding as traditional graph based learning methods, our new graph learning model minimizes the \u2113 1 -norm of spectral embedding with well motivation. The sparsity produced by the \u2113 1 -norm minimization results in the solutions with much clearer cluster structures, which are suitable for both image clustering and classification tasks. We introduce a new efficient iterative algorithm to solve the \u2113 1 -norm of spectral embedding minimization problem, and prove the convergence of the algorithm. More specifically, our algorithm adaptively re-weight the original weights of graph to discover clearer cluster structure. Experimental results on both toy data and real image data sets show the effectiveness and advantages of our proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126506",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 12,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Clustering algorithms",
                "Sparse matrices",
                "Convergence",
                "Linear matrix inequalities",
                "Distributed databases",
                "Algorithm design and analysis",
                "Iterative methods"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image classification",
                "iterative methods",
                "minimisation",
                "pattern clustering",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semisupervised learning",
                "\u21131-norm graph",
                "unsupervised learning",
                "graph learning model",
                "\u21131-norm minimization",
                "iterative algorithm",
                "spectral embedding minimization problem",
                "image clustering",
                "image classification",
                "image data sets"
            ]
        },
        "id": 288,
        "cited_by": []
    },
    {
        "title": "Semi-supervised learning and optimization for hypergraph matching",
        "authors": [
            "Marius Leordeanu",
            "Andrei Zanfir",
            "Cristian Sminchisescu"
        ],
        "abstract": "Graph and hypergraph matching are important problems in computer vision. They are successfully used in many applications requiring 2D or 3D feature matching, such as 3D reconstruction and object recognition. While graph matching is limited to using pairwise relationships, hypergraph matching permits the use of relationships between sets of features of any order. Consequently, it carries the promise to make matching more robust to changes in scale, deformations and outliers. In this paper we make two contributions. First, we present a first semi-supervised algorithm for learning the parameters that control the hypergraph matching model and demonstrate experimentally that it significantly improves the performance of current state-of-the-art methods. Second, we propose a novel efficient hypergraph matching algorithm, which outperforms the state-of-the-art, and, when used in combination with other higher-order matching algorithms, it consistently improves their performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126507",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 13,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Approximation methods",
                "Approximation algorithms",
                "Tensile stress",
                "Training",
                "Convergence",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "graph theory",
                "image matching",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semisupervised learning algorithm",
                "optimization",
                "hypergraph matching model",
                "computer vision",
                "2D feature matching",
                "3D feature matching",
                "3D reconstruction",
                "object recognition",
                "pairwise relationships"
            ]
        },
        "id": 289,
        "cited_by": [
            {
                "year": "2015",
                "id": 22
            },
            {
                "year": "2013",
                "id": 205
            },
            {
                "year": "2013",
                "id": 214
            }
        ]
    },
    {
        "title": "Evaluation of image features using a photorealistic virtual world",
        "authors": [
            "Biliana Kaneva",
            "Antonio Torralba",
            "William T. Freeman"
        ],
        "abstract": "Image features are widely used in computer vision applications. They need to be robust to scene changes and image transformations. Designing and comparing feature descriptors requires the ability to evaluate their performance with respect to those transformations. We want to know how robust the descriptors are to changes in the lighting, scene, or viewing conditions. For this, we need ground truth data of different scenes viewed under different camera or lighting conditions in a controlled way. Such data is very difficult to gather in a real-world setting. We propose using a photorealistic virtual world to gain complete and repeatable control of the environment in order to evaluate image features. We calibrate our virtual world evaluations by comparing against feature rankings made from photographic data of the same subject matter (the Statue of Liberty). We find very similar feature rankings between the two datasets. We then use our virtual world to study the effects on descriptor performance of controlled changes in viewpoint and illumination. We also study the effect of augmenting the descriptors with depth information to improve performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126508",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            },
            {
                "year": "2003",
                "id": 86
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 18,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "lighting",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image feature evaluation",
                "photorealistic virtual world",
                "computer vision",
                "feature ranking",
                "viewpoint",
                "illumination",
                "depth information",
                "descriptor augmentation",
                "3ds Maxs Mental Ray renderer"
            ]
        },
        "id": 290,
        "cited_by": [
            {
                "year": "2017",
                "id": 233
            },
            {
                "year": "2015",
                "id": 419
            }
        ]
    },
    {
        "title": "Kinecting the dots: Particle based scene flow from depth sensors",
        "authors": [
            "Simon Hadfield",
            "Richard Bowden"
        ],
        "abstract": "The motion field of a scene can be used for object segmentation and to provide features for classification tasks like action recognition. Scene flow is the full 3D motion field of the scene, and is more difficult to estimate than it's 2D counterpart, optical flow. Current approaches use a smoothness cost for regularisation, which tends to over-smooth at object boundaries. This paper presents a novel formulation for scene flow estimation, a collection of moving points in 3D space, modelled using a particle filter that supports multiple hypotheses and does not oversmooth the motion field. In addition, this paper is the first to address scene flow estimation, while making use of modern depth sensors and monocular appearance images, rather than traditional multi-viewpoint rigs. The algorithm is applied to an existing scene flow dataset, where it achieves comparable results to approaches utilising multiple views, while taking a fraction of the time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126509",
        "reference_list": [
            {
                "year": "2007",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 15,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Estimation",
                "Optical imaging",
                "Three dimensional displays",
                "Optical sensors",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image motion analysis",
                "image segmentation",
                "image sensors",
                "image sequences",
                "particle filtering (numerical methods)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth sensor",
                "object segmentation",
                "action recognition",
                "3D motion field",
                "particle filter",
                "monocular appearance images",
                "multiview point rigs",
                "optical flow",
                "particle based scene flow",
                "action recognition",
                "scene flow estimation"
            ]
        },
        "id": 291,
        "cited_by": [
            {
                "year": "2015",
                "id": 493
            }
        ]
    },
    {
        "title": "A FACS valid 3D dynamic action unit database with applications to 3D dynamic morphable facial modeling",
        "authors": [
            "Darren Cosker",
            "Eva Krumhuber",
            "Adrian Hilton"
        ],
        "abstract": "This paper presents the first dynamic 3D FACS data set for facial expression research, containing 10 subjects performing between 19 and 97 different AUs both individually and in combination. In total the corpus contains 519 AU sequences. The peak expression frame of each sequence has been manually FACS coded by certified FACS experts. This provides a ground truth for 3D FACS based AU recognition systems. In order to use this data, we describe the first framework for building dynamic 3D morphable models. This includes a novel Active Appearance Model (AAM) based 3D facial registration and mesh correspondence scheme. The approach overcomes limitations in existing methods that require facial markers or are prone to optical flow drift. We provide the first quantitative assessment of such 3D facial mesh registration techniques and show how our proposed method provides more reliable correspondence.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126510",
        "reference_list": [],
        "citation": {
            "ieee": 25,
            "other": 18,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Solid modeling",
                "Gold",
                "Optical imaging",
                "Data models",
                "Adaptive optics",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image sequences",
                "mesh generation",
                "solid modelling",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D dynamic morphable facial modeling",
                "3D dynamic action unit database",
                "3D FACS",
                "facial expression research",
                "peak expression frame",
                "AU recognition systems",
                "dynamic 3D morphable models",
                "active appearance model",
                "mesh correspondence scheme",
                "facial markers",
                "optical flow drift",
                "3D facial mesh registration techniques",
                "facial action coding system"
            ]
        },
        "id": 292,
        "cited_by": [
            {
                "year": "2017",
                "id": 325
            },
            {
                "year": "2015",
                "id": 334
            }
        ]
    },
    {
        "title": "Point-based calibration using a parametric representation of the general imaging model",
        "authors": [
            "Pedro Miraldo",
            "Helder Araujo",
            "Jo\u00e3o Queir\u00f3"
        ],
        "abstract": "Generic imaging models can be used to represent any camera. These models are specially suited for non-central cameras for which closed-form models do not exist. Current models are discrete and define a mapping between each pixel in the image and a straight line in 3D space. Due to difficulties in the calibration procedure and model complexity these methods have not been used in practice. The focus of our work was to relax these drawbacks. In this paper we modify the general imaging model using radial basis functions to interpolate image coordinates and 3D lines allowing both an increase in resolution (due to their continuous nature) and a more compact representation. Using this new variation of the general imaging model we also develop a new linear calibration procedure. In this process it is only required to match one 3D point to each image pixel. Also it is not required the calibration of every image pixel. As a result the complexity of the procedure is significantly decreased.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126511",
        "reference_list": [
            {
                "year": "2001",
                "id": 117
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Mathematical model",
                "Cameras",
                "Equations",
                "Three dimensional displays",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "image representation",
                "image resolution",
                "interpolation",
                "radial basis function networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "point-based calibration",
                "parametric representation",
                "closed-form models",
                "3D space",
                "radial basis functions",
                "image coordinates",
                "image resolution",
                "compact representation",
                "generic imaging model",
                "image pixel"
            ]
        },
        "id": 293,
        "cited_by": [
            {
                "year": "2015",
                "id": 262
            }
        ]
    },
    {
        "title": "Efficient Orthogonal Matching Pursuit using sparse random projections for scene and video classification",
        "authors": [
            "Shiv N. Vitaladevuni",
            "Pradeep Natarajan",
            "Rohit Prasad",
            "Prem Natarajan"
        ],
        "abstract": "Sparse projection has been shown to be highly effective in several domains, including image denoising and scene / object classification. However, practical application to large scale problems such as video analysis requires efficient versions of sparse projection algorithms such as Orthogonal Matching Pursuit (OMP). In particular, random projection based locality sensitive hashing (LSH) has been proposed for OMP. In this paper, we propose a novel technique called Comparison Hadamard random projection (CHRP) for further improving the efficiency of LSH within OMP. CHRP combines two techniques:(1) The Fast Johnson-Lindenstrauss Transform (FJLT) which uses a randomized Hadamard transform and sparse projection matrix for LSH, and (2) Achlioptas' random projection that uses only addition and comparison operations. Our approach provides the robustness of FJLT while completely avoiding multiplications. We empirically validate CHRP's efficacy by performing a suite of experiments for image denoising, scene classification, and video categorization. Our experiments indicate that CHRP significantly speeds-up OMP with negligible loss in classification accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126512",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 1,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Matching pursuit algorithms",
                "Dictionaries",
                "Artificial neural networks",
                "Approximation methods",
                "Sparse matrices",
                "Complexity theory"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image denoising",
                "iterative methods",
                "sparse matrices",
                "transforms",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "orthogonal matching pursuit",
                "sparse random projection algorithm",
                "scene classification",
                "video classification",
                "image denoising",
                "object classification",
                "video analysis",
                "random projection based locality sensitive hashing",
                "Comparison Hadamard random projection",
                "Fast Johnson-Lindenstrauss transform",
                "randomized Hadamard transform",
                "sparse projection matrix",
                "Achliopta random projection",
                "addition operations",
                "comparison operations",
                "video categorization"
            ]
        },
        "id": 294,
        "cited_by": []
    },
    {
        "title": "DTAM: Dense tracking and mapping in real-time",
        "authors": [
            "Richard A. Newcombe",
            "Steven J. Lovegrove",
            "Andrew J. Davison"
        ],
        "abstract": "DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense, every pixel methods. As a single hand-held RGB camera flies over a static scene, we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices. We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term, and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework. Interleaved, we track the camera's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model. Our algorithms are highly parallelisable throughout and DTAM achieves real-time performance using current commodity GPU hardware. We demonstrate that a dense model permits superior tracking performance under rapid motion compared to a state of the art method using features; and also show the additional usefulness of the dense model for real-time scene interaction in a physics-enhanced augmented reality application.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126513",
        "reference_list": [],
        "citation": {
            "ieee": 449,
            "other": 197,
            "total": 646
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Tracking",
                "Real time systems",
                "Image reconstruction",
                "Vectors",
                "Robustness",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "augmented reality",
                "cameras",
                "concave programming",
                "graphics processing units",
                "image motion analysis",
                "image reconstruction",
                "image texture",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense tracking and mapping",
                "real-time camera tracking",
                "real-time camera reconstruction",
                "hand-held RGB camera",
                "textured depth map",
                "video stream",
                "photometric data term",
                "energy functional",
                "nonconvex optimisation",
                "image alignment",
                "GPU hardware",
                "dense model",
                "real-time scene interaction",
                "physics-enhanced augmented reality"
            ]
        },
        "id": 295,
        "cited_by": [
            {
                "year": "2017",
                "id": 271
            },
            {
                "year": "2017",
                "id": 411
            },
            {
                "year": "2017",
                "id": 492
            },
            {
                "year": "2015",
                "id": 78
            },
            {
                "year": "2015",
                "id": 98
            },
            {
                "year": "2015",
                "id": 102
            },
            {
                "year": "2015",
                "id": 148
            },
            {
                "year": "2015",
                "id": 250
            },
            {
                "year": "2015",
                "id": 327
            },
            {
                "year": "2015",
                "id": 392
            },
            {
                "year": "2015",
                "id": 490
            },
            {
                "year": "2013",
                "id": 8
            },
            {
                "year": "2013",
                "id": 58
            },
            {
                "year": "2013",
                "id": 88
            },
            {
                "year": "2013",
                "id": 180
            },
            {
                "year": "2013",
                "id": 251
            },
            {
                "year": "2013",
                "id": 294
            },
            {
                "year": "2013",
                "id": 382
            },
            {
                "year": "2013",
                "id": 407
            }
        ]
    },
    {
        "title": "Tight convex relaxations for vector-valued labeling problems",
        "authors": [
            "Evgeny Strekalovskiy",
            "Bastian Goldluecke",
            "Daniel Cremers"
        ],
        "abstract": "The multi-label problem is of fundamental importance to computer vision, yet finding global minima of the associated energies is very hard and usually impossible in practice. Recently, progress has been made using continuous formulations of the multi-label problem and solving a convex relaxation globally, thereby getting a solution with optimality bounds. In this work, we develop a novel framework for continuous convex relaxations, where the label space is a continuous product space. In this setting, we can combine the memory efficient product relaxation of [9] with the much tighter relaxation of [5], which leads to solutions closer to the global optimum. Furthermore, the new setting allows us to formulate more general continuous regularizers, which can be freely combined in the different label dimensions. We also improve upon the relaxation of the products in the data term of [9], which removes the need for artificial smoothing and allows the use of exact solvers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126514",
        "reference_list": [
            {
                "year": "2009",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 7,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Smoothing methods",
                "Computer vision",
                "Minimization",
                "Transforms",
                "Adaptive optics",
                "Optical sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "relaxation theory",
                "smoothing methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tight convex relaxation",
                "vector-valued labeling problem",
                "multilabel problem",
                "computer vision",
                "continuous convex relaxation",
                "memory efficient product relaxation",
                "artificial smoothing"
            ]
        },
        "id": 296,
        "cited_by": []
    },
    {
        "title": "Image based detection of geometric changes in urban environments",
        "authors": [
            "Aparna Taneja",
            "Luca Ballan",
            "Marc Pollefeys"
        ],
        "abstract": "In this paper, we propose an efficient technique to detect changes in the geometry of an urban environment using some images observing its current state. The proposed method can be used to significantly optimize the process of updating the 3D model of a city changing over time, by restricting this process to only those areas where changes are detected. With this application in mind, we designed our algorithm to specifically detect only structural changes in the environment, ignoring any changes in its appearance, and ignoring also all the changes which are not relevant for update purposes, such as cars, people etc. As a by-product, the algorithm also provides a coarse geometry of the detected changes. The performance of the proposed method was tested on four different kinds of urban environments and compared with two alternative techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126515",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2007",
                "id": 142
            },
            {
                "year": "2007",
                "id": 123
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 12,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Three dimensional displays",
                "Solid modeling",
                "Equations",
                "Urban areas",
                "Mathematical model",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image registration",
                "object detection",
                "solid modelling",
                "structural engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image based detection",
                "geometric change detection",
                "urban environments",
                "3D model",
                "structural changes",
                "coarse geometry"
            ]
        },
        "id": 297,
        "cited_by": [
            {
                "year": "2017",
                "id": 412
            },
            {
                "year": "2015",
                "id": 140
            },
            {
                "year": "2013",
                "id": 62
            }
        ]
    },
    {
        "title": "Unsupervised learning of a scene-specific coarse gaze estimator",
        "authors": [
            "Ben Benfold",
            "Ian Reid"
        ],
        "abstract": "We present a method to estimate the coarse gaze directions of people from surveillance data. Unlike previous work we aim to do this without recourse to a large hand-labelled corpus of training data. In contrast we propose a method for learning a classifier without any hand labelled data using only the output from an automatic tracking system. A Conditional Random Field is used to model the interactions between the head motion, walking direction, and appearance to recover the gaze directions and simultaneously train randomised decision tree classifiers. Experiments demonstrate performance exceeding that of conventionally trained classifiers on two large surveillance datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126516",
        "reference_list": [
            {
                "year": "2007",
                "id": 275
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 15,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Angular velocity",
                "Head",
                "Vegetation",
                "Legged locomotion",
                "Data models",
                "Image color analysis",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "image classification",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised learning",
                "scene-specific coarse gaze estimator",
                "coarse gaze direction estimation",
                "classifier learning",
                "conditional random field",
                "head motion",
                "walking direction",
                "appearance",
                "randomised decision tree classifier"
            ]
        },
        "id": 298,
        "cited_by": [
            {
                "year": "2015",
                "id": 520
            },
            {
                "year": "2013",
                "id": 146
            }
        ]
    },
    {
        "title": "Double window optimisation for constant time visual SLAM",
        "authors": [
            "Hauke Strasdat",
            "Andrew J. Davison",
            "J.M.M. Montiel",
            "Kurt Konolige"
        ],
        "abstract": "We present a novel and general optimisation framework for visual SLAM, which scales for both local, highly accurate reconstruction and large-scale motion with long loop closures. We take a two-level approach that combines accurate pose-point constraints in the primary region of interest with a stabilising periphery of pose-pose soft constraints. Our algorithm automatically builds a suitable connected graph of keyposes and constraints, dynamically selects inner and outer window membership and optimises both simultaneously. We demonstrate in extensive simulation experiments that our method approaches the accuracy of offline bundle adjustment while maintaining constant-time operation, even in the hard case of very loopy monocular camera motion. Furthermore, we present a set of real experiments for various types of visual sensor and motion, including large scale SLAM with both monocular and stereo cameras, loopy local browsing with either monocular or RGB-D cameras, and dense RGB-D object model building.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126517",
        "reference_list": [],
        "citation": {
            "ieee": 73,
            "other": 41,
            "total": 114
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Simultaneous localization and mapping",
                "Cameras",
                "Barium",
                "Visualization",
                "Measurement",
                "Three dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image sensors",
                "pose estimation",
                "robot vision",
                "SLAM (robots)",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "double window optimisation",
                "constant time visual SLAM",
                "reconstruction",
                "pose-point constraints",
                "pose-pose soft constraints",
                "visual sensor",
                "monocular cameras",
                "stereo cameras",
                "RGB-D cameras",
                "dense RGB-D object model building"
            ]
        },
        "id": 299,
        "cited_by": [
            {
                "year": "2017",
                "id": 411
            }
        ]
    },
    {
        "title": "Probabilistic 3D object recognition with both positive and negative evidences",
        "authors": [
            "Sukhan Lee",
            "Zhaojin Lu",
            "Hyunwoo Kim"
        ],
        "abstract": "In real applications, sometimes, visual recognition may need to rely on incomplete or ambiguous features for a unique decision. Furthermore, the detected features may suffer a lot of uncertainties due to environment changes. In order to solve the problem with ambiguities and uncertainties in one computational framework, we propose a probabilistic 3D object recognition approach using both positive and negative evidences in cluttered environment. First of all, initial feature are selected as parallel and perpendicular line pairs to generate pose hypotheses as the multiple interpretations. Secondly, given a 3D polyhedral object model and the estimated pose, positive and negative evidences can be identified as additional information for probability computation of the multiple interpretations. More specifically, given the estimated pose, followed by visibility test, positive evidence is the feature that should be appeared around the pose, and negative evidence is the feature that should not be appeared due to self-occlusion. Where the probability is computed using Bayesian principle in terms of both likelihood and unlikelihood. The experimental results support the potential of the proposed approach in the real environment.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126518",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Object recognition",
                "Solid modeling",
                "Computational modeling",
                "Image edge detection",
                "Feature extraction",
                "Probabilistic logic"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "feature extraction",
                "geometry",
                "object recognition",
                "pose estimation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic 3D object recognition approach",
                "positive evidences",
                "negative evidences",
                "visual recognition",
                "feature detection",
                "parallel line pairs",
                "perpendicular line pairs",
                "3D polyhedral object model",
                "pose estimation",
                "visibility test",
                "Bayesian principle"
            ]
        },
        "id": 300,
        "cited_by": []
    },
    {
        "title": "Low order dynamics embedding for high dimensional time series",
        "authors": [
            "Fei Xiong",
            "Octavia I. Camps",
            "Mario Sznaier"
        ],
        "abstract": "This paper considers the problem of finding low order nonlinear embeddings of dynamic data, that is, data characterized by a temporal ordering. Examples where this problem arises include, among others, appearance-based multi-frame tracking, activity recognition from video and dynamic texture analysis/synthesis. Our main result is a semi-definite programming based manifold embedding algorithm that exploits the dynamical information encapsulated in the temporal ordering of the sequence to obtain embeddings with lower complexity that those produced by existing algorithms, at a comparable computational cost. In addition, the use of spatio-temporal information allows for minimizing the effects of outliers on the manifold structure and for handling fragmented sequences, where some of the data is missing, for instance due to occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126519",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Heuristic algorithms",
                "Trajectory",
                "Complexity theory",
                "Three dimensional displays",
                "Optimization",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image texture",
                "mathematical programming",
                "time series",
                "tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high dimensional time series",
                "nonlinear embeddings",
                "dynamic data",
                "temporal ordering",
                "activity recognition",
                "video",
                "dynamic texture analysis/synthesis",
                "semidefinite programming",
                "occlusion",
                "appearance-based multiframe tracking"
            ]
        },
        "id": 301,
        "cited_by": []
    },
    {
        "title": "Maximizing all margins: Pushing face recognition with Kernel Plurality",
        "authors": [
            "Ritwik Kumar",
            "Arunava Banerjee",
            "Baba C. Vemuri",
            "Hanspeter Pfister"
        ],
        "abstract": "We present two theses in this paper: First, performance of most existing face recognition algorithms improves if instead of the whole image, smaller patches are individually classified followed by label aggregation using voting. Second, weighted plurality 1 voting outperforms other popular voting methods if the weights are set such that they maximize the victory margin for the winner with respect to each of the losers. Moreover, this can be done while taking higher order relationships among patches into account using kernels. We call this scheme Kernel Plurality. We verify our proposals with detailed experimental results and show that our framework with Kernel Plurality improves the performance of various face recognition algorithms beyond what has been previously reported in the literature. Furthermore, on five different benchmark datasets - Yale A, CMU PIE, MERL Dome, Extended Yale B and Multi-PIE, we show that Kernel Plurality in conjunction with recent face recognition algorithms can provide state-of-the-art results in terms of face recognition rates.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126520",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 11,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Vectors",
                "Support vector machines",
                "Training",
                "Face recognition",
                "Proposals",
                "Stacking"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "kernel plurality",
                "classification",
                "label aggregation",
                "weighted plurality voting",
                "victory margin",
                "Yale A dataset",
                "CMU PIE dataset",
                "MERL Dome dataset",
                "Extended Yale B dataset",
                "MultiPIE dataset"
            ]
        },
        "id": 302,
        "cited_by": []
    },
    {
        "title": "Parallelizable inpainting and refinement of diffeomorphisms using Beltrami holomorphic flow",
        "authors": [
            "Tsz Wai Wong",
            "Xianfeng Gu",
            "Tony F. Chan",
            "Lok Ming Lui"
        ],
        "abstract": "In this paper, we propose novel algorithms for inpainting and refinement of diffeomorphisms. We first represent a diffeomorphism by its Beltrami coefficient. Then it is possible to refine and inpaint the diffeomorphism by processing this Beltrami coefficient. With the inpainted/refined Beltrami coefficient, we construct a new diffeomorphism using the exact Beltrami holomorphic flow algorithm proposed in this paper. We apply our algorithms on several practical applications, which include the inpainting of a highly distorted diffeomorphism, the inpainting of image sequences of deforming shapes, the super-resolution of diffeomorphisms and the global parameterization of cortical surfaces by combining local parameterizations. Experiments show that our algorithm can solve these problems with natural and smooth results. We demonstrate how our proposed method can be widely applied in areas from texture mapping to video processing, and from computer graphics to medical imaging.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126521",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "image enhancement",
                "image resolution",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parallelizable inpainting",
                "diffeomorphism refinement",
                "diffeomorphism inpainting",
                "Beltrami holomorphic flow algorithm",
                "image sequence inpainting",
                "diffeomorphism super-resolution",
                "global parameterization",
                "local parameterization",
                "texture mapping",
                "video processing",
                "computer graphics",
                "medical imaging"
            ]
        },
        "id": 303,
        "cited_by": []
    },
    {
        "title": "Optical flow estimation using learned sparse model",
        "authors": [
            "Kui Jia",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Optical flow estimation is a fundamental and ill-posed problem in computer vision. To recover a dense flow field, appropriate spatial constraints have to be enforced. Recent advances exploit higher order spatial regularization, and achieve the top performance on the Middlebury benchmark. In this work, we revisit learning-based approach, and propose a learned sparse model to patch-wisely regularize the flow field. In particular, our method is based on multi-scale spatial regularization, which benefits from first-order spatial regularity and our learned, higher order sparse model. To obtain accurate flow estimation, we propose a sequential optimization scheme to solve the corresponding energy minimization problem. Moreover, as the errors in intermediate flow estimates are usually dense with large variations, we further propose flow-driven and image-driven approaches to address the problem of outliers. Experiments on the Middlebury benchmark show that our method is competitive with the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126522",
        "reference_list": [
            {
                "year": "2009",
                "id": 213
            },
            {
                "year": "2005",
                "id": 5
            },
            {
                "year": "2007",
                "id": 64
            },
            {
                "year": "2009",
                "id": 292
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 10,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical imaging",
                "Adaptive optics",
                "Dictionaries",
                "Estimation",
                "Robustness",
                "Encoding",
                "Adaptation models"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "learning (artificial intelligence)",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optical flow estimation",
                "learned sparse model",
                "computer vision",
                "dense flow field",
                "spatial constraint",
                "higher order sparse model",
                "Middiebury benchmark",
                "learning-based approach",
                "multiscale spatial regularization",
                "sequential optimization scheme",
                "energy minimization problem",
                "flow-driven approach",
                "image-driven approach",
                "computer vision"
            ]
        },
        "id": 304,
        "cited_by": []
    },
    {
        "title": "Efficient similarity search for covariance matrices via the Jensen-Bregman LogDet Divergence",
        "authors": [
            "Anoop Cherian",
            "Suvrit Sra",
            "Arindam Banerjee",
            "Nikolaos Papanikolopoulos"
        ],
        "abstract": "Covariance matrices provide compact, informative feature descriptors for use in several computer vision applications, such as people-appearance tracking, diffusion-tensor imaging, activity recognition, among others. A key task in many of these applications is to compare different covariance matrices using a (dis)similarity function. A natural choice here is the Riemannian metric corresponding to the manifold inhabited by covariance matrices. But computations involving this metric are expensive, especially for large matrices and even more so, in gradient-based algorithms. To alleviate these difficulties, we advocate a novel dissimilarity measure for covariance matrices: the Jensen-Bregman LogDet Divergence. This divergence enjoys several useful theoretical properties, but its greatest benefits are: (i) lower computational costs (compared to standard approaches); and (ii) amenability for use in nearest-neighbor retrieval. We show numerous experiments to substantiate these claims.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126523",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 9,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Covariance matrix",
                "Measurement",
                "Manifolds",
                "Eigenvalues and eigenfunctions",
                "Vectors",
                "Accuracy",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "covariance matrices",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "similarity search",
                "covariance matrix",
                "Jensen-Bregman LogDet divergence",
                "informative feature descriptor",
                "computer vision application",
                "dissimilarity function",
                "Riemannian metric",
                "gradient-based algorithm",
                "computational cost",
                "nearest-neighbor retrieval"
            ]
        },
        "id": 305,
        "cited_by": [
            {
                "year": "2013",
                "id": 223
            }
        ]
    },
    {
        "title": "Learning cross-modality similarity for multinomial data",
        "authors": [
            "Yangqing Jia",
            "Mathieu Salzmann",
            "Trevor Darrell"
        ],
        "abstract": "Many applications involve multiple-modalities such as text and images that describe the problem of interest. In order to leverage the information present in all the modalities, one must model the relationships between them. While some techniques have been proposed to tackle this problem, they either are restricted to words describing visual objects only, or require full correspondences between the different modalities. As a consequence, they are unable to tackle more realistic scenarios where a narrative text is only loosely related to an image, and where only a few image-text pairs are available. In this paper, we propose a model that addresses both these challenges. Our model can be seen as a Markov random field of topic models, which connects the documents based on their similarity. As a consequence, the topics learned with our model are shared across connected documents, thus encoding the relations between different modalities. We demonstrate the effectiveness of our model for image retrieval from a loosely related text.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126524",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 3,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Internet",
                "Encyclopedias",
                "Data models",
                "Electronic publishing",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "image retrieval",
                "Markov processes",
                "random processes",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cross modality similarity learning",
                "multinomial data",
                "narrative text",
                "image text pair",
                "Markov random field",
                "connected document",
                "image retrieval"
            ]
        },
        "id": 306,
        "cited_by": [
            {
                "year": "2017",
                "id": 132
            },
            {
                "year": "2015",
                "id": 452
            },
            {
                "year": "2013",
                "id": 112
            }
        ]
    },
    {
        "title": "Video parsing for abnormality detection",
        "authors": [
            "Borislav Anti\u0107",
            "Bj\u00f6rn Ommer"
        ],
        "abstract": "Detecting abnormalities in video is a challenging problem since the class of all irregular objects and behaviors is infinite and thus no (or by far not enough) abnormal training samples are available. Consequently, a standard setting is to find abnormalities without actually knowing what they are because we have not been shown abnormal examples during training. However, although the training data does not define what an abnormality looks like, the main paradigm in this field is to directly search for individual abnormal local patches or image regions independent of another. To address this problem we parse video frames by establishing a set of hypotheses that jointly explain all the foreground while, at same time, trying to find normal training samples that explain the hypotheses. Consequently, we can avoid a direct detection of abnormalities. They are discovered indirectly as those hypotheses which are needed for covering the foreground without finding an explanation by normal samples for themselves. We present a probabilistic model that localizes abnormalities using statistical inference. On the challenging dataset of [15] it outperforms the state-of-the-art by 7% to achieve a frame-based abnormality classification performance of 91% and the localization performance improves by 32% to 76%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126525",
        "reference_list": [
            {
                "year": "2005",
                "id": 59
            },
            {
                "year": "2009",
                "id": 149
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2005",
                "id": 162
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 25,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Training data",
                "Silicon",
                "Support vector machines",
                "Adaptation models",
                "Probabilistic logic",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object detection",
                "statistical analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video parsing",
                "abnormality detection",
                "abnormal local patch",
                "abnormal image region",
                "probabilistic model",
                "statistical inference",
                "frame-based abnormality classification",
                "localization performance"
            ]
        },
        "id": 307,
        "cited_by": [
            {
                "year": "2017",
                "id": 305
            },
            {
                "year": "2017",
                "id": 381
            },
            {
                "year": "2017",
                "id": 461
            },
            {
                "year": "2013",
                "id": 339
            }
        ]
    },
    {
        "title": "Density-aware person detection and tracking in crowds",
        "authors": [
            "Mikel Rodriguez",
            "Ivan Laptev",
            "Josef Sivic",
            "Jean-Yves Audibert"
        ],
        "abstract": "We address the problem of person detection and tracking in crowded video scenes. While the detection of individual objects has been improved significantly over the recent years, crowd scenes remain particularly challenging for the detection and tracking tasks due to heavy occlusions, high person densities and significant variation in people's appearance. To address these challenges, we propose to leverage information on the global structure of the scene and to resolve all detections jointly. In particular, we explore constraints imposed by the crowd density and formulate person detection as the optimization of a joint energy function combining crowd density estimation and the localization of individual people. We demonstrate how the optimization of such an energy function significantly improves person detection and tracking in crowds. We validate our approach on a challenging video dataset of crowded scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126526",
        "reference_list": [
            {
                "year": "2009",
                "id": 194
            },
            {
                "year": "2009",
                "id": 29
            },
            {
                "year": "2009",
                "id": 178
            },
            {
                "year": "2011",
                "id": 156
            },
            {
                "year": "2005",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 82,
            "other": 58,
            "total": 140
        },
        "keywords": {
            "IEEE Keywords": [
                "Head",
                "Detectors",
                "Tracking",
                "Estimation",
                "Cameras",
                "Training",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "object detection",
                "object tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "density-aware person detection",
                "density-aware person tracking",
                "crowded video scenes",
                "occlusions",
                "high person densities",
                "people appearance variation",
                "joint energy function",
                "crowd density estimation",
                "individual people localization"
            ]
        },
        "id": 308,
        "cited_by": [
            {
                "year": "2017",
                "id": 196
            },
            {
                "year": "2017",
                "id": 268
            },
            {
                "year": "2017",
                "id": 436
            },
            {
                "year": "2017",
                "id": 541
            },
            {
                "year": "2011",
                "id": 156
            }
        ]
    },
    {
        "title": "The power of comparative reasoning",
        "authors": [
            "Jay Yagnik",
            "Dennis Strelow",
            "David A. Ross",
            "Ruei-sung Lin"
        ],
        "abstract": "Rank correlation measures are known for their resilience to perturbations in numeric values and are widely used in many evaluation metrics. Such ordinal measures have rarely been applied in treatment of numeric features as a representational transformation. We emphasize the benefits of ordinal representations of input features both theoretically and empirically. We present a family of algorithms for computing ordinal embeddings based on partial order statistics. Apart from having the stability benefits of ordinal measures, these embeddings are highly nonlinear, giving rise to sparse feature spaces highly favored by several machine learning methods. These embeddings are deterministic, data independent and by virtue of being based on partial order statistics, add another degree of resilience to noise. These machine-learning-free methods when applied to the task of fast similarity search outperform state-of-the-art machine learning methods with complex optimization setups. For solving classification problems, the embeddings provide a nonlinear transformation resulting in sparse binary codes that are well-suited for a large class of machine learning algorithms. These methods show significant improvement on VOC 2010 using simple linear classifiers which can be trained quickly. Our method can be extended to the case of polynomial kernels, while permitting very efficient computation. Further, since the popular Min Hash algorithm is a special case of our method, we demonstrate an efficient scheme for computing Min Hash on conjunctions of binary features. The actual method can be implemented in about 10 lines of code in most languages (2 lines in MATLAB), and does not require any data-driven optimization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126527",
        "reference_list": [],
        "citation": {
            "ieee": 29,
            "other": 16,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Indexes",
                "Measurement",
                "Polynomials",
                "Kernel",
                "Correlation",
                "Learning systems"
            ],
            "INSPEC: Controlled Indexing": [
                "correlation methods",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "matrix algebra",
                "optimisation",
                "pattern classification",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "comparative reasoning",
                "rank correlation",
                "evaluation metrics",
                "numeric features",
                "representational transformation",
                "partial order statistics",
                "machine-learning-free methods",
                "similarity search",
                "state-of-the-art machine learning method",
                "complex optimization",
                "classification problems",
                "nonlinear transformation",
                "sparse binary codes",
                "VOC 2010",
                "linear classifiers",
                "MinHash algorithm"
            ]
        },
        "id": 309,
        "cited_by": []
    },
    {
        "title": "Multi-task low-rank affinity pursuit for image segmentation",
        "authors": [
            "Bin Cheng",
            "Guangcan Liu",
            "Jingdong Wang",
            "Zhongyang Huang",
            "Shuicheng Yan"
        ],
        "abstract": "This paper investigates how to boost region-based image segmentation by pursuing a new solution to fuse multiple types of image features. A collaborative image segmentation framework, called multi-task low-rank affinity pursuit, is presented for such a purpose. Given an image described with multiple types of features, we aim at inferring a unified affinity matrix that implicitly encodes the segmentation of the image. This is achieved by seeking the sparsity-consistent low-rank affinities from the joint decompositions of multiple feature matrices into pairs of sparse and low-rank matrices, the latter of which is expressed as the production of the image feature matrix and its corresponding image affinity matrix. The inference process is formulated as a constrained nuclear norm and \u2113 2;1 -norm minimization problem, which is convex and can be solved efficiently with the Augmented Lagrange Multiplier method. Compared to previous methods, which are usually based on a single type of features, the proposed method seamlessly integrates multiple types of features to jointly produce the affinity matrix within a single inference step, and produces more accurate and reliable segmentation results. Experiments on the MSRC dataset and Berkeley segmentation dataset well validate the superiority of using multiple features over single feature and also the superiority of our method over conventional methods for feature fusion. Moreover, our method is shown to be very competitive while comparing to other state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126528",
        "reference_list": [
            {
                "year": "2009",
                "id": 84
            },
            {
                "year": "2009",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 55,
            "other": 36,
            "total": 91
        },
        "keywords": {
            "IEEE Keywords": [
                "Silicon",
                "Reliability"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image fusion",
                "image segmentation",
                "matrix algebra",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multitask low-rank affinity pursuit",
                "region-based image segmentation",
                "image feature fusion",
                "collaborative image segmentation",
                "unified affinity matrix",
                "image feature matrix",
                "image affinity matrix",
                "constrained nuclear norm",
                "\u21132;1-norm minimization problem",
                "convex",
                "augmented Lagrange multiplier method",
                "MSRC dataset",
                "Berkeley segmentation dataset"
            ]
        },
        "id": 310,
        "cited_by": [
            {
                "year": "2015",
                "id": 176
            },
            {
                "year": "2015",
                "id": 352
            },
            {
                "year": "2015",
                "id": 471
            },
            {
                "year": "2013",
                "id": 310
            },
            {
                "year": "2013",
                "id": 442
            }
        ]
    },
    {
        "title": "Multiview structure from motion in trajectory space",
        "authors": [
            "Aamer Zaheer",
            "Ijaz Akhter",
            "Mohammad Haris Baig",
            "Shabbir Marzban",
            "Sohaib Khan"
        ],
        "abstract": "Most nonrigid objects exhibit temporal regularities in their deformations. Recently it was proposed that these regularities can be parameterized by assuming that the nonrigid structure lies in a small dimensional trajectory space. In this paper, we propose a factorization approach for 3D reconstruction from multiple static cameras under the compact trajectory subspace representation. Proposed factorization is analogous to rank-3 factorization of rigid structure from motion problem, in transformed space. The benefit of our approach is that the 3D trajectory basis can be directly learned from the image observations. This also allows us to impute missing observations and denoise tracking errors without explicit estimation of the 3D structure. In contrast to standard triangulation based methods which require points to be visible in at least two cameras, our approach can reconstruct points, which remain occluded even in all the cameras for quite a long time. This makes our solution especially suitable for occlusion handling in motion capture systems. We demonstrate robustness of our method on challenging real and synthetic scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126529",
        "reference_list": [
            {
                "year": "2009",
                "id": 201
            },
            {
                "year": "2007",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Cameras",
                "Three dimensional displays",
                "Principal component analysis",
                "Discrete cosine transforms",
                "Image reconstruction",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image reconstruction",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview structure",
                "motion in trajectory space",
                "temporal regularity",
                "factorization approach",
                "3D reconstruction",
                "static cameras",
                "compact trajectory subspace representation",
                "tracking error denoising",
                "occlusion handling",
                "motion capture system"
            ]
        },
        "id": 311,
        "cited_by": []
    },
    {
        "title": "Optimal estimation of vanishing points in a Manhattan world",
        "authors": [
            "Faraz M. Mirzaei",
            "Stergios I. Roumeliotis"
        ],
        "abstract": "In this paper, we present an analytical method for computing the globally optimal estimates of orthogonal vanishing points in a \u201cManhattan world\u201d with a calibrated camera. We formulate this as constrained least-squares problem whose optimality conditions form a multivariate polynomial system. We solve this system analytically to compute all the critical points of the least-squares cost function, and hence the global minimum, i.e., the globally optimal estimate for the orthogonal vanishing points. The same optimal estimator is used in conjunction with RANSAC to generate orthogonal-vanishing-point hypotheses (from triplets of lines) and thus classify lines into parallel and mutually orthogonal groups. The proposed method is validated experimentally on the York Urban Database.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126530",
        "reference_list": [
            {
                "year": "2009",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 10,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Cameras",
                "Vectors",
                "Three dimensional displays",
                "Estimation",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image processing",
                "iterative methods",
                "least squares approximations",
                "polynomial approximation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "vanishing points estimation",
                "Manhattan world",
                "constrained least-squares problem",
                "multivariate polynomial system",
                "least-squares cost function",
                "RANSAC",
                "random sample consensus",
                "orthogonal-vanishing-point hypothesis",
                "calibrated camera"
            ]
        },
        "id": 312,
        "cited_by": [
            {
                "year": "2017",
                "id": 91
            },
            {
                "year": "2017",
                "id": 241
            }
        ]
    },
    {
        "title": "Dynamic subspace-based coordinated multicamera tracking",
        "authors": [
            "Mustafa Ayazoglu",
            "Binlong Li",
            "Caglayan Dicle",
            "Mario Sznaier",
            "Octavia I. Camps"
        ],
        "abstract": "This paper considers the problem of sustained multicamera tracking in the presence of occlusion and changes in the target motion model. The key insight of the proposed method is the fact that, under mild conditions, the 2D trajectories of the target in the image planes of each of the cameras are constrained to evolve in the same subspace. This observation allows for identifying, at each time instant, a single (piecewise) linear model that explains all the available 2D measurements. In turn, this model can be used in the context of a modified particle filter to predict future target locations. In the case where the target is occluded to some of the cameras, the missing measurements can be estimated using the facts that they must lie both in the subspace spanned by previous measurements and satisfy epipolar constraints. Hence, by exploiting both dynamical and geometrical constraints the proposed method can robustly handle substantial occlusion, without the need for performing 3D reconstruction, calibrated cameras or constraints on sensor separation. The performance of the proposed tracker is illustrated with several challenging examples involving targets that substantially change appearance and motion models while occluded to some of the cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126531",
        "reference_list": [
            {
                "year": "2003",
                "id": 138
            },
            {
                "year": "2009",
                "id": 198
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 8,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Cameras",
                "Three dimensional displays",
                "Vectors",
                "Trajectory",
                "Heuristic algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer graphics",
                "object tracking",
                "particle filtering (numerical methods)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic subspace-based coordinated multicamera tracking",
                "occlusion",
                "target motion model",
                "single linear model",
                "2D measurements",
                "particle filter",
                "target location prediction",
                "epipolar constraints",
                "dynamical constraints",
                "geometrical constraints"
            ]
        },
        "id": 313,
        "cited_by": []
    },
    {
        "title": "Globally optimal solution to multi-object tracking with merged measurements",
        "authors": [
            "Jo\u00e3o F. Henriques",
            "Rui Caseiro",
            "Jorge Batista"
        ],
        "abstract": "Multiple object tracking has been formulated recently as a global optimization problem, and solved efficiently with optimal methods such as the Hungarian Algorithm. A severe limitation is the inability to model multiple objects that are merged into a single measurement, and track them as a group, while retaining optimality. This work presents a new graph structure that encodes these multiple-match events as standard one-to-one matches, allowing computation of the solution in polynomial time. Since identities are lost when objects merge, an efficient method to identify groups is also presented, as a flow circulation problem. The problem of tracking individual objects across groups is then posed as a standard optimal assignment. Experiments show increased performance on the PETS 2006 and 2009 datasets compared to state-of-the-art algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126532",
        "reference_list": [
            {
                "year": "2007",
                "id": 97
            },
            {
                "year": "2005",
                "id": 68
            },
            {
                "year": "2009",
                "id": 199
            }
        ],
        "citation": {
            "ieee": 45,
            "other": 25,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimal matching",
                "Silicon",
                "Optimization",
                "Polynomials",
                "Joining processes",
                "Indexes",
                "Adaptation models"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object tracking",
                "optimisation",
                "polynomials",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "globally optimal solution",
                "multiple object tracking",
                "optimization problem",
                "Hungarian algorithm",
                "multiple-match event",
                "polynomial time",
                "flow circulation problem",
                "standard optimal assignment",
                "PETS 2009 dataset",
                "PETS 2006 dataset"
            ]
        },
        "id": 314,
        "cited_by": []
    },
    {
        "title": "Dynamic fluid surface acquisition using a camera array",
        "authors": [
            "Yuanyuan Ding",
            "Feng Li",
            "Yu Ji",
            "Jingyi Yu"
        ],
        "abstract": "Acquiring dynamic 3D fluid surfaces is a challenging problem in computer vision. Single or stereo camera based solutions are sensitive to refraction distortions, fast fluid motions, and calibration errors. In this paper, we present a multi-view based solution for robustly capturing fast evolving fluid wavefronts. We first construct a portable, 3\u00d73 camera array system as the main acquisition device. We elaborately design the system to allow high-resolution and high-speed capture. To recover fluid surfaces, we place a known pattern beneath the surface and position the camera array on top to observe the pattern. By tracking the distorted feature points over time and across cameras, we obtain spatial-temporal correspondence maps and we use them for specular carving to reconstruct the time-varying surface. In case one of the cameras loses track due to distortions or blurs, we use the rest of the cameras to construct the surface and then apply multi-perspective warping to locate the lost-track feature points so that we can continue using the camera in later frames. Our experiments on synthetic and real data demonstrate that our multi-view framework is robust and reliable.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126533",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 13,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Pipelines",
                "Graphics processing unit"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "computer vision",
                "distortion",
                "flow visualisation",
                "fluid dynamics",
                "spatiotemporal phenomena",
                "stereo image processing",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic fluid surface acquisition",
                "computer vision",
                "stereo camera",
                "refraction distortions",
                "fluid motions",
                "calibration errors",
                "camera array system",
                "distorted feature tracking",
                "spatial-temporal correspondence maps",
                "surface reconstruction",
                "multiperspective warping"
            ]
        },
        "id": 315,
        "cited_by": [
            {
                "year": "2015",
                "id": 249
            },
            {
                "year": "2015",
                "id": 384
            }
        ]
    },
    {
        "title": "In defense of soft-assignment coding",
        "authors": [
            "Lingqiao Liu",
            "Lei Wang",
            "Xinwang Liu"
        ],
        "abstract": "In object recognition, soft-assignment coding enjoys computational efficiency and conceptual simplicity. However, its classification performance is inferior to the newly developed sparse or local coding schemes. It would be highly desirable if its classification performance could become comparable to the state-of-the-art, leading to a coding scheme which perfectly combines computational efficiency and classification performance. To achieve this, we revisit soft-assignment coding from two key aspects: classification performance and probabilistic interpretation. For the first aspect, we argue that the inferiority of soft-assignment coding is due to its neglect of the underlying manifold structure of local features. To remedy this, we propose a simple modification to localize the soft-assignment coding, which surprisingly achieves comparable or even better performance than existing sparse or local coding schemes while maintaining its computational advantage. For the second aspect, based on our probabilistic interpretation of the soft-assignment coding, we give a probabilistic explanation to the magic max-pooling operation, which has successfully been used by sparse or local coding schemes but still poorly understood. This probability explanation motivates us to develop a new mix-order max-pooling operation which further improves the classification performance of the proposed coding scheme. As experimentally demonstrated, the localized soft-assignment coding achieves the state-of-the-art classification performance with the highest computational efficiency among the existing coding schemes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126534",
        "reference_list": [
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 46,
            "other": 12,
            "total": 58
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Visualization",
                "Vectors",
                "Image coding",
                "Probabilistic logic",
                "Educational institutions",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image coding",
                "object recognition",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "classification performance",
                "probabilistic interpretation",
                "local features",
                "magic max pooling operation",
                "mix order max pooling operation",
                "localized soft-assignment coding"
            ]
        },
        "id": 316,
        "cited_by": [
            {
                "year": "2015",
                "id": 131
            },
            {
                "year": "2015",
                "id": 143
            },
            {
                "year": "2013",
                "id": 35
            },
            {
                "year": "2013",
                "id": 337
            },
            {
                "year": "2013",
                "id": 424
            },
            {
                "year": "2013",
                "id": 444
            }
        ]
    },
    {
        "title": "Pose, illumination and expression invariant pairwise face-similarity measure via Doppelg\u00e4nger list comparison",
        "authors": [
            "Florian Schroff",
            "Tali Treibitz",
            "David Kriegman",
            "Serge Belongie"
        ],
        "abstract": "Face recognition approaches have traditionally focused on direct comparisons between aligned images, e.g. using pixel values or local image features. Such comparisons become prohibitively difficult when comparing faces across extreme differences in pose, illumination and expression. The goal of this work is to develop a face-similarity measure that is largely invariant to these differences. We propose a novel data driven method based on the insight that comparing images of faces is most meaningful when they are in comparable imaging conditions. To this end we describe an image of a face by an ordered list of identities from a Library. The order of the list is determined by the similarity of the Library images to the probe image. The lists act as a signature for each face image: similarity between face images is determined via the similarity of the signatures. Here the CMU Multi-PIE database, which includes images of 337 individuals in more than 2000 pose, lighting and illumination combinations, serves as the Library. We show improved performance over state of the art face-similarity measures based on local features, such as FPLBP, especially across large pose variations on FacePix and multi-PIE. On LFW we show improved performance in comparison with measures like SIFT (on fiducials), LBP, FPLBP and Gabor (C1).",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126535",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 15,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Probes",
                "Face",
                "Libraries",
                "Lighting",
                "Imaging",
                "Face recognition",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Doppelg\u00e4nger list comparison",
                "face recognition",
                "image pose",
                "image illumination",
                "image expression",
                "face-similarity measure",
                "data driven method",
                "library image",
                "CMU MultiPIE database",
                "FacePix",
                "SIFT",
                "FPLBP",
                "Gabor"
            ]
        },
        "id": 317,
        "cited_by": [
            {
                "year": "2013",
                "id": 14
            }
        ]
    },
    {
        "title": "The generalized trace-norm and its application to structure-from-motion problems",
        "authors": [
            "Roland Angst",
            "Christopher Zach",
            "Marc Pollefeys"
        ],
        "abstract": "In geometric computer vision, the structure from motion (SfM) problem can be formulated as a optimization problem with a rank constraint. It is well known that the trace norm of a matrix can act as a convex proxy for a low rank constraint. Hence, in recent work [7], the trace-norm relaxation has been applied to the SfM problem. However, SfM problems often exhibit a certain structure, for example a smooth camera path. Unfortunately, the trace norm relaxation can not make use of this additional structure. This observation motivates the main contribution of this paper. We present the so-called generalized trace norm which allows to encode prior knowledge about a specific problem into a convex regularization term which enforces a low rank solution while at the same time taking the problem structure into account. While deriving the generalized trace norm and stating its different formulations, we draw interesting connections to other fields, most importantly to the field of compressive sensing. Even though the generalized trace norm is a very general concept with a wide area of potential applications we are ultimately interested in applying it to SfM problems. Therefore, we also present an efficient algorithm to optimize the resulting generalized trace norm regularized optimization problems. Results show that the generalized trace norm indeed achieves its goals in providing a problem-dependent regularization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126536",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 11,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Transportation",
                "Vectors",
                "Computer vision",
                "Cost function",
                "Bayesian methods",
                "Covariance matrix"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "matrix algebra",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generalized trace-norm relaxation",
                "structure-from-motion problems",
                "geometric computer vision",
                "optimization problem",
                "rank constraint",
                "matrix",
                "SfM problem",
                "smooth camera path",
                "convex regularization term",
                "compressive sensing",
                "problem-dependent regularization"
            ]
        },
        "id": 318,
        "cited_by": [
            {
                "year": "2013",
                "id": 236
            },
            {
                "year": "2013",
                "id": 260
            },
            {
                "year": "2013",
                "id": 310
            }
        ]
    },
    {
        "title": "Digital anti-aging in face images",
        "authors": [
            "Guodong Guo"
        ],
        "abstract": "We study a problem called digital facial anti-aging, which aims at making human faces look younger in digital photos. This novel problem is different from the traditional age synthesis where new faces are synthesized at older ages using other example face images. In contrast, our facial anti-aging works on a single digital photo without using any other faces. The proposed system contains several modules. First, an input color face image is decomposed into three layers: face structure, aging detail, and color. Second, the specular highlight in the original face image is recovered. Third, the face structure, color, and the recovered specular highlight are combined to form an anti-aging face image. Further, the system can deal with hair coloring and eyebrow change if needed, since these factors influence human judges of ages. Our approach keeps facial identities and delivers high-quality outputs. We show that the anti-aging system can be built based on adapting and integrating the state-of-the-art methods that were originally proposed to solve other problems. The anti-aging faces are evaluated by humans to demonstrate the performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126537",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Image color analysis",
                "Eyebrows",
                "Aging",
                "Humans",
                "Indexes"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "digital facial antiaging",
                "digital photos",
                "input color face image decomposition",
                "face structure layer",
                "aging detail layer",
                "color layer",
                "specular highlight recovery",
                "hair coloring",
                "eyebrow"
            ]
        },
        "id": 319,
        "cited_by": []
    },
    {
        "title": "Robust topological features for deformation invariant image matching",
        "authors": [
            "Edgar Lobaton",
            "Ram Vasudevan",
            "Ron Alterovitz",
            "Ruzena Bajcsy"
        ],
        "abstract": "Local photometric descriptors are a crucial low level component of numerous computer vision algorithms. In practice, these descriptors are constructed to be invariant to a class of transformations. However, the development of a descriptor that is simultaneously robust to noise and invariant under general deformation has proven difficult. In this paper, we introduce the Topological-Attributed Relational Graph (T-ARG), a new local photometric descriptor constructed from homology that is provably invariant to locally bounded deformation. This new robust topological descriptor is backed by a formal mathematical framework. We apply T-ARG to a set of benchmark images to evaluate its performance. Results indicate that T-ARG significantly outperforms traditional descriptors for noisy, deforming images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126538",
        "reference_list": [
            {
                "year": "2005",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Noise",
                "Transforms",
                "Deformable models",
                "Topology",
                "Image edge detection",
                "Imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "graph theory",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "topological feature",
                "deformation invariant image matching",
                "photometric descriptor",
                "computer vision algorithm",
                "topological-attributed relational graph"
            ]
        },
        "id": 320,
        "cited_by": []
    },
    {
        "title": "Multiclass recognition and part localization with humans in the loop",
        "authors": [
            "Catherine Wah",
            "Steve Branson",
            "Pietro Perona",
            "Serge Belongie"
        ],
        "abstract": "We propose a visual recognition system that is designed for fine-grained visual categorization. The system is composed of a machine and a human user. The user, who is unable to carry out the recognition task by himself, is interactively asked to provide two heterogeneous forms of information: clicking on object parts and answering binary questions. The machine intelligently selects the most informative question to pose to the user in order to identify the object's class as quickly as possible. By leveraging computer vision and analyzing the user responses, the overall amount of human effort required, measured in seconds, is minimized. We demonstrate promising results on a challenging dataset of uncropped images, achieving a significant average reduction in human effort over previous methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126539",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2005",
                "id": 107
            },
            {
                "year": "2009",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 59,
            "other": 19,
            "total": 78
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiclass recognition",
                "part localization",
                "visual recognition system",
                "visual categorization",
                "human user",
                "informative question",
                "computer vision"
            ]
        },
        "id": 321,
        "cited_by": [
            {
                "year": "2017",
                "id": 167
            },
            {
                "year": "2015",
                "id": 273
            },
            {
                "year": "2013",
                "id": 1
            },
            {
                "year": "2013",
                "id": 163
            },
            {
                "year": "2013",
                "id": 213
            },
            {
                "year": "2013",
                "id": 314
            },
            {
                "year": "2013",
                "id": 320
            }
        ]
    },
    {
        "title": "An automatic assembly and completion framework for fragmented skulls",
        "authors": [
            "Zhao Yin",
            "Li Wei",
            "Xin Li",
            "Mary Manhein"
        ],
        "abstract": "We develop a completion pipeline for fragmented and damaged skulls. The goal of this work is to convert scanned incomplete skull fragments to a complete skull model for subsequent forensic or archeological tasks such as facial reconstruction. The proposed assembly and completion algorithms can also be used to repair other fragmented objects with inherent symmetry. A two-step assembly framework is proposed: (1) rough assembly by an ICP-like template matching algorithm integrated with the slippage features and spin-image descriptors; (2) assembly refinement by a global optimization on least square transformation error (LSTE) of break-curves. The assembled skull is finally repaired by a symmetry-based completion algorithm. Experiments on repairing scanned skull fragments demonstrate the efficacy and robustness of this framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126540",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Assembly",
                "Skull",
                "Maintenance engineering",
                "Geometry",
                "Pipelines",
                "Feature extraction",
                "Silicon"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "image reconstruction",
                "least squares approximations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "assembly algorithm",
                "fragmented skull",
                "damaged skull",
                "facial reconstruction",
                "completion algorithm",
                "ICP-like template matching algorithm",
                "slippage feature",
                "spin-image descriptor",
                "assembly refinement",
                "global optimization",
                "least square transformation error",
                "symmetry-based completion algorithm",
                "computer vision"
            ]
        },
        "id": 322,
        "cited_by": []
    },
    {
        "title": "Markov Random Field-based fitting of a subdivision-based geometric atlas",
        "authors": [
            "Uday Kurkure",
            "Yen H. Le",
            "Nikos Paragios",
            "Tao Ju",
            "James P. Carson",
            "Ioannis A. Kakadiaris"
        ],
        "abstract": "An accurate labeling of a multi-part, complex anatomical structure (e.g., brain) is required in order to compare data across images for spatial analysis. It can be achieved by fitting an object-specific geometric atlas that is constructed using a partitioned, high-resolution deformable mesh and tagging each of its polygons with a region label. Subdivision meshes have been used to construct such an atlas because they can provide a compact representation of a partitioned, multi-resolution, object-specific mesh structure using only a few control points. However, automated fitting of a subdivision mesh-based geometric atlas to an anatomical structure in an image is a difficult problem and has not been sufficiently addressed. In this paper, we propose a novel Markov Random Field-based method for fitting a planar, multi-part subdivision mesh to anatomical data. The optimal fitting of the atlas is obtained by determining the optimal locations of the control points. We also tackle the problem of landmark matching in tandem with atlas fitting by constructing a single graphical model to impose pose-invariant, landmark-based geometric constraints on atlas deformation. The atlas deformation is also governed by additional constraints imposed by the mesh's geometric properties and the object boundary. We demonstrate the potential of the proposed method on the difficult problem of segmenting a mouse brain and its interior regions in gene expression images which exhibit large intensity and shape variability. We obtain promising results when compared with manual annotations and prior methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126541",
        "reference_list": [
            {
                "year": "2007",
                "id": 234
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Shape",
                "Vectors",
                "Data models",
                "Gene expression",
                "Optimization",
                "Anatomical structure"
            ],
            "INSPEC: Controlled Indexing": [
                "biology computing",
                "image registration",
                "image segmentation",
                "Markov processes",
                "mesh generation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Markov random field",
                "subdivision-based geometric atlas",
                "spatial analysis",
                "deformable mesh",
                "subdivision mesh",
                "image anatomical structure",
                "graphical model",
                "landmark-based geometric constraint",
                "mouse brain segmentation",
                "gene expression image"
            ]
        },
        "id": 323,
        "cited_by": []
    },
    {
        "title": "BRISK: Binary Robust invariant scalable keypoints",
        "authors": [
            "Stefan Leutenegger",
            "Margarita Chli",
            "Roland Y. Siegwart"
        ],
        "abstract": "Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Established leaders in the field are the SIFT and SURF algorithms which exhibit great performance under a variety of image transformations, with SURF in particular considered as the most computationally efficient amongst the high-performance methods to date. In this paper we propose BRISK, a novel method for keypoint detection, description and matching. A comprehensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art algorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126542",
        "reference_list": [],
        "citation": {
            "ieee": 705,
            "other": 491,
            "total": 1196
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Boats",
                "Brightness",
                "Robustness",
                "Kernel",
                "Feature extraction",
                "Complexity theory"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image matching",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "binary robust invariant scalable keypoints",
                "keypoint generation",
                "computer vision application",
                "SIFT algorithm",
                "SURF algorithm",
                "image transformation",
                "BRISK method",
                "keypoint detection",
                "keypoint description",
                "keypoint matching",
                "scale-space FAST-based detector",
                "bit-string descriptor"
            ]
        },
        "id": 324,
        "cited_by": [
            {
                "year": "2017",
                "id": 349
            },
            {
                "year": "2015",
                "id": 65
            },
            {
                "year": "2015",
                "id": 258
            },
            {
                "year": "2013",
                "id": 149
            },
            {
                "year": "2013",
                "id": 326
            }
        ]
    },
    {
        "title": "HMDB: A large video database for human motion recognition",
        "authors": [
            "H. Kuehne",
            "H. Jhuang",
            "E. Garrote",
            "T. Poggio",
            "T. Serre"
        ],
        "abstract": "With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126543",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 416,
            "other": 273,
            "total": 689
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "YouTube",
                "Databases",
                "Training",
                "Visualization",
                "Humans",
                "Motion pictures"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "object recognition",
                "social networking (online)",
                "video databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "HMDB",
                "large video database",
                "human motion recognition",
                "computer vision research",
                "static image datasets",
                "image categories",
                "human action datasets",
                "action recognition databases",
                "digitized movies",
                "YouTube",
                "camera motion",
                "viewpoint",
                "video quality",
                "occlusion"
            ]
        },
        "id": 325,
        "cited_by": [
            {
                "year": "2017",
                "id": 10
            },
            {
                "year": "2017",
                "id": 69
            },
            {
                "year": "2017",
                "id": 74
            },
            {
                "year": "2017",
                "id": 225
            },
            {
                "year": "2017",
                "id": 226
            },
            {
                "year": "2017",
                "id": 306
            },
            {
                "year": "2017",
                "id": 351
            },
            {
                "year": "2017",
                "id": 383
            },
            {
                "year": "2017",
                "id": 448
            },
            {
                "year": "2017",
                "id": 463
            },
            {
                "year": "2017",
                "id": 590
            },
            {
                "year": "2015",
                "id": 113
            },
            {
                "year": "2015",
                "id": 272
            },
            {
                "year": "2015",
                "id": 353
            },
            {
                "year": "2015",
                "id": 359
            },
            {
                "year": "2015",
                "id": 362
            },
            {
                "year": "2015",
                "id": 368
            },
            {
                "year": "2015",
                "id": 512
            },
            {
                "year": "2015",
                "id": 513
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2013",
                "id": 280
            },
            {
                "year": "2013",
                "id": 337
            },
            {
                "year": "2013",
                "id": 398
            },
            {
                "year": "2013",
                "id": 429
            },
            {
                "year": "2013",
                "id": 443
            },
            {
                "year": "2013",
                "id": 444
            }
        ]
    },
    {
        "title": "ORB: An efficient alternative to SIFT or SURF",
        "authors": [
            "Ethan Rublee",
            "Vincent Rabaud",
            "Kurt Konolige",
            "Gary Bradski"
        ],
        "abstract": "Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126544",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 1398,
            "other": 936,
            "total": 2334
        },
        "keywords": {
            "IEEE Keywords": [
                "Boats"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "object detection",
                "object recognition",
                "tracking",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ORB",
                "SIFT",
                "SURF",
                "feature matching",
                "computer vision",
                "object recognition",
                "binary descriptor",
                "BRIEF",
                "noise resistance",
                "object detection",
                "patch-tracking",
                "smart phone"
            ]
        },
        "id": 326,
        "cited_by": [
            {
                "year": "2017",
                "id": 15
            },
            {
                "year": "2017",
                "id": 79
            },
            {
                "year": "2017",
                "id": 249
            },
            {
                "year": "2017",
                "id": 349
            },
            {
                "year": "2015",
                "id": 11
            },
            {
                "year": "2015",
                "id": 13
            },
            {
                "year": "2015",
                "id": 65
            },
            {
                "year": "2015",
                "id": 134
            },
            {
                "year": "2013",
                "id": 8
            },
            {
                "year": "2013",
                "id": 149
            },
            {
                "year": "2013",
                "id": 157
            },
            {
                "year": "2013",
                "id": 326
            }
        ]
    },
    {
        "title": "Isotonic CCA for sequence alignment and activity recognition",
        "authors": [
            "Shahriar Shariat",
            "Vladimir Pavlovic"
        ],
        "abstract": "This paper presents an approach for sequence alignment based on canonical correlation analysis(CCA). We show that a novel set of constraints imposed on traditional CCA leads to canonical solutions with the time warping property, i.e., non-decreasing monotonicity in time. This formulation generalizes the more traditional dynamic time warping (DTW) solutions to cases where the alignment is accomplished on arbitrary subsequence segments, optimally determined from data, instead on individual sequence samples. We then introduce a robust and efficient algorithm to find such alignments using non-negative least squares reductions. Experimental results show that this new method, when applied to MOCAP activity recognition problems, can yield improved recognition accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126545",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 4,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Motion segmentation",
                "Optimization",
                "Accuracy",
                "Robustness",
                "Gaussian noise"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image recognition",
                "least squares approximations",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "isotonic CCA",
                "sequence alignment",
                "activity recognition",
                "canonical correlation analysis",
                "time warping property",
                "dynamic time warping",
                "DTW",
                "arbitrary subsequence",
                "nonnegative least squares reductions",
                "computer vision"
            ]
        },
        "id": 327,
        "cited_by": []
    },
    {
        "title": "BiCoS: A Bi-level co-segmentation method for image classification",
        "authors": [
            "Yuning Chai",
            "Victor Lempitsky",
            "Andrew Zisserman"
        ],
        "abstract": "The objective of this paper is the unsupervised segmentation of image training sets into foreground and background in order to improve image classification performance. To this end we introduce a new scalable, alternation-based algorithm for co-segmentation, BiCoS, which is simpler than many of its predecessors, and yet has superior performance on standard benchmark image datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126546",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2007",
                "id": 125
            },
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 5,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Support vector machines",
                "Accuracy",
                "Training",
                "Image color analysis",
                "Visualization",
                "Birds"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "BiCoS",
                "bilevel cosegmentation method",
                "image classification",
                "unsupervised segmentation",
                "image training sets",
                "foreground",
                "background",
                "alternation-based algorithm"
            ]
        },
        "id": 328,
        "cited_by": [
            {
                "year": "2017",
                "id": 141
            },
            {
                "year": "2013",
                "id": 38
            },
            {
                "year": "2013",
                "id": 49
            },
            {
                "year": "2013",
                "id": 161
            },
            {
                "year": "2013",
                "id": 213
            }
        ]
    },
    {
        "title": "Shared shape spaces",
        "authors": [
            "Victor Adrian Prisacariu",
            "Ian Reid"
        ],
        "abstract": "We propose a method for simultaneous shape-constrained segmentation and parameter recovery. The parameters can describe anything from 3D shape to 3D pose and we place no restriction on the topology of the shapes, i.e. they can have holes or be made of multiple parts. We use Shared Gaussian Process Latent Variable Models to learn multimodal shape-parameter spaces. These allow non-linear embeddings of the high-dimensional shape and parameter spaces in low dimensional spaces in a fully probabilistic manner. We propose a method for exploring the multimodality in the joint space in an efficient manner, by learning a mapping from the latent space to a space that encodes the similarity between shapes. We further extend the SGP-LVM to a model that makes use of a hierarchy of embeddings and show that this yields faster convergence and greater accuracy over the standard non-hierarchical embedding. Shapes are represented implicitly using level sets, and inference is made tractable by compressing the level set embedding functions with discrete cosine transforms. We show state of the art results in various fields, ranging from pose recovery to gaze tracking and to monocular 3D reconstruction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126547",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 6,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image segmentation",
                "Discrete cosine transforms",
                "Three dimensional displays",
                "Manifolds",
                "Level set",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "discrete cosine transforms",
                "Gaussian processes",
                "image reconstruction",
                "image segmentation",
                "inference mechanisms",
                "set theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape-constrained segmentation",
                "parameter recovery",
                "3D shape",
                "3D pose",
                "shape topology",
                "shared Gaussian process latent variable model",
                "multimodal shape-parameter space",
                "level set",
                "pose recovery",
                "gaze tracking",
                "monocular 3D reconstruction",
                "discrete cosine transforms"
            ]
        },
        "id": 329,
        "cited_by": [
            {
                "year": "2015",
                "id": 245
            }
        ]
    },
    {
        "title": "A \u201cstring of feature graphs\u201d model for recognition of complex activities in natural videos",
        "authors": [
            "U. Gaur",
            "Y. Zhu",
            "B. Song",
            "A. Roy-Chowdhury"
        ],
        "abstract": "Videos usually consist of activities involving interactions between multiple actors, sometimes referred to as complex activities. Recognition of such activities requires modeling the spatio-temporal relationships between the actors and their individual variabilities. In this paper, we consider the problem of recognition of complex activities in a video given a query example. We propose a new feature model based on a string representation of the video which respects the spatio-temporal ordering. This ordered arrangement of local collections of features (e.g., cuboids, STIP), which are the characters in the string, are initially matched using graph-based spectral techniques. Final recognition is obtained by matching the string representations of the query and the test videos in a dynamic programming framework which allows for variability in sampling rates and speed of activity execution. The method does not require tracking or recognition of body parts, is able to identify the region of interest in a cluttered scene, and gives reasonable performance with even a single query example. We test our approach in an example-based video retrieval framework with two publicly available complex activity datasets and provide comparisons against other methods that have studied this problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126548",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2009",
                "id": 204
            },
            {
                "year": "2009",
                "id": 252
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 29,
            "total": 61
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Testing",
                "Feature extraction",
                "Dynamic programming",
                "Training",
                "Vehicles",
                "Clutter"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "feature extraction",
                "graph theory",
                "image matching",
                "image recognition",
                "image representation",
                "image sampling",
                "video retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature graph string",
                "complex activity recognition",
                "natural video",
                "spatio-temporal relationship",
                "feature model",
                "string representation",
                "spatio-temporal ordering",
                "features collection",
                "graph-based spectral technique",
                "dynamic programming",
                "sampling rate",
                "example-based video retrieval",
                "complex activity dataset"
            ]
        },
        "id": 330,
        "cited_by": []
    },
    {
        "title": "Regression from local features for viewpoint and pose estimation",
        "authors": [
            "Marwan Torki",
            "Ahmed Elgammal"
        ],
        "abstract": "In this paper we propose a framework for learning a regression function form a set of local features in an image. The regression is learned from an embedded representation that reflects the local features and their spatial arrangement as well as enforces supervised manifold constraints on the data. We applied the approach for viewpoint estimation on a Multiview car dataset, a head pose dataset and arm posture dataset. The experimental results show that this approach has superior results (up to 67% improvement) to the state-of-the-art approaches in very challenging datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126549",
        "reference_list": [
            {
                "year": "2009",
                "id": 144
            },
            {
                "year": "2007",
                "id": 146
            },
            {
                "year": "2009",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 14,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Kernel",
                "Training",
                "Estimation",
                "Accuracy",
                "Feature extraction",
                "Head"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation",
                "pose estimation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local feature regression",
                "pose estimation",
                "viewpoint estimation",
                "embedded representation",
                "multiview car dataset",
                "head pose dataset",
                "arm posture dataset",
                "image features"
            ]
        },
        "id": 331,
        "cited_by": [
            {
                "year": "2015",
                "id": 115
            },
            {
                "year": "2013",
                "id": 275
            }
        ]
    },
    {
        "title": "Probabilistic image segmentation with closedness constraints",
        "authors": [
            "Bjoern Andres",
            "J\u00f6rg H. Kappes",
            "Thorsten Beier",
            "Ullrich K\u00f6the",
            "Fred A. Hamprecht"
        ],
        "abstract": "We propose a novel graphical model for probabilistic image segmentation that contributes both to aspects of perceptual grouping in connection with image segmentation, and to globally optimal inference with higher-order graphical models. We represent image partitions in terms of cellular complexes in order to make the duality between connected regions and their contours explicit. This allows us to formulate a graphical model with higher-order factors that represent the requirement that all contours must be closed. The model induces a probability measure on the space of all partitions, concentrated on perceptually meaningful segmentations. We give a complete polyhedral characterization of the resulting global inference problem in terms of the multicut polytope and efficiently compute global optima by a cutting plane method. Competitive results for the Berkeley segmentation benchmark confirm the consistency of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126550",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2003",
                "id": 1
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 24,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Labeling",
                "Junctions",
                "Topology",
                "Graphical models",
                "Probabilistic logic",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "benchmark testing",
                "constraint handling",
                "image representation",
                "image segmentation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic image segmentation",
                "closedness constraint",
                "perceptual grouping",
                "globally optimal inference",
                "higher-order graphical model",
                "image partition",
                "cellular complex",
                "contours explicit",
                "probability measurement",
                "complete polyhedral characterization",
                "global inference problem",
                "cutting plane method",
                "Berkeley segmentation benchmark"
            ]
        },
        "id": 332,
        "cited_by": [
            {
                "year": "2017",
                "id": 493
            },
            {
                "year": "2015",
                "id": 195
            },
            {
                "year": "2015",
                "id": 365
            },
            {
                "year": "2013",
                "id": 230
            }
        ]
    },
    {
        "title": "Generalized ordering constraints for multilabel optimization",
        "authors": [
            "Evgeny Strekalovskiy",
            "Daniel Cremers"
        ],
        "abstract": "We propose a novel framework for imposing label ordering constraints in multilabel optimization. In particular, label jumps can be penalized differently depending on the jump direction. In contrast to the recently proposed MRF-based approaches, the proposed method arises from the viewpoint of spatially continuous optimization. It unifies and generalizes previous approaches to label ordering constraints: Firstly, it provides a common solution to three different problems which are otherwise solved by three separate approaches [4, 10, 14]. We provide an exact characterization of the penalization functions expressible with our approach. Secondly, we show that it naturally extends to three and higher dimensions of the image domain. Thirdly, it allows novel applications, such as the convex shape prior. Despite this generality, our model is easily adjustable to various label layouts and is also easy to implement. On a number of experiments we show that it works quite well, producing solutions comparable and superior to those obtained with previous approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126551",
        "reference_list": [
            {
                "year": "2009",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 9,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Optimization",
                "Shape",
                "Labeling",
                "Dynamic programming",
                "Accuracy",
                "Solids"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generalized ordering constraints",
                "multilabel optimization",
                "label jumps",
                "MRF-based approach",
                "label ordering"
            ]
        },
        "id": 333,
        "cited_by": []
    },
    {
        "title": "N-best maximal decoders for part models",
        "authors": [
            "Dennis Park",
            "Deva Ramanan"
        ],
        "abstract": "We describe a method for generating N-best configurations from part-based models, ensuring that they do not overlap according to some user-provided definition of overlap. We extend previous N-best algorithms from the speech community to incorporate non-maximal suppression cues, such that pixel-shifted copies of a single configuration are not returned. We use approximate algorithms that perform nearly identical to their exact counterparts, but are orders of magnitude faster. Our approach outperforms standard methods for generating multiple object configurations in an image. We use our method to generate multiple pose hypotheses for the problem of human pose estimation from video sequences. We present quantitative results that demonstrate that our framework significantly improves the accuracy of a state-of-the-art pose estimation algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126552",
        "reference_list": [],
        "citation": {
            "ieee": 42,
            "other": 14,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Approximation algorithms",
                "Partitioning algorithms",
                "Heuristic algorithms",
                "Decoding",
                "Inference algorithms",
                "Dynamic programming",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image sequences",
                "pose estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "n-best maximal decoders",
                "n-best configuration generation",
                "part-based models",
                "speech community",
                "nonmaximal suppression cues",
                "approximate algorithms",
                "human pose estimation",
                "video sequences"
            ]
        },
        "id": 334,
        "cited_by": [
            {
                "year": "2015",
                "id": 224
            },
            {
                "year": "2015",
                "id": 371
            },
            {
                "year": "2015",
                "id": 485
            },
            {
                "year": "2013",
                "id": 331
            },
            {
                "year": "2013",
                "id": 364
            }
        ]
    },
    {
        "title": "Integrating local classifiers through nonlinear dynamics on label graphs with an application to image segmentation",
        "authors": [
            "Yutian Chen",
            "Andrew Gelfand",
            "Charless C. Fowlkes",
            "Max Welling"
        ],
        "abstract": "We present a new method to combine possibly inconsistent locally (piecewise) trained conditional models p(y \u03b1 |x \u03b1 ) into pseudo-samples from a global model. Our method does not require training of a CRF, but instead generates samples by iterating forward a weakly chaotic dynamical system. The new method is illustrated on image segmentation tasks where classifiers based on local appearance cues are combined with pairwise boundary cues.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126553",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Joints",
                "Computational modeling",
                "Training",
                "Accuracy",
                "Data models",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image classification",
                "image segmentation",
                "time-varying systems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local classifier integration",
                "nonlinear dynamics",
                "label graph",
                "image segmentation",
                "conditional model",
                "weakly chaotic dynamical system",
                "pairwise boundary cues"
            ]
        },
        "id": 335,
        "cited_by": []
    },
    {
        "title": "Efficient learning of sparse, distributed, convolutional feature representations for object recognition",
        "authors": [
            "Kihyuk Sohn",
            "Dae Yon Jung",
            "Honglak Lee",
            "Alfred O. Hero"
        ],
        "abstract": "Informative image representations are important in achieving state-of-the-art performance in object recognition tasks. Among feature learning algorithms that are used to develop image representations, restricted Boltzmann machines (RBMs) have good expressive power and build effective representations. However, the difficulty of training RBMs has been a barrier to their wide use. To address this difficulty, we show the connections between mixture models and RBMs and present an efficient training method for RBMs that utilize these connections. To the best of our knowledge, this is the first work showing that RBMs can be trained with almost no hyperparameter tuning to provide classification performance similar to or significantly better than mixture models (e.g., Gaussian mixture models). Along with this efficient training, we evaluate the importance of convolutional training that can capture a larger spatial context with less redundancy, as compared to non-convolutional training. Overall, our method achieves state-of-the-art performance on both Caltech 101 / 256 datasets using a single type of feature.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126554",
        "reference_list": [
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 5,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Convolutional codes",
                "Feature extraction",
                "Encoding",
                "Clustering algorithms",
                "Context",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "Boltzmann machines",
                "image classification",
                "image representation",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature representations",
                "object recognition",
                "learning",
                "informative image representations",
                "restricted Boltzmann machines",
                "hyperparameter tuning",
                "classification performance",
                "mixture models",
                "Caltech 101-256 datasets"
            ]
        },
        "id": 336,
        "cited_by": []
    },
    {
        "title": "Ask the locals: Multi-way local pooling for image recognition",
        "authors": [
            "Y-Lan Boureau",
            "Nicolas Le Roux",
            "Francis Bach",
            "Jean Ponce",
            "Yann LeCun"
        ],
        "abstract": "Invariant representations in object recognition systems are generally obtained by pooling feature vectors over spatially local neighborhoods. But pooling is not local in the feature vector space, so that widely dissimilar features may be pooled together if they are in nearby locations. Recent approaches rely on sophisticated encoding methods and more specialized codebooks (or dictionaries), e.g., learned on subsets of descriptors which are close in feature space, to circumvent this problem. In this work, we argue that a common trait found in much recent work in image recognition or retrieval is that it leverages locality in feature space on top of purely spatial locality. We propose to apply this idea in its simplest form to an object recognition system based on the spatial pyramid framework, to increase the performance of small dictionaries with very little added engineering. State-of-the-art results on several object recognition benchmarks show the promise of this approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126555",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 276
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2009",
                "id": 55
            }
        ],
        "citation": {
            "ieee": 86,
            "other": 52,
            "total": 138
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Dictionaries",
                "Image coding",
                "Vectors",
                "Feature extraction",
                "Training",
                "Benchmark testing"
            ],
            "INSPEC: Controlled Indexing": [
                "encoding",
                "feature extraction",
                "image recognition",
                "object recognition",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiway local pooling",
                "image recognition",
                "invariant representations",
                "object recognition systems",
                "feature vectors",
                "spatially local neighborhoods",
                "feature vector space",
                "dissimilar features",
                "encoding methods",
                "codebooks",
                "image retrieval",
                "purely spatial locality",
                "spatial pyramid framework",
                "object recognition benchmarks"
            ]
        },
        "id": 337,
        "cited_by": [
            {
                "year": "2017",
                "id": 589
            },
            {
                "year": "2015",
                "id": 131
            },
            {
                "year": "2013",
                "id": 35
            },
            {
                "year": "2013",
                "id": 227
            },
            {
                "year": "2013",
                "id": 424
            }
        ]
    },
    {
        "title": "Scalable object-class retrieval with approximate and top-k ranking",
        "authors": [
            "Mohammad Rastegari",
            "Chen Fang",
            "Lorenzo Torresani"
        ],
        "abstract": "In this paper we address the problem of object-class retrieval in large image data sets: given a small set of training examples defining a visual category, the objective is to efficiently retrieve images of the same class from a large database. We propose two contrasting retrieval schemes achieving good accuracy and high efficiency. The first exploits sparse classification models expressed as linear combinations of a small number of features. These sparse models can be efficiently evaluated using inverted file indexing. Furthermore, we introduce a novel ranking procedure that provides a significant speedup over inverted file indexing when the goal is restricted to finding the top-k (i.e., the k highest ranked) images in the data set. We contrast these sparse retrieval models with a second scheme based on approximate ranking using vector quantization. Experimental results show that our algorithms for object-class retrieval can search a 10 million database in just a couple of seconds and produce categorization accuracy comparable to the best known class-recognition systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6126556",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 6,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Databases",
                "Accuracy",
                "Upper bound",
                "Training",
                "Quantization",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image retrieval",
                "indexing",
                "vector quantisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object-class retrieval",
                "approximate ranking",
                "top-k ranking",
                "image data set",
                "visual category",
                "image retrieval",
                "sparse classification model",
                "inverted file indexing",
                "ranking procedure",
                "vector quantization",
                "class-recognition systems"
            ]
        },
        "id": 338,
        "cited_by": [
            {
                "year": "2013",
                "id": 206
            }
        ]
    }
]