[
    {
        "title": "Regression based bandwidth selection for segmentation using Parzen windows",
        "authors": [
            "Singh",
            "Ahuja"
        ],
        "abstract": "We consider the problem of segmentation of images that can be modelled as piecewise continuous signals having unknown, nonstationary statistics. We propose a solution to this problem which first uses a regression framework to estimate the image PDF, and then mean-shift to find the modes of this PDF. The segmentation follows from mode identification wherein pixel clusters or image segments are identified with unique modes of the multimodal PDF. Each pixel is mapped to a mode using a convergent, iterative process. The effectiveness of the approach depends upon the accuracy of the (implicit) estimate of the underlying multimodal density function and thus on the bandwidth parameters used for its estimate using Parzen windows. Automatic selection of bandwidth parameters is a desired feature of the algorithm. We show that the proposed regression-based model admits a realistic framework to automatically choose bandwidth parameters which minimizes a global error criterion. We validate the theory presented with results on real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238307",
        "reference_list": [
            {
                "year": "2001",
                "id": 58
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 8,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Bandwidth",
                "Image segmentation",
                "Iterative algorithms",
                "Kernel",
                "Pixel",
                "Statistics",
                "Density functional theory",
                "Image converters",
                "Probability density function",
                "Clustering algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "regression analysis",
                "iterative methods",
                "realistic images",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "regression based bandwidth selection",
                "image segmentation",
                "piecewise continuous signal",
                "image PDF",
                "mode identification",
                "pixel clusters",
                "iterative process",
                "Parzen windows",
                "multimodal density function",
                "bandwidth parameters",
                "real image"
            ]
        },
        "id": 0,
        "cited_by": []
    },
    {
        "title": "Learning a classification model for segmentation",
        "authors": [
            "Ren",
            "Malik"
        ],
        "abstract": "We propose a two-class classification model for grouping. Human segmented natural images are used as positive examples. Negative examples of grouping are constructed by randomly matching human segmentations and images. In a preprocessing stage an image is over-segmented into super-pixels. We define a variety of features derived from the classical Gestalt cues, including contour, texture, brightness and good continuation. Information-theoretic analysis is applied to evaluate the power of these grouping cues. We train a linear classifier to combine these features. To demonstrate the power of the classification model, a simple algorithm is used to randomly search for good segmentations. Results are shown on a wide range of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238308",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2001",
                "id": 120
            }
        ],
        "citation": {
            "ieee": 423,
            "other": 287,
            "total": 710
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Humans",
                "Brightness",
                "Image databases",
                "Computer science",
                "Information analysis",
                "Partitioning algorithms",
                "Design optimization",
                "Computer vision",
                "Logistics"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "information analysis",
                "image classification",
                "image matching",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "classification model",
                "grouping",
                "human segmented natural image",
                "Gestalt cues",
                "information-theoretic analysis",
                "linear classifier",
                "image range",
                "image segmentation"
            ]
        },
        "id": 1,
        "cited_by": [
            {
                "year": "2015",
                "id": 153
            },
            {
                "year": "2015",
                "id": 390
            },
            {
                "year": "2013",
                "id": 48
            },
            {
                "year": "2013",
                "id": 163
            },
            {
                "year": "2013",
                "id": 266
            },
            {
                "year": "2011",
                "id": 56
            },
            {
                "year": "2011",
                "id": 167
            },
            {
                "year": "2011",
                "id": 268
            },
            {
                "year": "2011",
                "id": 272
            },
            {
                "year": "2011",
                "id": 280
            },
            {
                "year": "2011",
                "id": 332
            },
            {
                "year": "2009",
                "id": 31
            },
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2009",
                "id": 87
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2009",
                "id": 119
            },
            {
                "year": "2007",
                "id": 3
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2005",
                "id": 185
            }
        ]
    },
    {
        "title": "Image parsing: unifying segmentation, detection, and recognition",
        "authors": [
            "Zhuowen Tu",
            "Xiangrong Chen",
            "Yuille",
            "Zhu"
        ],
        "abstract": "We propose a general framework for parsing images into regions and objects. In this framework, the detection and recognition of objects proceed simultaneously with image segmentation in a competitive and cooperative manner. We illustrate our approach on natural images of complex city scenes where the objects of primary interest are faces and text. This method makes use of bottom-up proposals combined with top-down generative models using the data driven Markov chain Monte Carlo (DDMCMC) algorithm, which is guaranteed to converge to the optimal estimate asymptotically. More precisely, we define generative models for faces, text, and generic regions- e.g. shading, texture, and clutter. These models are activated by bottom-up proposals. The proposals for faces and text are learnt using a probabilistic version of AdaBoost. The DDMCMC combines reversible jump and diffusion dynamics to enable the generative models to explain the input images in a competitive and cooperative manner. Our experiments illustrate the advantages and importance of combining bottom-up and top-down models and of performing segmentation and object detection/recognition simultaneously.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238309",
        "reference_list": [
            {
                "year": "2001",
                "id": 60
            },
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 3,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image recognition",
                "Face detection",
                "Object detection",
                "Proposals",
                "Computer vision",
                "Image generation",
                "Layout",
                "Monte Carlo methods",
                "Cities and towns"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "object detection",
                "object recognition",
                "natural scenes",
                "face recognition",
                "Monte Carlo methods",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image parsing",
                "object detection",
                "image segmentation",
                "natural image",
                "data driven Markov chain Monte Carlo algorithm",
                "AdaBoost",
                "diffusion dynamics",
                "bottom-up model",
                "top-down model",
                "object recognition"
            ]
        },
        "id": 2,
        "cited_by": [
            {
                "year": "2005",
                "id": 16
            },
            {
                "year": "2005",
                "id": 79
            },
            {
                "year": "2005",
                "id": 131
            },
            {
                "year": "2005",
                "id": 149
            },
            {
                "year": "2005",
                "id": 159
            },
            {
                "year": "2005",
                "id": 174
            },
            {
                "year": "2005",
                "id": 178
            },
            {
                "year": "2005",
                "id": 185
            },
            {
                "year": "2005",
                "id": 232
            }
        ]
    },
    {
        "title": "Computing geodesics and minimal surfaces via graph cuts",
        "authors": [
            "Boykov",
            "Kolmogorov"
        ],
        "abstract": "Geodesic active contours and graph cuts are two standard image segmentation techniques. We introduce a new segmentation method combining some of their benefits. Our main intuition is that any cut on a graph embedded in some continuous space can be interpreted as a contour (in 2D) or a surface (in 3D). We show how to build a grid graph and set its edge weights so that the cost of cuts is arbitrarily close to the length (area) of the corresponding contours (surfaces) for any anisotropic Riemannian metric. There are two interesting consequences of this technical result. First, graph cut algorithms can be used to find globally minimum geodesic contours (minimal surfaces in 3D) under arbitrary Riemannian metric for a given set of boundary conditions. Second, we show how to minimize metrication artifacts in existing graph-cut based methods in vision. Theoretically speaking, our work provides an interesting link between several branches of mathematics -differential geometry, integral geometry, and combinatorial optimization. The main technical problem is solved using Cauchy-Crofton formula from integral geometry.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238310",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 155,
            "other": 166,
            "total": 321
        },
        "keywords": {
            "IEEE Keywords": [
                "Geophysics computing",
                "Active contours",
                "Image segmentation",
                "Anisotropic magnetoresistance",
                "Boundary conditions",
                "Computational geometry",
                "Computer vision",
                "Data visualization",
                "Computer science",
                "Costs"
            ],
            "INSPEC: Controlled Indexing": [
                "differential geometry",
                "image segmentation",
                "surface topography",
                "general relativity",
                "optimisation",
                "graph theory",
                "computational geometry",
                "computational complexity"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geodesic active contour",
                "image segmentation technique",
                "2D contour",
                "3D surface",
                "grid graph",
                "surface contour",
                "Riemannian metric",
                "metrication artifacts",
                "graph-cut based method",
                "differential geometry",
                "integral geometry",
                "combinatorial optimization",
                "Cauchy-Crofton formula",
                "minimal surface computing"
            ]
        },
        "id": 3,
        "cited_by": [
            {
                "year": "2015",
                "id": 173
            },
            {
                "year": "2015",
                "id": 197
            },
            {
                "year": "2009",
                "id": 65
            },
            {
                "year": "2009",
                "id": 66
            },
            {
                "year": "2009",
                "id": 93
            },
            {
                "year": "2009",
                "id": 100
            },
            {
                "year": "2009",
                "id": 234
            },
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2007",
                "id": 118
            },
            {
                "year": "2007",
                "id": 130
            },
            {
                "year": "2007",
                "id": 132
            },
            {
                "year": "2007",
                "id": 157
            },
            {
                "year": "2007",
                "id": 163
            },
            {
                "year": "2007",
                "id": 370
            },
            {
                "year": "2005",
                "id": 72
            },
            {
                "year": "2005",
                "id": 161
            },
            {
                "year": "2005",
                "id": 183
            }
        ]
    },
    {
        "title": "Epitomic analysis of appearance and shape",
        "authors": [
            "Jojic",
            "Frey",
            "Kannan"
        ],
        "abstract": "We present novel simple appearance and shape models that we call epitomes. The epitome of an image is its miniature, condensed version containing the essence of the textural and shape properties of the image. As opposed to previously used simple image models, such as templates or basis functions, the size of the epitome is considerably smaller than the size of the image or object it represents, but the epitome still contains most constitutive elements needed to reconstruct the image. A collection of images often shares an epitome, e.g., when images are a few consecutive frames from a video sequence, or when they are photographs of similar objects. A particular image in a collection is defined by its epitome and a smooth mapping from the epitome to the image pixels. When the epitomic representation is used within a hierarchical generative model, appropriate inference algorithms can be derived to extract the epitome from a single image or a collection of images and at the same time perform various inference tasks, such as image segmentation, motion estimation, object removal and super-resolution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238311",
        "reference_list": [
            {
                "year": "2001",
                "id": 110
            }
        ],
        "citation": {
            "ieee": 85,
            "other": 37,
            "total": 122
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image segmentation",
                "Pixel",
                "Computer vision",
                "Application software",
                "Deformable models",
                "Image reconstruction",
                "Video sequences",
                "Inference algorithms",
                "Motion estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "motion estimation",
                "image reconstruction",
                "image coding"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape model",
                "epitomic analysis",
                "image reconstruction",
                "video sequence",
                "image pixels",
                "inference algorithm",
                "image segmentation",
                "image motion estimation",
                "object removal",
                "super-resolution"
            ]
        },
        "id": 4,
        "cited_by": [
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2011",
                "id": 171
            },
            {
                "year": "2007",
                "id": 14
            }
        ]
    },
    {
        "title": "Segmenting foreground objects from a dynamic textured background via a robust Kalman filter",
        "authors": [
            "Jing Zhong",
            "Sclaroff"
        ],
        "abstract": "The algorithm presented aims to segment the foreground objects in video (e.g., people) given time-varying, textured backgrounds. Examples of time-varying backgrounds include waves on water, clouds moving, trees waving in the wind, automobile traffic, moving crowds, escalators, etc. We have developed a novel foreground-background segmentation algorithm that explicitly accounts for the nonstationary nature and clutter-like appearance of many dynamic textures. The dynamic texture is modeled by an autoregressive moving average model (ARMA). A robust Kalman filter algorithm iteratively estimates the intrinsic appearance of the dynamic texture, as well as the regions of the foreground objects. Preliminary experiments with this method have demonstrated promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238312",
        "reference_list": [
            {
                "year": "2001",
                "id": 49
            },
            {
                "year": "2001",
                "id": 163
            }
        ],
        "citation": {
            "ieee": 107,
            "other": 43,
            "total": 150
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Vehicle dynamics",
                "Autoregressive processes",
                "Object detection",
                "Iterative algorithms",
                "Statistical distributions",
                "Layout",
                "Computer science",
                "Clouds",
                "Automobiles"
            ],
            "INSPEC: Controlled Indexing": [
                "autoregressive moving average processes",
                "Kalman filters",
                "image segmentation",
                "algorithm theory",
                "iterative methods",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "foreground objects segmentation",
                "video object",
                "time-varying background",
                "automobile traffic",
                "escalators",
                "foreground-background segmentation algorithm",
                "dynamic textured background",
                "autoregressive moving average model",
                "robust Kalman filter",
                "ARMA model"
            ]
        },
        "id": 5,
        "cited_by": [
            {
                "year": "2009",
                "id": 8
            },
            {
                "year": "2009",
                "id": 156
            },
            {
                "year": "2009",
                "id": 266
            },
            {
                "year": "2007",
                "id": 375
            },
            {
                "year": "2005",
                "id": 87
            }
        ]
    },
    {
        "title": "Ranking prior likelihood distributions for Bayesian shape localization framework",
        "authors": [
            "Shuicheng Yan",
            "Mingjing Li",
            "Hongjiang Zhang",
            "Qiansheng Cheng"
        ],
        "abstract": "We formulate the shape localization problem in the Bayesian framework. In the learning stage, we propose the Constrained Rank-Boost approach to model the likelihood of local features associated with the key points of an object, like face, while preserve the prior ranking order between the ground truth position of a key point and its neighbors; in the inferring stage, a simple efficient iterative algorithm is proposed to uncover the MAP shape by locally modeling the likelihood distribution around each key point via our proposed variational locally weighted learning (VLWL) method. Our proposed framework has the following benefits: 1) compared to the classical PCA models, the likelihood presented by the ranking prior likelihood model has more discriminating power as to the optimal position and its neighbors, especially in the problem with ambiguity between the optimal positions and their neighbors; 2) the VLWL method guarantees that the posterior probability of the derived shape increases monotonously; and 3) the above two methods are both based on accurate probability formulation, which spontaneously leads to a robust confidence measure for the discovered shape. Moreover, we present a theoretical analysis for the convergence of the Constrained Rank-Boost. Extensive experiments compared with the active shape models demonstrate the accuracy, robustness, and stability of our proposed framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238313",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 0,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Shape measurement",
                "Robust stability",
                "Iterative algorithms",
                "Principal component analysis",
                "Robustness",
                "Power measurement",
                "Position measurement",
                "Convergence",
                "Constraint theory"
            ],
            "INSPEC: Controlled Indexing": [
                "principal component analysis",
                "Bayes methods",
                "maximum likelihood estimation",
                "face recognition",
                "algorithm theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape localization problem",
                "Bayesian framework",
                "Constrained Rank-Boost approach",
                "MAP shape",
                "prior likelihood distribution modeling",
                "variational locally weighted learning",
                "PCA model"
            ]
        },
        "id": 6,
        "cited_by": []
    },
    {
        "title": "Efficient, robust and accurate fitting of a 3D morphable model",
        "authors": [
            "Romdhani",
            "Vetter"
        ],
        "abstract": "3D morphable models, as a means to generate images of a class of objects and to analyze them, have become increasingly popular. The problematic part of this framework is the registration of the model to an image, a.k.a. the fitting. The characteristic features of a fitting algorithm are its efficiency, robustness, accuracy and automation. Many accurate algorithms based on gradient descent techniques exist which are unfortunately short on the other features. Recently, an efficient algorithm called inverse compositional image alignment (ICIA) algorithm, able to fit 2D images, was introduced. We extent this algorithm to fit 3D morphable models using a novel mathematical notation which facilitates the formulation of the fitting problem. This formulation enables us to avoid a simplification so far used in the ICIA, being as efficient and leading to improved fitting precision. Additionally, the algorithm is robust without sacrificing its efficiency and accuracy, thereby conforming to three of the four characteristics of a good fitting algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238314",
        "reference_list": [],
        "citation": {
            "ieee": 56,
            "other": 44,
            "total": 100
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Image analysis",
                "Computer vision",
                "Shape",
                "Image generation",
                "Automation",
                "Fitting",
                "Humans",
                "Lighting",
                "Approximation algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "solid modelling",
                "computer vision",
                "image colour analysis",
                "image texture",
                "image morphing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D morphable model",
                "inverse compositional image alignment algorithm",
                "mathematical notation",
                "improved fitting precision",
                "computer vision",
                "image texture",
                "image morphing"
            ]
        },
        "id": 7,
        "cited_by": [
            {
                "year": "2005",
                "id": 65
            },
            {
                "year": "2005",
                "id": 114
            }
        ]
    },
    {
        "title": "Statistical background subtraction for a mobile observer",
        "authors": [
            "Hayman",
            "Eklundh"
        ],
        "abstract": "Statistical background modelling and subtraction has proved to be a popular and effective class of algorithms for segmenting independently moving foreground objects out from a static background, without requiring any a priori information of the properties of foreground objects. We present two contributions on this topic, aimed towards robotics where an active head is mounted on a mobile vehicle. In periods when the vehicle's wheels are not driven, camera translation is virtually zero, and background subtraction techniques are applicable. This is also highly relevant to surveillance and video conferencing. The first part presents an efficient probabilistic framework for when the camera pans and tilts. A unified approach is developed for handling various sources of error, including motion blur, subpixel camera motion, mixed pixels at object boundaries, and also uncertainty in background stabilisation caused by noise, unmodelled radial distortion and small translations of the camera. The second contribution regards a Bayesian approach to specifically incorporate uncertainty concerning whether the background has yet been uncovered by moving foreground objects. This is an important requirement during initialisation of a system. We cannot assume that a background model is available in advance since that would involve storing models for each possible position, in every room, of the robot's operating environment.. Instead the background mode must be generated online, very possibly in the presence of moving objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238315",
        "reference_list": [
            {
                "year": "2001",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 80,
            "other": 51,
            "total": 131
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Uncertainty",
                "Mobile robots",
                "Vehicle driving",
                "Wheels",
                "Robot vision systems",
                "Subtraction techniques",
                "Surveillance",
                "Videoconference",
                "Background noise"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image motion analysis",
                "image segmentation",
                "image sequences",
                "mobile robots",
                "video cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "statistical background modelling",
                "robotics",
                "video conferencing",
                "radial distortion",
                "Bayesian approach",
                "image motion analysis",
                "image segmentation",
                "image sequences",
                "mobile robots",
                "video cameras"
            ]
        },
        "id": 8,
        "cited_by": [
            {
                "year": "2015",
                "id": 360
            },
            {
                "year": "2013",
                "id": 196
            },
            {
                "year": "2011",
                "id": 276
            },
            {
                "year": "2009",
                "id": 156
            }
        ]
    },
    {
        "title": "Joint region tracking with switching hypothesized measurements",
        "authors": [
            "Yang Wang",
            "Tan",
            "Loe"
        ],
        "abstract": "We propose a switching hypothesized measurements (SHM) model supporting multimodal probability distributions and present the application of the model in handling potential variability in visual environments when tracking multiple objects jointly. For a set of occlusion hypotheses, a frame is measured once under each hypothesis, resulting in a set of measurements at each time instant. A computationally efficient SHM filter is derived for online joint region tracking. Both occlusion relationships and states of the objects are recursively estimated from the history of hypothesized measurements. The reference image is updated adaptively to deal with appearance changes of the objects. The SHM model is generally applicable to various dynamic processes with multiple alternative measurement methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238316",
        "reference_list": [
            {
                "year": "2001",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Time measurement",
                "Probability distribution",
                "State-space methods",
                "State estimation",
                "History",
                "Superluminescent diodes",
                "Switches",
                "Filters",
                "Recursive estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "hidden feature removal",
                "Kalman filters",
                "state-space methods",
                "tracking filters"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint region tracking",
                "switching hypothesized measurements model",
                "multimodal probability distribution",
                "occlusion hypotheses",
                "recursive estimation"
            ]
        },
        "id": 9,
        "cited_by": []
    },
    {
        "title": "The local projective shape of smooth surfaces and their outlines",
        "authors": [
            "Lazebnik",
            "Ponce"
        ],
        "abstract": "We examine projectively invariant local properties of smooth curves and surfaces. Oriented projective differential geometry is proposed as a theoretical framework for establishing such invariants and describing the local shape of surfaces and their outlines. This framework is applied to two problems: a projective proof of Koenderink's famous characterization of convexities, concavities, and inflections of apparent contours; and the determination of the relative orientation of rim tangents at frontier points.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238317",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Information geometry",
                "Application software",
                "Computational geometry",
                "Computer vision",
                "Solids",
                "Cameras",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "differential geometry",
                "computational geometry",
                "image reconstruction",
                "computer vision",
                "surface fitting",
                "curve fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "oriented projective differential geometry",
                "rim tangents",
                "computational geometry",
                "image reconstruction",
                "computer vision",
                "surface fitting",
                "curve fitting"
            ]
        },
        "id": 10,
        "cited_by": []
    },
    {
        "title": "Active concept learning for image retrieval in dynamic databases",
        "authors": [
            "Dong",
            "Bhanu"
        ],
        "abstract": "Concept learning in content-based image retrieval (CBIR) systems is a challenging task. We present an active concept learning approach based on mixture model to deal with the two basic aspects of a database system: changing (image insertion or removal) nature of a database and user queries. To achieve concept learning, we develop a novel model selection method based on Bayesian analysis that evaluates the consistency of hypothesized models with the available information. The analysis of exploitation vs. exploration in the search space helps to find optimal model efficiently. Experimental results on Corel database show the efficacy of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238318",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 6,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Information retrieval",
                "Image retrieval",
                "Image databases",
                "Parameter estimation",
                "Bayesian methods",
                "Feedback",
                "Content based retrieval",
                "Spatial databases",
                "Intelligent systems",
                "Database systems"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "image retrieval",
                "visual databases",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic databases",
                "content-based image retrieval systems",
                "active concept learning approach",
                "user queries",
                "Bayesian analysis",
                "search space exploration",
                "Corel database"
            ]
        },
        "id": 11,
        "cited_by": []
    },
    {
        "title": "Video input driven animation (VIDA)",
        "authors": [
            "Sun",
            "Jepson",
            "Fiume"
        ],
        "abstract": "There are many challenges associated with the integration of synthetic and real imagery. One particularly difficult problem is the automatic extraction of salient parameters of natural phenomena in real video footage for subsequent application to synthetic objects. We can ensure that the hair and clothing of a synthetic actor placed in a meadow of swaying grass will move consistently with the wind that moved that grass. The video footage can be seen as a controller for the motion of synthetic features, a concept we call video input driven animation (VIDA). We propose a schema that analyzes an input video sequence, extracts parameters from the motion of objects in the video, and uses this information to drive the motion of synthetic objects. To validate the principles of VIDA, we approximate the inverse problem to harmonic oscillation, which we use to extract parameters of wind and of regular water waves. We observe the effect of wind on a tree in a video, estimate wind speed parameters from its motion, and then use this to make synthetic objects move. We also extract water elevation parameters from the observed motion of boats and apply the resulting water waves to synthetic boats.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238319",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 9,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Animation",
                "Data mining",
                "Motion analysis",
                "Boats",
                "Hair",
                "Clothing",
                "Motion control",
                "Information analysis",
                "Video sequences",
                "Inverse problems"
            ],
            "INSPEC: Controlled Indexing": [
                "computer animation",
                "image sequences",
                "water waves",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video input driven animation",
                "input video sequence",
                "parameter extraction",
                "harmonic oscillation",
                "synthetic object",
                "water waves",
                "computer vision"
            ]
        },
        "id": 12,
        "cited_by": []
    },
    {
        "title": "Automatic video summarization by graph modeling",
        "authors": [
            "Chong-Wah Ngo",
            "Yu-Fei Ma",
            "Hong-Jiang Zhang"
        ],
        "abstract": "We propose a unified approach for summarization based on the analysis of video structures and video highlights. Our approach emphasizes both the content balance and perceptual quality of a summary. Normalized cut algorithm is employed to globally and optimally partition a video into clusters. A motion attention model based on human perception is employed to compute the perceptual quality of shots and clusters. The clusters, together with the computed attention values, form a temporal graph similar to Markov chain that inherently describes the evolution and perceptual importance of video clusters. In our application, the flow of a temporal graph is utilized to group similar clusters into scenes, while the attention values are used as guidelines to select appropriate subshots in scenes for summarization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238320",
        "reference_list": [
            {
                "year": "2001",
                "id": 150
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 2,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Computer science",
                "Asia",
                "Partial response channels",
                "Clustering algorithms",
                "Partitioning algorithms",
                "Humans",
                "Guidelines",
                "Entropy",
                "Motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "Markov processes",
                "image segmentation",
                "pattern clustering",
                "visual perception"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic video summarization",
                "graph modeling",
                "video structure analysis",
                "normalized cut algorithm",
                "human perception",
                "temporal graph",
                "Markov chain",
                "video clusters"
            ]
        },
        "id": 13,
        "cited_by": []
    },
    {
        "title": "A non-iterative greedy algorithm for multi-frame point correspondence",
        "authors": [
            "Shafique",
            "Shah"
        ],
        "abstract": "We present a framework for finding point correspondences in monocular image sequences over multiple frames. The general problem of multiframe point correspondence is NP hard for three or more frames. A polynomial time algorithm for a restriction of this problem is presented, and is used as the basis of proposed greedy algorithm for the general problem. The greedy nature of the proposed algorithm allows it to be used in real time systems for tracking and surveillance etc. In addition, the proposed algorithm deals with the problems of occlusion, missed detections, and false positives, by using a single noniterative greedy optimization scheme, and hence, reduces the complexity of the overall algorithm as compared to most existing approaches, where multiple heuristics are used for the same purpose. While most greedy algorithms for point tracking do not allow for entry and exit of points from the scene, this is not a limitation for the proposed algorithm. Experiments with real and synthetic data show that the proposed algorithm outperforms the existing techniques and is applicable in more general settings.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238321",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 24,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Greedy algorithms",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "algorithm theory",
                "feature extraction",
                "optimisation",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "noniterative greedy algorithm",
                "multiframe point correspondence",
                "monocular image sequences",
                "NP hard problem",
                "polynomial time algorithm",
                "occlusion",
                "feature extraction",
                "optimisation",
                "tracking"
            ]
        },
        "id": 14,
        "cited_by": [
            {
                "year": "2003",
                "id": 125
            }
        ]
    },
    {
        "title": "Variational frameworks for DT-MRI estimation, regularization and visualization",
        "authors": [
            "Tschumperle",
            "Deriche"
        ],
        "abstract": "We address three crucial issues encountered in DT-MRI (diffusion tensor magnetic resonance imaging): diffusion tensor estimation, regularization and fiber bundle visualization. We first review related algorithms existing in the literature and propose then alternative variational formalisms that lead to new and improved schemes, thanks to the preservation of important tensor constraints (positivity, symmetry). We illustrate how our complete DT-MRI processing pipeline can be successfully used to construct and draw fiber bundles in the white matter of the brain, from a set of noisy raw MRl images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238323",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 20,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Diffusion tensor imaging",
                "Visualization",
                "Tensile stress",
                "Magnetic resonance imaging",
                "Pipelines",
                "Symmetric matrices",
                "Neurons",
                "Magnetic noise",
                "Biomedical imaging",
                "Motion measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "data visualisation",
                "tensors",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "DT-MRI",
                "diffusion tensor magnetic resonance imaging",
                "diffusion tensor estimation",
                "fiber bundle visualization",
                "variational framework",
                "processing pipeline",
                "MRl images",
                "human brain",
                "regularization process"
            ]
        },
        "id": 15,
        "cited_by": []
    },
    {
        "title": "Counting people in crowds with a real-time network of simple image sensors",
        "authors": [
            "Yang",
            "Gonzalez-Banos",
            "Guibas"
        ],
        "abstract": "Estimating the number of people in a crowded environment is a central task in civilian surveillance. Most vision-based counting techniques depend on detecting individuals in order to count, an unrealistic proposition in crowded settings. We propose an alternative approach that directly estimates the number of people. In our system, groups of image sensors segment foreground objects from the background, aggregate the resulting silhouettes over a network, and compute a planar projection of the scene's visual hull. We introduce a geometric algorithm that calculates bounds on the number of persons in each region of the projection, after phantom regions have been eliminated. The computational requirements scale well with the number of sensors and the number of people, and only limited amounts of data are transmitted over the network. Because of these properties, our system runs in real-time and can be deployed as an untethered wireless sensor network. We describe the major components of our system, and report preliminary experiments with our first prototype implementation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238325",
        "reference_list": [
            {
                "year": "2001",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 81,
            "other": 55,
            "total": 136
        },
        "keywords": {
            "IEEE Keywords": [
                "Intelligent networks",
                "Image sensors",
                "Computer networks",
                "Computer science",
                "Prototypes",
                "Resource management",
                "Bandwidth",
                "Sensor systems",
                "Research and development",
                "Surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "image sensors",
                "real-time systems",
                "wireless sensor networks",
                "motion estimation",
                "television interference"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time network",
                "crowded environment",
                "vision-based counting techniques",
                "image sensors",
                "geometric algorithm",
                "untethered wireless sensor network",
                "phantom region",
                "data transmission"
            ]
        },
        "id": 16,
        "cited_by": []
    },
    {
        "title": "Fragmentation in the vision of scenes",
        "authors": [
            "Geusebroek",
            "Smeulders"
        ],
        "abstract": "Natural images are highly structured in their spatial configuration. Where one would expect a different spatial distribution for every image, as each image has a different spatial layout, we show that the spatial statistics of recorded images can be explained by a single process of sequential fragmentation. The observation by a resolution limited sensory system turns out to have a profound influence on the observed statistics of natural images. The power-law and normal distribution represent the extreme cases of sequential fragmentation. Between these two extremes, spatial detail statistics deform from power-law to normal through the Weibull type distribution as receptive field size increases relative to image detail size.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238326",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 8,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Statistical distributions",
                "Machine vision",
                "Spatial resolution",
                "Image resolution",
                "Gaussian distribution",
                "Statistics",
                "Statistical analysis",
                "Intelligent systems",
                "Intelligent sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "natural scenes",
                "realistic images",
                "image sequences",
                "normal distribution",
                "Weibull distribution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "natural images",
                "spatial configuration",
                "sequential fragmentation",
                "sensory system",
                "power-law distribution",
                "normal distribution",
                "Weibull type distribution"
            ]
        },
        "id": 17,
        "cited_by": []
    },
    {
        "title": "Eye gaze estimation from a single image of one eye",
        "authors": [
            "Wang",
            "Sung",
            "Ronda Venkateswarlu"
        ],
        "abstract": "We present a novel approach, called the \"one-circle \" algorithm, for measuring the eye gaze using a monocular image that zooms in on only one eye of a person. Observing that the iris contour is a circle, we estimate the normal direction of this iris circle, considered as the eye gaze, from its elliptical image. From basic projective geometry, an ellipse can be back-projected into space onto two circles of different orientations. However, by using an anthropometric property of the eyeball, the correct solution can be disambiguated. This allows us to obtain a higher resolution image of the iris with a zoom-in camera and thereby achieving higher accuracies in the estimation. The robustness of our gaze determination approach was verified statistically by the extensive experiments on synthetic and real image data. The two key contributions are that we show the possibility of finding the unique eye gaze direction from a single image of one eye and that one can obtain better accuracy as a consequence of this.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238328",
        "reference_list": [],
        "citation": {
            "ieee": 49,
            "other": 23,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Iris",
                "Head",
                "Image edge detection",
                "Geometry",
                "Robustness",
                "Humans",
                "Image resolution",
                "Joining processes",
                "Cameras",
                "Sockets"
            ],
            "INSPEC: Controlled Indexing": [
                "eye",
                "image processing",
                "estimation theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "one circle algorithm",
                "eye gaze estimation",
                "monocular image",
                "iris contour",
                "elliptical image",
                "anthropometric property",
                "gaze determination approach",
                "real image data"
            ]
        },
        "id": 18,
        "cited_by": [
            {
                "year": "2017",
                "id": 331
            },
            {
                "year": "2011",
                "id": 19
            }
        ]
    },
    {
        "title": "Calibrating pan-tilt cameras in wide-area surveillance networks",
        "authors": [
            "Davis",
            "Chen"
        ],
        "abstract": "Pan-tilt cameras are often used as components of wide-area surveillance systems. It is necessary to calibrate these cameras in relation to one another in order to obtain a consistent representation of the entire space. Existing methods for calibrating pan-tilt cameras have assumed an idealized model of camera mechanics. In addition, most methods have been calibrated using only a small range of camera motion. We present a method for calibrating pan-tilt cameras that introduces a more complete model of camera motion. Pan and tilt rotations are modeled as occurring around arbitrary axes in space. In addition, the wide area surveillance system itself is used to build a large virtual calibration object, resulting in better calibration than would be possible with a single small calibration target. Finally, the proposed enhancements are validated experimentally, with comparisons showing the improvement provided over more traditional methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238329",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 26,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Intelligent networks",
                "Surveillance",
                "Calibration",
                "Target tracking",
                "Focusing",
                "Costs",
                "Solid modeling",
                "Optical imaging",
                "Geometrical optics"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "surveillance",
                "video cameras",
                "calibration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pan-tilt cameras",
                "wide-area surveillance systems",
                "camera motion",
                "virtual calibration object"
            ]
        },
        "id": 19,
        "cited_by": []
    },
    {
        "title": "Calibration of a hybrid camera network",
        "authors": [
            "Xilin Chen",
            "Jie Yang",
            "Waibel"
        ],
        "abstract": "Visual surveillance using a camera network has imposed new challenges to camera calibration. An essential problem is that a large number of cameras may not have a common field of view or even be synchronized well. We propose to use a hybrid camera network that consists of catadioptric and perspective cameras for a visual surveillance task. The relations between multiple views of a scene captured from different cameras can be then calibrated under the catadioptric camera's coordinate system. We address the important issue of how to calibrate the hybrid camera network. We calibrate the hybrid camera network in three steps. First, we calibrate the catadioptric camera using only the vanishing points. In order to reduce computational complexity, we calibrate the camera without the mirror first and then calibrate the catadioptric camera system. Second, we determine 3D positions of some points using as few as two spatial parallel lines and some equidistance points. Finally, we calibrate other perspective cameras based on these known spatial points.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238330",
        "reference_list": [
            {
                "year": "2001",
                "id": 18
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Surveillance",
                "Mirrors",
                "Layout",
                "Computer science",
                "Computational complexity",
                "Monitoring",
                "Target tracking",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "video cameras",
                "computer vision",
                "calibration",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual surveillance",
                "camera calibration",
                "hybrid camera network",
                "perspective cameras",
                "visual surveillance task",
                "catadioptric camera",
                "vanishing points",
                "computational complexity",
                "3D positions",
                "spatial parallel lines",
                "equidistance points"
            ]
        },
        "id": 20,
        "cited_by": []
    },
    {
        "title": "Image registration with global and local luminance alignment",
        "authors": [
            "Jiaya Jia",
            "Chi-Keung Tang"
        ],
        "abstract": "Inspired by tensor voting, we present luminance voting, a novel approach for image registration with global and local luminance alignment. The key to our modeless approach is the direct estimation of replacement function, by reducing the complex estimation problem to the robust 2D tensor voting in the corresponding voting spaces. No model for replacement function is assumed. Luminance data are first encoded into 2D ball tensors. Subject to the monotonic constraint only, we vote for an optimal replacement function by propagating the smoothness constraint using a dense tensor field. Our method effectively infers missing curve segments and rejects image outliers without assuming any simplifying or complex curve model. The voted replacement functions are used in our iterative registration algorithm for computing the best warping matrix. Unlike previous approaches, our robust method corrects exposure disparity even if the two overlapping images are initially misaligned. Luminance voting is effective in correcting exposure difference, eliminating vignettes, and thus improving image registration. We present results on a variety of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238331",
        "reference_list": [
            {
                "year": "2001",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 3,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Image registration",
                "Voting",
                "Tensile stress",
                "Robustness",
                "Layout",
                "Digital cameras",
                "Computer graphics",
                "Computer science",
                "Image segmentation",
                "Iterative algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "brightness",
                "tensors",
                "estimation theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "luminance voting",
                "image registration",
                "global luminance alignment",
                "local luminance alignment",
                "replacement function",
                "complex estimation problem",
                "2D tensor voting",
                "monotonic constraint",
                "dense tensor field",
                "complex curve model",
                "iterative registration algorithm",
                "best warping matrix"
            ]
        },
        "id": 21,
        "cited_by": [
            {
                "year": "2005",
                "id": 215
            }
        ]
    },
    {
        "title": "Highlight removal by illumination-constrained inpainting",
        "authors": [
            "PingTan",
            "Lin",
            "Long Quan",
            "Heung-Yeung Shum"
        ],
        "abstract": "We present a single-image highlight removal method that incorporates illumination-based constraints into image inpainting. Unlike occluded image regions filled by traditional inpainting, highlight pixels contain some useful information for guiding the inpainting process. Constraints provided by observed pixel colors, highlight color analysis and illumination color uniformity are employed in our method to improve estimation of the underlying diffuse color. The inclusion of these illumination constraints allows for better recovery of shading and textures by inpainting. Experimental results are given to demonstrate the performance of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238333",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Colored noise",
                "Lighting",
                "Optical polarization",
                "Surface fitting",
                "Optical reflection",
                "Histograms",
                "Rough surfaces",
                "Surface roughness",
                "Shape measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image texture",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single image highlight removal method",
                "illumination-based constraints",
                "image inpainting",
                "occluded image regions",
                "inpainting process",
                "pixel colors",
                "highlight color analysis",
                "illumination color uniformity",
                "image texture",
                "image shading"
            ]
        },
        "id": 22,
        "cited_by": [
            {
                "year": "2005",
                "id": 220
            }
        ]
    },
    {
        "title": "Surface reflectance modeling of real objects with interreflections",
        "authors": [
            "Machida",
            "Yokoya",
            "Takemura"
        ],
        "abstract": "In mixed reality, especially in augmented virtuality which virtualizes real objects, it is important to estimate object surface reflectance properties to render the objects under arbitrary illumination conditions. Though several methods have been explored to estimate the surface reflectance properties, it is still difficult to estimate surface reflectance parameters faithfully for complex objects which have nonuniform surface reflectance properties and exhibit interreflections. We describe a new method for densely estimating nonuniform surface reflectance properties of real objects constructed of convex and concave surfaces with interreflections. We use registered range and surface color texture images obtained by a laser rangefinder. Experiments show the usefulness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238335",
        "reference_list": [
            {
                "year": "2001",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflectivity",
                "Optical reflection",
                "Parameter estimation",
                "Shape",
                "Virtual reality",
                "Lighting",
                "Surface fitting",
                "Rendering (computer graphics)",
                "Surface treatment",
                "Robots"
            ],
            "INSPEC: Controlled Indexing": [
                "reflection",
                "solid modelling",
                "augmented reality",
                "rendering (computer graphics)",
                "image texture",
                "image colour analysis",
                "surface fitting",
                "laser ranging"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonuniform surface reflectance property estimation",
                "real object",
                "augmented virtuality",
                "arbitrary illumination condition",
                "interreflection",
                "surface reflectance parameter",
                "registered range image",
                "surface color texture image",
                "laser rangefinder"
            ]
        },
        "id": 23,
        "cited_by": [
            {
                "year": "2005",
                "id": 184
            },
            {
                "year": "2005",
                "id": 188
            }
        ]
    },
    {
        "title": "Variable bandwidth QMDPE and its application in robust optical flow estimation",
        "authors": [
            "Wang",
            "Suter"
        ],
        "abstract": "Robust estimators, such as least median of squared (LMedS) residuals, M-estimators, the least trimmed squares (LTS) etc., have been employed to estimate optical flow from image sequences in recent years. However, these robust estimators have a breakdown point of no more than 50%. We propose a novel robust estimator, called variable bandwidth quick maximum density power estimator (vbQMDPE), which can tolerate more than 50% outliers. We apply the novel proposed estimator to robust optical flow estimation. Our method yields better results than most other recently proposed methods, and it has the potential to better handle multiple motion effects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238337",
        "reference_list": [
            {
                "year": "2001",
                "id": 22
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 4,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Bandwidth",
                "Robustness",
                "Image motion analysis",
                "Optical computing",
                "Electric breakdown",
                "Equations",
                "Image sequences",
                "Computer vision",
                "Least squares approximation",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "estimation theory",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optical flow estimation",
                "robust estimator",
                "least median of squared residual",
                "LMedS",
                "M-estimator",
                "least trimmed square",
                "LTS",
                "image sequence",
                "variable bandwidth quick maximum density power estimator",
                "vbQMDPE",
                "multiple motion effect"
            ]
        },
        "id": 24,
        "cited_by": []
    },
    {
        "title": "Surface reconstruction from feature based stereo",
        "authors": [
            "Taylor"
        ],
        "abstract": "We describe an approach to recovering surface models of complex scenes from the quasisparse data returned by a feature based stereo system. The method can be used to merge stereo results obtained from different viewpoints into a single coherent surface mesh. The technique proceeds by exploiting the freespace theorem which provides a principled mechanism for reasoning about the structure of the scene based on quasisparse correspondences in multiple image. Effective methods for overcoming the difficulties posed by missing features and outliers are discussed. Results obtained by applying this approach to actual images are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238338",
        "reference_list": [
            {
                "year": "2001",
                "id": 132
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 11,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Layout",
                "Stereo vision",
                "Image reconstruction",
                "Stereo image processing",
                "Surface structures",
                "Pixel",
                "Laboratories",
                "Clustering algorithms",
                "Merging"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "feature extraction",
                "image texture",
                "computational geometry",
                "solid modelling",
                "stereo image processing",
                "surface fitting",
                "mesh generation",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface reconstruction",
                "feature based stereo",
                "surface model recovery",
                "quasisparse data",
                "coherent surface mesh",
                "freespace theorem"
            ]
        },
        "id": 25,
        "cited_by": []
    },
    {
        "title": "Gaze manipulation for one-to-one teleconferencing",
        "authors": [
            "Criminisi",
            "Shotton",
            "Blake",
            "Torr"
        ],
        "abstract": "A new algorithm is proposed for novel view generation in one-to-one teleconferencing applications. Given the video streams acquired by two cameras placed on either side of a computer monitor, the proposed algorithm synthesizes images from a virtual camera in arbitrary position (typically located within the monitor) to facilitate eye contact. Our technique is based on an improved, dynamic-programming, stereo algorithm for efficient novel-view generation. The two main contributions are: i) a new type of three-plane graph for dense-stereo dynamic-programming, that encourages correct occlusion labeling; ii) a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface. Furthermore, we present a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts (flicker); and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space. Examples are given that demonstrate the robustness of the new algorithm to spatial and temporal artefacts for long stereo video streams. These include demonstrations of synthesis of cyclopean views of extended conversational sequences. We further demonstrate synthesis from a freely translating virtual camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238340",
        "reference_list": [],
        "citation": {
            "ieee": 32,
            "other": 24,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Teleconferencing",
                "Cameras",
                "Streaming media",
                "Costs",
                "Application software",
                "Computer displays",
                "Computerized monitoring",
                "Heuristic algorithms",
                "Labeling",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "teleconferencing",
                "video coding",
                "video cameras",
                "dynamic programming",
                "rendering (computer graphics)",
                "image sequences",
                "image matching",
                "hidden feature removal",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gaze manipulation",
                "one-to-one teleconferencing",
                "video stream",
                "virtual camera",
                "novel-view generation",
                "three-plane graph",
                "dense-stereo dynamic-programming",
                "occlusion labeling",
                "minimum-cost surface",
                "background model temporal maintenance",
                "cost aggregation algorithm",
                "three-dimensional matching cost space",
                "spatial artefact",
                "temporal artefact",
                "cyclopean view synthesis",
                "conversational sequence"
            ]
        },
        "id": 26,
        "cited_by": [
            {
                "year": "2007",
                "id": 159
            }
        ]
    },
    {
        "title": "Preemptive RANSAC for live structure and motion estimation",
        "authors": [
            "Nister"
        ],
        "abstract": "A system capable of performing robust live ego-motion estimation for perspective cameras is presented. The system is powered by random sample consensus with preemptive scoring of the motion hypotheses. A general statement of the problem of efficient preemptive scoring is given. Then a theoretical investigation of preemptive scoring under a simple inlier-outlier model is performed. A practical preemption scheme is proposed and it is shown that the preemption is powerful enough to enable robust live structure and motion estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238341",
        "reference_list": [
            {
                "year": "2001",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 99,
            "other": 70,
            "total": 169
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Robustness",
                "Cameras",
                "Delay estimation",
                "Layout",
                "Computer vision",
                "Cost function",
                "Collaboration",
                "Government",
                "Real time systems"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "video cameras",
                "random processes",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "random sample consensus",
                "preemptive RANSAC",
                "live structure",
                "ego-motion estimation",
                "preemptive scoring",
                "motion hypotheses",
                "inlier-outlier model"
            ]
        },
        "id": 27,
        "cited_by": [
            {
                "year": "2011",
                "id": 164
            },
            {
                "year": "2009",
                "id": 137
            },
            {
                "year": "2009",
                "id": 269
            },
            {
                "year": "2009",
                "id": 282
            },
            {
                "year": "2007",
                "id": 386
            },
            {
                "year": "2005",
                "id": 81
            },
            {
                "year": "2005",
                "id": 225
            }
        ]
    },
    {
        "title": "Multiple-view structure and motion from line correspondences",
        "authors": [
            "Bartoli",
            "Sturm"
        ],
        "abstract": "We address the problem of camera motion and structure reconstruction from line correspondences across multiple views, from initialization to final bundle adjustment. One of the main difficulties when dealing with line features is their algebraic representation. First, we consider the triangulation problem. Based on Plucker coordinates to represent the lines, we propose a maximum likelihood algorithm, relying on linearising the Plucker constraint, and on a Plucker correction procedure to compute the closest Plucker coordinates to a given 6-vector. Second, we consider the bundle adjustment problem. Previous overparameterizations of 3D lines induce gauge freedoms and/or internal consistency constraints. We propose the orthonormal representation, which allows handy nonlinear optimization of 3D lines using the minimum 4 parameters, within an unconstrained nonlinear optimizer. We compare our algorithms to existing ones on simulated and real data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238342",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 10,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Tensile stress",
                "Layout",
                "Maximum likelihood estimation",
                "Calibration",
                "Geometry",
                "Motion estimation",
                "Kalman filters",
                "Filtering"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "mesh generation",
                "computational geometry",
                "optimisation",
                "video cameras",
                "image motion analysis",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple-view structure reconstruction",
                "3D line correspondence",
                "camera motion",
                "algebraic representation",
                "triangulation problem",
                "Plucker coordinate",
                "maximum likelihood algorithm",
                "Plucker correction procedure",
                "bundle adjustment problem",
                "internal consistency constraint",
                "orthonormal representation",
                "nonlinear optimization"
            ]
        },
        "id": 28,
        "cited_by": []
    },
    {
        "title": "Modeling textured motion : particle, wave and sketch",
        "authors": [
            "Yizhou Wang",
            "Zhu"
        ],
        "abstract": "We present a generative model for textured motion phenomena, such as falling snow, wavy river and dancing grass, etc. Firstly, we represent an image as a linear superposition of image bases selected from a generic and over-complete dictionary. The dictionary contains Gabor bases for point/particle elements and Fourier bases for wave-elements. These bases compete to explain the input images. The transform from a raw image to a base or a token representation leads to large dimension reduction. Secondly, we introduce a unified motion equation to characterize the motion of these bases and the interactions between waves and particles, e.g. a ball floating on water. We use statistical learning algorithm to identify the structure of moving objects and their trajectories automatically. Then novel sequences can be synthesized easily from the motion and image models. Thirdly, we replace the dictionary of Gabor and Fourier bases with symbolic sketches (also bases). With the same image and motion model, we can render realistic and stylish cartoon animation. In our view, cartoon and sketch are symbolic visualization of the inner representation for visual perception. The success of the cartoon animation, in turn, suggests that our image and motion models capture the essence of visual perception of textured motion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238343",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Motion analysis",
                "Snow",
                "Rivers",
                "Animation",
                "Image motion analysis",
                "Graphics",
                "Rendering (computer graphics)",
                "Visual perception",
                "Image texture analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "image motion analysis",
                "image sequences",
                "computer animation",
                "data visualisation",
                "rendering (computer graphics)",
                "image representation",
                "natural scenes",
                "Fourier analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "textured motion phenomena",
                "unified motion equation",
                "statistical learning algorithm",
                "Gabor dictionary",
                "Fourier base dictionary",
                "image model",
                "motion model",
                "cartoon animation",
                "symbolic visualization",
                "visual perception inner representation"
            ]
        },
        "id": 29,
        "cited_by": []
    },
    {
        "title": "Circular motion geometry by minimal 2 points in 4 images",
        "authors": [
            "Guang Jiang",
            "Long Quan",
            "Hung-tat Tsui"
        ],
        "abstract": "We describe a new and simple method of recovering the geometry of uncalibrated circular motion or single axis motion using a minimal data set of 2 points in 4 images. This problem has been solved using nonminimal data either by computing the fundamental matrix and trifocal tensor in 3 images, or by fitting conics to tracked points in 5 images. Our new method first computes a planar homography from a minimum of 2 points in 4 images. It is shown that two eigenvectors of this homography are the images of the circular points. Then, other fixed image entities and rotation angles can be straightforwardly computed. The crux of the method lies in relating this planar homography from two different points to a homology naturally induced by corresponding points on different conic loci from a circular motion. The experiments on real image sequences demonstrate the simplicity, accuracy and robustness of the new method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238345",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image sequences",
                "Tensile stress",
                "Transmission line matrix methods",
                "Data engineering",
                "Computer science",
                "Physics",
                "Computational geometry",
                "Robustness",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image sequences",
                "optimisation",
                "solid modelling",
                "realistic images",
                "image reconstruction",
                "matrix algebra",
                "eigenvalues and eigenfunctions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "circular motion geometry",
                "single axis motion",
                "minimal data set",
                "fundamental matrix",
                "trifocal tensor",
                "planar homography",
                "eigenvector",
                "image entity",
                "real image sequence"
            ]
        },
        "id": 30,
        "cited_by": []
    },
    {
        "title": "A cylindrical surface model to rectify the bound document image",
        "authors": [
            "Huaigu Cao",
            "Xiaoqing Ding",
            "Changsong Liu"
        ],
        "abstract": "We propose a novel approach on how to rectify the photo image of the bound document. The surface of the document is modeled by a cylindrical surface. By the geometry of camera image formation, the equations using the cue of directrixes to map the points on the surface in the 3D scene to the points on the image plane are achieved. Baselines of the horizontal text line are extracted as projections of directrixes to estimate the bending extent of the surface, and then the images are rectified. The proposed method needs no auxiliary device. Experimental results are presented to demonstrate the feasibility and the application of the method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238346",
        "reference_list": [
            {
                "year": "2001",
                "id": 154
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 0,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Digital cameras",
                "Glass",
                "Optical character recognition software",
                "Surface emitting lasers",
                "Rough surfaces",
                "Surface roughness",
                "Books",
                "Laboratories",
                "Intelligent systems",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "image scanners",
                "computational geometry",
                "feature extraction",
                "optical character recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cylindrical surface model",
                "bound document image rectification",
                "camera image formation geometry",
                "directrixes cue",
                "3D scene",
                "horizontal text line extraction",
                "image scanners",
                "optical character recognition"
            ]
        },
        "id": 31,
        "cited_by": [
            {
                "year": "2015",
                "id": 438
            },
            {
                "year": "2005",
                "id": 146
            }
        ]
    },
    {
        "title": "Edit distance from graph spectra",
        "authors": [
            "Robles-Kelly",
            "Hancock"
        ],
        "abstract": "We are concerned with computing graph edit distance. One of the criticisms that can be leveled at existing methods for computing graph edit distance is that it lacks the formality and rigour of the computation of string edit distance. Hence, our aim is to convert graphs to string sequences so that standard string edit distance techniques can be used. To do this we use graph spectral seriation method to convert the adjacency matrix into a string or sequence order. We pose the problem of graph-matching as maximum a posteriori probability alignment of the seriation sequences for pairs of graphs. This treatment leads to an expression for the edit costs. We compute the edit distance by finding the sequence of string edit operations, which minimise the cost of the path traversing the edit lattice. The edit costs are defined in terms of the a posteriori probability of visiting a site on the lattice. We demonstrate the method with results on a data-set of Delaunay graphs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238347",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 14,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Sequences",
                "Costs",
                "Computer science",
                "Lattices",
                "Simulated annealing",
                "Graph theory",
                "Matrix converters",
                "Image segmentation",
                "Noise robustness",
                "Focusing"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern matching",
                "graph theory",
                "image segmentation",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph edit distance computing",
                "graph spectral seriation method",
                "adjacency matrix",
                "string sequence",
                "graph-matching problem",
                "posteriori probability alignment",
                "Delaunay graph data-set",
                "search problems"
            ]
        },
        "id": 32,
        "cited_by": []
    },
    {
        "title": "Minimum risk distance measure for object recognition",
        "authors": [
            "Mahamud",
            "Hebert"
        ],
        "abstract": "The optimal distance measure for a given discrimination task under the nearest neighbor framework has been shown to be the likelihood that a pair of measurements have different class labels [S. Mahamud et al., (2002)]. For implementation and efficiency considerations, the optimal distance measure was approximated by combining more elementary distance measures defined on simple feature spaces. We address two important issues that arise in practice for such an approach: (a) What form should the elementary distance measure in each feature space take? We motivate the need to use the optimal distance measure in simple feature spaces as the elementary distance measures; such distance measures have the desirable property that they are invariant to distance-respecting transformations, (b) How do we combine the elementary distance measures ? We present the precise statistical assumptions under which a linear logistic model holds exactly. We benchmark our model with three other methods on a challenging face discrimination task and show that our approach is competitive with the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238349",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 9,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Nearest neighbor searches",
                "Extraterrestrial measurements",
                "Tin",
                "Principal component analysis",
                "Computer science",
                "Logistics",
                "Face detection",
                "Neural networks",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "principal component analysis",
                "face recognition",
                "distance measurement",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "minimum risk distance measure",
                "object recognition",
                "elementary distance measure",
                "distance-respecting transformations",
                "linear logistic model",
                "principal component analysis",
                "face recognition"
            ]
        },
        "id": 33,
        "cited_by": [
            {
                "year": "2005",
                "id": 179
            }
        ]
    },
    {
        "title": "A multi-scale generative model for animate shapes and parts",
        "authors": [
            "Dubinskiy",
            "Zhu"
        ],
        "abstract": "We present a multiscale generative model for representing animate shapes and extracting meaningful parts of objects. The model assumes that animate shapes (2D simple dosed curves) are formed by a linear superposition of a number of shape bases. These shape bases resemble the multiscale Gabor bases in image pyramid representation, are well localized in both spatial and frequency domains, and form an over-complete dictionary. This model is simpler than the popular B-spline representation since it does not engage a domain partition. Thus it eliminates the interference between adjacent B-spline bases, and becomes a true linear additive model. We pursue the bases by reconstructing the shape in a coarse-to-fine procedure through curve evolution. These shape bases are further organized in a tree-structure, where the bases in each subtree sum up to an intuitive part of the object. To build probabilistic model for a class of objects, we propose a Markov random field model at each level of the tree representation to account for the spatial relationship between bases. Thus the final model integrates a Markov tree (generative) model over scales and a Markov random field over space. We adopt EM-type algorithm for learning the meaningful parts for a shape class, and show some results on shape synthesis.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238350",
        "reference_list": [
            {
                "year": "2001",
                "id": 102
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Animation",
                "Shape",
                "Spline",
                "Principal component analysis",
                "Polynomials",
                "Markov random fields",
                "Frequency domain analysis",
                "Dictionaries",
                "Interference elimination",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image reconstruction",
                "feature extraction",
                "splines (mathematics)",
                "Markov processes",
                "curve fitting",
                "computer animation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale generative model",
                "animate shape representation",
                "feature extraction",
                "multiscale Gabor bases",
                "image pyramid representation",
                "B-spline bases",
                "shape reconstruction",
                "Markov random field model",
                "tree representation",
                "Markov tree model",
                "EM-type algorithm"
            ]
        },
        "id": 34,
        "cited_by": []
    },
    {
        "title": "Recognition with local features: the kernel recipe",
        "authors": [
            "Wallraven",
            "Caputo",
            "Graf"
        ],
        "abstract": "Recent developments in computer vision have shown that local features can provide efficient representations suitable for robust object recognition. Support vector machines have been established as powerful learning algorithms with good generalization capabilities. We combine these two approaches and propose a general kernel method for recognition with local features. We show that the proposed kernel satisfies the Mercer condition and that it is, suitable for many established local feature frameworks. Large-scale recognition results are presented on three different databases, which demonstrate that SVMs with the proposed kernel perform better than standard matching techniques on local features. In addition, experiments on noisy and occluded images show that local feature representations significantly outperform global approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238351",
        "reference_list": [
            {
                "year": "2001",
                "id": 60
            },
            {
                "year": "2001",
                "id": 31
            },
            {
                "year": "2001",
                "id": 196
            },
            {
                "year": "2001",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 114,
            "other": 80,
            "total": 194
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Computer vision",
                "Object recognition",
                "Machine learning",
                "Cybernetics",
                "Support vector machines",
                "Image recognition",
                "Robustness",
                "Large-scale systems",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "feature extraction",
                "image representation",
                "support vector machines",
                "computer vision",
                "image classification",
                "visual databases",
                "hidden feature removal"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust object recognition",
                "computer vision",
                "support vector machines",
                "learning algorithms",
                "feature representations",
                "image classification",
                "image databases",
                "occlusion",
                "Gaussian noise"
            ]
        },
        "id": 35,
        "cited_by": [
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2011",
                "id": 231
            },
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2005",
                "id": 17
            },
            {
                "year": "2005",
                "id": 178
            },
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2005",
                "id": 208
            }
        ]
    },
    {
        "title": "Images as bags of pixels",
        "authors": [
            "Jebara"
        ],
        "abstract": "We propose modeling images and related visual objects as bags of pixels or sets of vectors. For instance, gray scale images are modeled as a collection or bag of (X, Y, I) pixel vectors. This representation implies a permutational invariance over the bag of pixels, which is naturally handled by endowing each image with a permutation matrix. Each matrix permits the image to span a manifold of multiple configurations, capturing the vector set's invariance to orderings or permutation transformations. Permutation configurations are optimized while jointly modeling many images via maximum likelihood. The solution is a uniquely solvable convex program, which computes correspondence simultaneously for all images (as opposed to traditional pairwise correspondence solutions). Maximum likelihood performs a nonlinear dimensionality reduction, choosing permutations that compact the permuted image vectors into a volumetrically minimal subspace. This is highly suitable for principal components analysis which, when applied to the permutationally invariant bag of pixels representation, outperforms PCA on appearance-based vectorization by orders of magnitude. Furthermore, the bag of pixels subspace benefits from automatic correspondence estimation, giving rise to meaningful linear variations such as morphings, translations, and jointly spatio-textural image transformations. Results are shown for several datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238352",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 9,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Pixel",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "maximum likelihood estimation",
                "principal component analysis",
                "vectors",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gray scale image modelling",
                "permutational invariance",
                "permutation matrix",
                "maximum likelihood estimation",
                "permuted image pixel vector representation",
                "principal components analysis",
                "image transformations"
            ]
        },
        "id": 36,
        "cited_by": []
    },
    {
        "title": "Context-based vision system for place and object recognition",
        "authors": [
            "Torralba",
            "Murphy",
            "Freeman",
            "Rubin"
        ],
        "abstract": "While navigating in an environment, a vision system has to be able to recognize where it is and what the main objects in the scene are. We present a context-based vision system for place and object recognition. The goal is to identify familiar locations (e.g., office 610, conference room 941, main street), to categorize new environments (office, corridor, street) and to use that information to provide contextual priors for object recognition (e.g., tables are more likely in an office than a street). We present a low-dimensional global image representation that provides relevant information for place recognition and categorization, and show how such contextual information introduces strong priors that simplify object recognition. We have trained the system to recognize over 60 locations (indoors and outdoors) and to suggest the presence and locations of more than 20 different object types. The algorithm has been integrated into a mobile system that provides realtime feedback to the user.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238354",
        "reference_list": [],
        "citation": {
            "ieee": 241,
            "other": 202,
            "total": 443
        },
        "keywords": {
            "IEEE Keywords": [
                "Machine vision",
                "Object recognition",
                "Layout",
                "Artificial intelligence",
                "Image recognition",
                "Mobile robots",
                "Robot kinematics",
                "Face recognition",
                "Navigation",
                "Image representation"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image representation",
                "robot vision",
                "mobile robots",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "context-based vision system",
                "place recognition",
                "object recognition",
                "low-dimensional global image representation",
                "image categorization",
                "mobile system",
                "realtime feedback"
            ]
        },
        "id": 37,
        "cited_by": [
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2009",
                "id": 119
            },
            {
                "year": "2009",
                "id": 285
            },
            {
                "year": "2007",
                "id": 222
            },
            {
                "year": "2007",
                "id": 231
            },
            {
                "year": "2007",
                "id": 270
            },
            {
                "year": "2005",
                "id": 115
            }
        ]
    },
    {
        "title": "Object recognition with informative features and linear classification",
        "authors": [
            "Vidal-Naquet",
            "Ullman"
        ],
        "abstract": "We show that efficient object recognition can be obtained by combining informative features with linear classification. The results demonstrate the superiority of informative class-specific features, as compared with generic type features such as wavelets, for the task of object recognition. We show that information rich features can reach optimal performance with simple linear separation rules, while generic feature based classifiers require more complex classification schemes. This is significant because efficient and optimal methods have been developed for spaces that allow linear separation. To compare different strategies for feature extraction, we trained and compared classifiers working in feature spaces of the same low dimensionality, using two feature types (image fragments vs. wavelets) and two classification rules (linear hyperplane and a Bayesian network). The results show that by maximizing the individual information of the features, it is possible to obtain efficient classification by a simple linear separating rule, as well as more efficient learning.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238356",
        "reference_list": [],
        "citation": {
            "ieee": 73,
            "other": 63,
            "total": 136
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "feature extraction",
                "pattern classification",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "informative class-specific features",
                "linear classification",
                "feature extraction",
                "linear hyperplane classification rule",
                "Bayesian network classification rule",
                "support vector machines"
            ]
        },
        "id": 38,
        "cited_by": [
            {
                "year": "2009",
                "id": 146
            },
            {
                "year": "2005",
                "id": 37
            },
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2005",
                "id": 230
            },
            {
                "year": "2005",
                "id": 237
            }
        ]
    },
    {
        "title": "Meshfree particle method",
        "authors": [
            "Huafeng Liu",
            "Pengcheng Shi"
        ],
        "abstract": "Many of the computer vision algorithms have been posed in various forms of differential equations, derived from minimization of specific energy functionals, and the finite element representation and computation have become the de facto numerical strategies for solving these problems. However, for cases where domain mappings between numerical iterations or image frames involve large geometrical shape changes, such as deformable models for object segmentation and nonrigid motion tracking, these strategies may exhibit considerable loss of accuracy when the mesh elements become extremely skewed or compressed. We present a new computational paradigm, the meshfree particle method, where the object representation and the numerical calculation are purely based on the nodal points and do not require the meshing of the analysis domain. This meshfree strategy can naturally handle large deformation and domain discontinuity issues and achieve desired numerical accuracy through adaptive node and polynomial shape function refinement. We discuss in detail the element-free Galerkin method, including the shape function construction using the moving least square approximation and the Galerkin weak form formulation, and we demonstrate its applications to deformable model based segmentation and mechanically motivated left ventricular motion analysis.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238357",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Computer vision",
                "Deformable models",
                "Moment methods",
                "Differential equations",
                "Minimization methods",
                "Finite element methods",
                "Object segmentation",
                "Tracking",
                "Image coding"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "mesh generation",
                "computational geometry",
                "Galerkin method",
                "least squares approximations",
                "image motion analysis",
                "medical image processing",
                "image segmentation",
                "image representation",
                "differential equations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision algorithms",
                "differential equations",
                "energy functionals minimization",
                "finite element representation",
                "domain mappings",
                "numerical iterations",
                "image frames",
                "geometrical shape changes",
                "deformable models",
                "object segmentation",
                "nonrigid motion tracking",
                "mesh elements",
                "meshfree particle method",
                "object representation",
                "numerical calculation",
                "nodal points",
                "adaptive node",
                "polynomial shape function refinement",
                "element-free Galerkin method",
                "shape function construction",
                "moving least square approximation",
                "Galerkin weak form formulation",
                "model based segmentation",
                "left ventricular motion analysis"
            ]
        },
        "id": 39,
        "cited_by": []
    },
    {
        "title": "Minimally-supervised classification using multiple observation sets",
        "authors": [
            "Stauffer"
        ],
        "abstract": "We discuss building complex classifiers from a single labeled example and vast number of unlabeled observation sets, each derived from observation of a single process or object. When data can be measured by observation, it is often plentiful and it is often possible to make more than one observation of the state of a process or object. We discuss how to exploit the variability across such sets of observations of the same object to estimate class labels for unlabeled examples given a minimal number of labeled examples. In contrast to similar semisupervised classification procedures that define the likelihood that two observations share a label as a function of the embedded distance between the two observations, this method uses the Naive Bayes estimate of how often the two observations did result from the same observed process. Exploiting this additional source of information in an iterative estimation procedure can generalize complex classification models from single labeled observations. Some examples involving classification of tracked objects in a low-dimensional feature space given thousands of unlabeled observation sets are used to illustrate the effectiveness of this method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238358",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Information resources",
                "Humans",
                "Artificial intelligence",
                "Laboratories",
                "Cognitive science",
                "Optical sensors",
                "Image motion analysis",
                "Active noise reduction",
                "Eyes",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "tracking",
                "estimation theory",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "minimally-supervised classification",
                "unlabeled observation sets",
                "semisupervised classification procedures",
                "Naive Bayes estimate",
                "iterative estimation procedure",
                "single labeled observations",
                "tracked objects classification",
                "low-dimensional feature space",
                "multiple observation sets"
            ]
        },
        "id": 40,
        "cited_by": []
    },
    {
        "title": "Learning how to inpaint from global image statistics",
        "authors": [
            "Levin",
            "Zomet",
            "Weiss"
        ],
        "abstract": "Inpainting is the problem of filling-in holes in images. Considerable progress has been made by techniques that use the immediate boundary of the hole and some prior information on images to solve this problem. These algorithms successfully solve the local inpainting problem but they must, by definition, give the same completion to any two holes that have the same boundary, even when the rest of the image is vastly different. We address a different, more global inpainting problem. How can we use the rest of the image in order to learn how to inpaint? We approach this problem from the context of statistical learning. Given a training image we build an exponential family distribution over images that is based on the histograms of local features. We then use this image specific distribution to inpaint the hole by finding the most probable image given the boundary and the distribution. The optimization is done using loopy belief propagation. We show that our method can successfully complete holes while taking into account the specific image statistics. In particular it can give vastly different completions even when the local neighborhoods are identical.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238360",
        "reference_list": [],
        "citation": {
            "ieee": 79,
            "other": 40,
            "total": 119
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Painting",
                "Image restoration",
                "Computer science",
                "Statistical learning",
                "Histograms",
                "Belief propagation",
                "Statistical distributions",
                "Computer errors",
                "Boundary conditions"
            ],
            "INSPEC: Controlled Indexing": [
                "exponential distribution",
                "optimisation",
                "image restoration",
                "image texture",
                "statistical analysis",
                "belief maintenance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "filling-in holes problem",
                "local inpainting problem",
                "global inpainting problem",
                "statistical learning",
                "training image",
                "exponential family distribution",
                "local feature histograms",
                "image specific distribution",
                "loopy belief propagation",
                "global image statistics"
            ]
        },
        "id": 41,
        "cited_by": [
            {
                "year": "2007",
                "id": 93
            },
            {
                "year": "2007",
                "id": 140
            }
        ]
    },
    {
        "title": "Multiclass spectral clustering",
        "authors": [
            "Yu",
            "Shi"
        ],
        "abstract": "We propose a principled account on multiclass spectral clustering. Given a discrete clustering formulation, we first solve a relaxed continuous optimization problem by eigen-decomposition. We clarify the role of eigenvectors as a generator of all optimal solutions through orthonormal transforms. We then solve an optimal discretization problem, which seeks a discrete solution closest to the continuous optima. The discretization is efficiently computed in an iterative fashion using singular value decomposition and nonmaximum suppression. The resulting discrete solutions are nearly global-optimal. Our method is robust to random initialization and converges faster than other clustering methods. Experiments on real image segmentation are reported.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238361",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 204,
            "other": 148,
            "total": 352
        },
        "keywords": {
            "IEEE Keywords": [
                "Discrete transforms",
                "Karhunen-Loeve transforms",
                "Image segmentation",
                "Computer vision",
                "Robots",
                "Information science",
                "Singular value decomposition",
                "Robustness",
                "Image converters",
                "Clustering methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "pattern clustering",
                "eigenvalues and eigenfunctions",
                "singular value decomposition",
                "realistic images",
                "optimisation",
                "iterative methods",
                "convergence"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiclass spectral clustering",
                "discrete clustering formulation",
                "relaxed continuous optimization problem",
                "eigen-decomposition",
                "eigenvectors",
                "orthonormal transforms",
                "optimal discretization problem",
                "continuous optima",
                "singular value decomposition",
                "nonmaximum suppression",
                "random initialization",
                "real image segmentation"
            ]
        },
        "id": 42,
        "cited_by": [
            {
                "year": "2017",
                "id": 217
            },
            {
                "year": "2009",
                "id": 104
            },
            {
                "year": "2009",
                "id": 164
            },
            {
                "year": "2007",
                "id": 90
            },
            {
                "year": "2007",
                "id": 118
            },
            {
                "year": "2007",
                "id": 119
            },
            {
                "year": "2007",
                "id": 221
            },
            {
                "year": "2005",
                "id": 162
            }
        ]
    },
    {
        "title": "Graph partition by Swendsen-Wang cuts",
        "authors": [
            "Barbu",
            "Zhu"
        ],
        "abstract": "Vision tasks, such as segmentation, grouping, recognition, can be formulated as graph partition problems. The recent literature witnessed two popular graph cut algorithms: the Ncut using spectral graph analysis and the minimum-cut using the maximum flow algorithm. We present a third major approach by generalizing the Swendsen-Wang method - a well celebrated algorithm in statistical mechanics. Our algorithm simulates ergodic, reversible Markov chain jumps in the space of graph partitions to sample a posterior probability. At each step, the algorithm splits, merges, or regroups a sizable subgraph, and achieves fast mixing at low temperature enabling a fast annealing procedure. Experiments show it converges in 2-30 seconds on a PC for image segmentation. This is 400 times faster than the single-site update Gibbs sampler, and 20-40 times faster than the DDMCMC algorithm. The algorithm can optimize over the number of models and works for general forms of posterior probabilities, so it is more general than the existing graph cut approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238362",
        "reference_list": [],
        "citation": {
            "ieee": 44,
            "other": 20,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Partitioning algorithms",
                "Spectral analysis",
                "Temperature",
                "Image segmentation",
                "Annealing",
                "Image converters",
                "Computer vision",
                "Clustering algorithms",
                "Bayesian methods",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "computer vision",
                "simulated annealing",
                "Markov processes",
                "convergence",
                "image segmentation",
                "statistical mechanics",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "vision tasks",
                "graph partition problems",
                "graph cut algorithms",
                "Ncut algorithm",
                "spectral graph analysis",
                "minimum-cut algorithm",
                "maximum flow algorithm",
                "Swendsen-Wang method",
                "statistical mechanics",
                "reversible Markov chain jumps",
                "posterior probability",
                "annealing procedure",
                "image segmentation",
                "single-site update Gibbs sampler"
            ]
        },
        "id": 43,
        "cited_by": [
            {
                "year": "2007",
                "id": 277
            },
            {
                "year": "2005",
                "id": 79
            }
        ]
    },
    {
        "title": "Obstacle detection using projective invariant and vanishing lines",
        "authors": [
            "Okada",
            "Taniguchi",
            "Furukawa",
            "Onoguchi"
        ],
        "abstract": "We present a novel method for detecting vehicles as obstacles in various road scenes using a single onboard camera. Vehicles are detected by testing whether the motion of a set of three horizontal line segments, which are always on the vehicles, satisfies the motion constraint of the ground plane or that of the surface plane of the vehicles. The motion constraint of each plane is derived from the projective invariant combined with the vanishing line of the plane that is a prior knowledge of road scenes. The proposed method is implemented into a newly developed onboard LSI. Experimental results for real road scenes under various conditions show the effectiveness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238363",
        "reference_list": [],
        "citation": {
            "ieee": 25,
            "other": 5,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Vehicle detection",
                "Road vehicles",
                "Motion detection",
                "Motion estimation",
                "Cameras",
                "Robustness",
                "Land vehicles",
                "Mobile robots",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "road vehicles",
                "object detection",
                "tracking",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "obstacle detection",
                "projective invariant",
                "vanishing lines",
                "road scenes",
                "onboard camera",
                "motion constraint",
                "ground plane",
                "surface plane",
                "onboard LSI"
            ]
        },
        "id": 44,
        "cited_by": []
    },
    {
        "title": "Using temporal coherence to build models of animals",
        "authors": [
            "Ramanan",
            "Forsyth"
        ],
        "abstract": "We describe a system that can build appearance models of animals automatically from a video sequence of the relevant animal with no explicit supervisory information. The video sequence need not have any form of special background. Animals are modeled as a 2D kinematic chain of rectangular segments, where the number of segments and the topology of the chain are unknown. The system detects possible segments, clusters segments whose appearance is coherent over time, and then builds a spatial model of such segment clusters. The resulting representation of the spatial configuration of the animal in each frame can be seen either as a track - in which case the system described should be viewed as a generalized tracker, that is capable of modeling objects while tracking them - or as the source of an appearance model which can be used to build detectors for the particular animal. This is because knowing a video sequence is temporally coherent - i.e. that a particular animal is present through the sequence - is a strong supervisory signal. The method is shown to be successful as a tracker on video sequences of real scenes showing three different animals. For the same reason it is successful as a tracker, the method results in detectors that can be used to find each animal fairly reliably within the Corel collection of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238364",
        "reference_list": [
            {
                "year": "2001",
                "id": 93
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 12,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Coherence",
                "Animals",
                "Object detection",
                "Video sequences",
                "Detectors",
                "Computer science",
                "Kinematics",
                "Topology",
                "Layout",
                "Region 4"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image segmentation",
                "tracking",
                "object detection",
                "realistic images",
                "image representation",
                "pattern clustering",
                "spatiotemporal phenomena"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "animal appearance models",
                "video sequence",
                "supervisory information",
                "2D kinematic chain",
                "rectangular segments",
                "object modeling",
                "object tracking",
                "real scenes",
                "Corel image collection",
                "temporal coherence",
                "segment clusters",
                "spatial model"
            ]
        },
        "id": 45,
        "cited_by": [
            {
                "year": "2005",
                "id": 4
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2005",
                "id": 86
            },
            {
                "year": "2005",
                "id": 133
            }
        ]
    },
    {
        "title": "On-line selection of discriminative tracking features",
        "authors": [
            "Collins",
            "Liu"
        ],
        "abstract": "We present a method for evaluating multiple feature spaces while tracking, and for adjusting the set of features used to improve tracking performance. Our hypothesis is that the features that best discriminate between object and background are also best for tracking the object. We develop an online feature selection mechanism based on the two-class variance ratio measure, applied to log likelihood distributions computed with respect to a given feature from samples of object and background pixels. This feature selection mechanism is embedded in a tracking system that adaptively selects the top-ranked discriminative features for tracking. Examples are presented to illustrate how the method adapts to changing appearances of both tracked object and scene background.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238365",
        "reference_list": [],
        "citation": {
            "ieee": 109,
            "other": 48,
            "total": 157
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Layout",
                "Cameras",
                "Contracts",
                "Robustness",
                "Skin",
                "Space exploration",
                "Distributed computing",
                "Filtering",
                "Particle tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "tracking",
                "object detection",
                "feature extraction",
                "image sequences",
                "image colour analysis",
                "image motion analysis",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature spaces",
                "online feature selection mechanism",
                "two-class variance ratio measure",
                "log likelihood distributions",
                "background pixels",
                "tracking system",
                "top-ranked discriminative tracking features",
                "scene background"
            ]
        },
        "id": 46,
        "cited_by": [
            {
                "year": "2011",
                "id": 167
            },
            {
                "year": "2009",
                "id": 92
            },
            {
                "year": "2009",
                "id": 184
            },
            {
                "year": "2009",
                "id": 199
            },
            {
                "year": "2007",
                "id": 110
            }
        ]
    },
    {
        "title": "A sparse probabilistic learning algorithm for real-time tracking",
        "authors": [
            "Williams",
            "Blake",
            "Cipolla"
        ],
        "abstract": "We address the problem of applying powerful pattern recognition algorithms based on kernels to efficient visual tracking. Recently S. Avidan, (2001) has shown that object recognizers using kernel-SVMs can be elegantly adapted to localization by means of spatial perturbation of the SVM, using optic flow. Whereas Avidan's SVM applies to each frame of a video independently of other frames, the benefits of temporal fusion of data are well known. Using a fully probabilistic 'relevance vector machine' (RVM) to generate observations with Gaussian distributions that can be fused over time is addressed. To improve performance further, rather than adapting a recognizer, we build a localizer directly using the regression form of the RVM. A classification SVM is used in tandem, for object verification, and this provides the capability of automatic initialization and recovery. The approach is demonstrated in real-time face and vehicle tracking systems. The 'sparsity' of the RVMs means that only a fraction of CPU time is required to track at frame rate. Tracker output is demonstrated in a camera management task in which zoom and pan are controlled in response to speaker/vehicle position and orientation, over an extended period. The advantages of temporal fusion in this system are demonstrated.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238366",
        "reference_list": [
            {
                "year": "2001",
                "id": 199
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 25,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Vehicles",
                "Pattern recognition",
                "Kernel",
                "Image motion analysis",
                "Fusion power generation",
                "Gaussian distribution",
                "Support vector machine classification",
                "Real time systems",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian distribution",
                "support vector machines",
                "video cameras",
                "optical tracking",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pattern recognition",
                "visual tracking",
                "object recognition",
                "optic flow",
                "temporal data fusion",
                "relevance vector machine",
                "Gaussian distribution",
                "real-time face tracking",
                "vehicle tracking system",
                "camera management task",
                "sparse probabilistic learning algorithm"
            ]
        },
        "id": 47,
        "cited_by": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2009",
                "id": 289
            },
            {
                "year": "2007",
                "id": 110
            },
            {
                "year": "2007",
                "id": 266
            },
            {
                "year": "2007",
                "id": 379
            }
        ]
    },
    {
        "title": "Dominant sets and hierarchical clustering",
        "authors": [
            "Pavan",
            "Pelillo"
        ],
        "abstract": "Dominant sets are a new graph-theoretic concept that has proven to be relevant in partitional (flat) clustering as well as image segmentation problems. However, in many computer vision applications, such as the organization of an image database, it is important to provide the data to be clustered with a hierarchical organization, and it is not clear how to do this within the dominant set framework. We address precisely this problem, and present a simple and elegant solution to it. To this end, we consider a family of (continuous) quadratic programs, which contain a parameterized regularization term that controls the global shape of the energy landscape. When the regularization parameter is zero the local solutions are known to be in one-to-one correspondence with dominant sets, but when it is positive an interesting picture emerges. We determine bounds for the regularization parameter that allow us to exclude from the set of local solutions those inducing clusters of size smaller than a prescribed threshold. This suggests a new (divisive) hierarchical approach to clustering, which is based on the idea of properly varying the regularization parameter during the clustering process. Straightforward dynamics from evolutionary game theory are used to locate the solutions of the quadratic programs at each level of the hierarchy. We apply the proposed framework to the problem of organizing a shape database. Experiments with three different similarity matrices (and databases) reported in the literature have been conducted, and the results confirm the effectiveness of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238367",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 21,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Shape control",
                "Image segmentation",
                "Application software",
                "Image databases",
                "Game theory",
                "Clustering algorithms",
                "Organizing",
                "Tree graphs",
                "Partitioning algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern clustering",
                "image segmentation",
                "visual databases",
                "graph theory",
                "game theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dominant sets",
                "graph-theory",
                "hierarchical clustering",
                "image segmentation",
                "computer vision",
                "image database",
                "quadratic programs",
                "regularization parameter",
                "game theory"
            ]
        },
        "id": 48,
        "cited_by": []
    },
    {
        "title": "Applying the information bottleneck principle to unsupervised clustering of discrete and continuous image representations",
        "authors": [
            "Gordon",
            "Greenspan",
            "Goldberger"
        ],
        "abstract": "We present a method for unsupervised clustering of image databases. The method is based on a recently introduced information-theoretic principle, the information bottleneck (IB) principle. Image archives are clustered such that the mutual information between the clusters and the image content is maximally preserved. The IB principle is applied to both discrete and continuous image representations, using discrete image histograms and probabilistic continuous image modeling based on mixture of Gaussian densities, respectively. Experimental results demonstrate the performance of the proposed method for image clustering on a large image database. Several clustering algorithms derived from the IB principle are explored and compared.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238368",
        "reference_list": [],
        "citation": {
            "ieee": 24,
            "other": 18,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Image representation",
                "Histograms",
                "Image databases",
                "Pixel",
                "Jacobian matrices",
                "Clustering algorithms",
                "Image retrieval",
                "Clustering methods",
                "Mutual information",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "pattern clustering",
                "visual databases",
                "Gaussian distribution",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised image clustering",
                "image databases",
                "information bottleneck principle",
                "image archives",
                "discrete image representation",
                "continuous image representation",
                "image histogram",
                "Gaussian densities"
            ]
        },
        "id": 49,
        "cited_by": [
            {
                "year": "2007",
                "id": 27
            },
            {
                "year": "2007",
                "id": 283
            }
        ]
    },
    {
        "title": "Feature selection for unsupervised and supervised inference: the emergence of sparsity in a weighted-based approach",
        "authors": [
            "Wolf",
            "Shashua"
        ],
        "abstract": "The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science including - examples in computer vision, text processing and more recently bioinformatics are abundant. We present a definition of \"relevancy\" based on spectral properties of the Affinity (or Laplacian) of the features' measurement matrix. The feature selection process is then based on a continuous ranking of the features defined by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a \"biased nonnegativity\" of a key matrix in the process. As a result, a simple least-squares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maxima over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence shows that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238369",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 4,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Face recognition",
                "Engines",
                "Text processing",
                "Bioinformatics",
                "Sparse matrices",
                "Speech recognition",
                "Support vector machines",
                "Support vector machine classification",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "sparse matrices",
                "learning (artificial intelligence)",
                "least mean squares methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature selection algorithm",
                "features measurement matrix",
                "least-squares optimization",
                "sparse matrix",
                "local maxima",
                "unsupervised inference",
                "supervised inference"
            ]
        },
        "id": 50,
        "cited_by": [
            {
                "year": "2005",
                "id": 179
            }
        ]
    },
    {
        "title": "Learning a locality preserving subspace for visual recognition",
        "authors": [
            "Xiaofei He",
            "Shuicheng Yan",
            "Yuxiao Hu",
            "Hong-Jiang Zhang"
        ],
        "abstract": "We have demonstrated that the face recognition performance can be improved significantly in low dimensional linear subspaces. Conventionally, principal component analysis (PCA) and linear discriminant analysis (LDA) are considered effective in deriving such a face subspace. However, both of them effectively see only the Euclidean structure of face space. We propose a new approach to mapping face images into a subspace obtained by locality preserving projections (LPP) for face analysis. We call this Laplacianface approach. Different from PCA and LDA, LPP finds an embedding that preserves local information, and obtains a face space that best detects the essential manifold structure. In this way, the unwanted variations resulting from changes in lighting, facial expression, and pose may be eliminated or reduced. We compare the proposed Laplacianface approach with eigenface and fisherface methods on three test datasets. Experimental results show that the proposed Laplacianface approach provides a better representation and achieves lower error rates in face recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238370",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 2,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis",
                "Linear discriminant analysis",
                "Face recognition",
                "Face detection",
                "Testing",
                "Computer vision",
                "Helium",
                "Asia",
                "Computer science",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "principal component analysis",
                "visual databases",
                "image representation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "principal component analysis",
                "PCA",
                "linear discriminant analysis",
                "LDA",
                "Euclidean structure",
                "face images mapping",
                "locality preserving projections",
                "LPP",
                "Laplacianface approach",
                "eigenface method",
                "fisherface method"
            ]
        },
        "id": 51,
        "cited_by": [
            {
                "year": "2007",
                "id": 218
            },
            {
                "year": "2005",
                "id": 158
            }
        ]
    },
    {
        "title": "A model-based approach for automated feature extraction in fundus images",
        "authors": [
            "Li",
            "Chutatape"
        ],
        "abstract": "A new approach to automatically extract the main features in color fundus images is proposed. The optic disk is localized by principal component analysis (PCA) and its shape is detected by a modified active shape model (ASM). Exudates are extracted by the combined region growing and edge detection. A fundus coordinate system is further set up based on fovea localization to provide a better description of the features in fundus images. The success rates achieved are 99%, 94%, and 100% for disk localization, disk boundary detection, and fovea localization respectively. The sensitivity and specificity for exudate detection are 100% and 71%. The success of the proposed algorithms can be attributed to utilization of the model-based methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238371",
        "reference_list": [],
        "citation": {
            "ieee": 45,
            "other": 25,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Optical sensors",
                "Image edge detection",
                "Principal component analysis",
                "Lesions",
                "Geometrical optics",
                "Optical devices",
                "Active shape model",
                "Blood vessels",
                "Biomedical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "biomedical optical imaging",
                "edge detection",
                "principal component analysis",
                "image colour analysis",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based feature extraction",
                "color fundus images",
                "optic disk localization",
                "principal component analysis",
                "PCA",
                "eye diseases",
                "active shape model",
                "edge detection",
                "disk boundary detection",
                "exudate detection"
            ]
        },
        "id": 52,
        "cited_by": []
    },
    {
        "title": "Information theoretic focal length selection for real-time active 3D object tracking",
        "authors": [
            "Denzler",
            "Zobel",
            "Niemann"
        ],
        "abstract": "Active object tracking, for example, in surveillance tasks, becomes more and more important these days. Besides the tracking algorithms themselves methodologies have to be developed for reasonable active control of the degrees of freedom of all involved cameras. We present an information theoretic approach that allows the optimal selection of the focal lengths of two cameras during active 3D object tracking. The selection is based on the uncertainty in the 3D estimation. This allows us to resolve the trade-off between small and large focal length: in the former case, the chance is increased to keep the object in the field of view of the cameras. In the latter one, 3D estimation becomes more reliable. Also, more details are provided, for example for recognizing the objects. Beyond a rigorous mathematical framework we present real-time experiments demonstrating that we gain an improvement in 3D trajectory estimation by up to 42% in comparison with tracking using a fixed focal length.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238372",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 12,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Uncertainty",
                "Surveillance",
                "Object recognition",
                "Pattern recognition",
                "Trajectory",
                "Face recognition",
                "Size control",
                "Computer vision",
                "State estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "tracking",
                "motion estimation",
                "object recognition",
                "information theory",
                "video cameras",
                "surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surveillance",
                "cameras focal length",
                "3D trajectory estimation",
                "object recognition",
                "information theory",
                "real-time active 3D object tracking"
            ]
        },
        "id": 53,
        "cited_by": []
    },
    {
        "title": "Shape gradients for histogram segmentation using active contours",
        "authors": [
            "Jehan-Besson",
            "Barlaud",
            "Aubert",
            "Faugeras"
        ],
        "abstract": "We consider the problem of image segmentation using active contours through the minimization of an energy criterion involving both region and boundary functionals. These functionals are derived through a shape derivative approach instead of classical calculus of variation. The equations can be elegantly derived without converting the region integrals into boundary integrals. From the derivative, we deduce the evolution equation of an active contour that makes it evolve towards a minimum of the criterion. We focus more particularly on statistical features globally attached to the region and especially to the probability density functions of image features such as the color histogram of a region. A theoretical framework is set for the minimization of the distance between two histograms for matching or tracking purposes. An application of this framework to the segmentation of color histograms in video sequences is then proposed. We briefly describe our numerical scheme and show some experimental results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238375",
        "reference_list": [
            {
                "year": "2001",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 10,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Histograms",
                "Active contours",
                "Integral equations",
                "Image segmentation",
                "Image converters",
                "Partial differential equations",
                "Telecommunication computing",
                "Computer vision",
                "Calculus"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image sequences",
                "partial differential equations",
                "minimisation",
                "boundary integral equations",
                "gradient methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "histogram segmentation",
                "active contours",
                "minimization",
                "region functionals",
                "boundary functionals",
                "boundary integrals",
                "probability density function",
                "color histogram",
                "video sequences",
                "shape gradient method",
                "image segmentation"
            ]
        },
        "id": 54,
        "cited_by": [
            {
                "year": "2009",
                "id": 97
            },
            {
                "year": "2009",
                "id": 196
            }
        ]
    },
    {
        "title": "Fast intensity-based 2D-3D image registration of clinical data using light",
        "authors": [
            "Russakoff",
            "Rohlfing",
            "Maurer"
        ],
        "abstract": "Registration of a preoperative CT (3D) image to one or more X-ray projection (2D) images, a special case of the pose estimation problem, has been attempted in a variety of ways with varying degrees of success. Recently, there has been a great deal of interest in intensity-based methods. One of the drawbacks to such methods is the need to create digitally reconstructed radiographs (DRRs) at each step of the optimization process. DRRs are typically generated by ray casting, an operation that requires O(n/sup 3/) time, where we assume that n is approximately the size (in voxels) of one side of the DRR as well as one side of the CT volume. We address this issue by extending light field rendering techniques from the computer graphics community to generate DRRs instead of conventional rendered images. Using light fields allows most of the computation to be performed in a preprocessing step; after this precomputation, very accurate DRRs can be generated in O(n/sup 2/) time. Another important issue for 2D-3D registration algorithms is validation. Previously reported 2D-3D registration algorithms were validated using synthetic data or phantoms but not clinical data. We present an intensity-based 2D-3D registration system that generates DRRs using light fields; we validate its performance using clinical data with a known gold standard transformation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238376",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 12,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Image registration",
                "Computed tomography",
                "Rendering (computer graphics)",
                "X-ray imaging",
                "Image reconstruction",
                "Radiography",
                "Optimization methods",
                "Casting",
                "Computer graphics",
                "Imaging phantoms"
            ],
            "INSPEC: Controlled Indexing": [
                "computerised tomography",
                "rendering (computer graphics)",
                "medical image processing",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "2D-3D image registration algorithm",
                "clinical data",
                "2D X-ray projection image",
                "preoperative 3D CT image registration",
                "intensity-based method",
                "digitally reconstructed radiograph",
                "ray casting",
                "light field rendering technique",
                "computer graphics",
                "gold standard transformation"
            ]
        },
        "id": 55,
        "cited_by": []
    },
    {
        "title": "Integrated edge and junction detection with the boundary tensor",
        "authors": [
            "Kothe"
        ],
        "abstract": "The boundaries of image regions necessarily consist of edges (in particular, step and roof edges), corners, and junctions. Currently, different algorithms are used to detect each boundary type separately, but the integration of the results into a single boundary representation is difficult. Therefore, a method for the simultaneous detection of all boundary types is needed. We propose to combine responses of suitable polar separable filters into what we will call the boundary tensor. The trace of this tensor is a measure of boundary strength, while the small eigenvalue and its difference to the large one represent corner/junction and edge strengths respectively. We prove that the edge strength measure behaves like a rotationally invariant quadrature filter. A number of examples demonstrate the properties of the new method and illustrate its application to image segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238377",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 21,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Image edge detection",
                "Detectors",
                "Filters",
                "Eigenvalues and eigenfunctions",
                "Rotation measurement",
                "Image segmentation",
                "Object detection",
                "Object recognition",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "quadrature mirror filters",
                "image segmentation",
                "edge detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "integrated edge detection",
                "junction detection",
                "boundary tensor",
                "image boundary detection",
                "image segmentation",
                "quadrature filters"
            ]
        },
        "id": 56,
        "cited_by": []
    },
    {
        "title": "Space-time interest points",
        "authors": [
            "Laptev",
            "Lindeberg"
        ],
        "abstract": "Local image features or interest points provide compact and abstract representations of patterns in an image. We propose to extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for its interpretation. To detect spatio-temporal events, we build on the idea of the Harris and Forstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We then estimate the spatio-temporal extents of the detected events and compute their scale-invariant spatio-temporal descriptors. Using such descriptors, we classify events and construct video representation in terms of labeled space-time points. For the problem of human motion analysis, we illustrate how the proposed method allows for detection of walking people in scenes with occlusions and dynamic backgrounds.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238378",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 457,
            "other": 294,
            "total": 751
        },
        "keywords": {
            "IEEE Keywords": [
                "Event detection",
                "Computer vision",
                "Image motion analysis",
                "Motion analysis",
                "Layout",
                "Indexing",
                "Optical computing",
                "Spatiotemporal phenomena",
                "Acceleration",
                "Videoconference"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image representation",
                "spatiotemporal phenomena",
                "computer vision",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image pattern representation",
                "spatial interest point",
                "spatio-temporal domain",
                "video data representation",
                "spatio-temporal event detection",
                "scale-invariant spatio-temporal descriptor",
                "human motion analysis"
            ]
        },
        "id": 57,
        "cited_by": [
            {
                "year": "2015",
                "id": 356
            },
            {
                "year": "2015",
                "id": 501
            },
            {
                "year": "2013",
                "id": 169
            },
            {
                "year": "2013",
                "id": 228
            },
            {
                "year": "2013",
                "id": 232
            },
            {
                "year": "2013",
                "id": 396
            },
            {
                "year": "2011",
                "id": 99
            },
            {
                "year": "2011",
                "id": 225
            },
            {
                "year": "2011",
                "id": 275
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2009",
                "id": 118
            },
            {
                "year": "2009",
                "id": 211
            },
            {
                "year": "2009",
                "id": 247
            },
            {
                "year": "2009",
                "id": 248
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2005",
                "id": 19
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2005",
                "id": 105
            },
            {
                "year": "2005",
                "id": 182
            }
        ]
    },
    {
        "title": "Good continuations in digital image level lines",
        "authors": [
            "Cao"
        ],
        "abstract": "We propose a probabilistic algorithm able to detect the curves that are unexpectedly smooth in a set of digital curves. The only parameter is a false alarm rate, influencing the detection only by its logarithm. We experiment the good continuation criterion on image level lines. One of the conclusion is that, accordingly to Gestalt theory, one can detect edges in a way that is widely independent of contrast. We also use the same kind of method to detect corners and junctions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238380",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Digital images",
                "Computer vision",
                "Image edge detection",
                "Robustness",
                "Image segmentation",
                "Image analysis",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "digital image level line",
                "probabilistic algorithm",
                "digital curve detection",
                "good continuation criterion",
                "Gestalt theory",
                "edge detection"
            ]
        },
        "id": 58,
        "cited_by": []
    },
    {
        "title": "On the use of marginal statistics of subband images",
        "authors": [
            "Gluckman"
        ],
        "abstract": "A commonly used representation of a visual pattern is the set of marginal probability distributions of the output of a bank of filters (Gaussian, Laplacian, Gabor etc.). This representation has been used effectively for a variety of vision tasks including texture classification, texture synthesis, object detection and image retrieval. We examine the ability of this representation to discriminate between an arbitrary pair of visual stimuli. Examples of patterns are derived that provably possess the same marginal statistical properties, yet are \"visually distinct.\" These results suggest the need for either employing a large and diverse filter bank or incorporating joint statistics in order to represent a large class of visual patterns.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238381",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Filter bank",
                "Statistical distributions",
                "Histograms",
                "Gabor filters",
                "Laplace equations",
                "Object detection",
                "Image retrieval",
                "Visual system",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "statistical distributions",
                "image retrieval",
                "object detection",
                "computer vision",
                "image classification",
                "image texture",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subband image marginal statistics",
                "visual pattern representation",
                "marginal probability distribution",
                "Gaussian filter",
                "Laplacian filter",
                "Gabor filter",
                "texture classification",
                "texture synthesis",
                "object detection",
                "image retrieval"
            ]
        },
        "id": 59,
        "cited_by": []
    },
    {
        "title": "Mean shift based clustering in high dimensions: a texture classification example",
        "authors": [
            "Georgescu",
            "Shimshoni",
            "Meer"
        ],
        "abstract": "Feature space analysis is the main module in many computer vision tasks. The most popular technique, k-means clustering, however, has two inherent limitations: the clusters are constrained to be spherically symmetric and their number has to be known a priori. In nonparametric clustering methods, like the one based on mean shift, these limitations are eliminated but the amount of computation becomes prohibitively large as the dimension of the space increases. We exploit a recently proposed approximation technique, locality-sensitive hashing (LSH), to reduce the computational complexity of adaptive mean shift. In our implementation of LSH the optimal parameters of the data structure are determined by a pilot learning procedure, and the partitions are data driven. As an application, the performance of mode and k-means based textons are compared in a texture classification study.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238382",
        "reference_list": [
            {
                "year": "2001",
                "id": 58
            },
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 128,
            "other": 108,
            "total": 236
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Space technology",
                "Computational complexity",
                "Clustering algorithms",
                "Robustness",
                "Computer science",
                "Industrial engineering",
                "Technology management",
                "Engineering management",
                "Clustering methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "pattern clustering",
                "image classification",
                "computer vision",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mean shift based clustering",
                "texture classification",
                "feature space analysis",
                "computer vision"
            ]
        },
        "id": 60,
        "cited_by": [
            {
                "year": "2007",
                "id": 257
            },
            {
                "year": "2005",
                "id": 77
            }
        ]
    },
    {
        "title": "Improved fast gauss transform and efficient kernel density estimation",
        "authors": [
            "Yang",
            "Duraiswami",
            "Gumerov",
            "Davis"
        ],
        "abstract": "Evaluating sums of multivariate Gaussians is a common computational task in computer vision and pattern recognition, including in the general and powerful kernel density estimation technique. The quadratic computational complexity of the summation is a significant barrier to the scalability of this algorithm to practical applications. The fast Gauss transform (FGT) has successfully accelerated the kernel density estimation to linear running time for low-dimensional problems. Unfortunately, the cost of a direct extension of the FGT to higher-dimensional problems grows exponentially with dimension, making it impractical for dimensions above 3. We develop an improved fast Gauss transform to efficiently estimate sums of Gaussians in higher dimensions, where a new multivariate expansion scheme and an adaptive space subdivision technique dramatically improve the performance. The improved FGT has been applied to the mean shift algorithm achieving linear computational complexity. Experimental results demonstrate the efficiency and effectiveness of our algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238383",
        "reference_list": [],
        "citation": {
            "ieee": 125,
            "other": 86,
            "total": 211
        },
        "keywords": {
            "IEEE Keywords": [
                "Gaussian processes",
                "Kernel",
                "Computer vision",
                "Application software",
                "Density functional theory",
                "Pattern recognition",
                "Computational complexity",
                "Parametric statistics",
                "Bandwidth",
                "Nails"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "computational complexity",
                "Gaussian processes",
                "estimation theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "pattern recognition",
                "kernel density estimation",
                "quadratic computational complexity",
                "fast Gauss transform",
                "multivariate expansion scheme",
                "adaptive space subdivision technique",
                "mean shift algorithm"
            ]
        },
        "id": 61,
        "cited_by": [
            {
                "year": "2015",
                "id": 479
            },
            {
                "year": "2013",
                "id": 20
            },
            {
                "year": "2013",
                "id": 159
            },
            {
                "year": "2011",
                "id": 145
            },
            {
                "year": "2009",
                "id": 99
            },
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2007",
                "id": 113
            },
            {
                "year": "2007",
                "id": 138
            }
        ]
    },
    {
        "title": "Unsupervised image translation",
        "authors": [
            "Resales",
            "Achan",
            "Frey"
        ],
        "abstract": "An interesting and potentially useful vision/graphics task is to render an input image in an enhanced form or also in an unusual style; for example with increased sharpness or with some artistic qualities. In previous work [10, 5], researchers showed that by estimating the mapping from an input image to a registered (aligned) image of the same scene in a different style or resolution, the mapping could be used to render a new input image in that style or resolution. Frequently a registered pair is not available, but instead the user may have only a source image of an unrelated scene that contains the desired style. In this case, the task of inferring the output image is much more difficult since the algorithm must both infer correspondences between features in the input image and the source image, and infer the unknown mapping between the images. We describe a Bayesian technique for inferring the most likely output image. The prior on the output image P(X) is a patch-based Markov random field obtained from the source image. The likelihood of the input P(Y/spl bsol/X) is a Bayesian network that can represent different rendering styles. We describe a computationally efficient, probabilistic inference and learning algorithm for inferring the most likely output image and learning the rendering style. We also show that current techniques for image restoration or reconstruction proposed in the vision literature (e.g., image super-resolution or de-noising) and image-based nonphotorealistic rendering could be seen as special cases of our model. We demonstrate our technique using several tasks, including rendering a photograph in the artistic style of an unrelated scene, de-noising, and texture transfer.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238384",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 4,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Rendering (computer graphics)",
                "Layout",
                "Image resolution",
                "Bayesian methods",
                "Noise reduction",
                "Graphics",
                "Markov random fields",
                "Inference algorithms",
                "Image restoration",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "image denoising",
                "image restoration",
                "image registration",
                "image resolution",
                "image texture",
                "rendering (computer graphics)",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised image translation",
                "Bayesian technique",
                "patch-based Markov random field",
                "rendering style",
                "probabilistic inference",
                "learning algorithm",
                "image restoration",
                "image reconstruction",
                "image de-noising",
                "image-based nonphotorealistic rendering"
            ]
        },
        "id": 62,
        "cited_by": [
            {
                "year": "2017",
                "id": 234
            }
        ]
    },
    {
        "title": "On exploiting occlusions in multiple-view geometry",
        "authors": [
            "Favaro",
            "Duci",
            "Ma",
            "Soatto"
        ],
        "abstract": "Occlusions are commonplace in man-made and natural environments; they often result in photometric features where a line terminates at an occluding boundary, resembling a \"T\". We show that the 2-D motion of such T-junctions in multiple views carries nontrivial information on the 3-D structure of the scene and its motion relative to the camera. We show how the constraint among multiple views of T-junctions can be used to reliably detect them and differentiate them from ordinary point features. Finally, we propose an integrated algorithm to recursively and causally estimate structure and motion in the presence of T-junctions along with other point-features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238386",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 5,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Photometry",
                "Layout",
                "Image edge detection",
                "Motion detection",
                "Cameras",
                "Motion estimation",
                "Recursive estimation",
                "Image segmentation",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "hidden feature removal",
                "feature extraction",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple-view geometry",
                "camera motion estimation",
                "3-D structure",
                "occlusion",
                "T-junction detection",
                "point-feature detection"
            ]
        },
        "id": 63,
        "cited_by": []
    },
    {
        "title": "An efficient image similarity measure based on approximations of KL-divergence between two gaussian mixtures",
        "authors": [
            "Goldberger",
            "Gordon",
            "Greenspan"
        ],
        "abstract": "We present two new methods for approximating the Kullback-Liebler (KL) divergence between two mixtures of Gaussians. The first method is based on matching between the Gaussian elements of the two Gaussian mixture densities. The second method is based on the unscented transform. The proposed methods are utilized for image retrieval tasks. Continuous probabilistic image modeling based on mixtures of Gaussians together with KL measure for image similarity, can be used for image retrieval tasks with remarkable performance. The efficiency and the performance of the KL approximation methods proposed are demonstrated on both simulated data and real image data sets. The experimental results indicate that our proposed approximations outperform previously suggested methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238387",
        "reference_list": [
            {
                "year": "2001",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 95,
            "other": 76,
            "total": 171
        },
        "keywords": {
            "IEEE Keywords": [
                "Image retrieval",
                "Jacobian matrices",
                "Image matching",
                "Image representation",
                "Histograms",
                "Approximation methods",
                "Information retrieval",
                "Statistics",
                "Computational complexity",
                "Content based retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image retrieval",
                "Gaussian distribution",
                "approximation theory",
                "content-based retrieval",
                "image representation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Kullback-Liebler divergence approximation method",
                "Gaussian element matching",
                "Gaussian mixture density",
                "unscented based transform",
                "image retrieval",
                "continuous probabilistic image modeling",
                "image similarity",
                "simulated data set",
                "real image data set"
            ]
        },
        "id": 64,
        "cited_by": [
            {
                "year": "2013",
                "id": 210
            },
            {
                "year": "2011",
                "id": 222
            },
            {
                "year": "2009",
                "id": 171
            },
            {
                "year": "2007",
                "id": 107
            },
            {
                "year": "2005",
                "id": 163
            }
        ]
    },
    {
        "title": "Machine learning and multiscale methods in the identification of bivalve larvae",
        "authors": [
            "Tiwari",
            "Gallager"
        ],
        "abstract": "We describe a novel application of support vector machines and multiscale texture and color invariants to a problem in biological oceanography: the identification of 6 species of bivalve larvae. Our data consists of polarized color images of scallop and other bivalve larvae (between 2 and 17 days old) collected from the ocean by a shipboard optical imaging system of our design. Larvae of scallops, clams, and oysters are small (100 microns) with few distinguishing features when observed under standard light microscopy. However, the use of polarized light with a full wave retardation plate produces a vivid color, birefringence pattern. The patterns display very subtle differences between species, often not discernable to human observers. We show that a soft-margin support vector machine with Gaussian RBF kernel is a good discriminator on a feature set extracted from Gabor wavelet transforms and color distribution angles of each image. By constraining the Gabor center frequencies to be low, the resulting system can attain classification accuracy in excess of 90% for vertically oriented images, and in excess of 80% for randomly oriented images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238388",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 7,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Machine learning",
                "Support vector machines",
                "Optical polarization",
                "Color",
                "Oceans",
                "Optical imaging",
                "Optical design",
                "Microscopy",
                "Birefringence",
                "Displays"
            ],
            "INSPEC: Controlled Indexing": [
                "support vector machines",
                "image texture",
                "image colour analysis",
                "image classification",
                "biology computing",
                "feature extraction",
                "wavelet transforms",
                "oceanography",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "support vector machine",
                "multiscale texture",
                "image color invariants",
                "biological oceanography",
                "bivalve larvae identification",
                "optical imaging system",
                "polarized light",
                "birefringence pattern",
                "feature set extraction",
                "Gabor wavelet transform"
            ]
        },
        "id": 65,
        "cited_by": []
    },
    {
        "title": "A segmentation algorithm for contrast-enhanced images",
        "authors": [
            "Kim",
            "Zabih"
        ],
        "abstract": "Medical imaging often involves the injection of contrast agents and the subsequent analysis of tissue enhancement patterns. Many important types of tissue have characteristic enhancement patterns; for example, in magnetic resonance (MR) mammography, malignancies exhibit a characteristic \"wash out\" temporal pattern, while in MR angiography, arteries, veins and parenchyma each have their own distinctive temporal signature. In such image sequences, there are substantial changes in intensities; however, this change is due primarily to the contrast agent rather than the motion of scene elements. As a result, the task of segmenting contrast-enhanced images poses interesting new challenges for computer vision. We propose a new image segmentation algorithm for image sequences with contrast enhancement, using a model-based time series analysis of individual pixels. We use energy minimization via graph cuts to efficiently ensure spatial coherence. The energy is minimized in an expectation-maximization fashion that alternates between segmenting the image into a number of nonoverlapping regions and finding the temporal profile parameters which best describe the behavior of each region. Preliminary experiments on MR mammography and MR angiography studies show the algorithm's ability to find an accurate segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238389",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 8,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image analysis",
                "Mammography",
                "Angiography",
                "Image sequences",
                "Biomedical imaging",
                "Pattern analysis",
                "Magnetic analysis",
                "Magnetic resonance",
                "Arteries"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image sequences",
                "image enhancement",
                "image colour analysis",
                "image resolution",
                "biomedical MRI",
                "time series",
                "image motion analysis",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation algorithm",
                "contrast-enhanced images",
                "medical imaging",
                "computer vision",
                "image sequences",
                "model-based time series analysis",
                "graph cuts",
                "spatial coherence",
                "temporal profile parameters",
                "MR mammography",
                "MR angiography"
            ]
        },
        "id": 66,
        "cited_by": []
    },
    {
        "title": "Reinforcement learning for combining relevance feedback techniques",
        "authors": [
            "Peng-Yeng Yin",
            "Bhanu",
            "Kuang-Cheng Chang",
            "Dong"
        ],
        "abstract": "Relevance feedback (RF) is an interactive process which refines the retrievals by utilizing user's feedback history. Most researchers strive to develop new RF techniques and ignore the advantages of existing ones. We propose an image relevance reinforcement learning (IRRL) model for integrating existing RF techniques. Various integration schemes are presented and a long-term shared memory is used to exploit the retrieval experience from multiple users. Also, a concept digesting method is proposed to reduce the complexity of storage demand. The experimental results manifest that the integration of multiple RF approaches gives better retrieval performance than using one RF technique alone, and that the sharing of relevance knowledge between multiple query sessions also provides significant contributions for improvement. Further, the storage demand is significantly reduced by the concept digesting technique. This shows the scalability of the proposed model against a growing-size database.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238390",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Learning",
                "Feedback",
                "Radio frequency",
                "Image databases",
                "Spatial databases",
                "Information retrieval",
                "Image retrieval",
                "History",
                "Bismuth",
                "Information management"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "image retrieval",
                "relevance feedback",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relevance feedback techniques",
                "image relevance reinforcement learning",
                "storage demand",
                "concept digesting technique"
            ]
        },
        "id": 67,
        "cited_by": []
    },
    {
        "title": "Automatically labeling video data using multi-class active learning",
        "authors": [
            "Yan",
            "Jie Yang",
            "Hauptmann"
        ],
        "abstract": "Labeling video data is an essential prerequisite for many vision applications that depend on training data, such as visual information retrieval, object recognition, and human activity modelling. However, manually creating labels is not only time-consuming but also subject to human errors, and eventually, becomes impossible for a very large amount of data (e.g. 24/7 surveillance video). To minimize the human effort in labeling, we propose a unified multiclass active learning approach for automatically labeling video data. We include extending active learning from binary classes to multiple classes and evaluating several practical sample selection strategies. The experimental results show that the proposed approach works effectively even with a significantly reduced amount of labeled data. The best sample selection strategy can achieve more than a 50% error reduction over random sample selection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238391",
        "reference_list": [],
        "citation": {
            "ieee": 51,
            "other": 31,
            "total": 82
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Humans",
                "Geriatrics",
                "Computer vision",
                "Application software",
                "Information retrieval",
                "Object recognition",
                "Computer errors",
                "Robustness",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "video signal processing",
                "image sampling",
                "image sequences",
                "learning (artificial intelligence)",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video data labeling",
                "multiclass active learning",
                "visual information retrieval",
                "object recognition",
                "human activity modelling",
                "sample selection strategy",
                "computer vision",
                "image sequences"
            ]
        },
        "id": 68,
        "cited_by": [
            {
                "year": "2017",
                "id": 458
            },
            {
                "year": "2015",
                "id": 316
            },
            {
                "year": "2013",
                "id": 26
            },
            {
                "year": "2009",
                "id": 135
            }
        ]
    },
    {
        "title": "Fast vehicle detection with probabilistic feature grouping and its application to vehicle tracking",
        "authors": [
            "Kim",
            "Malik"
        ],
        "abstract": "Generating vehicle trajectories from video data is an important application of ITS (intelligent transportation systems). We introduce a new tracking approach which uses model-based 3-D vehicle detection and description algorithm. Our vehicle detection and description algorithm is based on a probabilistic line feature grouping, and it is faster (by up to an order of magnitude) and more flexible than previous image-based algorithms. We present the system implementation and the vehicle detection and tracking results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238392",
        "reference_list": [
            {
                "year": "2001",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 73,
            "other": 21,
            "total": 94
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicle detection",
                "Computer vision",
                "Traffic control",
                "Intelligent transportation systems",
                "Application software",
                "Cameras",
                "Detectors",
                "Trajectory",
                "Intelligent vehicles",
                "Vehicle driving"
            ],
            "INSPEC: Controlled Indexing": [
                "automated highways",
                "video signal processing",
                "tracking",
                "feature extraction",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast vehicle detection",
                "probabilistic feature grouping",
                "vehicle tracking",
                "vehicle trajectory",
                "video data",
                "ITS",
                "intelligent transportation systems",
                "model-based 3-D vehicle detection",
                "vehicle description algorithm",
                "computer vision"
            ]
        },
        "id": 69,
        "cited_by": [
            {
                "year": "2005",
                "id": 131
            },
            {
                "year": "2005",
                "id": 147
            }
        ]
    },
    {
        "title": "An automatic drowning detection surveillance system for challenging outdoor pool environments",
        "authors": [
            "Eng",
            "Toh",
            "Kam",
            "Wang",
            "Yau"
        ],
        "abstract": "Automatically understanding events happening at a site is the ultimate goal of visual surveillance system. We investigate the challenges faced by automated surveillance systems operating in hostile conditions and demonstrate the developed algorithms via a system that detects water crises within highly dynamic aquatic environments. An efficient segmentation algorithm based on robust block-based background modelling and thresholding-with-hysteresis methodology enables swimmers to be reliably detected amid reflections, ripples, splashes and rapid lighting changes. Partial occlusions are resolved using a Markov Random Field framework that enhances the tracking capability of the system. Visual indicators of water crises are identified based on professional knowledge of water crises detection, based on which a set of swimmer descriptors has been defined. Through seamlessly fusing the extracted swimmer descriptors based on a novel functional link network, the system achieves promising results for water crises detection. The developed algorithms have been incorporated into a live system with robust performance for different hostile environments faced by an outdoor swimming pool.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238393",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 11,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Surveillance",
                "Face detection",
                "Event detection",
                "Humans",
                "Robustness",
                "Markov random fields",
                "Cameras",
                "Optical reflection",
                "Target tracking",
                "Terrorism"
            ],
            "INSPEC: Controlled Indexing": [
                "surveillance",
                "image segmentation",
                "Markov processes",
                "random processes",
                "hidden feature removal"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic drowning detection surveillance system",
                "outdoor pool environments",
                "water crises detection",
                "segmentation algorithm",
                "block-based background modeling",
                "thresholding-with-hysteresis methodology",
                "partial occlusions",
                "Markov Random Field framework",
                "visual indicators"
            ]
        },
        "id": 70,
        "cited_by": []
    },
    {
        "title": "The catchment feature model for multimodal language analysis",
        "authors": [
            "Quek"
        ],
        "abstract": "The catchment feature model (CFM) addresses two questions in multimodal interaction: how do we bridge video and audio processing with the realities of human multimodal communication, and how information from the different modes may be fused. We discuss the need for our model, motivate the CFM from psycholinguistic research, and present the model. In contrast to 'whole gesture' recognition, the CFM applies a feature decomposition approach that facilitates cross-modal fusion at the level of discourse planning and conceptualization. We present our experimental framework for CFM-based research, and cite three concrete examples of catchment features (CF), and propose new directions of multimodal research based on the model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238394",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Feedback",
                "Psychology",
                "Speech",
                "Shape control",
                "Bridges",
                "Concrete",
                "Biological system modeling",
                "Face",
                "Eyebrows"
            ],
            "INSPEC: Controlled Indexing": [
                "audio signal processing",
                "video signal processing",
                "natural languages",
                "feature extraction",
                "gesture recognition",
                "computer vision",
                "linguistics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "catchment feature model",
                "multimodal language analysis",
                "video processing",
                "audio processing",
                "human multimodal communication",
                "feature decomposition approach",
                "cross-modal fusion",
                "gesture recognition",
                "computer vision"
            ]
        },
        "id": 71,
        "cited_by": []
    },
    {
        "title": "Cumulative residual entropy, a new measure of information & its application to image alignment",
        "authors": [
            "Wang",
            "Vemuri",
            "Rao",
            "Chen"
        ],
        "abstract": "We use the cumulative distribution of a random variable to define the information content in it and use it to develop a novel measure of information that parallels Shannon entropy, which we dub cumulative residual entropy (CRE). The key features of CRE may be summarized as, (1) its definition is valid in both the continuous and discrete domains, (2) it is mathematically more general than the Shannon entropy and (3) its computation from sample data is easy and these computations converge asymptotically to the true values. We define the cross-CRE (CCRE) between two random variables and apply it to solve the uni- and multimodal image alignment problem for parameterized (rigid, affine and projective) transformations. The key strengths of the CCRE over using the now popular mutual information method (based on Shannon's entropy) are that the former has significantly larger noise immunity and a much larger convergence range over the field of parameterized transformations. These strengths of CCRE are demonstrated via experiments on synthesized and real image data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238395",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 12,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Random variables",
                "Coordinate measuring machines",
                "Gain measurement",
                "Density measurement",
                "Mathematics",
                "Image converters",
                "Mutual information",
                "Convergence",
                "Information theory"
            ],
            "INSPEC: Controlled Indexing": [
                "information theory",
                "convergence",
                "image registration",
                "image matching",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cumulative residual entropy",
                "image alignment",
                "cumulative distribution",
                "information content",
                "Shannon entropy",
                "mutual information method",
                "convergence",
                "real image data",
                "noise immunity"
            ]
        },
        "id": 72,
        "cited_by": []
    },
    {
        "title": "Nonmetric lens distortion calibration: closed-form solutions, robust estimation and model selection",
        "authors": [
            "El-Melegy",
            "Farag"
        ],
        "abstract": "We address the problem of calibrating camera lens distortion, which can be significant in medium to wide angle lenses. While almost all existing nonmetric distortion calibration methods need user involvement in one form or another, we present an automatic approach based on the robust the-least-median-of-squares (LMedS) estimator. Our approach is thus less sensitive to erroneous input data such as image curves that are mistakenly considered as projections of 3D linear segments. Our approach uniquely uses fast, closed-form solutions to the distortion coefficients, which serve as an initial point for a nonlinear optimization algorithm to straighten imaged lines. Moreover we propose a method for distortion model selection based on geometrical inference. Successful experiments to evaluate the performance of this approach on synthetic and real data are reported.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238396",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 6,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Lenses",
                "Calibration",
                "Robustness",
                "Nonlinear distortion",
                "Cameras",
                "Layout",
                "Computer vision",
                "Image segmentation",
                "Closed-form solution",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "optical distortion",
                "photographic lenses",
                "estimation theory",
                "least mean squares methods",
                "optimisation",
                "computational geometry",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonmetric lens distortion calibration",
                "robust estimation",
                "least-median-of-squares estimator",
                "image curves",
                "nonlinear optimization algorithm",
                "straighten imaged lines",
                "distortion model selection",
                "geometrical inference"
            ]
        },
        "id": 73,
        "cited_by": []
    },
    {
        "title": "Determining reflectance and light position from a single image without distant illumination assumption",
        "authors": [
            "Hara",
            "Nishino",
            "Ikeuchi"
        ],
        "abstract": "Several techniques have been developed for recovering reflectance properties of real surfaces under unknown illumination conditions. However, in most cases, those techniques assume that the light sources are located at infinity, which cannot be applied to, for example, photometric modelling of indoor environments. We propose two methods to estimate the surface reflectance property of an object, as well as the position of a light source from a single image without the distant illumination assumption. Given a color image of an object with specular reflection as an input, the first method estimates the light source position by fitting to the Lambertian diffuse component, while separating the specular and diffuse components by using an iterative relaxation scheme. Moreover, we extend the above method by using a single specular image as an input, thus removing its constraints on the diffuse reflectance property and the number of light sources. This method simultaneously recovers the reflectance properties and the light source positions by optimizing the linearity of a log-transformed Torrance-Sparrow model. By estimating the object's reflectance property and the light source position, we can freely generate synthetic images of the target object under arbitrary source directions and source-surface distances.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238397",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflectivity",
                "Lighting",
                "Light sources",
                "H infinity control",
                "Photometry",
                "Indoor environments",
                "Surface fitting",
                "Color",
                "Optical reflection",
                "Iterative methods"
            ],
            "INSPEC: Controlled Indexing": [
                "reflectivity",
                "light sources",
                "optimisation",
                "image texture",
                "iterative methods",
                "realistic images",
                "relaxation theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "light source position",
                "surface reflectance property",
                "Lambertian diffuse component",
                "iterative relaxation scheme",
                "log-transformed Torrance-Sparrow model",
                "realistic images"
            ]
        },
        "id": 74,
        "cited_by": [
            {
                "year": "2009",
                "id": 74
            },
            {
                "year": "2007",
                "id": 61
            },
            {
                "year": "2005",
                "id": 184
            },
            {
                "year": "2003",
                "id": 129
            }
        ]
    },
    {
        "title": "Phenomenological eigenfunctions for image irradiance",
        "authors": [
            "Nillius",
            "Eklundh"
        ],
        "abstract": "We present a framework for calculating low-dimensional bases to represent image irradiance from surfaces with isotropic reflectance under arbitrary illumination. By representing the illumination and the bidirectional reflectance distribution function (BRDF) in frequency space, a model for the image irradiance is derived. This model is then reduced in dimensionality by analytically constructing the principal component basis for all images given the variations in both the illumination and the surface material. The principal component basis are constructed in such a way that all the symmetries (Helmholtz reciprocity and isotropy) of the BRDF are preserved in the basis functions. Using the framework we calculate a basis using a database of natural illumination and the CURET database containing BRDFs of real world surface materials.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238398",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Eigenvalues and eigenfunctions",
                "Lighting",
                "Reflectivity",
                "Computer vision",
                "Frequency",
                "Image analysis",
                "Space technology",
                "Low pass filters",
                "Laboratories",
                "Numerical analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "Helmholtz equations",
                "principal component analysis",
                "visual databases",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "eigenfunctions",
                "image irradiance",
                "isotropic reflectance",
                "bidirectional reflectance distribution function",
                "frequency space",
                "principal component basis",
                "Helmholtz reciprocity",
                "Helmholtz isotropy",
                "basis functions",
                "CURET database"
            ]
        },
        "id": 75,
        "cited_by": []
    },
    {
        "title": "Dealing with textureless regions and specular highlights - a progressive space carving scheme using a novel photo-consistency measure",
        "authors": [
            "Ruigang Yang",
            "Pollefeys",
            "Welch"
        ],
        "abstract": "We present two extensions to the space carving framework. The first is a progressive scheme to better reconstruct surfaces lacking sufficient textures. The second is a novel photo-consistency measure that is valid for both specular and diffuse surfaces, under unknown lighting conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238399",
        "reference_list": [
            {
                "year": "2001",
                "id": 52
            },
            {
                "year": "2001",
                "id": 111
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 6,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Extraterrestrial measurements",
                "Surface reconstruction",
                "Image reconstruction",
                "Layout",
                "Surface texture",
                "Cameras",
                "Rendering (computer graphics)",
                "Testing",
                "Computer science",
                "Volume measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image texture",
                "rendering (computer graphics)",
                "computer vision",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "specular highlight",
                "textureless region",
                "photo-consistency measure",
                "space carving framework",
                "surface reconstruction",
                "specular surface",
                "diffuse surface"
            ]
        },
        "id": 76,
        "cited_by": [
            {
                "year": "2007",
                "id": 53
            },
            {
                "year": "2005",
                "id": 45
            },
            {
                "year": "2005",
                "id": 56
            }
        ]
    },
    {
        "title": "Outlier correction in image sequences for the affine camera",
        "authors": [
            "Huynh",
            "Hartley",
            "Heyden"
        ],
        "abstract": "It is widely known that, for the affine camera model, both shape and motion can be factorized directly from the so-called image measurement matrix constructed from image point coordinates. The ability to extract both shape and motion from this matrix by a single SVD operation makes this shape-from-motion approach attractive; however, it can not deal with missing feature points and, in the presence of outliers, a direct SVD to the matrix would yield highly unreliable shape and motion components. Here, we present an outlier correction scheme that iteratively updates the elements of the image measurement matrix. The magnitude and sign of the update to each element is dependent upon the residual robustly estimated in each iteration. The result is that outliers are corrected and retained, giving improved reconstruction and smaller reprojection errors. Our iterative outlier correction scheme has been applied to both synthesized and real video sequences. The results obtained are remarkably good.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238400",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 4,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Image sequences",
                "Cameras",
                "Shape measurement",
                "Motion measurement",
                "Jacobian matrices",
                "Coordinate measuring machines",
                "Matrix decomposition",
                "Video sequences",
                "Iterative methods",
                "Australia"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "video cameras",
                "image motion analysis",
                "image reconstruction",
                "singular value decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "outlier correction scheme",
                "image sequences",
                "affine camera model",
                "image measurement matrix",
                "image shape",
                "image motion",
                "image point coordinate",
                "SVD operation",
                "shape-from-motion approach",
                "image reconstruction",
                "synthesized video sequences",
                "real video sequences"
            ]
        },
        "id": 77,
        "cited_by": []
    },
    {
        "title": "Voxel carving for specular surfaces",
        "authors": [
            "Bonfort",
            "Sturm"
        ],
        "abstract": "We present an novel algorithm that reconstructs voxels of a general 3D specular surface from multiple images of a calibrated camera. A calibrated scene (i.e. points whose 3D coordinates are known) is reflected by the unknown specular surface onto the image plane of the camera. For every viewpoint, surface normals are associated to the voxels traversed by each projection ray formed by the reflection of a scene point. A decision process then discards voxels whose associated surface normals are not consistent with one another. The output of the algorithm is a collection of voxels and surface normals in 3D space, whose quality and size depend on user-set thresholds. The method has been tested on synthetic and real images. Visual and quantified experimental results are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238401",
        "reference_list": [
            {
                "year": "2001",
                "id": 113
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 23,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Sea surface",
                "Surface fitting",
                "Cameras",
                "Layout",
                "Reflection",
                "Image reconstruction",
                "Reconstruction algorithms",
                "Surface texture",
                "Mirrors"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "realistic images",
                "surface fitting",
                "image texture",
                "ray tracing",
                "computational geometry",
                "video cameras",
                "calibration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "voxel carving",
                "specular surface",
                "image reconstruction",
                "multiple images",
                "calibrated camera",
                "calibrated scene",
                "surface normal",
                "projection ray",
                "3D space",
                "synthetic image",
                "real image"
            ]
        },
        "id": 78,
        "cited_by": [
            {
                "year": "2013",
                "id": 312
            },
            {
                "year": "2011",
                "id": 73
            },
            {
                "year": "2007",
                "id": 43
            }
        ]
    },
    {
        "title": "Variational stereovision and 3D scene flow estimation with statistical similarity measures",
        "authors": [
            "Pons",
            "Keriven",
            "Faugeras",
            "Hermosillo"
        ],
        "abstract": "We present a common variational framework for dense depth recovery and dense three-dimensional motion field estimation from multiple video sequences, which is robust to camera spectral sensitivity differences and illumination changes. For this purpose, we first show that both problems reduce to a generic image matching problem after backprojecting the input images onto suitable surfaces. We then solve this matching problem in the case of statistical similarity criteria that can handle frequently occurring nonaffine image intensities dependencies. Our method leads to an efficient and elegant implementation based on fast recursive filters. We obtain good results on real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238402",
        "reference_list": [
            {
                "year": "2001",
                "id": 111
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 7,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Fluid flow measurement",
                "Image motion analysis",
                "Motion estimation",
                "Video sequences",
                "Cameras",
                "Shape",
                "Brightness",
                "Lighting",
                "Image matching"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "computer vision",
                "image sequences",
                "image matching",
                "image motion analysis",
                "image texture",
                "recursive filters",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational stereovision",
                "3D scene flow estimation",
                "statistical similarity measure",
                "depth recovery",
                "three-dimensional motion field estimation",
                "multiple video sequences",
                "camera spectral sensitivity",
                "illumination",
                "image matching problem",
                "nonaffine image intensity",
                "recursive filter",
                "real image"
            ]
        },
        "id": 79,
        "cited_by": [
            {
                "year": "2007",
                "id": 160
            },
            {
                "year": "2005",
                "id": 181
            }
        ]
    },
    {
        "title": "Two-frame wide baseline matching",
        "authors": [
            "Jiangjian Xiao",
            "Shah"
        ],
        "abstract": "We describe a novel approach to automatically recover corresponding feature points and epipolar geometry over two wide baseline frames. Our contributions consist of several aspects: First, the use of an affine invariant feature, edge-corner, is introduced to provide a robust and consistent matching primitives. Second, based on SVD decomposition of affine matrix, the affine matching space between two corners can be approximately divided into two independent spaces by rotation angle and scaling factor. Employing this property, a two-stage affine matching algorithm is designed to obtain robust matches over two frames. Third, using the epipolar geometry estimated by these matches, more corresponding feature points are determined. Based on these robust correspondences, the fundamental matrix is refined, and a series of virtual views of the scene are synthesized. Finally, several experiments are presented to illustrate that a number of robust correspondences can be stably determined for two wide baseline images under significant camera motions with illumination changes, occlusions, and self-similarities. After testing a number of examples and comparing with the existing methods, the experimental results strongly demonstrate that our matching method outperforms the state-of-art algorithms for all of the test cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238403",
        "reference_list": [
            {
                "year": "2001",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 10,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Image edge detection",
                "Transmission line matrix methods",
                "Lighting",
                "Computer vision",
                "Geometry",
                "Matrix decomposition",
                "Algorithm design and analysis",
                "Layout",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "singular value decomposition",
                "computational geometry",
                "feature extraction",
                "edge detection",
                "image motion analysis",
                "natural scenes",
                "hidden feature removal",
                "rendering (computer graphics)",
                "video cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "two-frame wide baseline matching",
                "epipolar geometry",
                "edge-corner detection",
                "image matching",
                "SVD decomposition",
                "affine matrix",
                "affine matching space",
                "feature points",
                "camera motion",
                "illumination",
                "occlusion"
            ]
        },
        "id": 80,
        "cited_by": [
            {
                "year": "2015",
                "id": 248
            },
            {
                "year": "2009",
                "id": 161
            }
        ]
    },
    {
        "title": "Fast stereo matching using reliability-based dynamic programming and consistency constraints",
        "authors": [
            "Gong",
            "Yee-Hong Yang"
        ],
        "abstract": "A method for solving binocular and multiview stereo matching problems is presented here. A weak consistency constraint is proposed, which expresses the visibility constraint in the image space. It can be proved that the weak consistency constraint holds for scenes that can be represented by a set of 3D points. As well, also proposed is a new reliability measure for dynamic programming techniques, which evaluates the reliability of a given match. A novel reliability-based dynamic programming algorithm is derived accordingly, which can selectively assign disparity values to pixels when the reliabilities of the corresponding matches exceed a given threshold. Consistency constraints and the new reliability-based dynamic programming algorithm can be combined in an iterative approach. The experimental results show that the iterative approach can produce dense (60-90%) and reliable (total error rate of 0.1-1.1%) matching for binocular stereo datasets. It can also generate promising disparity maps for trinocular and multiview stereo datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238404",
        "reference_list": [],
        "citation": {
            "ieee": 39,
            "other": 13,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Dynamic programming",
                "Heuristic algorithms",
                "Iterative algorithms",
                "Stereo vision",
                "Iterative methods",
                "Computational efficiency",
                "Layout",
                "Error analysis",
                "Taxonomy",
                "Rendering (computer graphics)"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "stereo image processing",
                "natural scenes",
                "constraint theory",
                "dynamic programming",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo matching",
                "reliability-based dynamic programming",
                "consistency constraint",
                "visibility constraint",
                "image space",
                "iterative approach",
                "binocular stereo dataset",
                "disparity map",
                "trinocular stereo dataset",
                "multiview stereo dataset"
            ]
        },
        "id": 81,
        "cited_by": []
    },
    {
        "title": "Shape and motion under varying illumination: unifying structure from motion, photometric stereo, and multiview stereo",
        "authors": [
            "Li Zhang",
            "Curless",
            "Hertzmann",
            "Seitz"
        ],
        "abstract": "We present an algorithm for computing optical flow, shape, motion, lighting, and albedo from an image sequence of a rigidly-moving Lambertian object under distant illumination. The problem is formulated in a manner that subsumes structure from motion, multiview stereo, and photometric stereo as special cases. The algorithm utilizes both spatial and temporal intensity variation as cues: the former constrains flow and the latter constrains surface orientation; combining both cues enables dense reconstruction of both textured and textureless surfaces. The algorithm works by iteratively estimating affine camera parameters, illumination, shape, and albedo in an alternating fashion. Results are demonstrated on videos of hand-held objects moving in front of a fixed light and camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238405",
        "reference_list": [
            {
                "year": "2001",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 58,
            "other": 21,
            "total": 79
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Lighting",
                "Photometry",
                "Optical computing",
                "Iterative algorithms",
                "Surface reconstruction",
                "Surface texture",
                "Cameras",
                "Optical variables control",
                "Image motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "stereo image processing",
                "image texture",
                "image sequences",
                "image reconstruction",
                "video cameras",
                "albedo",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image shape",
                "image motion",
                "photometric stereo",
                "multiview stereo",
                "optical flow computation",
                "image sequence",
                "Lambertian object",
                "distant illumination",
                "spatial intensity variation",
                "temporal intensity variation",
                "surface orientation",
                "image reconstruction",
                "textured surface",
                "textureless surface",
                "affine camera parameter"
            ]
        },
        "id": 82,
        "cited_by": [
            {
                "year": "2005",
                "id": 29
            },
            {
                "year": "2005",
                "id": 213
            },
            {
                "year": "2005",
                "id": 218
            }
        ]
    },
    {
        "title": "Unsupervised improvement of visual detectors using cotraining",
        "authors": [
            "Levin",
            "Viola",
            "Freund"
        ],
        "abstract": "One significant challenge in the construction of visual detection systems is the acquisition of sufficient labeled data. We describe a new technique for training visual detectors which requires only a small quantity of labeled data, and then uses unlabeled data to improve performance over time. Unsupervised improvement is based on the cotraining framework of Blum and Mitchell, in which two disparate classifiers are trained simultaneously. Unlabeled examples which are confidently labeled by one classifier are added, with labels, to the training set of the other classifier. Experiments are presented on the realistic task of automobile detection in roadway surveillance video. In this application, cotraining reduces the false positive rate by a factor of 2 to 11 from the classifier trained with labeled data alone.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238406",
        "reference_list": [],
        "citation": {
            "ieee": 63,
            "other": 54,
            "total": 117
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Face detection",
                "Costs",
                "Cameras",
                "Automobiles",
                "Surveillance",
                "Computer science",
                "Data acquisition",
                "Training data",
                "History"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "object detection",
                "pattern classification",
                "surveillance",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised improvement",
                "visual detection system",
                "labeled data acquisition",
                "visual detector training",
                "pattern classifier",
                "automobile detection",
                "roadway surveillance video"
            ]
        },
        "id": 83,
        "cited_by": [
            {
                "year": "2011",
                "id": 215
            },
            {
                "year": "2007",
                "id": 115
            },
            {
                "year": "2007",
                "id": 295
            }
        ]
    },
    {
        "title": "Selection of scale-invariant parts for object class recognition",
        "authors": [
            "Dorko",
            "Schmid"
        ],
        "abstract": "We introduce a novel method for constructing and selecting scale-invariant object parts. Scale-invariant local descriptors are first grouped into basic parts. A classifier is then learned for each of these parts, and feature selection is used to determine the most discriminative ones. This approach allows robust pan detection, and it is invariant under scale changes-that is, neither the training images nor the test images have to be normalized. The proposed method is evaluated in car detection tasks with significant variations in viewing conditions, and promising results are demonstrated. Different local regions, classifiers and feature selection methods are quantitatively compared. Our evaluation shows that local invariant descriptors are an appropriate representation for object classes such as cars, and it underlines the importance of feature selection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238407",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 94,
            "other": 73,
            "total": 167
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Robustness",
                "Testing",
                "Machine learning",
                "Feature extraction",
                "Brightness",
                "Character recognition",
                "Image recognition",
                "Image segmentation",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "pattern classification",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object class recognition",
                "scale-invariant object part",
                "scale-invariant local descriptor",
                "pattern classifier",
                "feature selection",
                "car detection task"
            ]
        },
        "id": 84,
        "cited_by": [
            {
                "year": "2017",
                "id": 146
            },
            {
                "year": "2011",
                "id": 176
            },
            {
                "year": "2009",
                "id": 125
            },
            {
                "year": "2009",
                "id": 164
            },
            {
                "year": "2007",
                "id": 48
            },
            {
                "year": "2007",
                "id": 259
            },
            {
                "year": "2007",
                "id": 288
            },
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2005",
                "id": 115
            },
            {
                "year": "2005",
                "id": 234
            }
        ]
    },
    {
        "title": "Inferring 3D structure with a statistical image-based shape model",
        "authors": [
            "Grauman",
            "Shakhnarovich",
            "Darrell"
        ],
        "abstract": "We present an image-based approach to infer 3D structure parameters using a probabilistic \"shape+structure\" model. The 3D shape of an object class is represented by sets of contours from silhouette views simultaneously observed from multiple calibrated cameras, while structural features of interest on the object are denoted by a number of 3D locations. A prior density over the multiview shape and corresponding structure is constructed with a mixture of probabilistic principal components analyzers. Given a novel set of contours, we infer the unknown structure parameters from the new shape's Bayesian reconstruction. Model matching and parameter inference are done entirely in the image domain and require no explicit 3D construction. Our shape model enables accurate estimation of structure despite segmentation errors or missing views in the input silhouettes, and it works even with only a single input view. Using a training set of thousands of pedestrian images generated from a synthetic model, we can accurately infer the 3D locations of 19 joints on the body based on observed silhouette contours from real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238408",
        "reference_list": [],
        "citation": {
            "ieee": 62,
            "other": 50,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image reconstruction",
                "Image segmentation",
                "Cameras",
                "Bayesian methods",
                "Computer science",
                "Artificial intelligence",
                "Laboratories",
                "Image generation",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "principal component analysis",
                "video cameras",
                "image matching",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D structure parameter inference",
                "statistical image-based shape model",
                "multiple calibrated camera",
                "probabilistic principal component analyzer",
                "Bayesian reconstruction",
                "model matching",
                "pedestrian images training set",
                "silhouette contour",
                "real image"
            ]
        },
        "id": 85,
        "cited_by": [
            {
                "year": "2009",
                "id": 174
            },
            {
                "year": "2007",
                "id": 107
            },
            {
                "year": "2005",
                "id": 93
            }
        ]
    },
    {
        "title": "Affine-invariant local descriptors and neighborhood statistics for texture recognition",
        "authors": [
            "Lazebnik",
            "Schmid",
            "Ponce"
        ],
        "abstract": "We present a framework for texture recognition based on local affine-invariant descriptors and their spatial layout. At modelling time, a generative model of local descriptors is learned from sample images using the EM algorithm. The EM framework allows the incorporation of unsegmented multitexture images into the training set. The second modelling step consists of gathering co-occurrence statistics of neighboring descriptors. At recognition time, initial probabilities computed from the generative model are refined using a relaxation step that incorporates co-occurrence statistics. Performance is evaluated on images of an indoor scene and pictures of wild animals.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238409",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            },
            {
                "year": "2001",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 30,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Detectors",
                "Image recognition",
                "Shape",
                "Probability",
                "Layout",
                "Animals",
                "Image retrieval",
                "Image segmentation",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "image recognition",
                "learning (artificial intelligence)",
                "statistical analysis",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "affine-invariant local descriptors",
                "neighborhood statistics",
                "texture recognition",
                "spatial layout",
                "unsegmented multitexture images",
                "training set",
                "probability",
                "texture modelling"
            ]
        },
        "id": 86,
        "cited_by": [
            {
                "year": "2011",
                "id": 290
            },
            {
                "year": "2007",
                "id": 179
            }
        ]
    },
    {
        "title": "Shape representation via harmonic embedding",
        "authors": [
            "Duci",
            "Yezzi",
            "Mitter",
            "Soatto"
        ],
        "abstract": "We present a novel representation of shape for closed planar contours explicitly designed to possess a linear structure. This greatly simplifies linear operations such as averaging, principal component analysis or differentiation in the space of shapes. The representation relies upon embedding the contour on a subset of the space of harmonic functions of which the original contour is the zero level set.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238410",
        "reference_list": [
            {
                "year": "2001",
                "id": 7
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 7,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Level set",
                "Principal component analysis",
                "Computer science",
                "Space technology",
                "Nonlinear equations",
                "Geometry",
                "Topology",
                "Laplace equations",
                "Design engineering"
            ],
            "INSPEC: Controlled Indexing": [
                "principal component analysis",
                "harmonic analysis",
                "image representation",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape representation",
                "harmonic embedding",
                "closed planar contours",
                "principal component analysis",
                "differentiation",
                "zero level set"
            ]
        },
        "id": 87,
        "cited_by": []
    },
    {
        "title": "Learning pedestrian models for silhouette refinement",
        "authors": [
            "Lee",
            "Dalley",
            "Tieu"
        ],
        "abstract": "We present a model-based method for accurate extraction of pedestrian silhouettes from video sequences. Our approach is based on two assumptions, 1) there is a common appearance to all pedestrians, and 2) each individual looks like him/herself over a short amount of time. These assumptions allow us to learn pedestrian models that encompass both a pedestrian population appearance and the individual appearance variations. Using our models, we are able to produce pedestrian silhouettes that have fewer noise pixels and missing parts. We apply our silhouette extraction approach to the NIST gait data set and show that under the gait recognition task, our model-based silhouettes result in much higher recognition rates than silhouettes directly extracted from background subtraction, or any nonmodel-based smoothing schemes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238411",
        "reference_list": [],
        "citation": {
            "ieee": 42,
            "other": 24,
            "total": 66
        },
        "keywords": {
            "IEEE Keywords": [
                "Legged locomotion",
                "Background noise",
                "Shape",
                "Data mining",
                "Smoothing methods",
                "Colored noise",
                "Video sequences",
                "NIST",
                "Noise shaping",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "gait analysis",
                "image sequences",
                "image motion analysis",
                "feature extraction",
                "image segmentation",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based pedestrian silhouette extraction",
                "video sequences",
                "noise pixels",
                "gait recognition",
                "nonmodel-based smoothing scheme",
                "pedestrian shape representation",
                "image segmentation"
            ]
        },
        "id": 88,
        "cited_by": []
    },
    {
        "title": "A Bayesian network framework for relational shape matching",
        "authors": [
            "Rangarajan",
            "Coughlan",
            "Yuille"
        ],
        "abstract": "A Bayesian network formulation for relational shape matching is presented. The main advantage of the relational shape matching approach is the obviation of the nonrigid spatial mappings used by recent nonrigid matching approaches. The basic variables that need to be estimated in the relational shape matching objective function are the global rotation and scale and the local displacements and correspondences. The new Bethe free energy approach is used to estimate the pairwise correspondences between links of the template graphs and the data. The resulting framework is useful in both registration and recognition contexts. Results are shown on hand-drawn templates and on 2D transverse T1-weighted MR images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238412",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 14,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Shape",
                "Image segmentation",
                "Spline",
                "Topology",
                "Biomedical computing",
                "Biomedical imaging",
                "Optical imaging",
                "Object recognition",
                "Indexing"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "image matching",
                "image registration",
                "magnetic resonance imaging",
                "graph theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian network formulation",
                "relational shape matching",
                "nonrigid spatial mapping",
                "objective function",
                "Bethe free energy approach",
                "image correspondence estimation",
                "template graph",
                "image registration",
                "image recognition",
                "2D transverse T1-weighted MR image"
            ]
        },
        "id": 89,
        "cited_by": []
    },
    {
        "title": "Unified subspace analysis for face recognition",
        "authors": [
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "We propose a face difference model that decomposes face difference into three components, intrinsic difference, transformation difference, and noise. Using the face difference model and a detailed subspace analysis on the three components we develop a unified framework for subspace analysis. Using this framework we discover the inherent relationship among different subspace methods and their unique contributions to the extraction of discriminating information from the face difference. This eventually leads to the construction of a 3D parameter space that uses three subspace dimensions as axis. Within this parameter space, we develop a unified subspace analysis method that achieves better recognition performance than the standard subspace methods on over 2000 face images from the FERET database.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238413",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 8,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Principal component analysis",
                "Linear discriminant analysis",
                "Bayesian methods",
                "Data mining",
                "Performance analysis",
                "Image recognition",
                "Standards development",
                "Image databases",
                "Karhunen-Loeve transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "principal component analysis",
                "feature extraction",
                "Bayes methods",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unified subspace analysis",
                "face recognition",
                "face difference model",
                "intrinsic difference",
                "transformation difference",
                "noise",
                "3D parameter space construction",
                "FERET database",
                "feature extraction",
                "principal component analysis",
                "Bayesian algorithm"
            ]
        },
        "id": 90,
        "cited_by": [
            {
                "year": "2005",
                "id": 9
            }
        ]
    },
    {
        "title": "Face sketch synthesis and recognition",
        "authors": [
            "Xiaoou Tang",
            "Xiaogang Wang"
        ],
        "abstract": "We propose a novel face photo retrieval system using sketch drawings. By transforming a photo image into a sketch, we reduce the difference between photo and sketch significantly, thus allow effective matching between the two. To improve the synthesis performance, we separate shape and texture information in a face photo, and conduct transformation on them respectively. Finally a Bayesian classifier is used to recognize the probing sketch from the synthesized pseudo-sketches. Experiments on a data set containing 606 people clearly demonstrate the efficacy of the algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238414",
        "reference_list": [],
        "citation": {
            "ieee": 69,
            "other": 27,
            "total": 96
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Shape",
                "Information retrieval",
                "Bayesian methods",
                "Image databases",
                "Testing",
                "Spatial databases",
                "Face detection",
                "Image reconstruction",
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image texture",
                "image retrieval",
                "belief networks",
                "eigenvalues and eigenfunctions",
                "image matching",
                "learning (artificial intelligence)",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face sketch synthesis",
                "face recognition",
                "face photo retrieval system",
                "image matching",
                "image texture information",
                "Bayesian classifier",
                "eigentransformation algorithm",
                "training data set"
            ]
        },
        "id": 91,
        "cited_by": [
            {
                "year": "2005",
                "id": 221
            }
        ]
    },
    {
        "title": "Dynamic stroke information analysis for video-based handwritten Chinese character recognition",
        "authors": [
            "Feng Lin",
            "Xiaoou Tang"
        ],
        "abstract": "Video-based handwritten character recognition (VCR) system is a new type of character recognition system with many unique advantages over online character recognition system. Its main problem is to effectively extract stroke dynamic information from video data for character recognition. We propose a new stroke extraction algorithm through dynamic stroke information analysis for a VCR system. The experimental results on over 3000 video character sequences show that our system can extract the Chinese character stroke dynamic information similar to an online system.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238415",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Information analysis",
                "Character recognition",
                "Data mining",
                "Writing",
                "Video recording",
                "Handwriting recognition",
                "Cameras",
                "Video sequences",
                "Tracking",
                "Ink"
            ],
            "INSPEC: Controlled Indexing": [
                "handwritten character recognition",
                "natural languages",
                "feature extraction",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic stroke information analysis",
                "video-based handwritten Chinese character recognition system",
                "stroke extraction algorithm",
                "video character sequences"
            ]
        },
        "id": 92,
        "cited_by": []
    },
    {
        "title": "Surface classification using conformal structures",
        "authors": [
            "Gu",
            "Yau"
        ],
        "abstract": "3D surface classification is a fundamental problem in computer vision and computational geometry. Surfaces can be classified by different transformation groups. Traditional classification methods mainly use topological transformation groups and Euclidean transformation groups. We introduce a novel method to classify surfaces by conformal transformation groups. Conformal equivalent class is refiner than topological equivalent class and coarser than isometric equivalent class, making it suitable for practical classification purposes. For general surfaces, the gradient fields of conformal maps form a vector space, which has a natural structure invariant under conformal transformations. We present an algorithm to compute this conformal structure, which can be represented as matrices, and use it to classify surfaces. The result is intrinsic to the geometry, invariant to triangulation and insensitive to resolution. To the best of our knowledge, this is the first paper to classify surfaces with arbitrary topologies by global conformal invariants. The method introduced here can also be used for surface matching problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238416",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 10,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Surface treatment",
                "Computer vision",
                "Computational geometry",
                "Mathematics",
                "Topology",
                "Large-scale systems",
                "Spatial databases",
                "Euclidean distance",
                "Noise robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "conformal mapping",
                "computational geometry",
                "surface fitting",
                "image matching",
                "equivalence classes",
                "vectors",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D surface classification",
                "computer vision",
                "computational geometry",
                "conformal transformation group",
                "conformal equivalent class",
                "gradient field",
                "vector space",
                "triangulation",
                "conformal invariants",
                "surface matching problem",
                "topological transformation"
            ]
        },
        "id": 93,
        "cited_by": [
            {
                "year": "2005",
                "id": 50
            }
        ]
    },
    {
        "title": "Boosting chain learning for object detection",
        "authors": [
            "Rong Xiao",
            "Long Zhu",
            "Hong-Jiang Zhang"
        ],
        "abstract": "A general classification framework, called boosting chain, is proposed for learning boosting cascade. In this framework, a \"chain\" structure is introduced to integrate historical knowledge into successive boosting learning. Moreover, a linear optimization scheme is proposed to address the problems of redundancy in boosting learning and threshold adjusting in cascade coupling. By this means, the resulting classifier consists of fewer weak classifiers yet achieves lower error rates than boosting cascade in both training and test. Experimental comparisons of boosting chain and boosting cascade are provided through a face detection problem. The promising results clearly demonstrate the effectiveness made by boosting chain.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238417",
        "reference_list": [],
        "citation": {
            "ieee": 48,
            "other": 16,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Object detection",
                "Detectors",
                "Face detection",
                "Support vector machines",
                "Computational efficiency",
                "Iterative algorithms",
                "Support vector machine classification",
                "Asia",
                "Redundancy"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "learning (artificial intelligence)",
                "pattern classification",
                "optimisation",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "boosting chain learning",
                "object detection",
                "pattern classification",
                "linear optimization",
                "redundancy",
                "cascade coupling",
                "face detection problem",
                "bootstrap training"
            ]
        },
        "id": 94,
        "cited_by": [
            {
                "year": "2017",
                "id": 204
            },
            {
                "year": "2015",
                "id": 375
            },
            {
                "year": "2007",
                "id": 21
            },
            {
                "year": "2005",
                "id": 57
            },
            {
                "year": "2005",
                "id": 156
            }
        ]
    },
    {
        "title": "Texture segmentation by multiscale aggregation of filter responses and shape elements",
        "authors": [
            "Galun",
            "Sharon",
            "Basri",
            "Brandt"
        ],
        "abstract": "Texture segmentation is a difficult problem, as is apparent from camouflage pictures. A textured region can contain texture elements of various sizes, each of which can itself be textured. We approach this problem using a bottom-up aggregation framework that combines structural characteristics of texture elements with filter responses. Our process adaptively identifies the shape of texture elements and characterize them by their size, aspect ratio, orientation, brightness, etc., and then uses various statistics of these properties to distinguish between different textures. At the same time our process uses the statistics of filter responses to characterize textures. In our process the shape measures and the filter responses crosstalk extensively. In addition, a top-down cleaning process is applied to avoid mixing the statistics of neighboring segments. We tested our algorithm on real images and demonstrate that it can accurately segment regions that contain challenging textures.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238418",
        "reference_list": [],
        "citation": {
            "ieee": 56,
            "other": 38,
            "total": 94
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Image segmentation",
                "Filter bank",
                "Shape measurement",
                "Brightness",
                "Crosstalk",
                "Computer vision",
                "Computer science",
                "Cleaning",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "image segmentation",
                "image resolution",
                "filtering theory",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "texture segmentation",
                "multiscale aggregation",
                "filter response",
                "camouflage pictures",
                "bottom-up aggregation",
                "structural characteristics",
                "texture element shape",
                "statistical analysis"
            ]
        },
        "id": 95,
        "cited_by": [
            {
                "year": "2009",
                "id": 107
            },
            {
                "year": "2007",
                "id": 87
            }
        ]
    },
    {
        "title": "Recognizing action at a distance",
        "authors": [
            "Efros",
            "Berg",
            "Mori",
            "Malik"
        ],
        "abstract": "Our goal is to recognize human action at a distance, at resolutions where a whole person may be, say, 30 pixels tall. We introduce a novel motion descriptor based on optical flow measurements in a spatiotemporal volume for each stabilized human figure, and an associated similarity measure to be used in a nearest-neighbor framework. Making use of noisy optical flow measurements is the key challenge, which is addressed by treating optical flow not as precise pixel displacements, but rather as a spatial pattern of noisy measurements which are carefully smoothed and aggregated to form our spatiotemporal motion descriptor. To classify the action being performed by a human figure in a query sequence, we retrieve nearest neighbor(s) from a database of stored, annotated video sequences. We can also use these retrieved exemplars to transfer 2D/3D skeletons onto the figures in the query sequence, as well as two forms of data-based action synthesis \"do as I do\" and \"do as I say\". Results are demonstrated on ballet, tennis as well as football datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238420",
        "reference_list": [],
        "citation": {
            "ieee": 426,
            "other": 294,
            "total": 720
        },
        "keywords": {
            "IEEE Keywords": [
                "Fluid flow measurement",
                "Motion measurement",
                "Humans",
                "Image motion analysis",
                "Volume measurement",
                "Spatiotemporal phenomena",
                "Optical noise",
                "Displacement measurement",
                "Information retrieval",
                "Spatial databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image retrieval",
                "image motion analysis",
                "computer vision",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "distant action recognition",
                "human action",
                "motion descriptor",
                "optical flow measurements",
                "spatiotemporal volume",
                "stabilized human figure",
                "similarity measure",
                "nearest-neighbor framework",
                "noisy optical flow",
                "spatial pattern",
                "action classification",
                "query sequence",
                "nearest neighbor retrieval",
                "video sequences database",
                "retrieved exemplars",
                "2D skeletons",
                "3D skeletons",
                "data-based action synthesis",
                "ballet dataset",
                "tennis dataset",
                "football dataset"
            ]
        },
        "id": 96,
        "cited_by": [
            {
                "year": "2017",
                "id": 226
            },
            {
                "year": "2017",
                "id": 415
            },
            {
                "year": "2017",
                "id": 461
            },
            {
                "year": "2015",
                "id": 500
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2013",
                "id": 280
            },
            {
                "year": "2013",
                "id": 396
            },
            {
                "year": "2011",
                "id": 51
            },
            {
                "year": "2011",
                "id": 89
            },
            {
                "year": "2011",
                "id": 127
            },
            {
                "year": "2011",
                "id": 156
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2009",
                "id": 62
            },
            {
                "year": "2009",
                "id": 140
            },
            {
                "year": "2009",
                "id": 144
            },
            {
                "year": "2009",
                "id": 186
            },
            {
                "year": "2009",
                "id": 193
            },
            {
                "year": "2007",
                "id": 10
            },
            {
                "year": "2007",
                "id": 26
            },
            {
                "year": "2007",
                "id": 84
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2007",
                "id": 206
            },
            {
                "year": "2007",
                "id": 209
            },
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2005",
                "id": 19
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2005",
                "id": 94
            },
            {
                "year": "2005",
                "id": 182
            }
        ]
    },
    {
        "title": "Detecting pedestrians using patterns of motion and appearance",
        "authors": [
            "Viola",
            "Jones",
            "Snow"
        ],
        "abstract": "This paper describes a pedestrian detection system that integrates image intensity information with motion information. We use a detection style algorithm that scans a detector over two consecutive frames of a video sequence. The detector is trained (using AdaBoost) to take advantage of both motion and appearance information to detect a walking person. Past approaches have built detectors based on appearance information, but ours is the first to combine both sources of information in a single detector. The implementation described runs at about 4 frames/second, detects pedestrians at very small scales (as small as 20/spl times/15 pixels), and has a very low false positive rate. Our approach builds on the detection work of Viola and Jones. Novel contributions of this paper include: i) development of a representation of image motion which is extremely efficient, and ii) implementation of a state of the art pedestrian detection system which operates on low resolution images under difficult conditions (such as rain and snow).",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238422",
        "reference_list": [],
        "citation": {
            "ieee": 458,
            "other": 284,
            "total": 742
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion detection",
                "Detectors",
                "Face detection",
                "Image resolution",
                "Humans",
                "Motion analysis",
                "Snow",
                "Rain",
                "Pattern recognition",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image representation",
                "image sequences",
                "image resolution",
                "object detection",
                "feature extraction",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pedestrian detection",
                "motion patterns",
                "motion appearance",
                "image intensity information",
                "motion information",
                "detection style algorithm",
                "detector scanning",
                "video sequence frames",
                "AdaBoost",
                "walking person",
                "image motion representation",
                "low resolution images",
                "300 pixels",
                "20 pixels",
                "15 pixels"
            ]
        },
        "id": 97,
        "cited_by": [
            {
                "year": "2009",
                "id": 4
            },
            {
                "year": "2009",
                "id": 92
            },
            {
                "year": "2009",
                "id": 205
            },
            {
                "year": "2007",
                "id": 101
            },
            {
                "year": "2007",
                "id": 195
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2005",
                "id": 94
            },
            {
                "year": "2005",
                "id": 106
            },
            {
                "year": "2005",
                "id": 112
            },
            {
                "year": "2005",
                "id": 178
            }
        ]
    },
    {
        "title": "Recognition of group activities using dynamic probabilistic networks",
        "authors": [
            "Shaogang Gong",
            "Tao Xiang"
        ],
        "abstract": "Dynamic Probabilistic Networks (DPNs) are exploited for modeling the temporal relationships among a set of different object temporal events in the scene for a coherent and robust scene-level behaviour interpretation. In particular, we develop a Dynamically Multi-Linked Hidden Markov Model (DML-HMM) to interpret group activities involving multiple objects captured in an outdoor scene. The model is based on the discovery of salient dynamic interlinks among multiple temporal events using DPNs. Object temporal events are detected and labeled using Gaussian Mixture Models with automatic model order selection. A DML-HMM is built using Schwarz's Bayesian Information Criterion based factorisation resulting in its topology being intrinsically determined by the underlying causality and temporal order among different object events. Our experiments demonstrate that its performance on modelling group activities in a noisy outdoor scene is superior compared to that of a Multi-Observation Hidden Markov Model (MOHMM), a Parallel Hidden Markov Model (PaHMM) and a Coupled Hidden Markov Model (CHMM).",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238423",
        "reference_list": [],
        "citation": {
            "ieee": 63,
            "other": 29,
            "total": 92
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Layout",
                "State-space methods",
                "Bayesian methods",
                "Character recognition",
                "Network topology",
                "Computer science",
                "Robustness",
                "Object detection",
                "Event detection"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden Markov models",
                "Bayes methods",
                "computer vision",
                "object detection",
                "image recognition",
                "topology",
                "probability",
                "Gaussian processes",
                "causality"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "group activity recognition",
                "dynamic probabilistic networks",
                "DPN",
                "temporal relationship modeling",
                "scene-level behaviour interpretation",
                "Dynamically Multi-Linked Hidden Markov Model",
                "DML-HMM",
                "group activity interpretation",
                "object capture",
                "outdoor scene",
                "salient dynamic interlinks",
                "multiple temporal events",
                "object temporal events",
                "Gaussian mixture models",
                "automatic model order selection",
                "Schwarz Bayesian information criterion",
                "factorisation",
                "topology",
                "object events"
            ]
        },
        "id": 98,
        "cited_by": [
            {
                "year": "2011",
                "id": 94
            },
            {
                "year": "2005",
                "id": 162
            },
            {
                "year": "2005",
                "id": 236
            }
        ]
    },
    {
        "title": "Fast pose estimation with parameter-sensitive hashing",
        "authors": [
            "Shakhnarovich",
            "Viola",
            "Darrell"
        ],
        "abstract": "Example-based methods are effective for parameter estimation problems when the underlying system is simple or the dimensionality of the input is low. For complex and high-dimensional problems such as pose estimation, the number of required examples and the computational complexity rapidly become prohibitively high. We introduce a new algorithm that learns a set of hashing functions that efficiently index examples relevant to a particular estimation task. Our algorithm extends locality-sensitive hashing, a recently developed method to find approximate neighbors in time sublinear in the number of examples. This method depends critically on the choice of hash functions that are optimally relevant to a particular estimation problem. Experiments demonstrate that the resulting algorithm, which we call parameter-sensitive hashing, can rapidly and accurately estimate the articulated pose of human figures from a large database of example images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238424",
        "reference_list": [
            {
                "year": "2001",
                "id": 161
            }
        ],
        "citation": {
            "ieee": 264,
            "other": 185,
            "total": 449
        },
        "keywords": {
            "IEEE Keywords": [
                "Parameter estimation",
                "Computational complexity",
                "Computer vision",
                "Computer science",
                "Artificial intelligence",
                "Humans",
                "Image databases",
                "Video sequences",
                "Biological system modeling",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "visual databases",
                "computer vision",
                "parameter estimation",
                "file organisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast pose estimation",
                "parameter sensitive hashing",
                "example-based methods",
                "parameter estimation problems",
                "computational complexity",
                "hashing functions",
                "example indexing",
                "locality-sensitive hashing",
                "hash functions",
                "human figures",
                "database"
            ]
        },
        "id": 99,
        "cited_by": [
            {
                "year": "2017",
                "id": 278
            },
            {
                "year": "2017",
                "id": 584
            },
            {
                "year": "2015",
                "id": 208
            },
            {
                "year": "2015",
                "id": 467
            },
            {
                "year": "2011",
                "id": 107
            },
            {
                "year": "2011",
                "id": 138
            },
            {
                "year": "2011",
                "id": 157
            },
            {
                "year": "2009",
                "id": 5
            },
            {
                "year": "2009",
                "id": 38
            },
            {
                "year": "2009",
                "id": 136
            },
            {
                "year": "2009",
                "id": 173
            },
            {
                "year": "2009",
                "id": 174
            },
            {
                "year": "2009",
                "id": 183
            },
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2007",
                "id": 36
            },
            {
                "year": "2007",
                "id": 201
            },
            {
                "year": "2007",
                "id": 214
            },
            {
                "year": "2007",
                "id": 233
            },
            {
                "year": "2007",
                "id": 235
            },
            {
                "year": "2005",
                "id": 106
            },
            {
                "year": "2005",
                "id": 133
            },
            {
                "year": "2005",
                "id": 185
            },
            {
                "year": "2003",
                "id": 60
            }
        ]
    },
    {
        "title": "Towards gauge invariant bundle adjustment: a solution based on gauge dependent damping",
        "authors": [
            "Bartoli"
        ],
        "abstract": "Bundle adjustment is used to obtain accurate visual reconstructions by minimizing the reprojection error. The coordinate frame ambiguity, or more generality the gauge freedoms, has been dealt with in different manners. It has often been reported that standard bundle adjustment algorithms were not gauge invariant: two iterations within different gauges can lead to geometrically very different results. Surprisingly, most algorithms do not exploit gauge freedoms to improve performances. We consider this issue. We analyze theoretically the impact of the gauge on standard algorithms. We show that a sufficiently general damping matrix in Levenberg-Marquardt iteration can be used to implicitly reproduce a gauge transformation. We show that if the damping matrix is chosen such that the decrease in the reprojection error is maximized, then the iteration is gauge invariant. Experimental results on simulated and real data show that our gauge invariant bundle adjustment algorithm outperforms existing ones in terms of stability.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238425",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Damping",
                "Image reconstruction",
                "Cameras",
                "Symmetric matrices",
                "Least squares methods",
                "Nonlinear equations",
                "Geometry",
                "Algorithm design and analysis",
                "Computational modeling",
                "Stability"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "iterative methods",
                "matrix algebra",
                "damping",
                "photogrammetry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gauge invariant bundle adjustment",
                "gauge dependent damping",
                "visual reconstructions",
                "reprojection error",
                "coordinate frame ambiguity",
                "gauge freedoms",
                "gauge invariance",
                "damping matrix",
                "Levenberg-Marquardt iteration",
                "gauge transformation"
            ]
        },
        "id": 100,
        "cited_by": []
    },
    {
        "title": "Mirrors in motion: epipolar geometry and motion estimation",
        "authors": [
            "Geyer",
            "Daniilidis"
        ],
        "abstract": "In this paper we consider the images taken from pairs of parabolic catadioptric cameras separated by discrete motions. Despite the nonlinearity of the projection model, the epipolar geometry arising from such a system, like the perspective case, can be encoded in a bilinear form, the catadioptric fundamental matrix. We show that all such matrices have equal Lorentzian singular values, and they define a nine-dimensional manifold in the space of 4 /spl times/ 4 matrices. Furthermore, this manifold can be identified with a quotient of two Lie groups. We present a method to estimate a matrix in this space, so as to obtain an estimate of the motion. We show that the estimation procedures are robust to modest deviations from the ideal assumptions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238426",
        "reference_list": [
            {
                "year": "2001",
                "id": 16
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 11,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Mirrors",
                "Geometry",
                "Motion estimation",
                "Cameras",
                "Navigation",
                "Robustness",
                "Robot vision systems",
                "Visualization",
                "Solid modeling",
                "Robot sensing systems"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "Lie groups",
                "cameras",
                "encoding",
                "computational geometry",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mirror motion",
                "epipolar geometry",
                "motion estimation",
                "parabolic catadioptric cameras",
                "discrete motions",
                "nonlinearity",
                "projection model",
                "bilinear coding",
                "catadioptric fundamental matrix",
                "Lorentzian singular values",
                "9D manifold",
                "Lie groups",
                "matrix estimation"
            ]
        },
        "id": 101,
        "cited_by": []
    },
    {
        "title": "Autocalibration of a projector-screen-camera system: theory and algorithm for screen-to-camera homography estimation",
        "authors": [
            "Okatani",
            "Deguchi"
        ],
        "abstract": "This paper deals with the autocalibration of a system that consists of a planar screen, multiple projectors, and a camera. In the system, either multiple projectors or a single moving projector projects patterns on a screen while a stationary camera placed in front of the screen takes images of the patterns. We treat the case in which the patterns that the projectors project toward space are assumed to be known (i.e., the projectors are calibrated), whereas poses of the projectors are unknown. Under these conditions, we consider the problem of estimating screen-to-camera homography from the images alone. This is intended for cases where there is no clue on the screen surface that enables direct estimation of the screen-to-camera homography. One application is a 6DOF input device; poses of a multibeam projector freely moving in space are computed from the images of beam spots on the screen. The primary contribution of the paper is theoretical results on the uniqueness of solutions and a noniterative algorithm for the problem. The effectiveness of the method is shown by experimental results on synthetic as well as on real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238427",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 5,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Magnetic sensors",
                "Laser beams",
                "Sensor systems",
                "Computer vision",
                "Layout",
                "Laser theory",
                "Mice"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "optical projectors",
                "computer vision",
                "cameras",
                "estimation theory",
                "image recognition",
                "screens (display)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "autocalibration",
                "projector-screen-camera system",
                "screen to camera homography estimation",
                "planar screen",
                "multiple projectors",
                "moving projector",
                "stationary camera",
                "pattern images",
                "6DOF input device",
                "multibeam projector",
                "beam spots",
                "noniterative algorithm",
                "synthetic images",
                "real images"
            ]
        },
        "id": 102,
        "cited_by": []
    },
    {
        "title": "Camera calibration using spheres: a semi-definite programming approach",
        "authors": [
            "Agrawal",
            "Davis"
        ],
        "abstract": "Vision algorithms utilizing camera networks with a common field of view are becoming increasingly feasible and important. Calibration of such camera networks is a challenging and cumbersome task. The current approaches for calibration using planes or a known 3D target may not be feasible as these objects may not be simultaneously visible in all the cameras. In this paper, we present a new algorithm to calibrate cameras using occluding contours of spheres. In general, an occluding contour of a sphere projects to an ellipse in the image. Our algorithm uses the projection of the occluding contours of three spheres and solves for the intrinsic parameters and the locations of the spheres. The problem is formulated in the dual space and the parameters are solved for optimally and efficiently using semidefinite programming. The technique is flexible, accurate and easy to use. In addition, since the contour of a sphere is simultaneously visible in all the cameras, our approach can greatly simplify calibration of multiple cameras with a common field of view. Experimental results from computer simulated data and real world data, both for a single camera and multiple cameras, are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238428",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 35,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Computer vision",
                "Geometry",
                "Layout",
                "Costs",
                "Machine vision",
                "Computer science",
                "Educational institutions",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "calibration",
                "computer vision",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera calibration",
                "spheres",
                "semidefinite programming approach",
                "vision algorithms",
                "camera networks",
                "common view field",
                "3D target",
                "occluding contours",
                "ellipse",
                "intrinsic parameters",
                "sphere location"
            ]
        },
        "id": 103,
        "cited_by": [
            {
                "year": "2005",
                "id": 76
            }
        ]
    },
    {
        "title": "Gamut constrained illuminant estimation",
        "authors": [
            "Finlayson",
            "Hordley",
            "Tastl"
        ],
        "abstract": "This paper presents a novel solution to the illuminant estimation problem: the problem of how, given an image of a scene taken under an unknown illuminant, we can recover an estimate of that light. The work is founded on previous gamut mapping solutions to the problem which solve for a scene illuminant by determining the set of diagonal mappings which take image data captured under an unknown light to a gamut of reference colours taken under a known light. Unfortunately a diagonal model is not always a valid model of illumination change and so previous approaches sometimes return a null solution. In addition, previous methods are difficult to implement. We address these problems by recasting the problem as one of illuminant classification: we define a priori a set of plausible lights thus ensuring that a scene illuminant estimate will always be found. A plausible light is represented by the gamut of colours observable under it and the illuminant in an image is classified by determining the plausible light whose gamut is most consistent with the image data. We show that this step (the main computational burden of the algorithm) can be performed simply, quickly, and efficiently by means of a non-negative least-squares optimisation. We report results on a large set of real images which show that it provides excellent illuminant estimation, outperforming previous algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238429",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Color",
                "Lighting",
                "Neural networks",
                "Computer vision",
                "Cameras",
                "Laboratories",
                "Object recognition",
                "Image segmentation",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image colour analysis",
                "computer vision",
                "least squares approximations",
                "lighting",
                "brightness",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gamut constraint",
                "illuminant estimation",
                "scene image",
                "light estimate",
                "gamut mapping solutions",
                "scene illuminant",
                "diagonal mappings",
                "image data capture",
                "reference colours",
                "illumination change",
                "illuminant classification",
                "plausible lights",
                "nonnegative least-squares optimisation"
            ]
        },
        "id": 104,
        "cited_by": []
    },
    {
        "title": "Appearance sampling for obtaining a set of basis images for variable illumination",
        "authors": [
            "Sato",
            "Okabe",
            "Sato",
            "Ikeuchi"
        ],
        "abstract": "Previous studies have demonstrated that the appearance of an object under varying illumination conditions can be represented by a low-dimensional linear subspace. A set of basis images spanning such a linear subspace can be obtained by applying the principal component analysis (PCA) for a large number of images taken under different lighting conditions. While the approaches based on PCA have been used successfully for object recognition under varying illumination conditions, little is known about how many images would be required in order to obtain the basis images correctly. In this study, we present a novel method for analytically obtaining a set of basis images of an object for arbitrary illumination from input images of the object taken under a point light source. The main contribution of our work is that we show that a set of lighting directions can be determined for sampling images of an object depending on the spectrum of the object's BRDF in the angular frequency domain such that a set of harmonic images can be obtained analytically based on the sampling theorem on spherical harmonics. In addition, unlike the previously proposed techniques based on spherical harmonics, our method does not require the 3D shape and reflectance properties of an object used for rendering harmonics images of the object synthetically.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238430",
        "reference_list": [
            {
                "year": "2001",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 7,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Image sampling",
                "Lighting",
                "Principal component analysis",
                "Image analysis",
                "Object recognition",
                "Light sources",
                "Frequency domain analysis",
                "Harmonic analysis",
                "Shape",
                "Reflectivity"
            ],
            "INSPEC: Controlled Indexing": [
                "image sampling",
                "computer vision",
                "object recognition",
                "principal component analysis",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "appearance sampling",
                "basis images",
                "variable illumination",
                "object illumination",
                "low-dimensional linear subspace",
                "principal component analysis",
                "lighting conditions",
                "object recognition",
                "arbitrary illumination",
                "input images",
                "image sampling",
                "BRDF",
                "angular frequency domain",
                "harmonic images",
                "sampling theorem",
                "spherical harmonics"
            ]
        },
        "id": 105,
        "cited_by": [
            {
                "year": "2007",
                "id": 96
            }
        ]
    },
    {
        "title": "A theory of multiplexed illumination",
        "authors": [
            "Schechner",
            "Nayar",
            "Belhumeur"
        ],
        "abstract": "Imaging of objects under variable lighting directions is an important and frequent practice in computer vision and image-based rendering. We introduce an approach that significantly improves the quality of such images. Traditional methods for acquiring images under variable illumination directions use only a single light source per acquired image. In contrast, our approach is based on a multiplexing principle, in which multiple light sources illuminate the object simultaneously from different directions. Thus, the object irradiance is much higher. The acquired images are then computationally demultiplexed. The number of image acquisitions is the same as in the single-source method. The approach is useful for imaging dim object areas. We give the optimal code by which the illumination should be multiplexed to obtain the highest quality output. For n images corresponding to n light sources, the noise is reduced by /spl radic/(n)/2 relative to the signal. This noise reduction translates to a faster acquisition time or an increase in density of illumination direction samples. It also enables one to use lighting with high directional resolution using practical setups, as we demonstrate in our experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238431",
        "reference_list": [
            {
                "year": "2001",
                "id": 80
            },
            {
                "year": "2001",
                "id": 195
            }
        ],
        "citation": {
            "ieee": 44,
            "other": 21,
            "total": 65
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Light sources",
                "Computer vision",
                "Rendering (computer graphics)",
                "Noise reduction",
                "Object recognition",
                "Demultiplexing",
                "Stress",
                "Computer science",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "rendering (computer graphics)",
                "image resolution",
                "image denoising",
                "multiplexing",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiplexed illumination theory",
                "object imaging",
                "variable lighting directions",
                "computer vision",
                "image-based rendering",
                "image quality",
                "image acquisition",
                "light source",
                "multiplexing principle",
                "object irradiance",
                "computational demultiplexing",
                "dim object area imaging",
                "optimal code",
                "image noise reduction",
                "acquisition time"
            ]
        },
        "id": 106,
        "cited_by": [
            {
                "year": "2011",
                "id": 2
            },
            {
                "year": "2011",
                "id": 87
            },
            {
                "year": "2007",
                "id": 43
            },
            {
                "year": "2007",
                "id": 250
            }
        ]
    },
    {
        "title": "Incorporating the Torrance and Sparrow model of reflectance in uncalibrated photometric stereo",
        "authors": [
            "Georghiades"
        ],
        "abstract": "Under the Lambertian reflectance model, uncalibrated photometric stereo with unknown light sources is inherently ambiguous. In this paper, we consider the use of a more general reflectance model, namely the Torrance and Sparrow model, in uncalibrated photometric stereo. We demonstrate that this can not only resolve the ambiguity when the light sources are unknown, but can also result in more accurate surface reconstructions and can capture the reflectance properties of a large number of nonLambertian surfaces. Our method uses single light source images with unknown lighting and no knowledge about the parameters of the reflectance model. It can recover the 3D shape of surfaces (up to the binary convex/concave ambiguity) together with their reflectance properties. We have successfully tested our algorithm on a variety of nonLambertian surfaces demonstrating the effectiveness of our approach. In the case of human faces, the estimated skin reflectance has been shown to closely resemble the measured skin reflectance reported in the literature. We also demonstrate improved recognition results on 4050 images of 10 faces with variable lighting and viewpoint when the synthetic image-based representations of the faces are generated using the surface reconstructions and reflectance properties recovered while assuming the extended reflectance model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238432",
        "reference_list": [],
        "citation": {
            "ieee": 62,
            "other": 46,
            "total": 108
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflectivity",
                "Photometry",
                "Surface reconstruction",
                "Light sources",
                "Image reconstruction",
                "Skin",
                "Shape",
                "Testing",
                "Humans",
                "Face"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "stereo image processing",
                "computer vision",
                "image reconstruction",
                "face recognition",
                "photometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Torrance and Sparrow reflectance model",
                "uncalibrated photometric stereo",
                "Lambertian reflectance model",
                "unknown light sources",
                "general reflectance model",
                "surface reconstructions",
                "reflectance properties",
                "nonLambertian surfaces",
                "single light source images",
                "3D shape",
                "human faces",
                "skin reflectance",
                "image-based representations"
            ]
        },
        "id": 107,
        "cited_by": [
            {
                "year": "2009",
                "id": 217
            },
            {
                "year": "2007",
                "id": 61
            },
            {
                "year": "2005",
                "id": 29
            },
            {
                "year": "2005",
                "id": 241
            }
        ]
    },
    {
        "title": "\"Perspective shape from shading\" and viscosity solutions",
        "authors": [
            "Prados",
            "Faugeras"
        ],
        "abstract": "This article proposes a solution of the Lambertian shape from shading (SFS) problem in the case of a pinhole camera model (performing a perspective projection). Our approach is based upon the notion of viscosity solutions of Hamilton-Jacobi equations. This approach allows us to naturally deal with nonsmooth solutions and provides a mathematical framework for proving correctness of our algorithms. Our work extends previous work in the area in three aspects. First, it models the camera as a pinhole whereas most authors assume an orthographic projection, thereby extending the applicability of shape from shading methods to more realistic images. In particular it extends the work of E. Prados et al. (2002) and E. Rouy et al. (1992). Second, by adapting the brightness equation to the perspective problem, we obtain a new partial differential equation (PDE). Results about the existence and uniqueness of its solution are also obtained. Third, it allows us to come up with a new approximation scheme and a new algorithm for computing numerical approximations of the \"continuous\" solution as well as a proof of their convergence toward that solution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238433",
        "reference_list": [],
        "citation": {
            "ieee": 29,
            "other": 39,
            "total": 68
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Viscosity",
                "Cameras",
                "Brightness",
                "Convergence of numerical methods",
                "Partial differential equations",
                "Reflectivity",
                "Layout",
                "Differential equations",
                "Approximation algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Jacobian matrices",
                "realistic images",
                "approximation theory",
                "partial differential equations",
                "convergence",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "perspective shape",
                "shading",
                "viscosity solutions",
                "Lambertian shape solution",
                "SFS problem",
                "pinhole camera model",
                "perspective projection",
                "Hamilton-Jacobi equations",
                "nonsmooth solutions",
                "brightness equation",
                "partial differential equation",
                "numerical approximation",
                "convergence",
                "continuous solution",
                "computer vision"
            ]
        },
        "id": 108,
        "cited_by": [
            {
                "year": "2017",
                "id": 409
            },
            {
                "year": "2009",
                "id": 225
            },
            {
                "year": "2005",
                "id": 78
            }
        ]
    },
    {
        "title": "The Beltrami flow over implicit manifolds",
        "authors": [
            "Sochen",
            "Deriche",
            "Perez"
        ],
        "abstract": "In many medical computer vision tasks the relevant data is attached to a specific tissue such as the colon or the cortex. This situation calls for regularization techniques which are defined over surfaces. We introduce in this paper the Beltrami flow over implicit manifolds. This new regularization technique overcomes the over-smoothing of the L/sub 2/ flow and the staircasing effects of the L/sub 1/ flow, that were recently suggested via the harmonic map methods. The key of our approach is first to clarify the link between the intrinsic Polyakov action and the implicit harmonic energy functional and then use the geometrical understanding of the Beltrami flow to generalize it to images on implicitly defined non flat surfaces. It is shown that once again the Beltrami flow interpolates between the L/sub 2/ and L/sub 1/ flows on non flat surfaces. The implementation scheme of this flow is presented and various experimental results obtained on a set of various real images illustrate the performances of the approach as well as the differences with the harmonic map flows. This extension of the Beltrami flow to the case of non flat surfaces opens new perspectives in the regularization of noisy data defined on manifolds.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238434",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 16,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "medical image processing",
                "computer vision",
                "image denoising",
                "harmonic analysis",
                "differential geometry",
                "interpolation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Beltrami flow",
                "implicit manifolds",
                "medical computer vision tasks",
                "tissue",
                "colon",
                "cortex",
                "regularization techniques",
                "oversmoothing",
                "L/sub 2/ flow",
                "staircasing effects",
                "L/sub 1/ flow",
                "harmonic map methods",
                "intrinsic Polyakov action",
                "implicit harmonic energy functional",
                "geometrical understanding",
                "nonflat surfaces",
                "implementation scheme",
                "noisy data"
            ]
        },
        "id": 109,
        "cited_by": []
    },
    {
        "title": "Image statistics and anisotropic diffusion",
        "authors": [
            "Scharr",
            "Black",
            "Haussecker"
        ],
        "abstract": "Many sensing techniques and image processing applications are characterized by noisy, or corrupted, image data. Anisotropic diffusion is a popular, and theoretically well understood, technique for denoising such images. Diffusion approaches however require the selection of an \"edge stopping\" function, the definition of which is typically ad hoc. We exploit and extend recent work on the statistics of natural images to define principled edge stopping functions for different types of imagery. We consider a variety of anisotropic diffusion schemes and note that they compute spatial derivatives at fixed scales from which we estimate the appropriate algorithm-specific image statistics. Going beyond traditional work on image statistics, we also model the statistics of the eigenvalues of the local structure tensor. Novel edge-stopping functions are derived from these image statistics giving a principled way of formulating anisotropic diffusion problems in which all edge-stopping parameters are learned from training data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238435",
        "reference_list": [
            {
                "year": "2001",
                "id": 201
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 20,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Anisotropic magnetoresistance",
                "Training data",
                "Filters",
                "Acoustic noise",
                "Noise reduction",
                "Statistical distributions",
                "Inference algorithms",
                "Layout",
                "Probability"
            ],
            "INSPEC: Controlled Indexing": [
                "image denoising",
                "image reconstruction",
                "computer vision",
                "edge detection",
                "eigenvalues and eigenfunctions",
                "statistics",
                "diffusion",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image statistics",
                "anisotropic diffusion",
                "sensing techniques",
                "image processing applications",
                "noisy image data",
                "corrupted image data",
                "image denoising",
                "diffusion approaches",
                "edge stopping function",
                "natural images",
                "eigenvalues",
                "local structure tensor",
                "image reconstruction",
                "noise statistics",
                "spatial statistics"
            ]
        },
        "id": 110,
        "cited_by": [
            {
                "year": "2007",
                "id": 140
            }
        ]
    },
    {
        "title": "Combining gradient and albedo data for rotation invariant classification of 3D surface texture",
        "authors": [
            "Jiahua Wu",
            "Chantler"
        ],
        "abstract": "We present a new texture classification scheme which is invariant to surface-rotation. Many texture classification approaches have been presented in the past that are image-rotation invariant. However, image rotation is not necessarily the same as surface rotation. We have therefore developed a classifier that uses invariants that are derived from surface properties rather than image properties. Previously we developed a scheme that used surface gradient (normal) fields estimated using photometric stereo. In this paper we augment these data with albedo information and also employ an additional feature set: the radial spectrum. We used 30 real textures to test the new classifier. A classification accuracy of 91% was achieved when albedo and gradient 1D polar and radial features were combined. The best performance was also achieved by using 2D albedo and gradient spectra. The classification accuracy is 99%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238437",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface texture",
                "Photometry",
                "Testing",
                "Stereo vision",
                "Lighting",
                "Image databases",
                "Histograms",
                "Frequency domain analysis",
                "Computer vision",
                "Reflectivity"
            ],
            "INSPEC: Controlled Indexing": [
                "albedo",
                "rotation",
                "lighting",
                "image classification",
                "image texture",
                "feature extraction",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gradient data",
                "albedo data",
                "rotation invariant classification",
                "3D surface texture",
                "texture classification",
                "surface rotation",
                "image rotation invariant",
                "image properties",
                "surface gradient",
                "photometric stereo",
                "albedo information",
                "feature set",
                "radial spectrum",
                "classifier testing",
                "2D albedo spectra",
                "2D gradient spectra"
            ]
        },
        "id": 111,
        "cited_by": []
    },
    {
        "title": "Reflectance-based classification of color edges",
        "authors": [
            "Gevers"
        ],
        "abstract": "We aim at using color information to classify the physical nature of edges in video. To achieve physics-based edge classification, we first propose a novel approach to color edge detection by automatic noise-adaptive thresholding derived from sensor noise analysis. Then, we present a taxonomy on color edge types. As a result, a parameter-free edge classifier is obtained by labeling color transitions into one of the following types: (1) shadow-geometry, (2) highlight edges, (3) material edges. The proposed method is empirically verified on images showing complex real world scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238438",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Colored noise",
                "Reflection",
                "Taxonomy",
                "Lighting",
                "Labeling",
                "Layout",
                "Image segmentation",
                "Reflectivity",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "reflectivity",
                "lighting",
                "image colour analysis",
                "image classification",
                "image denoising",
                "edge detection",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "reflectance-based image classification",
                "color edges",
                "color information",
                "video edges",
                "color edge detection",
                "automatic noise-adaptive thresholding",
                "sensor noise analysis",
                "taxonomy",
                "parameter-free edge classifier",
                "color transitions",
                "shadow geometry",
                "edge highlighting",
                "material edges",
                "real world scenes",
                "illuminant color"
            ]
        },
        "id": 112,
        "cited_by": []
    },
    {
        "title": "A new perspective [on] shape-from-shading",
        "authors": [
            "Tankus",
            "Sochen",
            "Yeshurun"
        ],
        "abstract": "Shape-from-shading (SFS) is a fundamental problem in computer vision. The vast majority of research in this field have assumed orthography as its projection model. This paper reexamines the basis of SFS, the image irradiance equation, under an assumption of perspective projection. The paper also shows that the perspective image irradiance equation depends merely on the natural logarithm of the depth function (and not on the depth function itself), and as such it is invariant to scale changes of the depth function. We then suggest a simple reconstruction algorithm based on the perspective formula, and compare it to existing orthographic SFS algorithms. This simple algorithm obtained lower error rates than legacy SFS algorithms, and equated with and sometimes surpassed state-of-the-art algorithms. These findings lend support to the assumption that transition to a more realistic set of assumptions improves reconstruction significantly.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238439",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 0,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Equations",
                "Image reconstruction",
                "Surface reconstruction",
                "Computer vision",
                "Computer science",
                "Reflectivity",
                "Mathematics",
                "Reconstruction algorithms",
                "Error analysis",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "edge detection",
                "brightness"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape-from-shading",
                "computer vision",
                "orthography",
                "projection model",
                "image irradiance equation",
                "perspective projection",
                "natural logarithm",
                "depth function",
                "image reconstruction",
                "legacy algorithms"
            ]
        },
        "id": 113,
        "cited_by": []
    },
    {
        "title": "Separating reflection components of textured surfaces using a single image",
        "authors": [
            "Tan",
            "Ikeuchi"
        ],
        "abstract": "The presence of highlights, which in dielectric inhomogeneous objects are linear combination of specular and diffuse reflection components, is inevitable. A number of methods have been developed to separate these reflection components. To our knowledge, all methods that use a single input image require explicit color segmentation to deal with multicolored surfaces. Unfortunately, for complex textured images, current color segmentation algorithms are still problematic to segment correctly. Consequently, a method without explicit color segmentation becomes indispensable, and this paper presents such a method. The method is based solely on colors, particularly chromaticity, without requiring any geometrical parameter information. One of the basic ideas is to compare the intensity logarithmic differentiation of specular-free images and input images iteratively. The specular-free image is a pseudo-code of diffuse components that can be generated by shifting a pixel's intensity and chromaticity nonlinearly while retaining its hue. All processes in the method are done locally, involving a maximum of only two pixels. The experimental results on natural images show that the proposed method is accurate and robust under known scene illumination chromaticity. Unlike the existing methods that use a single image, our method is effective for textured objects with complex multicolored scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238440",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 13,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface texture",
                "Optical reflection",
                "Image segmentation",
                "Lighting",
                "Filters",
                "Polarization",
                "Colored noise",
                "Equations",
                "Dielectrics",
                "Color"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image texture",
                "image colour analysis",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "reflection component separation",
                "textured surfaces",
                "single image",
                "dielectric inhomogeneous objects",
                "linear combination",
                "specular reflection",
                "diffuse reflection",
                "color segmentation",
                "multicolored surfaces",
                "chromaticity",
                "intensity logarithmic differentiation",
                "specular-free images",
                "pseudocode",
                "diffuse components",
                "pixel intensity",
                "hue retention",
                "natural images",
                "scene illumination",
                "textured objects"
            ]
        },
        "id": 114,
        "cited_by": [
            {
                "year": "2005",
                "id": 184
            },
            {
                "year": "2005",
                "id": 220
            }
        ]
    },
    {
        "title": "Robust regression with projection based M-estimators",
        "authors": [
            "Haifeng Chen",
            "Meer"
        ],
        "abstract": "The robust regression techniques in the RANSAC family are popular today in computer vision, but their performance depends on a user supplied threshold. We eliminate this drawback of RANSAC by reformulating another robust method, the M-estimator, as a projection pursuit optimization problem. The projection based pbM-estimator automatically derives the threshold from univariate kernel density estimates. Nevertheless, the performance of the pbM-estimator equals or exceeds that of RANSAC techniques tuned to the optimal threshold, a value which is never available in practice. Experiments were performed both with synthetic and real data in the affine motion and fundamental matrix estimation tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238441",
        "reference_list": [],
        "citation": {
            "ieee": 24,
            "other": 7,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Sampling methods",
                "Computer vision",
                "Motion estimation",
                "Noise robustness",
                "Parameter estimation",
                "Optimization methods",
                "Kernel",
                "Noise measurement",
                "Measurement standards",
                "Upper bound"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "regression analysis",
                "image motion analysis",
                "optimisation",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust regression",
                "projection based M-estimators",
                "RANSAC family",
                "computer vision",
                "pbM-estimator",
                "univariate kernel density estimates",
                "affine motion",
                "matrix estimation"
            ]
        },
        "id": 115,
        "cited_by": [
            {
                "year": "2009",
                "id": 52
            }
        ]
    },
    {
        "title": "Variational space-time motion segmentation",
        "authors": [
            "Cremers",
            "Soatto"
        ],
        "abstract": "We propose a variational method for segmenting image sequences into spatiotemporal domains of homogeneous motion. To this end, we formulate the problem of motion estimation in the framework of Bayesian inference, using a prior which favors domain boundaries of minimal surface area. We derive a cost functional which depends on a surface in space-time separating a set of motion regions, as well as a set of vectors modeling the motion in each region. We propose a multiphase level set formulation of this functional, in which the surface and the motion regions are represented implicitly by a vector-valued level set function. Joint minimization of the proposed functional results in an eigenvalue problem for the motion model of each region and in a gradient descent evolution for the separating interface. Numerical results on real-world sequences demonstrate that minimization of a single cost functional generates a segmentation of space-time into multiple motion regions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238442",
        "reference_list": [
            {
                "year": "2001",
                "id": 22
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 19,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Computer vision",
                "Cost function",
                "Level set",
                "Image segmentation",
                "Image sequences",
                "Spatiotemporal phenomena",
                "Motion estimation",
                "Bayesian methods",
                "Eigenvalues and eigenfunctions"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "image sequences",
                "image segmentation",
                "variational techniques",
                "inference mechanisms",
                "computer vision",
                "minimisation",
                "Bayes methods",
                "eigenvalues and eigenfunctions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational space-time motion segmentation",
                "image sequence",
                "spatiotemporal domains",
                "homogeneous motion",
                "motion estimation",
                "Bayesian inference",
                "domain boundaries",
                "minimal surface area",
                "cost functional",
                "vectors",
                "multiphase level set formulation",
                "vector-valued level set function",
                "joint minimization",
                "eigenvalue problem",
                "numerical results",
                "real-world sequences"
            ]
        },
        "id": 116,
        "cited_by": [
            {
                "year": "2007",
                "id": 341
            },
            {
                "year": "2005",
                "id": 4
            }
        ]
    },
    {
        "title": "How to deal with point correspondences and tangential velocities in the level set framework",
        "authors": [
            "Pons",
            "Hermosillo",
            "Keriven",
            "Faugeras"
        ],
        "abstract": "In this paper, we overcome a major drawback of the level set framework: the lack of point correspondences. We maintain explicit backward correspondences from the evolving interface to the initial one by advecting the initial point coordinates with the same speed as the level set function. Our method leads to a system of coupled Eulerian partial differential equations. We show in a variety of numerical experiments that it can handle both normal and tangential velocities, large deformations, shocks, rarefactions and topological changes. Applications are many in computer vision and elsewhere since our method can upgrade virtually any level set evolution. We complement our work with the design of non zero tangential velocities that preserve the relative area of interface patches; this feature may be crucial in such applications as computational geometry, grid generation or unfolding of the organs' surfaces, e.g. brain, in medical imaging.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238443",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 4,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Level set",
                "Gold",
                "Application software",
                "Computer vision",
                "Partial differential equations",
                "Electric shock",
                "Computational geometry",
                "Mesh generation",
                "Biomedical imaging",
                "Lagrangian functions"
            ],
            "INSPEC: Controlled Indexing": [
                "medical image processing",
                "partial differential equations",
                "computer vision",
                "computational geometry",
                "brain"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "point correspondences",
                "level set framework",
                "backward correspondence",
                "level set function",
                "coupled Eulerian partial differential equations",
                "normal velocity",
                "tangential velocity",
                "deformation",
                "shocks",
                "rarefactions",
                "topological changes",
                "computer vision",
                "nonzero tangential velocities",
                "interface patches",
                "computational geometry",
                "grid generation",
                "organ surface unfolding",
                "brain",
                "medical imaging"
            ]
        },
        "id": 117,
        "cited_by": [
            {
                "year": "2005",
                "id": 183
            }
        ]
    },
    {
        "title": "Comparison of graph cuts with belief propagation for stereo, using identical MRF parameters",
        "authors": [
            "Tappen",
            "Freeman"
        ],
        "abstract": "Recent stereo algorithms have achieved impressive results by modelling the disparity image as a Markov Random Field (MRF). An important component of an MRF-based approach is the inference algorithm used to find the most likely setting of each node in the MRF. Algorithms have been proposed which use graph cuts or belief propagation for inference. These stereo algorithms differ in both the inference algorithm used and the formulation of the MRF. It is unknown whether to attribute the responsibility for differences in performance to the MRF or the inference algorithm. We address this through controlled experiments by comparing the belief propagation algorithm and the graph cuts algorithm on the same MRF's, which have been created for calculating stereo disparities. We find that the labellings produced by the two algorithms are comparable. The solutions produced by graph cuts have a lower energy than those produced with belief propagation, but this does not necessarily lead to increased performance relative to the ground truth.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238444",
        "reference_list": [],
        "citation": {
            "ieee": 152,
            "other": 107,
            "total": 259
        },
        "keywords": {
            "IEEE Keywords": [
                "Belief propagation",
                "Inference algorithms",
                "Stereo vision",
                "Markov random fields",
                "Computer vision",
                "Pixel",
                "Labeling",
                "Testing",
                "Computational efficiency",
                "Costs"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing",
                "graph theory",
                "Markov processes",
                "random processes",
                "belief networks",
                "inference mechanisms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph cuts",
                "belief propagation",
                "identical MRF parameters",
                "disparity image modelling",
                "Markov Random Field",
                "inference algorithm",
                "controlled experiments",
                "stereo disparities",
                "computational vision"
            ]
        },
        "id": 118,
        "cited_by": [
            {
                "year": "2013",
                "id": 359
            },
            {
                "year": "2011",
                "id": 174
            },
            {
                "year": "2005",
                "id": 55
            },
            {
                "year": "2005",
                "id": 136
            },
            {
                "year": "2005",
                "id": 161
            }
        ]
    },
    {
        "title": "Controlling model complexity in flow estimation",
        "authors": [
            "Duric",
            "Li",
            "Wechsler",
            "Cherkassky"
        ],
        "abstract": "This paper describes a novel application of statistical learning theory (SLT) to control model complexity in flow estimation. SLT provides analytical generalization bounds suitable for practical model selection from small and noisy data sets of image measurements (normal flow). The method addresses the aperture problem by using the penalized risk (ridge regression). We demonstrate an application of this method on both synthetic and real image sequences and use it for motion interpolation and extrapolation. Our experimental results show that our approach compares favorably against alternative model selection methods such as the Akaike's final prediction error, Schwartz's criterion, generalized cross-validation, and Shibata's model selector.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238445",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Motion estimation",
                "Image sequences",
                "Interpolation",
                "Extrapolation",
                "Mathematical model",
                "Computer errors",
                "Motion measurement",
                "Statistical learning",
                "Fluid flow measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "interpolation",
                "extrapolation",
                "image segmentation",
                "flow measurement",
                "learning (artificial intelligence)",
                "motion estimation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model complexity control",
                "flow estimation",
                "statistical learning theory",
                "analytical generalization bounds",
                "model selection",
                "small data sets",
                "noisy data sets",
                "image measurements",
                "normal flow",
                "aperture problem",
                "penalized risk",
                "ridge regression",
                "image sequences",
                "motion interpolation",
                "motion extrapolation",
                "final prediction error",
                "generalized cross-validation",
                "model selector"
            ]
        },
        "id": 119,
        "cited_by": []
    },
    {
        "title": "Model-based multiple view reconstruction of people",
        "authors": [
            "Starck",
            "Hilton"
        ],
        "abstract": "This paper presents a framework to reconstruct a scene captured in multiple camera views based on a prior model of the scene geometry. The framework is applied to the capture of animated models of people. A multiple camera studio is used to simultaneously capture a moving person from multiple viewpoints. A humanoid computer graphics model is animated to match the pose at each time frame. Constrained optimisation is then used to recover the multiple view correspondence from silhouette, stereo and feature cues, updating the geometry and appearance of the model. The key contribution of this paper is a model-based computer vision framework for the reconstruction of shape and appearance from multiple views. This is compared to current model-free approaches for multiple view scene capture. The technique demonstrates improved scene reconstruction in the presence of visual ambiguities and provides the means to capture a dynamic scene with a consistent model that is instrumented with an animation structure to edit the scene dynamics or to synthesise new content.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238446",
        "reference_list": [
            {
                "year": "2001",
                "id": 53
            }
        ],
        "citation": {
            "ieee": 37,
            "other": 36,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Animation",
                "Cameras",
                "Solid modeling",
                "Geometry",
                "Computer graphics",
                "Constraint optimization",
                "Stereo vision",
                "Computer vision",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image motion analysis",
                "computer vision",
                "computer animation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based multiple view reconstruction",
                "scene reconstruction",
                "camera views",
                "scene geometry",
                "animated models",
                "multiple camera studio",
                "moving person",
                "humanoid computer graphics model",
                "pose matching",
                "constrained optimisation",
                "silhouette",
                "stereo",
                "feature cues",
                "computer vision",
                "shape reconstruction",
                "model-free approaches",
                "scene capture",
                "scene dynamics",
                "people"
            ]
        },
        "id": 120,
        "cited_by": [
            {
                "year": "2015",
                "id": 256
            },
            {
                "year": "2007",
                "id": 268
            }
        ]
    },
    {
        "title": "Landmark-based shape deformation with topology-preserving constraints",
        "authors": [
            "Song Wang",
            "Jim Xiuquan Ji",
            "Zhi-Pei Liang"
        ],
        "abstract": "This paper presents a novel approach for landmark-based shape deformation, in which fitting error and shape difference are formulated into a support vector machine (SVM) regression problem. To well describe nonrigid shape deformation, this paper measures the shape difference using a thin-plate spline model. The proposed approach is capable of preserving the topology of the template shape in the deformation. This property is achieved by inserting a set of additional points and imposing a set of linear equality and/or inequality constraints. The underlying optimization problem is solved using a quadratic programming algorithm. The proposed method has been tested using practical data in the context of shape-based image segmentation. Some relevant practical issues, such as missing detected landmarks and selection of the regularization parameter are also briefly discussed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238447",
        "reference_list": [
            {
                "year": "2001",
                "id": 131
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Topology",
                "Image segmentation",
                "Shape measurement",
                "Deformable models",
                "Biomedical imaging",
                "Computer vision",
                "Application software",
                "Active contours",
                "Support vector machines",
                "Active shape model"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "support vector machines",
                "regression analysis",
                "quadratic programming",
                "feature extraction",
                "splines (mathematics)",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "landmark-based shape deformation",
                "topology-preserving constraints",
                "fitting error",
                "shape difference",
                "support vector machine",
                "SVM regression problem",
                "nonrigid shape deformation",
                "thin-plate spline model",
                "template shape",
                "linear equality",
                "inequality constraints",
                "optimization problem",
                "quadratic programming algorithm",
                "shape-based image segmentation",
                "regularization parameter"
            ]
        },
        "id": 121,
        "cited_by": [
            {
                "year": "2009",
                "id": 238
            }
        ]
    },
    {
        "title": "Reliable recovery of piled box-like objects via parabolically deformable superquadrics",
        "authors": [
            "Katsoulas"
        ],
        "abstract": "Automatic unloading of piled box-like objects is undoubtedly of great importance to the industry. In this contribution a system addressing this problem is described: we employ a laser range finder for data acquisition, and globally deformable superquadrics for object modeling. Our technique is based on a hypothesis generation and refinement scheme. The vertices of the piled objects are extracted and superquadric seeds are aligned at these vertices. The model parameter recovery task is decomposed into two subproblems, each dealing with a subset of the model's parameter set. Both region and boundary based information sources are used for parameter estimation. Compared to a widespread strategy for superquadric recovery, our method shows advantages in terms of robustness and computational efficiency. In addition, our system exhibits versatility with regard to existing industrial systems, since it can effectively deal with both neatly placed and jumbled configurations of objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238448",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 4,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Parameter estimation",
                "Robustness",
                "Image segmentation",
                "Laser modes",
                "Deformable models",
                "Computational efficiency",
                "Service robots",
                "Parametric statistics",
                "Computer vision",
                "Pattern recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "data acquisition",
                "robot vision",
                "laser ranging",
                "edge detection",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "reliable recovery",
                "piled box-like objects",
                "parabolical deformation",
                "deformable superquadrics",
                "automatic unloading",
                "laser range finder",
                "data acquisition",
                "object modeling",
                "hypothesis generation",
                "refinement scheme",
                "superquadric seeds",
                "model parameter recovery",
                "information sources",
                "parameter estimation",
                "industrial systems",
                "depalletizing problem",
                "robotic bin picking problem",
                "automatic recovery",
                "distribution centers"
            ]
        },
        "id": 122,
        "cited_by": []
    },
    {
        "title": "View-invariant alignment and matching of video sequences",
        "authors": [
            "Rao",
            "Gritai",
            "Shah",
            "Syeda-Mahmood"
        ],
        "abstract": "In this paper, we propose a novel method to establish temporal correspondence between the frames of two videos. 3D epipolar geometry is used to eliminate the distortion generated by the projection from 3D to 2D. Although the fundamental matrix contains the extrinsic property of the projective geometry between views, it is sensitive to noise. Therefore, we propose the use of a rank constraint of corresponding points in two views to measure the similarity between trajectories. This rank constraint shows more robustness and avoids computation of the fundamental matrix. A dynamic programming approach using the similarity measurement is proposed to find the nonlinear time-warping function for videos containing human activities. In this way, videos of different individuals taken at different times and from distinct viewpoints can be synchronized. A temporal pyramid of trajectories is applied to improve the accuracy of the view-invariant dynamic time-warping approach. We show various applications of this approach such as video synthesis, human action recognition, and computer aider training. Compared to state-of-the-art techniques, our method shows a great improvement.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238449",
        "reference_list": [
            {
                "year": "2001",
                "id": 113
            }
        ],
        "citation": {
            "ieee": 59,
            "other": 36,
            "total": 95
        },
        "keywords": {
            "IEEE Keywords": [
                "Video sequences",
                "Humans",
                "Geometry",
                "Cameras",
                "Nonlinear distortion",
                "Distortion measurement",
                "Noise robustness",
                "Dynamic programming",
                "Application software",
                "Image retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image motion analysis",
                "image matching",
                "video signal processing",
                "dynamic programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "view-invariant alignment",
                "video sequence matching",
                "temporal correspondence",
                "video frames",
                "3D epipolar geometry",
                "distortion elimination",
                "noise sensitivity",
                "rank constraint",
                "trajectory similarity",
                "fundamental matrix",
                "dynamic programming",
                "nonlinear time-warping function",
                "human activities",
                "temporal pyramid",
                "video synthesis",
                "human action recognition",
                "computer aider training",
                "spatiotemporal alignment"
            ]
        },
        "id": 123,
        "cited_by": [
            {
                "year": "2011",
                "id": 72
            },
            {
                "year": "2005",
                "id": 18
            },
            {
                "year": "2005",
                "id": 19
            },
            {
                "year": "2005",
                "id": 105
            }
        ]
    },
    {
        "title": "High resolution terrain mapping using low attitude aerial stereo imagery",
        "authors": [
            "Il-Kyun Jung",
            "Lacroix"
        ],
        "abstract": "This paper presents an approach to build high resolution digital elevation maps from a sequence of unregistered low altitude stereovision image pairs. The approach first uses a visual motion estimation algorithm that determines the 3D motions of the cameras between consecutive acquisitions, on the basis of visually detected and matched environment features. An extended Kalman filter then estimates both the 6 position parameters and the 3D positions of the memorized features as images are acquired. Details are given on the filter implementation and on the estimation of the uncertainties on the feature observations and motion estimations. Experimental results show that the precision of the method enables to build spatially consistent very large maps.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238450",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 12,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Terrain mapping",
                "Simultaneous localization and mapping",
                "Motion estimation",
                "Robot sensing systems",
                "Robot kinematics",
                "Cameras",
                "Motion detection",
                "Filters",
                "Uncertainty"
            ],
            "INSPEC: Controlled Indexing": [
                "terrain mapping",
                "stereo image processing",
                "image resolution",
                "image sequences",
                "image matching",
                "feature extraction",
                "motion estimation",
                "Kalman filters",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high resolution terrain mapping",
                "low attitude aerial stereo imagery",
                "digital elevation maps",
                "stereovision image pairs",
                "image sequence",
                "visual motion estimation algorithm",
                "3D motions",
                "cameras",
                "extended Kalman filter",
                "position parameters",
                "3D positions",
                "filter implementation",
                "uncertainty estimation",
                "feature observations",
                "experimental results",
                "large maps",
                "sensor position",
                "sensor orientation",
                "SLAM problem",
                "landmarks",
                "error estimates"
            ]
        },
        "id": 124,
        "cited_by": []
    },
    {
        "title": "Tracking across multiple cameras with disjoint views",
        "authors": [
            "Javed",
            "Rasheed",
            "Shafique",
            "Shah"
        ],
        "abstract": "Conventional tracking approaches assume proximity in space, time and appearance of objects in successive observations. However, observations of objects are often widely separated in time and space when viewed from multiple non-overlapping cameras. To address this problem, we present a novel approach for establishing object correspondence across non-overlapping cameras. Our multicamera tracking algorithm exploits the redundance in paths that people and cars tend to follow, e.g. roads, walk-ways or corridors, by using motion trends and appearance of objects, to establish correspondence. Our system does not require any inter-camera calibration, instead the system learns the camera topology and path probabilities of objects using Parzen windows, during a training phase. Once the training is complete, correspondences are assigned using the maximum a posteriori (MAP) estimation framework. The learned parameters are updated with changing trajectory patterns. Experiments with real world videos are reported, which validate the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238451",
        "reference_list": [
            {
                "year": "2001",
                "id": 45
            },
            {
                "year": "2003",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 130,
            "other": 94,
            "total": 224
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Computer vision",
                "Calibration",
                "Surveillance",
                "Tracking",
                "Roads",
                "Topology",
                "Videos",
                "Kernel",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "object detection",
                "maximum likelihood estimation",
                "topology",
                "cameras",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple cameras",
                "disjoint views",
                "space proximity",
                "time proximity",
                "appearance proximity",
                "nonoverlapping cameras",
                "object correspondence",
                "multicamera tracking algorithm",
                "path redundance",
                "motion trends",
                "camera topology learning",
                "path probabilities",
                "Parzen windows",
                "maximum a posteriori estimation",
                "trajectory patterns",
                "videos"
            ]
        },
        "id": 125,
        "cited_by": [
            {
                "year": "2013",
                "id": 445
            },
            {
                "year": "2007",
                "id": 98
            },
            {
                "year": "2005",
                "id": 240
            }
        ]
    },
    {
        "title": "Facial expression decomposition",
        "authors": [
            "Hongcheng Wang",
            "Ahuja"
        ],
        "abstract": "In this paper, we propose a novel approach for facial expression decomposition - higher-order singular value decomposition (HOSVD), a natural generalization of matrix SVD. We learn the expression subspace and person subspace from a corpus of images showing seven basic facial expressions, rather than resort to expert-coded facial expression parameters. We propose a simultaneous face and facial expression recognition algorithm, which can classify the given image into one of the seven basic facial expression categories, and then other facial expressions of the new person can be synthesized using the learned expression subspace model. The contributions of this work lie mainly in two aspects. First, we propose a new HOSVD based approach to model the mapping between persons and expressions, used for facial expression synthesis for a new person. Second, we realize simultaneous face and facial expression recognition as a result of facial expression decomposition. Experimental results are presented that illustrate the capability of the person subspace and expression subspace in both synthesis and recognition tasks. As a quantitative measure of the quality of synthesis, we propose using gradient minimum square error (GMSE) which measures the gradient difference between the original and synthesized images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238452",
        "reference_list": [],
        "citation": {
            "ieee": 56,
            "other": 15,
            "total": 71
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Singular value decomposition",
                "Image recognition",
                "Humans",
                "Computer vision",
                "Facial features",
                "Matrix decomposition",
                "Cognition",
                "Psychology",
                "Face detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "emotion recognition",
                "singular value decomposition",
                "gradient methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial expression decomposition",
                "higher-order singular value decomposition",
                "natural generalization",
                "matrix SVD",
                "expression subspace",
                "image corpus",
                "facial expression recognition algorithm",
                "face recognition algorithm",
                "image classification",
                "facial expression synthesis",
                "gradient minimum square error",
                "image gradient difference"
            ]
        },
        "id": 126,
        "cited_by": [
            {
                "year": "2007",
                "id": 377
            }
        ]
    },
    {
        "title": "Plane-based calibration algorithm for multi-camera systems via factorization of homography matrices",
        "authors": [
            "Ueshiba",
            "Tomita"
        ],
        "abstract": "A new calibration algorithm for multicamera systems using a planar reference pattern is proposed. The algorithm is an extension of Sturm-Maybank-Zhang style plane-based calibration technique for use with multiple cameras. Rigid displacements between the cameras are recovered as well as the intrinsic parameters only by capturing with the cameras a model plane with known reference points placed at three or more locations. Thus the algorithm yields a simple calibration means for stereo vision systems with an arbitrary number of cameras while maintaining the handiness and flexibility of the original method. The algorithm is based on factorization of homography matrices between the model and image planes into the camera and plane parameters. To compensate for the indetermination of scaling factors, each homography matrix is rescaled by a double eigenvalue of a planar homology defined by two views and two model planes. The obtained parameters are finally refined by a nonlinear maximum likelihood estimation (MLE) process. The validity of the proposed technique was verified through simulation and experiments with real data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238453",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 17,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Transmission line matrix methods",
                "Maximum likelihood estimation",
                "Stereo vision",
                "Eigenvalues and eigenfunctions",
                "Layout",
                "Image reconstruction",
                "Geometry",
                "Degradation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing",
                "maximum likelihood estimation",
                "image reconstruction",
                "matrix decomposition",
                "eigenvalues and eigenfunctions",
                "calibration",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "plane-based calibration algorithm",
                "multicamera systems",
                "factorization",
                "homography matrices",
                "planar reference pattern",
                "Sturm-Maybank-Zhang style",
                "rigid displacements",
                "reference points",
                "stereo vision systems",
                "scaling factors",
                "double eigenvalue",
                "planar homology",
                "nonlinear maximum likelihood estimation"
            ]
        },
        "id": 127,
        "cited_by": []
    },
    {
        "title": "Tales of shape and radiance in multiview stereo",
        "authors": [
            "Soatto",
            "Yezzi",
            "Hailin Jin"
        ],
        "abstract": "To what extent can three-dimensional shape and radiance be inferred from a collection of images? Can the two be estimated separately while retaining optimality? How should the optimality criterion be computed? When is it necessary to employ an explicit model of the reflectance properties of a scene? In this paper we introduce a separation principle for shape and radiance estimation that applies to Lambertian scenes and holds for any choice of norm. When the scene is not Lambertian, however, shape cannot be decoupled from radiance, and therefore matching image-to-image is not possible directly. We employ a rank constraint on the radiance tensor, which is commonly used in computer graphics, and construct a novel cost functional whose minimization leads to an estimate of both shape and radiance for nonLambertian objects, which we validate experimentally.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238454",
        "reference_list": [
            {
                "year": "2001",
                "id": 157
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 13,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Layout",
                "Photometry",
                "Cost function",
                "Equations",
                "Tensile stress",
                "Computer graphics",
                "Geometry",
                "Cameras",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing",
                "edge detection",
                "image matching",
                "brightness"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview stereo",
                "3D shape",
                "3D radiance",
                "image collection",
                "reflectance properties",
                "separation principle",
                "shape estimation",
                "radiance estimation",
                "Lambertian scenes",
                "rank constraint",
                "radiance tensor",
                "computer graphics",
                "nonLambertian objects",
                "geometry",
                "photometry",
                "feature-based stereo reconstruction"
            ]
        },
        "id": 128,
        "cited_by": [
            {
                "year": "2017",
                "id": 402
            },
            {
                "year": "2007",
                "id": 163
            }
        ]
    },
    {
        "title": "Polarization-based inverse rendering from a single view",
        "authors": [
            "Miyazaki",
            "Tan",
            "Hara",
            "Ikeuchi"
        ],
        "abstract": "This paper presents a method to estimate geometrical, photometrical, and environmental information of a single-viewed object in one integrated framework under fixed viewing position and fixed illumination direction. These three types of information are important to render a photorealistic image of a real object. Photometrical information represents the texture and the surface roughness of an object, while geometrical and environmental information represent the 3D shape of an object and the illumination distribution, respectively. The proposed method estimates the 3D shape by computing the surface normal from polarization data, calculates the texture of the object from the diffuse only reflection component, determines the illumination directions from the position of the brightest intensity in the specular reflection component, and finally computes the surface roughness of the object by using the estimated illumination distribution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238455",
        "reference_list": [
            {
                "year": "2001",
                "id": 80
            },
            {
                "year": "2003",
                "id": 74
            },
            {
                "year": "2003",
                "id": 180
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 37,
            "total": 77
        },
        "keywords": {
            "IEEE Keywords": [
                "Polarization",
                "Lighting",
                "Rough surfaces",
                "Surface roughness",
                "Surface texture",
                "Rendering (computer graphics)",
                "Photometry",
                "Shape",
                "Distributed computing",
                "Reflection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "rendering (computer graphics)",
                "realistic images",
                "image texture",
                "surface roughness",
                "image representation",
                "reflection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "polarization-based inverse rendering",
                "geometrical information",
                "photometrical information",
                "environmental information",
                "single-viewed object",
                "fixed viewing position",
                "fixed illumination direction",
                "photorealistic image",
                "image texture",
                "surface roughness",
                "3D shape",
                "illumination distribution",
                "reflection",
                "Rahmann method"
            ]
        },
        "id": 129,
        "cited_by": [
            {
                "year": "2017",
                "id": 240
            },
            {
                "year": "2015",
                "id": 376
            },
            {
                "year": "2005",
                "id": 40
            },
            {
                "year": "2005",
                "id": 184
            },
            {
                "year": "2003",
                "id": 180
            }
        ]
    },
    {
        "title": "On the epipolar geometry of the Crossed-Slits projection",
        "authors": [
            "Feldman",
            "Pajdla",
            "Weinshall"
        ],
        "abstract": "The Crossed-Slits (X-Slits) camera is defined by two nonintersecting slits, which replace the pinhole in the common perspective camera. Each point in space is projected to the image plane by a ray which passes through the point and the two slits. The X-Slits projection model includes the pushbroom camera as a special case. In addition, it describes a certain class of panoramic images, which are generated from sequences obtained by translating pinhole cameras. In this paper we develop the epipolar geometry of the X-Slits projection model. We show an object which is similar to the fundamental matrix; our matrix, however, describes a quadratic relation between corresponding image points (using the Veronese mapping). Similarly the equivalent of epipolar lines are conics in the image plane. Unlike the pin-hole case, epipolar surfaces do not usually exist in the sense that matching epipolar lines lie on a single surface; we analyze the cases when epipolar surfaces exist, and characterize their properties. Finally, we demonstrate the matching of points in pairs of X-Slits panoramic images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238456",
        "reference_list": [
            {
                "year": "2001",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 8,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Cameras",
                "Stereo vision",
                "Image generation",
                "Image sampling",
                "Layout",
                "Computer science",
                "Cybernetics",
                "Solid modeling",
                "Eyes"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing",
                "image matching",
                "image sequences",
                "matrix algebra",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "epipolar geometry",
                "crossed-slits projection",
                "crossed-slits camera",
                "X-slits camera",
                "image plane",
                "pushbroom camera",
                "panoramic images",
                "pinhole cameras",
                "image points",
                "Veronese mapping",
                "epipolar lines",
                "conics",
                "epipolar surfaces",
                "stereo vision",
                "Julesz random dot stereograms",
                "Plucker matrix"
            ]
        },
        "id": 130,
        "cited_by": [
            {
                "year": "2013",
                "id": 57
            },
            {
                "year": "2013",
                "id": 60
            },
            {
                "year": "2013",
                "id": 251
            },
            {
                "year": "2009",
                "id": 234
            }
        ]
    },
    {
        "title": "Spectral partitioning for structure from motion",
        "authors": [
            "Steedly",
            "Essa",
            "Dellaert"
        ],
        "abstract": "We propose a spectral partitioning approach for large-scale optimization problems, specifically structure from motion. In structure from motion, partitioning methods reduce the problem into smaller and better conditioned subproblems which can be efficiently optimized. Our partitioning method uses only the Hessian of the reprojection error and its eigenvector. We show that partitioned systems that preserve the eigenvectors corresponding to small eigenvalues result in lower residual error when optimized. We create partitions by clustering the entries of the eigenvectors of the Hessian corresponding to small eigenvalues. This is a more general technique than relying on domain knowledge and heuristics such as bottom-up structure from motion approaches. Simultaneously, it takes advantage of more information than generic matrix partitioning algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238457",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 5,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Hessian matrices",
                "optimisation",
                "image motion analysis",
                "eigenvalues and eigenfunctions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spectral partitioning",
                "large-scale optimization problems",
                "Hessian-based partitioning",
                "reprojection error",
                "eigenvector",
                "domain knowledge",
                "heuristics",
                "structure from motion",
                "matrix partitioning algorithms",
                "Fiedler vector",
                "Newton-Raphson method",
                "computer vision",
                "video camera"
            ]
        },
        "id": 131,
        "cited_by": []
    },
    {
        "title": "Scene modeling based on constraint system decomposition techniques",
        "authors": [
            "Wilczkowiak",
            "Trombettoni",
            "Jermann",
            "Sturm",
            "Boyer"
        ],
        "abstract": "We present a new approach to 3D scene modeling based on geometric constraints. Contrary to the existing methods, we can quickly obtain 3D scene models that respect the given constraints exactly. Our system can describe a large variety of linear and nonlinear constraints in a flexible way. To deal with the constraints, we decided to exploit the properties of the GPDOF algorithm developed in the Constraint Programming community (Trombettoni, 1998). The approach is based on a dictionary of so-called r-methods, based on theorems of geometry, which can solve a subset of geometric constraints in a very efficient way. GPDOF is used to find, in polynomial-time, a reduced parameterization of a scene, and to decompose the equation system, induced by constraints, into a sequence of r-methods. We have validated our approach in reconstructing, from images, 3D models of buildings based on linear and quadratic geometric constraints.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238458",
        "reference_list": [
            {
                "year": "2001",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 9,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image reconstruction",
                "Solid modeling",
                "Equations",
                "Computer vision",
                "Calibration",
                "Dictionaries",
                "Geometry",
                "Constraint theory",
                "Polynomials"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "image segmentation",
                "stereo image processing",
                "computational geometry",
                "computational complexity",
                "constraint handling",
                "solid modelling",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "constraint system decomposition techniques",
                "3D scene modeling",
                "linear constraints",
                "nonlinear constraints",
                "GPDOF algorithm",
                "constraint programming",
                "r-methods",
                "quadratic geometric constraints",
                "computer vision",
                "computer graphics",
                "computer-aided design",
                "scene reconstruction"
            ]
        },
        "id": 132,
        "cited_by": []
    },
    {
        "title": "Combinatorial constraints on multiple projections of set points",
        "authors": [
            "Werner"
        ],
        "abstract": "Multiple projections of a scene cannot be arbitrary, the allowed configurations being given by matching constraints. This paper presents new matching constraints on multiple projections of a rigid point set by uncalibrated cameras, obtained by formulation in the oriented projective rather than projective geometry. They follow from consistency of orientations of camera rays and from the fact that the scene is the affine rather that projective space. For their non-parametric nature, we call them combinatorial. The constraints are derived in a unified theoretical framework using the theory of oriented matroids. For example, we present constraints on 4 point correspondences for 2D camera resectioning, on 3 correspondences in two 1D cameras, and on 4 correspondences in two 2D cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238459",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Geometry",
                "Image reconstruction",
                "Tensile stress",
                "Constraint theory",
                "H infinity control",
                "Solid modeling",
                "Gold",
                "Typesetting"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "image reconstruction",
                "combinatorial mathematics",
                "computational geometry",
                "cameras",
                "constraint handling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "combinatorial constraints",
                "multiple projections",
                "set points",
                "matching constraints",
                "uncalibrated cameras",
                "oriented projective geometry",
                "projective space",
                "oriented matroids",
                "2D camera resectioning",
                "chirality",
                "quasiaffine reconstruction",
                "orientation-based combinatorial geometry",
                "homography matrix",
                "photogrammetry",
                "computer vision",
                "minimal nonrealizable configurations",
                "partial scene chirotope"
            ]
        },
        "id": 133,
        "cited_by": []
    },
    {
        "title": "Multiview reconstruction of space curves",
        "authors": [
            "Kahl",
            "August"
        ],
        "abstract": "Is the real problem in resolving correspondence using current stereo algorithms the lack of the \"right\" matching criterion? In studying the related task of reconstructing three-dimensional space curves from their projections in multiple views, we suggest that the problem is more basic: matching and reconstruction are coupled, and so reconstruction algorithms should exploit this rather than assuming that matching can be successfully performed before reconstruction. To realize this coupling, a generative model of curves is introduced which has two key components: (i) a prior distribution of general space curves and (ii) an image formation model which describes how 3D curves are projected onto the image plane. A novel aspect of the image formation model is that it uses an exact description of the gradient field of a piecewise constant image. Based on this forward model, a fully automatic algorithm for solving the inverse problem is developed for an arbitrary number of views. The resulting algorithm is robust to partial occlusion, deficiencies in image curve extraction and it does not rely on photometric information. The relative motion of the cameras is assumed to be given. Several experiments are carried out on various realistic scenarios. In particular, we focus on scenes where traditional correlation-based methods would fail.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238461",
        "reference_list": [
            {
                "year": "2001",
                "id": 182
            },
            {
                "year": "2001",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 14,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Layout",
                "Cameras",
                "Computer vision",
                "Geometry",
                "Inverse problems",
                "Stereo vision",
                "Orbital robotics",
                "Reconstruction algorithms",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "image reconstruction",
                "feature extraction",
                "stereo image processing",
                "inverse problems",
                "curve fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview reconstruction",
                "stereo algorithms",
                "matching criterion",
                "3D space curves",
                "image formation model",
                "gradient field",
                "piecewise constant image",
                "inverse problem",
                "image curve extraction",
                "cameras",
                "correlation-based methods",
                "binocular stereo vision",
                "correspondence problem",
                "3D shape",
                "scene geometry",
                "object recognition",
                "motion estimation",
                "surface contour",
                "projective geometry"
            ]
        },
        "id": 134,
        "cited_by": [
            {
                "year": "2017",
                "id": 324
            },
            {
                "year": "2013",
                "id": 252
            }
        ]
    },
    {
        "title": "What does motion reveal about transparency?",
        "authors": [
            "Ben-Ezra",
            "Nayar"
        ],
        "abstract": "The perception of transparent objects from images is known to be a very hard problem in vision. Given a single image, it is difficult to even detect the presence of transparent objects in the scene. In this paper, we explore what can be said about transparent objects by a moving observer. We show how features that are imaged through a transparent object behave differently from those that are rigidly attached to the scene. We present a novel model-based approach to recover the shapes and the poses of transparent objects from known motion. The objects can be complex in that they may be composed of multiple layers with different refractive indices. We have conducted numerous simulations to verify the practical feasibility of our algorithm. We have applied it to real scenes that include transparent objects and recovered the shapes of the objects with high accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238462",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 19,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Painting",
                "Optical refraction",
                "Optical control",
                "Shape measurement",
                "Computer science",
                "Object detection",
                "Graphics",
                "Rendering (computer graphics)",
                "Interpolation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "rendering (computer graphics)",
                "motion compensation",
                "object recognition",
                "feature extraction",
                "transparency",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "transparent objects",
                "refractive indices",
                "3D scene",
                "environment matting",
                "camera motion",
                "computer vision",
                "graphics rendering",
                "visual inspection",
                "feature position",
                "photogrammetry",
                "image features",
                "scene features"
            ]
        },
        "id": 135,
        "cited_by": [
            {
                "year": "2015",
                "id": 384
            },
            {
                "year": "2011",
                "id": 44
            },
            {
                "year": "2007",
                "id": 43
            }
        ]
    },
    {
        "title": "Visual correspondence using energy minimization and mutual information",
        "authors": [
            "Junhwan Kim",
            "Kolmogorov",
            "Zabih"
        ],
        "abstract": "We address visual correspondence problems without assuming that scene points have similar intensities in different views. This situation is common, usually due to nonLambertian scenes or to differences between cameras. We use maximization of mutual information, a powerful technique for registering images that requires no a priori model of the relationship between scene intensities in different views. However, it has proven difficult to use mutual information to compute dense visual correspondence. Comparing fixed-size windows via mutual information suffers from the well-known problems of fixed windows, namely poor performance at discontinuities and in low-texture regions. In this paper, we show how to compute visual correspondence using mutual information without suffering from these problems. Using a simple approximation, mutual information can be incorporated into the standard energy minimization framework used in early vision. The energy can then be efficiently minimized using graph cuts, which preserve discontinuities and handle low-texture regions. The resulting algorithm combines the accurate disparity maps that come from graph cuts with the tolerance for intensity changes that comes from mutual information.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238463",
        "reference_list": [],
        "citation": {
            "ieee": 42,
            "other": 1,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Mutual information",
                "Layout",
                "Cameras",
                "Pixel",
                "Stereo vision",
                "Reflectivity",
                "Brightness",
                "Computer science",
                "Computer vision",
                "Minimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing",
                "image texture",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual correspondence",
                "energy minimization",
                "mutual information",
                "nonLambertian scenes",
                "cameras",
                "fixed-size windows",
                "low-texture regions",
                "scene reflectance",
                "stereo algorithm",
                "computer vision",
                "pose estimation",
                "object recognition",
                "affine transforms",
                "lightness compensation"
            ]
        },
        "id": 136,
        "cited_by": [
            {
                "year": "2017",
                "id": 140
            },
            {
                "year": "2015",
                "id": 386
            },
            {
                "year": "2009",
                "id": 227
            },
            {
                "year": "2007",
                "id": 19
            },
            {
                "year": "2007",
                "id": 94
            },
            {
                "year": "2007",
                "id": 381
            }
        ]
    },
    {
        "title": "Entropy-of-likelihood feature selection for image correspondence",
        "authors": [
            "Toews",
            "Arbel"
        ],
        "abstract": "Feature points for image correspondence are often selected according to subjective criteria (e.g. edge density, nostrils). In this paper, we present a general, nonsubjective criterion for selecting informative feature points, based on the correspondence model itself. We describe the approach within the framework of the Bayesian Markov random field (MRF) model, where the degree of feature point information is encoded by the entropy of the likelihood term. We propose that feature selection according to minimum entropy-of-likelihood (EOL) is less likely to lead to correspondence ambiguity, thus improving the optimization process in terms of speed and quality of solution. Experimental results demonstrate the criterion's ability to select optimal features points in a wide variety of image contexts (e.g. objects, faces). Comparison with the automatic Kanade-Lucas-Tomasi feature selection criterion shows correspondence to be significantly faster with feature points selected according to minimum EOL in difficult correspondence problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238464",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Entropy",
                "Context modeling",
                "Probability distribution",
                "Markov random fields",
                "Image matching",
                "Facial features",
                "Eyes",
                "Nose",
                "Information theory"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image matching",
                "object detection",
                "Bayes methods",
                "Markov processes",
                "minimum entropy methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "entropy-of-likelihood feature selection",
                "image correspondence",
                "feature points",
                "edge density",
                "nostrils",
                "subjective criteria",
                "nonsubjective criterion",
                "Bayesian Markov random field model",
                "correspondence ambiguity",
                "optimization process",
                "Kanade-Lucas-Tomasi feature selection criterion",
                "image matching",
                "contour-based approaches"
            ]
        },
        "id": 137,
        "cited_by": []
    },
    {
        "title": "A Caratheodory-Fejer approach to robust multiframe tracking",
        "authors": [
            "Camps",
            "Hwasup Lim",
            "Mazzaro",
            "Sznaier"
        ],
        "abstract": "A requirement common to most dynamic vision applications is the ability to track objects in a sequence of frames. This problem has been extensively studied in the past few years, leading to several techniques, such as unscented particle filter based trackers, that exploit a combination of the (assumed) target dynamics, empirically learned noise distributions and past position observations. While successful in many scenarios, these trackers remain fragile to occlusion and model uncertainty in the target dynamics. As we show in this paper, these difficulties can be addressed by modeling the dynamics of the target as an unknown operator that satisfies certain interpolation conditions. Results from interpolation theory can then be used to find this operator by solving a convex optimization problem. As illustrated with several examples, combining this operator with Kalman and UPF techniques leads to both robustness improvement and computational complexity reduction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238465",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 1,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Target tracking",
                "Kalman filters",
                "Particle filters",
                "Particle tracking",
                "Interpolation",
                "Position measurement",
                "Noise measurement",
                "Humans",
                "Uncertainty"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "computational complexity",
                "Kalman filters",
                "interpolation",
                "optimisation",
                "clutter",
                "optical tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Caratheodory-Fejer approach",
                "multiframe tracking",
                "unscented particle filter based trackers",
                "target dynamics",
                "occlusion",
                "convex optimization problem",
                "Kalman filter-based trackers",
                "computational complexity reduction",
                "canonical configurations",
                "linear matrix inequality optimization",
                "clutter"
            ]
        },
        "id": 138,
        "cited_by": [
            {
                "year": "2011",
                "id": 313
            },
            {
                "year": "2007",
                "id": 93
            },
            {
                "year": "2005",
                "id": 99
            }
        ]
    },
    {
        "title": "Tracking objects using density matching and shape priors",
        "authors": [
            "Tao Zhang",
            "Freedman"
        ],
        "abstract": "We present a novel method for tracking objects by combining density matching with shape priors. Density matching is a tracking method which operates by maximizing the Bhattacharyya similarity measure between the photometric distribution from an estimated image region and a model photometric distribution. Such trackers can be expressed as PDE-based curve evolutions, which can be implemented using level sets. Shape priors can be combined with this level-set implementation of density matching by representing the shape priors as a series of level sets; a variational approach allows for a natural, parametrization-independent shape term to be derived. Experimental results on real image sequences are shown.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238466",
        "reference_list": [
            {
                "year": "2001",
                "id": 110
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 11,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Level set",
                "Photometry",
                "Active contours",
                "Image segmentation",
                "Image sequences",
                "Computer vision",
                "Computer science",
                "Density measurement",
                "Biomedical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "image sequences",
                "splines (mathematics)",
                "Kalman filters",
                "partial differential equations",
                "image representation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object tracking",
                "Bhattacharyya similarity",
                "photometric distribution",
                "image region",
                "PDE-based curve evolutions",
                "image sequences",
                "shape priors",
                "active contours",
                "density matching",
                "level set method",
                "B-splines",
                "segmentation method",
                "Euclidean similarity transformation",
                "Euclidean distance",
                "differential equations",
                "Kullbach-Leibler distance",
                "computer vision",
                "shape energy",
                "Mahalanobis distance"
            ]
        },
        "id": 139,
        "cited_by": [
            {
                "year": "2009",
                "id": 196
            },
            {
                "year": "2007",
                "id": 177
            },
            {
                "year": "2007",
                "id": 376
            }
        ]
    },
    {
        "title": "Filtering using a tree-based estimator",
        "authors": [
            "Stenger",
            "Thayananthan",
            "Torr",
            "Cipolla"
        ],
        "abstract": "Within this paper a new framework for Bayesian tracking is presented, which approximates the posterior distribution at multiple resolutions. We propose a tree-based representation of the distribution, where the leaves define a partition of the state space with piecewise constant density. The advantage of this representation is that regions with low probability mass can be rapidly discarded in a hierarchical search, and the distribution can be approximated to arbitrary precision. We demonstrate the effectiveness of the technique by using it for tracking 3D articulated and nonrigid motion in front of cluttered background. More specifically, we are interested in estimating the joint angles, position and orientation of a 3D hand model in order to drive an avatar.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238467",
        "reference_list": [
            {
                "year": "2001",
                "id": 148
            },
            {
                "year": "2001",
                "id": 161
            }
        ],
        "citation": {
            "ieee": 49,
            "other": 35,
            "total": 84
        },
        "keywords": {
            "IEEE Keywords": [
                "Filtering",
                "State-space methods",
                "Particle tracking",
                "Particle filters",
                "Bayesian methods",
                "Avatars",
                "Video sequences",
                "Parameter estimation",
                "Robustness",
                "Target tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "motion estimation",
                "Kalman filters",
                "Bayes methods",
                "trees (mathematics)",
                "stereo image processing",
                "position measurement",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tree-based estimator",
                "Bayesian tracking",
                "posterior distribution",
                "tree-based representation",
                "state space",
                "piecewise constant density",
                "probability mass",
                "hierarchical search",
                "nonrigid motion",
                "3D hand model joint angles",
                "3D hand model position",
                "3D hand model orientation",
                "image sequences",
                "monocular video sequences",
                "particle filters",
                "Kalman filter",
                "Hausdorff distance",
                "template matching",
                "tree-based detection",
                "Monte Carlo methods",
                "piecewise linear approximation"
            ]
        },
        "id": 140,
        "cited_by": [
            {
                "year": "2009",
                "id": 176
            },
            {
                "year": "2005",
                "id": 64
            },
            {
                "year": "2005",
                "id": 133
            }
        ]
    },
    {
        "title": "Constraining human body tracking",
        "authors": [
            "Demirdjian",
            "Ko",
            "Darrell"
        ],
        "abstract": "Our paper addresses the problem of enforcing constraints in human body tracking. A projection technique is derived to impose kinematic constraints on independent multibody motion: we show that for small motions the multibody articulated motion space can be approximated by a linear manifold estimated directly from the previous body pose. We propose a learning approach to model nonlinear constraints; we train a support vector classifier from motion capture data to model the boundary of the space of valid poses. Linear and nonlinear body pose constraints are enforced by first projecting unconstrained motions onto the articulated motion space and then optimizing to find points on this linear manifold that lie within the non-linear constraint surface modeled by the SVM classifier.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238468",
        "reference_list": [
            {
                "year": "2001",
                "id": 190
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 15,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Motion estimation",
                "Biological system modeling",
                "Support vector machines",
                "Support vector machine classification",
                "Tracking",
                "Kinematics",
                "Joints",
                "Space technology",
                "Stereo vision"
            ],
            "INSPEC: Controlled Indexing": [
                "optical tracking",
                "computer vision",
                "learning (artificial intelligence)",
                "motion estimation",
                "image classification",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human body tracking",
                "projection technique",
                "kinematic constraints",
                "multibody motion",
                "articulated motion space",
                "linear manifold",
                "nonlinear constraints",
                "support vector machine classifier",
                "linear body pose constraints",
                "nonlinear body pose constraints",
                "unconstrained motions",
                "vision-based tracking",
                "motion capture",
                "human-computer interface",
                "articulated motion estimation",
                "unconstrained fitting error minimization",
                "stereo cameras",
                "gesture recognition"
            ]
        },
        "id": 141,
        "cited_by": []
    },
    {
        "title": "A background layer model for object tracking through occlusion",
        "authors": [
            "Yue Zhou",
            "Hai Tao"
        ],
        "abstract": "Motion layer estimation has recently emerged as a promising object tracking method. In this paper, we extend previous research on layer-based tracker by introducing the concept of background occluding layers and explicitly inferring depth ordering of foreground layers. The background occluding layers lie in front of, behind, and in between foreground layers. Each pixel in the background regions belongs to one of these layers and occludes all the foreground layers behind it. Together with the foreground ordering, the complete information necessary for reliably tracking objects through occlusion is included in our representation. An MAP estimation framework is developed to simultaneously update the motion layer parameters, the ordering parameters, and the background occluding layers. Experimental results show that under various conditions with occlusion, including situations with moving objects undergoing complex motions or having complex interactions, our tracking algorithm is able to handle many difficult tracking tasks reliably.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238469",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 6,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Motion estimation",
                "Hidden Markov models",
                "Shape",
                "Motion analysis",
                "Parameter estimation",
                "Gaussian distribution",
                "Markov random fields",
                "Layout",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "computer vision",
                "hidden Markov models",
                "hidden feature removal",
                "optical tracking",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "background layer model",
                "object tracking",
                "occlusion",
                "motion layer estimation",
                "background occluding layers",
                "foreground layers depth ordering",
                "maximum a posteriori estimation",
                "hidden Markov model",
                "Gaussian distribution",
                "foreground shape estimation",
                "background shape estimation",
                "appearance estimation",
                "vehicle tracking",
                "human tracking"
            ]
        },
        "id": 142,
        "cited_by": [
            {
                "year": "2009",
                "id": 184
            },
            {
                "year": "2007",
                "id": 192
            }
        ]
    },
    {
        "title": "Bayesian clustering of optical flow fields",
        "authors": [
            "Hoey",
            "Little"
        ],
        "abstract": "We present a method for unsupervised learning of classes of motions in video. We project optical flow fields to a complete, orthogonal, a-priori set of basis functions in a probabilistic fashion, which improves the estimation of the projections by incorporating uncertainties in the flows. We then cluster the projections using a mixture of feature-weighted Gaussians over optical flow fields. The resulting model extracts a concise probabilistic description of the major classes of optical flow present. The method is demonstrated on a video of a person's facial expressions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238470",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Image motion analysis",
                "Uncertainty",
                "Unsupervised learning",
                "Gaussian processes",
                "Humans",
                "Optical computing",
                "Computer science",
                "Streaming media",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Bayes methods",
                "unsupervised learning",
                "image classification",
                "pattern clustering",
                "Gaussian processes",
                "image sequences",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian clustering",
                "optical flow fields",
                "unsupervised learning",
                "motion classes",
                "video",
                "feature-weighted Gaussians",
                "facial expressions",
                "expectation-maximization algorithm",
                "Zernike polynomials",
                "hidden Markov model"
            ]
        },
        "id": 143,
        "cited_by": []
    },
    {
        "title": "Tracking articulated body by dynamic Markov network",
        "authors": [
            "Ying Wu",
            "Gang Hua",
            "Ting Yu"
        ],
        "abstract": "A new method for visual tracking of articulated objects is presented. Analyzing articulated motion is challenging because the dimensionality increase potentially demands tremendous increase of computation. To ease this problem, we propose an approach that analyzes subparts locally while reinforcing the structural constraints at the mean time. The computational model of the proposed approach is based on a dynamic Markov network, a generative model which characterizes the dynamics and the image observations of each individual subpart as well as the motion constraints among different subparts. Probabilistic variational analysis of the model reveals a mean field approximation to the posterior densities of each subparts given visual evidence, and provides a computationally efficient way for such a difficult Bayesian inference problem. In addition, we design mean field Monte Carlo (MFMC) algorithms, in which a set of low dimensional particle filters interact with each other and solve the high dimensional problem collaboratively. Extensive experiments on tracking human body parts demonstrate the effectiveness, significance and computational efficiency of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238471",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 2,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Markov random fields",
                "Subspace constraints",
                "Motion analysis",
                "Computer networks",
                "Computational modeling",
                "Character generation",
                "Bayesian methods",
                "Algorithm design and analysis",
                "Monte Carlo methods",
                "Inference algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Monte Carlo methods",
                "Bayes methods",
                "optical tracking",
                "object detection",
                "Markov processes",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "articulated body tracking",
                "dynamic Markov network",
                "visual tracking",
                "mean time structural constraints",
                "Bayesian inference problem",
                "mean field Monte Carlo algorithms",
                "particle filters",
                "perceptual interfaces",
                "smart video surveillance",
                "automatic video footage",
                "object tracking"
            ]
        },
        "id": 144,
        "cited_by": [
            {
                "year": "2009",
                "id": 205
            }
        ]
    },
    {
        "title": "Tracking articulated hand motion with eigen dynamics analysis",
        "authors": [
            "Hanning Zhou",
            "Huang"
        ],
        "abstract": "This paper introduces the concept of eigen-dynamics and proposes an eigen dynamics analysis (EDA) method to learn the dynamics of natural hand motion from labelled sets of motion captured with a data glove. The result is parameterized with a high-order stochastic linear dynamic system (LDS) consisting of five lower-order LDS. Each corresponding to one eigen-dynamics. Based on the EDA model, we construct a dynamic Bayesian network (DBN) to analyze the generative process of a image sequence of natural hand motion. Using the DBN, a hand tracking system is implemented. Experiments on both synthesized and real-world data demonstrate the robustness and effectiveness of these techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238472",
        "reference_list": [
            {
                "year": "2001",
                "id": 89
            },
            {
                "year": "2001",
                "id": 51
            },
            {
                "year": "2001",
                "id": 163
            },
            {
                "year": "2001",
                "id": 161
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 12,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Motion analysis",
                "Electronic design automation and methodology",
                "Data gloves",
                "Stochastic systems",
                "Bayesian methods",
                "Image analysis",
                "Image motion analysis",
                "Image sequence analysis",
                "Image sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Bayes methods",
                "Monte Carlo methods",
                "principal component analysis",
                "image sequences",
                "image motion analysis",
                "data gloves",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "articulated hand motion tracking",
                "eigen dynamics analysis",
                "stochastic linear dynamic system",
                "dynamic Bayesian network",
                "image sequence",
                "principal component analysis",
                "switching linear dynamic system",
                "iterative closest point algorithm",
                "likelihood edge",
                "sequential Monte Carlo method"
            ]
        },
        "id": 145,
        "cited_by": []
    },
    {
        "title": "Maintaining multimodality through mixture tracking",
        "authors": [
            "Vermaak",
            "Doucet",
            "Perez"
        ],
        "abstract": "In recent years particle filters have become a tremendously popular tool to perform tracking for nonlinear and/or nonGaussian models. This is due to their simplicity, generality and success over a wide range of challenging applications. Particle filters, and Monte Carlo methods in general, are however poor at consistently maintaining the multimodality of the target distributions that may arise due to ambiguity or the presence of multiple objects. To address this shortcoming this paper proposes to model the target distribution as a nonparametric mixture model, and presents the general tracking recursion in this case. It is shown how a Monte Carlo implementation of the general recursion leads to a mixture of particle filters that interact only in the computation of the mixture weights, thus leading to an efficient numerical algorithm, where all the results pertaining to standard particle filters apply. The ability of the new method to maintain posterior multimodality is illustrated on a synthetic example and a real world tracking problem involving the tracking of football players in a video sequence.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238473",
        "reference_list": [
            {
                "year": "2001",
                "id": 200
            },
            {
                "year": "2001",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 138,
            "other": 100,
            "total": 238
        },
        "keywords": {
            "IEEE Keywords": [
                "Radar tracking",
                "Particle filters",
                "Filtering",
                "Target tracking",
                "Monte Carlo methods",
                "Recursive estimation",
                "Video sequences",
                "Laser radar",
                "Bayesian methods",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "object detection",
                "Bayes methods",
                "Monte Carlo methods",
                "target tracking",
                "recursive estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mixture tracking",
                "particle filters",
                "nonGaussian models",
                "Monte Carlo methods",
                "target distributions",
                "nonparametric mixture model",
                "tracking recursion",
                "mixture weights",
                "posterior multimodality",
                "video sequence",
                "face tracking",
                "aircraft tracking",
                "mobile robot",
                "Bayesian sequential estimation"
            ]
        },
        "id": 146,
        "cited_by": [
            {
                "year": "2009",
                "id": 194
            },
            {
                "year": "2007",
                "id": 99
            },
            {
                "year": "2007",
                "id": 113
            }
        ]
    },
    {
        "title": "Using prior shape and intensity profile in medical image segmentation",
        "authors": [
            "Yunmei Chen",
            "Feng Huang",
            "Tagare",
            "Murali Rao",
            "Wilson",
            "Geiser"
        ],
        "abstract": "In this note we present a coupled optimization model for boundary determination. One part of the model incorporates a prior shape into a geometric active contour model with a fixed parameter. The second part determines the 'best' parameter used in the first part by maximizing the mutual information of the image geometry between the prior and an aligned novel image over all the alignments that are the solutions of the first part corresponding to different parameters. We also present an alternative method, which generates an intensity model formed as the average of a set of aligned training images. Experimental results on cardiac ultrasound images are presented. These results indicate that the proposed model provides close agreement with expert traced borders, and the parameter determined in this model for one image can be used for images with similar properties. The existence of a solution to the proposed minimization problem is also discussed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238474",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical imaging",
                "Image segmentation",
                "Active contours",
                "Active shape model",
                "Deformable models",
                "Medical diagnostic imaging",
                "Bismuth",
                "Solid modeling",
                "Ultrasonic imaging",
                "Anatomical structure"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "optimisation",
                "image registration",
                "image matching",
                "medical image processing",
                "Bayes methods",
                "edge detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "prior shape",
                "intensity profile",
                "medical image segmentation",
                "coupled optimization model",
                "boundary determination",
                "geometric active contour model",
                "image geometry mutual information",
                "cardiac ultrasound images",
                "Bayesian formulation",
                "Gaussian model",
                "Fourier coefficients",
                "Fourier parameterized shape models",
                "image registration",
                "image matching"
            ]
        },
        "id": 147,
        "cited_by": []
    },
    {
        "title": "A new paradigm for recognizing 3-D objects from range data",
        "authors": [
            "Ruiz-Correa",
            "Shapiro",
            "Meila"
        ],
        "abstract": "Most of the work on 3D object recognition from range data has used an alignment-verification approach in which a specific 3D object is matched to an exact instance of the same object in a scene. This approach has been successfully used in industrial machine vision, but it is not capable of dealing with the complexities of recognizing classes of similar objects. This paper undertakes this task by proposing and testing a component-based methodology encompassing three main ingredients: 1) a new way of learning and extracting shape-class components from surface shape information; 2) a new shape representation called a symbolic surface signature that summarizes the geometric relationships among components; and 3) an abstract representation of shape classes formed by a hierarchy of classifiers that learn object-class parts and their spatial relationships from examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238475",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 17,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Humans",
                "Object recognition",
                "Computer vision",
                "Shape measurement",
                "Character recognition",
                "Head",
                "Rabbits",
                "Dogs",
                "Statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "robot vision",
                "learning by example",
                "object recognition",
                "image matching",
                "image representation",
                "stereo image processing",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object recognition",
                "alignment-verification approach",
                "industrial machine vision",
                "shape-class components",
                "surface shape information",
                "shape representation",
                "symbolic surface signature",
                "shape classes abstract representation",
                "robot vision",
                "autonomous navigation",
                "automated inspection",
                "satellite image analysis",
                "Hilbert space",
                "support vector machine",
                "Mercel kernel"
            ]
        },
        "id": 148,
        "cited_by": []
    },
    {
        "title": "A Bayesian approach to unsupervised one-shot learning of object categories",
        "authors": [
            "Li Fe-Fei",
            "Fergus",
            "Perona"
        ],
        "abstract": "Learning visual models of object categories notoriously requires thousands of training examples; this is due to the diversity and richness of object appearance which requires models containing hundreds of parameters. We present a method for learning object categories from just a few images (1 /spl sim/ 5). It is based on incorporating \"generic\" knowledge which may be obtained from previously learnt models of unrelated categories. We operate in a variational Bayesian framework: object categories are represented by probabilistic models, and \"prior\" knowledge is represented as a probability density function on the parameters of these models. The \"posterior\" model for an object category is obtained by updating the prior in the light of one or more observations. Our ideas are demonstrated on four diverse categories (human faces, airplanes, motorcycles, spotted cats). Initially three categories are learnt from hundreds of training examples, and a \"prior\" is estimated from these. Then the model of the fourth category is learnt from 1 to 5 training examples, and is used for detecting new exemplars a set of test images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238476",
        "reference_list": [],
        "citation": {
            "ieee": 52,
            "other": 38,
            "total": 90
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Humans",
                "Computer vision",
                "Probability density function",
                "Cats",
                "Face detection",
                "Airplanes",
                "Motorcycles",
                "Testing",
                "Animals"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "unsupervised learning",
                "Bayes methods",
                "Markov processes",
                "Monte Carlo methods",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian method",
                "unsupervised one-shot learning",
                "object categories",
                "learning visual models",
                "probabilistic models",
                "prior knowledge",
                "probability density function",
                "posterior model",
                "human faces",
                "airplanes",
                "motorcycles",
                "spotted cats",
                "computer vision",
                "maximum likelihood",
                "Markov-chain Monte Carlo method",
                "Direchlet distribution",
                "Wishart density",
                "Digamma function",
                "expectation maximization"
            ]
        },
        "id": 149,
        "cited_by": [
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 286
            },
            {
                "year": "2011",
                "id": 321
            },
            {
                "year": "2009",
                "id": 37
            },
            {
                "year": "2007",
                "id": 202
            },
            {
                "year": "2005",
                "id": 37
            },
            {
                "year": "2005",
                "id": 174
            },
            {
                "year": "2005",
                "id": 178
            },
            {
                "year": "2005",
                "id": 179
            },
            {
                "year": "2005",
                "id": 230
            },
            {
                "year": "2005",
                "id": 237
            }
        ]
    },
    {
        "title": "An affine invariant deformable shape representation for general curves",
        "authors": [
            "Ericsson",
            "Astrom"
        ],
        "abstract": "Automatic construction of shape models from examples has been the focus of intense research during the last couple of years. These methods have proved to be useful for shape segmentation, tracking and shape understanding. In this paper novel theory to automate shape modelling is described. The theory is intrinsically defined for curves although curves are infinite dimensional objects. The theory is independent of parameterisation and affine transformations. We suggest a method for implementing the ideas and compare it to minimising the description length of the model (MDL). It turns out that the accuracy of the two methods is comparable. Both the MDL and our approach can get stuck at local minima. Our algorithm is less computational expensive and relatively good solutions are obtained after a few iterations. The MDL is, however, better suited at fine-tuning the parameters given good initial estimates to the problem. It is shown that a combination of the two methods outperforms either on its own.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238477",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Mathematics",
                "Mathematical model",
                "Focusing",
                "Image segmentation",
                "Biomedical imaging",
                "Humans",
                "Interpolation",
                "Testing",
                "Level measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "image reconstruction",
                "image segmentation",
                "feature extraction",
                "curve fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable shape representation",
                "general curves",
                "automatic shape models construction",
                "shape segmentation",
                "shape tracking",
                "shape understanding",
                "affine transformations",
                "image segmentation",
                "handwriting recognition",
                "Dijkstra's algorithm",
                "affine invariant active shape",
                "shape modelling",
                "shape boundary",
                "minimum description length",
                "curve parameterisation",
                "shape variation"
            ]
        },
        "id": 150,
        "cited_by": []
    },
    {
        "title": "Discriminative random fields: a discriminative framework for contextual interaction in classification",
        "authors": [
            "Sanjiv Kumar",
            "Hebert"
        ],
        "abstract": "In this work we present discriminative random fields (DRFs), a discriminative framework for the classification of image regions by incorporating neighborhood interactions in the labels as well as the observed data. The discriminative random fields offer several advantages over the conventional Markov random field (MRF) framework. First, the DRFs allow to relax the strong assumption of conditional independence of the observed data generally used in the MRF framework for tractability. This assumption is too restrictive for a large number of applications in vision. Second, the DRFs derive their classification power by exploiting the probabilistic discriminative models instead of the generative models used in the MRF framework. Finally, all the parameters in the DRF model are estimated simultaneously from the training data unlike the MRF framework where likelihood parameters are usually learned separately from the field parameters. We illustrate the advantages of the DRFs over the MRF framework in an application of man-made structure detection in natural images taken from the Corel database.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238478",
        "reference_list": [],
        "citation": {
            "ieee": 124,
            "other": 42,
            "total": 166
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Markov random fields",
                "Application software",
                "Image segmentation",
                "Computer vision",
                "Context modeling",
                "Robots",
                "Power generation",
                "Training data",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "image segmentation",
                "random processes",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative random fields",
                "contextual interaction",
                "image regions",
                "neighborhood interactions",
                "Markov random field",
                "computer vision",
                "probabilistic discriminative models",
                "generative models",
                "likelihood parameters",
                "man-made structure detection",
                "natural images",
                "Corel database",
                "contextual constraints",
                "probabilistic generative framework",
                "local posterior",
                "hierarchical texture segmentation",
                "conditional random field",
                "hidden Markov model based labeling",
                "text sequences",
                "generalized linear models",
                "maximum posterior marginal solution",
                "Ising model",
                "Bayes rule",
                "kernel classifiers",
                "image classification"
            ]
        },
        "id": 151,
        "cited_by": [
            {
                "year": "2017",
                "id": 1
            },
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2007",
                "id": 251
            },
            {
                "year": "2007",
                "id": 344
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2005",
                "id": 86
            },
            {
                "year": "2005",
                "id": 115
            },
            {
                "year": "2005",
                "id": 159
            },
            {
                "year": "2005",
                "id": 236
            }
        ]
    },
    {
        "title": "Eye design in the plenoptic space of light rays",
        "authors": [
            "Neumann",
            "Fermuller",
            "Aloimonos"
        ],
        "abstract": "Natural eye designs are optimized with regard to the tasks the eye-carrying organism has to perform for survival. This optimization has been performed by the process of natural evolution over many millions of years. Every eye captures a subset of the space of light rays. The information contained in this subset and the accuracy to which the eye can extract the necessary information determines an upper limit on how well an organism can perform a given task. In this work we propose a new methodology for camera design. By interpreting eyes as sample patterns in light ray space we can phrase the problem of eye design in a signal processing framework. This allows us to develop mathematical criteria for optimal eye design, which in turn enables us to build the best eye for a given task without the trial and error phase of natural evolution. The principle is evaluated on the task of 3D ego-motion estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238623",
        "reference_list": [
            {
                "year": "2001",
                "id": 0
            },
            {
                "year": "2001",
                "id": 117
            },
            {
                "year": "2001",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Eyes",
                "Organisms",
                "Data mining",
                "Cameras",
                "Signal design",
                "Assembly",
                "Information geometry",
                "Optical signal processing",
                "Computer vision",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "motion estimation",
                "image representation",
                "image resolution",
                "cameras",
                "eye",
                "signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "plenoptic space",
                "light rays",
                "natural eye designs",
                "eye-carrying organism",
                "natural evolution process",
                "camera design",
                "signal processing framework",
                "optimal eye design mathematical criteria",
                "3D ego-motion estimation",
                "sensory ecology",
                "compound eyes",
                "captured image",
                "light gathering power",
                "visual acuity",
                "image representation",
                "plenoptic video geometry",
                "plenoptic image formation",
                "mixed spherical-Cartesian coordinate system",
                "light field reconstruction",
                "B-splines",
                "sampling operators",
                "filter optimization",
                "signal processing tool",
                "natural image statistics",
                "optical nanotechnology",
                "image resolution",
                "square-summable sequences"
            ]
        },
        "id": 152,
        "cited_by": []
    },
    {
        "title": "Adaptive dynamic range imaging: optical control of pixel exposures over space and time",
        "authors": [
            "Nayar",
            "Branzoi"
        ],
        "abstract": "This paper presents a new approach to imaging that significantly enhances the dynamic range of a camera. The key idea is to adapt the exposure of each pixel on the image detector, based on the radiance value of the corresponding scene point. This adaptation is done in the optical domain, that is, during image formation. In practice, this is achieved using a spatial light modulator whose transmittance can be varied with high resolution over space and time. A real-time control algorithm is developed that uses acquired images to automatically adjust the transmittance function of the spatial modulator. Each captured image and its corresponding transmittance function are used to compute a very high dynamic range image that is linear in scene radiance. We have implemented a video-rate adaptive dynamic range camera that consists of a color CCD detector and a controllable liquid crystal light modulator. Experiments have been conducted in scenarios with complex and harsh lighting conditions. The results indicate that adaptive imaging can have a significant impact on vision applications such as monitoring, tracking, recognition, and navigation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238624",
        "reference_list": [
            {
                "year": "2001",
                "id": 0
            },
            {
                "year": "2001",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 34,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Programmable control",
                "Adaptive control",
                "Dynamic range",
                "Optical imaging",
                "Optical control",
                "Pixel",
                "Optical modulation",
                "Layout",
                "Cameras",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image recognition",
                "image resolution",
                "motion estimation",
                "object detection",
                "cameras",
                "computer graphics",
                "brightness",
                "optical control"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive dynamic range imaging",
                "optical control",
                "pixel exposures",
                "radiance value",
                "scene point",
                "optical domain",
                "image formation",
                "spatial light modulator",
                "real-time control algorithm",
                "transmittance function",
                "scene radiance",
                "video-rate adaptive dynamic range camera",
                "color CCD detector",
                "controllable liquid crystal light modulator",
                "tracking",
                "navigation",
                "brightness values",
                "computer vision",
                "color channel",
                "computer graphics",
                "digital cameras",
                "motion estimation",
                "object recognition",
                "multiple image detectors",
                "controllable optical attenuator",
                "detector irradiance",
                "effective optical transmittance",
                "spatiotemporal attenuator control",
                "scene dynamics",
                "attenuator blurring",
                "automatic gain control",
                "LCD attenuator",
                "video surveillance",
                "video monitoring"
            ]
        },
        "id": 153,
        "cited_by": [
            {
                "year": "2011",
                "id": 169
            },
            {
                "year": "2007",
                "id": 151
            },
            {
                "year": "2007",
                "id": 317
            }
        ]
    },
    {
        "title": "Image-based rendering using image-based priors",
        "authors": [
            "Fitzgibbon",
            "Wexler",
            "Zisserman"
        ],
        "abstract": "Given a set of images acquired from known viewpoints, we describe a method for synthesizing the image which would be seen from a new viewpoint. In contrast to existing techniques, which explicitly reconstruct the 3D geometry of the scene, we transform the problem to the reconstruction of colour rather than depth. This retains the benefits of geometric constraints, but projects out the ambiguities in depth estimation which occur in textureless regions. On the other hand, regularization is still needed in order to generate high-quality images. The paper's second contribution is to constrain the generated views to lie in the space of images whose texture statistics are those of the input images. This amounts to an image-based prior on the reconstruction which regularizes the solution, yielding realistic synthetic views. Examples are given of new view generation for cameras interpolated between the acquisition viewpoints - which enables synthetic steadicam stabilization of a sequence with a high level of realism.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238625",
        "reference_list": [],
        "citation": {
            "ieee": 51,
            "other": 23,
            "total": 74
        },
        "keywords": {
            "IEEE Keywords": [
                "Rendering (computer graphics)",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "image texture",
                "image colour analysis",
                "rendering (computer graphics)",
                "stereo image processing",
                "Bayes methods",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image-based rendering",
                "image-based priors",
                "image synthesizing",
                "3D scene geometry",
                "colour reconstruction",
                "geometric constraints",
                "depth estimation",
                "textureless regions",
                "image space",
                "texture statistics",
                "image-based prior",
                "synthetic steadicam stabilization",
                "stereo reconstruction",
                "implicit geometry techniques",
                "natural images",
                "Bayesian image reconstruction",
                "photoconsistency constraint",
                "texture reference",
                "texture rectification",
                "inverse problem",
                "iterated conditional modes algorithm",
                "Huber kernels",
                "interpolation",
                "extrapolation"
            ]
        },
        "id": 154,
        "cited_by": [
            {
                "year": "2005",
                "id": 111
            }
        ]
    },
    {
        "title": "Photo-consistent 3D fire by Flame-Sheet decomposition",
        "authors": [
            "Hasinoff",
            "Kutulakos"
        ],
        "abstract": "This paper considers the problem of reconstructing visually realistic 3D models of fire from a very small set of simultaneous views (even two). By modeling fire as a semitransparent 3D density field, we show that fire reconstruction is equivalent to a severely under-constrained computerized tomography problem, for which traditional methods break down. Our approach is based on the observation that every pair of photographs of a semitransparent scene defines a unique density field, called a Flame Sheet, that (1) concentrates all its density on one connected, semitransparent surface, (2) reproduces the two photos exactly, and (3) is the most spatially-coherent density field that does so. From this observation, we reduce fire reconstruction to the convex combination of sheet-like density fields, each of which is derived from the Flame Sheet of two input photos. Experimental results suggest that this method enables high-quality view extrapolation without over-fitting artifacts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238626",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 10,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Fires",
                "Image reconstruction",
                "Layout",
                "Computed tomography",
                "Extrapolation",
                "Data mining",
                "Videos",
                "Combustion",
                "Computer vision",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "realistic images",
                "stereo image processing",
                "solid modelling",
                "fires"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photo-consistent 3D fire",
                "flame sheet decomposition",
                "semitransparent 3D density field",
                "fire reconstruction",
                "underconstrained computerized tomography",
                "semitransparent scene",
                "semitransparent surface",
                "spatially-coherent density field",
                "sheet-like density fields convex combination",
                "view extrapolation",
                "computational modeling",
                "computer graphics",
                "visual modes",
                "image-based representation",
                "orthographic cameras",
                "convexity property",
                "volume rendering",
                "video cameras"
            ]
        },
        "id": 155,
        "cited_by": []
    },
    {
        "title": "Dense matching of multiple wide-baseline views",
        "authors": [
            "Strecha",
            "Tuytelaars",
            "Van Gool"
        ],
        "abstract": "This paper describes a PDE-based method for dense depth extraction from multiple wide-baseline images. Emphasis lies on the usage of only a small amount of images. The integration of these multiple wide-baseline views is guided by the relative confidence that the system has in the matching to different views. This weighting is fine-grained in that it is determined for every pixel at every iteration. Reliable information spreads fast at the expense of less reliable data, both in terms of spatial communications within a view and in terms of information exchange between the views. Changes in intensity between images can be handled in a similar fine grained fashion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238627",
        "reference_list": [],
        "citation": {
            "ieee": 49,
            "other": 27,
            "total": 76
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Stereo vision",
                "Layout",
                "Digital cameras",
                "Image resolution",
                "Stereo image processing",
                "Video recording",
                "Video sequences",
                "Pixel",
                "Level set"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "image texture",
                "image reconstruction",
                "feature extraction",
                "stereo image processing",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense matching",
                "multiple wide-baseline views",
                "PDE-based method",
                "dense depth extraction",
                "spatial communications",
                "information exchange",
                "video sequences",
                "shape-from-video technique",
                "digital cameras",
                "stereo matching",
                "video-based reconstruction",
                "uniqueness constraint",
                "ordering constraint",
                "inhomogeneous time diffusion",
                "depth parameterization",
                "RGB color space",
                "affine invariant feature matching",
                "occlusions",
                "texture extraction"
            ]
        },
        "id": 156,
        "cited_by": [
            {
                "year": "2015",
                "id": 248
            },
            {
                "year": "2013",
                "id": 96
            },
            {
                "year": "2007",
                "id": 53
            }
        ]
    },
    {
        "title": "Dense shape reconstruction of a moving object under arbitrary, unknown lighting",
        "authors": [
            "Simakov",
            "Frolova",
            "Basri"
        ],
        "abstract": "We present a method for shape reconstruction from several images of a moving object. The reconstruction is dense (up to image resolution). The method assumes that the motion is known, e.g., by tracking a small number of feature points on the object. The object is assumed Lambertian (completely matte), light sources should not be very close to the object but otherwise arbitrary, and no knowledge of lighting conditions is required. An object changes its appearance significantly when it changes its orientation relative to light sources, causing violation of the common brightness constancy assumption. While a lot of effort is devoted to deal with this violation, we demonstrate how to exploit it to recover 3D structure from 2D images. We propose a new correspondence measure that enables point matching across views of a moving object. The method has been tested both on computer simulated examples and on a real object.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238628",
        "reference_list": [
            {
                "year": "2001",
                "id": 111
            },
            {
                "year": "2001",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 15,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image reconstruction",
                "Brightness",
                "Light sources",
                "Stereo vision",
                "Information resources",
                "Cameras",
                "Calibration",
                "Computer science",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "image resolution",
                "image motion analysis",
                "image matching",
                "image sequences",
                "feature extraction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense shape reconstruction",
                "moving object",
                "arbitrary unknown lighting",
                "image resolution",
                "Lambertian object",
                "light sources",
                "brightness constancy assumption",
                "3D structure",
                "2D images",
                "illumination",
                "light distribution",
                "epipolar geometry",
                "camera calibration",
                "shape recovery",
                "correspondence measure",
                "object motion",
                "Markov random fields",
                "video sequence"
            ]
        },
        "id": 157,
        "cited_by": []
    },
    {
        "title": "Linear multiview reconstruction of points, lines, planes and cameras using a reference plane",
        "authors": [
            "Rother"
        ],
        "abstract": "This paper presents a new linear method for reconstructing simultaneously 3D features (points, lines and planes) and cameras from many perspective views by solving a single linear system. It assumes that a real or virtual reference plane is visible in all views. We call it the Direct Reference Plane (DRP) method. It is well known that the projection relationship between uncalibrated cameras and 3D features is nonlinear in the absence of a reference plane. With a known reference plane, points and cameras have a linear relationship, as shown by Rother and Carlsson (2001). The main contribution of this paper is that lines and cameras, as well as, planes and cameras also have a linear relationship. Consequently, all 3D features and all cameras can be reconstructed simultaneously from a single linear system, which handles missing image measurements naturally. A further contribution is an extensive experimental comparison, using real data, of different reference plane and nonreference plane reconstruction methods. For difficult reference plane scenarios, with point or line features, the DRP method is superior to all compared methods. Finally, an extensive list of reference plane scenarios is presented, which shows the wide applicability of the DRP method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238629",
        "reference_list": [
            {
                "year": "2001",
                "id": 56
            },
            {
                "year": "2001",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 5,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Layout",
                "Linear systems",
                "Computer vision",
                "Reconstruction algorithms",
                "Laboratories",
                "Motion measurement",
                "Large-scale systems",
                "Information resources"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "stereo image processing",
                "computational geometry",
                "feature extraction",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear multiview reconstruction",
                "3D features simultaneous reconstruction",
                "single linear system",
                "virtual reference plane",
                "real reference plane",
                "direct reference plane method",
                "nonreference plane reconstruction methods",
                "reference plane scenarios",
                "computer vision",
                "camera constraints",
                "matching constraints",
                "dual structure constraints",
                "factorization methods",
                "scene parameters",
                "motion parameters",
                "measurement matrix"
            ]
        },
        "id": 158,
        "cited_by": []
    },
    {
        "title": "Recognising panoramas",
        "authors": [
            "Brown",
            "Lowe"
        ],
        "abstract": "The problem considered in this paper is the fully automatic construction of panoramas. Fundamentally, this problem requires recognition, as we need to know which parts of the panorama join up. Previous approaches have used human input or restrictions on the image sequence for the matching step. In this work we use object recognition techniques based on invariant local features to select matching images, and a probabilistic model for verification. Because of this our method is insensitive to the ordering, orientation, scale and illumination of the images. It is also insensitive to 'noise' images which are not part of the panorama at all, that is, it recognises panoramas. This suggests a useful application for photographers: the system takes as input the images on an entire flash card or film, recognises images that form part of a panorama, and stitches them with no user input whatsoever.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238630",
        "reference_list": [],
        "citation": {
            "ieee": 327,
            "other": 200,
            "total": 527
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "image matching",
                "image sequences",
                "feature extraction",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "panorama recognition",
                "automatic panorama construction",
                "image sequence",
                "image matching",
                "object recognition techniques",
                "invariant local features",
                "image ordering",
                "image orientation",
                "image scale",
                "image illumination",
                "photographers",
                "panoramic image mosaicing",
                "camera matrix",
                "digital cameras",
                "normalised cross-correlation",
                "local intensity values",
                "nonlinear least squares problem",
                "Leverberg-Marquardt algorithm",
                "multiband blending",
                "automatic panorama stitching",
                "scale invariant feature transform",
                "panoramic image geometry",
                "feature extraction",
                "feature matching"
            ]
        },
        "id": 159,
        "cited_by": [
            {
                "year": "2013",
                "id": 265
            },
            {
                "year": "2011",
                "id": 290
            },
            {
                "year": "2009",
                "id": 25
            },
            {
                "year": "2009",
                "id": 161
            },
            {
                "year": "2007",
                "id": 48
            },
            {
                "year": "2007",
                "id": 102
            },
            {
                "year": "2007",
                "id": 241
            },
            {
                "year": "2007",
                "id": 305
            },
            {
                "year": "2007",
                "id": 382
            },
            {
                "year": "2005",
                "id": 103
            },
            {
                "year": "2005",
                "id": 117
            }
        ]
    },
    {
        "title": "Towards a mathematical theory of primal sketch and sketchability",
        "authors": [
            "Cheng-en Guo",
            "Song-Chun Zhu",
            "Ying Nian Wu"
        ],
        "abstract": "In this paper, we present a mathematical theory for Marr's primal sketch. We first conduct a theoretical study of the descriptive Markov random field model and the generative wavelet/sparse coding model from the perspective of entropy and complexity. The competition between the two types of models defines the concept of \"sketchability\", which divides image into texture and geometry. We then propose a primal sketch model that integrates the two models and, in addition, a Gestalt field model for spatial organization. We also propose a sketching pursuit process that coordinates the competition between two pursuit algorithms: the matching pursuit (Mallat and Zhang, 1993) and the filter pursuit (Zhu, et al., 1997), that seek to explain the image by bases and filters respectively. The model can be used to learn a dictionary of image primitives, or textons in Julesz's language, for natural images. The primal sketch model is not only parsimonious for image representation, but produces meaningful sketches over a large number of generic images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238631",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 3,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Markov random fields",
                "Geometry",
                "Shape",
                "Pixel",
                "Statistics",
                "Solid modeling",
                "Matched filters",
                "Dictionaries",
                "Layout",
                "Codes"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image texture",
                "image representation",
                "image matching",
                "Markov processes",
                "random processes",
                "wavelet transforms",
                "sparse matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mathematical theory",
                "sketchability",
                "Marr primal sketch",
                "Markov random field model",
                "generative wavelet coding model",
                "sparse coding model",
                "entropy",
                "complexity",
                "texture",
                "geometry",
                "primal sketch model",
                "Gestalt field model",
                "spatial organization",
                "pursuit algorithms",
                "matching pursuit",
                "filter pursuit",
                "image base",
                "image filter",
                "image primitives",
                "Julesz language",
                "image representation",
                "computer vision"
            ]
        },
        "id": 160,
        "cited_by": [
            {
                "year": "2005",
                "id": 192
            }
        ]
    },
    {
        "title": "Dynamic texture segmentation",
        "authors": [
            "Doretto",
            "Cremers",
            "Favaro",
            "Soatto"
        ],
        "abstract": "We address the problem of segmenting a sequence of images of natural scenes into disjoint regions that are characterized by constant spatio-temporal statistics. We model the spatio-temporal dynamics in each region by Gauss-Markov models, and infer the model parameters as well as the boundary of the regions in a variational optimization framework. Numerical results demonstrate that - in contrast to purely texture-based segmentation schemes - our method is effective in segmenting regions that differ in their dynamics even when spatial statistics are identical.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238632",
        "reference_list": [
            {
                "year": "2001",
                "id": 89
            },
            {
                "year": "2001",
                "id": 163
            }
        ],
        "citation": {
            "ieee": 69,
            "other": 47,
            "total": 116
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Statistics",
                "Layout",
                "Vehicle dynamics",
                "Gaussian processes",
                "Marine vehicles",
                "Remotely operated vehicles",
                "Mobile robots",
                "Statistical distributions",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "image texture",
                "natural scenes",
                "Gaussian processes",
                "Markov processes",
                "variational techniques",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic texture segmentation",
                "image sequence",
                "natural scenes",
                "disjoint regions",
                "spatiotemporal statistics",
                "spatiotemporal dynamics",
                "Gauss-Markov models",
                "model parameters",
                "variational optimization framework",
                "numerical results",
                "spatial statistics"
            ]
        },
        "id": 161,
        "cited_by": [
            {
                "year": "2015",
                "id": 8
            },
            {
                "year": "2011",
                "id": 154
            },
            {
                "year": "2007",
                "id": 254
            },
            {
                "year": "2005",
                "id": 82
            }
        ]
    },
    {
        "title": "Learning and inferring image segmentations using the GBP typical cut algorithm",
        "authors": [
            "Shental",
            "Zomet",
            "Hertz",
            "Weiss"
        ],
        "abstract": "Significant progress in image segmentation has been made by viewing the problem in the framework of graph partitioning. In particular, spectral clustering methods such as \"normalized cuts\" (ncuts) can efficiently calculate good segmentations using eigenvector calculations. However, spectral methods when applied to images with local connectivity often oversegment homogenous regions. More importantly, they lack a straightforward probabilistic interpretation which makes it difficult to automatically set parameters using training data. In this paper we revisit the typical cut criterion proposed by Blatt et al. (1997) and Gdalyahu et al (2001). We show that computing the typical cut is equivalent to performing inference in an undirected graphical model. This equivalence allows us to use the powerful machinery of graphical models for learning and inferring image segmentations. For inferring segmentations we show that the generalized belief propagation (GBP) algorithm can give excellent results with a runtime that is usually faster than the ncut eigensolver. For learning segmentations we derive a maximum likelihood learning algorithm to learn affinity matrices from labelled datasets. We illustrate both learning and inference on challenging real and synthetic images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238633",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 11,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Graphical models",
                "Inference algorithms",
                "Clustering algorithms",
                "Partitioning algorithms",
                "Clustering methods",
                "Training data",
                "Machinery",
                "Machine learning",
                "Belief propagation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "learning (artificial intelligence)",
                "inference mechanisms",
                "graph colouring",
                "directed graphs"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "cut algorithm",
                "graph partitioning",
                "spectral clustering methods",
                "normalized cuts",
                "eigenvector calculations",
                "local connectivity",
                "oversegment homogenous regions",
                "probabilistic interpretation",
                "training data",
                "graphical model",
                "inferring segmentation",
                "generalized belief propagation",
                "GBP algorithm",
                "ncut eigensolver",
                "learning segmentation",
                "maximum likelihood learning algorithm",
                "affinity matrices",
                "real images",
                "synthetic images"
            ]
        },
        "id": 162,
        "cited_by": []
    },
    {
        "title": "Geometric segmentation of perspective images based on symmetry groups",
        "authors": [
            "Yang",
            "Shankar Rao",
            "Kun Huang",
            "Wei Hong",
            "Yi Ma"
        ],
        "abstract": "Symmetry is an effective geometric cue to facilitate conventional segmentation techniques on images of man-made environment. Based on three fundamental principles that summarize the relations between symmetry and perspective imaging, namely, structure from symmetry, symmetry hypothesis testing, and global symmetry testing, we develop a prototype system which is able to automatically segment symmetric objects in space from single 2D perspective images. The result of such a segmentation is a hierarchy of geometric primitives, called symmetry cells and complexes, whose 3D structure and pose are fully recovered. Such a geometrically meaningful segmentation may greatly facilitate applications such as feature matching and robot navigation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238634",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Automatic testing",
                "System testing",
                "Tiles",
                "Prototypes",
                "Orbital robotics",
                "Robotics and automation",
                "Navigation",
                "Humans",
                "Information geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "computational geometry",
                "feature extraction",
                "image matching",
                "symmetry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric segmentation",
                "symmetry groups",
                "image segmentation",
                "geometric cue",
                "man-made environment",
                "perspective imaging",
                "symmetry hypothesis testing",
                "prototype system",
                "2D perspective images",
                "geometric primitives",
                "symmetry cells",
                "symmetry complexes",
                "3D structure",
                "feature matching",
                "robot navigation"
            ]
        },
        "id": 163,
        "cited_by": []
    },
    {
        "title": "Natural image statistics for natural image segmentation",
        "authors": [
            "Heiler",
            "Schnorr"
        ],
        "abstract": "Building on recent progress in modeling filter response statistics of natural images we integrate a statistical model into a variational framework for image segmentation. Incorporated in a sound probabilistic distance measure the model drives level sets toward meaningful segmentations of complex textures and natural scenes. Since each region comprises two model parameters only the approach is computationally efficient and enables the application of variational segmentation to a considerably larger class of real-world images. We validate the statistical basis of our approach on thousands of natural images and demonstrate that our model outperforms recent variational segmentation methods based on second-order statistics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238635",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 6,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Layout",
                "Computer vision",
                "Parametric statistics",
                "Mathematical model",
                "Level set",
                "Bayesian methods",
                "Image databases",
                "Computer graphics",
                "Pattern recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "natural scenes",
                "image segmentation",
                "image texture",
                "feature extraction",
                "Bayes methods",
                "variational techniques",
                "statistics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "modeling filter response statistics",
                "natural images",
                "statistical model",
                "variational framework",
                "probabilistic distance measure",
                "complex textures",
                "natural scenes",
                "model parameters",
                "variational segmentation",
                "real-world images",
                "statistical basis",
                "second-order statistics",
                "computer vision",
                "shape modeling",
                "tracking",
                "appearance-based recognition",
                "Bayesan inference"
            ]
        },
        "id": 164,
        "cited_by": []
    },
    {
        "title": "Unsupervised non-parametric region segmentation using level sets",
        "authors": [
            "Kadir",
            "Brady"
        ],
        "abstract": "We present a novel non-parametric unsupervised segmentation algorithm based on region competition (Zhu and Yuille, 1996); but implemented within a level sets framework (Osher and Sethian, 1988). The key novelty of the algorithm is that it can solve N /spl ges/ 2 class segmentation problems using just one embedded surface; this is achieved by controlling the merging and splitting behaviour of the level sets according to a minimum description length (MDL) (Leclerc (1989) and Rissanen (1985)) cost function. This is in contrast to N class region-based level set segmentation methods to date which operate by evolving multiple coupled embedded surfaces in parallel (Chan et al., 2002). Furthermore, it operates in an unsupervised manner; it is necessary neither to specify the value of N nor the class models a-priori. We argue that the level sets methodology provides a more convenient framework for the implementation of the region competition algorithm, which is conventionally implemented using region membership arrays due to the lack of a intrinsic curve representation. Finally, we generalise the Gaussian region model used in standard region competition to the non-parametric case. The region boundary motion and merge equations become simple expressions containing cross-entropy and entropy terms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238636",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 20,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Level set",
                "Image segmentation",
                "Merging",
                "Computer vision",
                "Cost function",
                "Equations",
                "Entropy",
                "Application software",
                "Active contours",
                "Topology"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "nonparametric statistics",
                "unsupervised learning",
                "edge detection",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric region segmentation",
                "image segmentation",
                "segmentation algorithm",
                "level sets framework",
                "class segmentation problems",
                "embedded surface",
                "splitting behaviour",
                "minimum description length",
                "MDL cost function",
                "region-based level set segmentation",
                "multiple coupled embedded surfaces",
                "region competition algorithm",
                "region membership arrays",
                "intrinsic curve representation",
                "Gaussian region model",
                "region boundary motion",
                "merge equations",
                "cross-entropy",
                "computer vision",
                "unsupervised segmentation"
            ]
        },
        "id": 165,
        "cited_by": [
            {
                "year": "2009",
                "id": 100
            }
        ]
    },
    {
        "title": "Computing MAP trajectories by representing, propagating and combining PDFs over groups",
        "authors": [
            "Smith",
            "Drummond",
            "Roussopoulos"
        ],
        "abstract": "This paper addresses the problem of computing the trajectory of a camera from sparse positional measurements that have been obtained from visual localisation, and dense differential measurements from odometry or inertial sensors. A fast method is presented for fusing these two sources of information to obtain the maximum a posteriori estimate of the trajectory. A formalism is introduced for representing probability density functions over Euclidean transformations, and it is shown how these density functions can be propagated along the data sequence and how multiple estimates of a transformation can be combined. A three-pass algorithm is described which makes use of these results to yield the trajectory of the camera. Simulation results are presented which are validated against a physical analogue of the vision problem, and results are then shown from sequences of approximately 1,800 frames captured from a video camera mounted on a go-kart. Several of these frames are processed using computer vision to obtain estimates of the position of the go-kart. The algorithm fuses these estimates with odometry from the entire sequence in 150 ms to obtain the trajectory of the kart.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238637",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Cameras",
                "Position measurement",
                "Information resources",
                "Maximum a posteriori estimation",
                "Probability density function",
                "Density functional theory",
                "Computational modeling",
                "Computer vision",
                "Fuses"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "object detection",
                "probability",
                "image sequences",
                "video cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "MAP trajectories",
                "PDF",
                "camera trajectory",
                "sparse positional measurements",
                "visual localisation",
                "differential measurements",
                "odometry",
                "inertial sensors",
                "a posteriori estimate",
                "probability density functions",
                "Euclidean transformations",
                "data sequence",
                "three-pass algorithm",
                "physical analogue",
                "vision problem",
                "camera frames",
                "video camera",
                "computer vision",
                "go-kart trajectory",
                "150 ms"
            ]
        },
        "id": 166,
        "cited_by": []
    },
    {
        "title": "Markov-based failure prediction for human motion analysis",
        "authors": [
            "Dockstader",
            "Imennov",
            "Tekalp"
        ],
        "abstract": "This paper presents a new method of detecting and predicting motion tracking failures with applications in human motion and gait analysis. We define a tracking failure as an event and describe its temporal characteristics using a hidden Markov model (HMM). This stochastic model is trained using previous examples of tracking failures. We derive vector observations for the HMM using the noise covariance matrices characterizing a tracked, 3D structural model of the human body. We show a causal relationship between the conditional output probability of the HMM, as transformed using a logarithmic mapping function, and impending tracking failures. Results are illustrated on several multi-view sequences of complex human motion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238638",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Motion analysis",
                "Hidden Markov models",
                "Tracking",
                "Biological system modeling",
                "Biomedical engineering",
                "Motion detection",
                "Robustness",
                "Predictive models",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "hidden Markov models",
                "gait analysis",
                "object detection",
                "tracking",
                "stochastic processes",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Markov-based failure prediction",
                "human motion analysis",
                "motion tracking failures",
                "computer vision",
                "gait analysis",
                "failure tracking",
                "temporal characteristics",
                "hidden Markov model",
                "HMM",
                "stochastic model",
                "vector observations",
                "noise covariance matrices",
                "3D structural model",
                "human body",
                "conditional output probability",
                "logarithmic mapping function",
                "multiview sequences"
            ]
        },
        "id": 167,
        "cited_by": []
    },
    {
        "title": "SVM-based nonparametric discriminant analysis, an application to face detection",
        "authors": [
            "Fransens",
            "De Prins",
            "Van Gool"
        ],
        "abstract": "Detecting the dominant normal directions to the decision surface is an established technique for feature selection in high dimensional classification problems. Several approaches have been proposed to render this strategy more amenable to practice, but they still show a number of important shortcomings from a pragmatic point of view. This paper introduces a novel such approach, which combines the normal directions idea with support vector machine classifiers. The two make a natural and powerful match, as SVs are located nearby, and fully describe the decision surfaces. The approach can be included elegantly into the training of performant classifiers from extensive datasets. The potential is corroborated by experiments, both on synthetic and real data, the latter on a face detection experiment. In this experiment we demonstrate how our approach can lead to a significant reduction of CPU-time, with neglectable loss of classification performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238639",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 12,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Face detection",
                "Linear discriminant analysis",
                "Feature extraction",
                "Application software",
                "Computer vision",
                "Support vector machines",
                "Support vector machine classification",
                "Performance loss",
                "Probability density function",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "support vector machines",
                "nonparametric statistics",
                "feature extraction",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric discriminant analysis",
                "face detection",
                "normal directions",
                "decision surface",
                "feature selection",
                "high dimensional classification",
                "support vector machine",
                "SVM",
                "image classification",
                "linear discriminant analysis",
                "linear feature",
                "feature extraction"
            ]
        },
        "id": 168,
        "cited_by": []
    },
    {
        "title": "Facial expression understanding in image sequences using dynamic and active visual information fusion",
        "authors": [
            "Yongmian Zhang",
            "Qiang Ji"
        ],
        "abstract": "This paper explores the use of multisensory information fusion technique with dynamic Bayesian networks (DBNs) for modeling and understanding the temporal behaviors of facial expressions in image sequences. Our approach to the facial expression understanding lies in a probabilistic framework by integrating the DBNs with the facial action units (AUs) from psychological view. The DBNs provide a coherent and unified hierarchical probabilistic framework to represent spatial and temporal information related to facial expressions, and to actively select the most informative visual cues from the available information to minimize the ambiguity in recognition. The recognition of facial expressions is accomplished by fusing not only from the current visual observations, but also from the previous visual evidences. Consequently, the recognition becomes more robust and accurate through modeling the temporal behavior of facial expressions. Experimental results demonstrate that our approach is more admissible for facial expression analysis in image sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238640",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Image sequences",
                "Face recognition",
                "Humans",
                "Hidden Markov models",
                "Image analysis",
                "Facial features",
                "Emotion recognition",
                "Motion measurement",
                "Tracking",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "emotion recognition",
                "face recognition",
                "image sequences",
                "image representation",
                "feature extraction",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial expression",
                "image sequences",
                "dynamic visual information",
                "active visual information",
                "multisensory information fusion",
                "dynamic Bayesian networks",
                "DBN",
                "temporal behavior",
                "facial action unit",
                "psychological view",
                "hierarchical probabilistic framework",
                "spatial information",
                "temporal information",
                "visual cues",
                "image recognition",
                "visual observations",
                "facial features",
                "computer vision",
                "spatiotemporal analysis"
            ]
        },
        "id": 169,
        "cited_by": []
    },
    {
        "title": "Background modeling and subtraction of dynamic scenes",
        "authors": [
            "Monnet",
            "Mittal",
            "Paragios",
            "Visvanathan Ramesh"
        ],
        "abstract": "Background modeling and subtraction is a core component in motion analysis. The central idea behind such module is to create a probabilistic representation of the static scene that is compared with the current input to perform subtraction. Such approach is efficient when the scene to be modeled refers to a static structure with limited perturbation. In this paper, we address the problem of modeling dynamic scenes where the assumption of a static background is not valid. Waving trees, beaches, escalators, natural scenes with rain or snow are examples. Inspired by the work proposed by Doretto et al. (2003), we propose an on-line auto-regressive model to capture and predict the behavior of such scenes. Towards detection of events we introduce a new metric that is based on a state-driven comparison between the prediction and the actual frame. Promising results demonstrate the potentials of the proposed framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238641",
        "reference_list": [
            {
                "year": "2001",
                "id": 49
            },
            {
                "year": "2001",
                "id": 169
            },
            {
                "year": "2001",
                "id": 163
            },
            {
                "year": "2001",
                "id": 40
            }
        ],
        "citation": {
            "ieee": 131,
            "other": 101,
            "total": 232
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Predictive models",
                "Vehicle dynamics",
                "Motion analysis",
                "Rain",
                "Lighting",
                "Educational institutions",
                "Snow",
                "Event detection",
                "Performance analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "feature extraction",
                "image representation",
                "object detection",
                "natural scenes",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time video analysis",
                "computer vision",
                "background modeling",
                "background subtraction",
                "dynamic scenes",
                "motion analysis",
                "scene modeling",
                "probabilistic representation",
                "static scene",
                "static structure",
                "perturbation",
                "static background",
                "waving trees",
                "natural scenes",
                "on-line auto-regressive model",
                "image detection",
                "state-driven comparison"
            ]
        },
        "id": 170,
        "cited_by": [
            {
                "year": "2009",
                "id": 8
            },
            {
                "year": "2009",
                "id": 156
            },
            {
                "year": "2009",
                "id": 266
            },
            {
                "year": "2007",
                "id": 123
            }
        ]
    },
    {
        "title": "Surface reconstruction by integrating 3D and 2D data of multiple views",
        "authors": [
            "Lhuillier",
            "Long Quan"
        ],
        "abstract": "Surface representation is needed for almost all modeling and visualization applications, but unfortunately, 3D data from a passive vision system are often insufficient for a traditional surface reconstruction technique that is designed for densely scanned 3D point data. In this paper, we develop a new method for surface reconstruction by combining both 3D data and 2D image information. The silhouette information extracted from 2D images can also be integrated as an option if it is available. The new method is a variational approach with a new functional integrating 3D stereo data with 2D image information. This gives a more robust approach than existing methods using only pure 2D information or 3D stereo data. We also propose a bounded regularization method to implement efficiently the surface evolution by level-set methods. The properties of the algorithms are discussed, proved for some cases, and empirically demonstrated through intensive experiments on real sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238642",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 11,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Stereo vision",
                "Surface fitting",
                "Computer vision",
                "Layout",
                "Data visualization",
                "Application software",
                "Cameras",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing",
                "image reconstruction",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface reconstruction",
                "multiple views",
                "surface representation",
                "visualization",
                "passive vision system",
                "2D image information",
                "silhouette information",
                "variational approach",
                "3D stereo data",
                "bounded regularization method",
                "surface evolution",
                "level-set methods",
                "image sequences"
            ]
        },
        "id": 171,
        "cited_by": [
            {
                "year": "2011",
                "id": 257
            },
            {
                "year": "2007",
                "id": 142
            },
            {
                "year": "2005",
                "id": 161
            }
        ]
    },
    {
        "title": "Recovery of epipolar geometry as a manifold fitting problem",
        "authors": [
            "Goshen",
            "Shimshoni",
            "Anandan",
            "Keren"
        ],
        "abstract": "The introduction of the joint image manifold allows to treat the problem of recovering camera motion and epipolar geometry as the problem of fitting a manifold to the data measured in a stereo pair. The manifold has a singularity and boundary, therefore care must be taken when fitting it. This paper reviews the notion of joint image manifold, and how previous motion recovery methods can be viewed in its context, and then offers a new fitting method, which improves upon previous results, especially when the extent of the data and/or the motion are small.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238643",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Stereo vision",
                "Manifolds",
                "Cameras",
                "Computer vision",
                "Polynomials",
                "Fitting",
                "Industrial engineering",
                "Computer science",
                "Computational geometry",
                "Motion measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "motion compensation",
                "stereo image processing",
                "image matching",
                "computational geometry",
                "cameras",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "epipolar geometry",
                "manifold fitting problem",
                "camera motion recovery",
                "stereo pair",
                "singularity",
                "motion recovery methods",
                "computer vision",
                "3D structure",
                "joint image fitting",
                "geometric method",
                "maximum likelihood method",
                "algebraic method"
            ]
        },
        "id": 172,
        "cited_by": [
            {
                "year": "2005",
                "id": 62
            }
        ]
    },
    {
        "title": "Assessing accuracy factors in deformable 2D/3D medical image registration using a statistical pelvis model",
        "authors": [
            "Jianhua Yao",
            "Taylor"
        ],
        "abstract": "Deformable 2D-3D medical image registration is an essential technique in computer integrated surgery (CIS) to fuse 3D pre-operative data with 2D intra-operative data. Several factors may affect the accuracy of 2D-3D registration, including the number of 2D views, the angle between views, the view angle relative to anatomical objects, the co-registration error between views, the image noise, and the image distortion. In this paper, we investigate and assess the relationship between these factors and the accuracy of 2D-3D registration. We proposed a deformable 2D-3D registration method based on a statistical model. We conducted experiments using a hemi-pelvis model and simulated X-ray images. Some discussions are provided on how to improve the accuracy of 2D-3D registration based on our assessment.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238644",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical imaging",
                "Image registration",
                "Pelvis",
                "Deformable models",
                "X-ray imaging",
                "Surgery",
                "Medical diagnostic imaging",
                "Computational Intelligence Society",
                "Computed tomography",
                "Fuses"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "surgery",
                "diagnostic radiography",
                "medical image processing",
                "image registration",
                "statistical analysis",
                "bone",
                "X-ray imaging"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "accuracy assessment",
                "statistical pelvis model",
                "computer integrated surgery",
                "3D preoperative data",
                "2D intraoperative data",
                "deformable 2D-3D medical image registration",
                "view angle",
                "anatomical objects",
                "co-registration error",
                "image noise",
                "image distortion",
                "hemi-pelvis model",
                "simulated X-ray images"
            ]
        },
        "id": 173,
        "cited_by": [
            {
                "year": "2011",
                "id": 107
            }
        ]
    },
    {
        "title": "Stochastic refinement of the visual hull to satisfy photometric and silhouette consistency constraints",
        "authors": [
            "Isidro",
            "Sclaroff"
        ],
        "abstract": "An iterative method for reconstructing a 3D polygonal mesh and color texture map from multiple views of an object is presented. In each iteration, the method first estimates a texture map given the current shape estimate. The texture map and its associated residual error image are obtained via maximum a posteriori estimation and reprojection of the multiple views into texture space. Next, the surface shape is adjusted to minimize residual error in texture space. The surface is deformed towards a photometrically-consistent solution via a series of 1D epipolar searches at randomly selected surface points. The texture space formulation has improved computational complexity over standard image-based error approaches, and allows computation of the reprojection error and uncertainty for any point on the surface. Moreover, shape adjustments can be constrained such that the recovered model's silhouette matches those of the input images. Experiments with real world imagery demonstrate the validity of the approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238645",
        "reference_list": [
            {
                "year": "2001",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 19,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Photometry",
                "Image reconstruction",
                "Shape",
                "Surface reconstruction",
                "Surface texture",
                "Application software",
                "Cameras",
                "Uncertainty",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "image texture",
                "image colour analysis",
                "mesh generation",
                "stereo image processing",
                "maximum likelihood estimation",
                "edge detection",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stochastic refinement",
                "visual hull",
                "photometric constraints",
                "silhouette consistency constraints",
                "3D polygonal mesh",
                "color texture map",
                "multiple views",
                "current shape estimate",
                "residual error image",
                "maximum a posteriori estimation",
                "MAP framework",
                "surface shape",
                "texture space",
                "1D epipolar searches",
                "surface points",
                "computational complexity",
                "image-based error approach",
                "shape adjustments",
                "silhouette matches",
                "real world imagery",
                "computer vision",
                "image reconstruction"
            ]
        },
        "id": 174,
        "cited_by": [
            {
                "year": "2005",
                "id": 45
            },
            {
                "year": "2005",
                "id": 161
            },
            {
                "year": "2005",
                "id": 213
            },
            {
                "year": "2005",
                "id": 228
            }
        ]
    },
    {
        "title": "Capturing subtle facial motions in 3D face tracking",
        "authors": [
            "Zhen Wen",
            "Huang"
        ],
        "abstract": "Facial motions produce not only facial feature points motions, but also subtle appearance changes such as wrinkles and shading changes. These subtle changes are important yet difficult issues for both analysis (tracking) and synthesis (animation). Previous approaches were mostly based on models learned from extensive training appearance examples. However, the space of all possible facial motion appearance is huge. Thus, it is not feasible to collect samples covering all possible variations due to lighting conditions, individualities, and head poses. Therefore, it is difficult to adapt such models to new conditions. In this paper, we present an adaptive technique for analyzing subtle facial appearance changes. We propose a new ratio-image based appearance feature, which is independent of a person's face albedo. This feature is used to track face appearance variations based on exemplars. To adapt the exemplar appearance model to new people and lighting conditions, we develop an online EM-based algorithm. Experiments show that the proposed method improves classification results in a facial expression recognition task, where a variety of people and lighting conditions are involved.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238646",
        "reference_list": [
            {
                "year": "2001",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 15,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Shape",
                "Head",
                "Principal component analysis",
                "Face recognition",
                "Robustness",
                "Active appearance model",
                "Facial features",
                "Animation",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "feature extraction",
                "image texture",
                "image colour analysis",
                "gesture recognition",
                "image classification",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subtle facial motions",
                "3D face tracking",
                "facial feature point motions",
                "wrinkles",
                "shading changes",
                "image analysis",
                "image tracking",
                "animation",
                "facial motion appearance",
                "lighting conditions",
                "head poses",
                "adaptive technique",
                "appearance feature",
                "face albedo",
                "exemplar appearance model",
                "online EM-based algorithm",
                "image classification",
                "facial expression",
                "image recognition",
                "computer vision"
            ]
        },
        "id": 175,
        "cited_by": [
            {
                "year": "2005",
                "id": 50
            }
        ]
    },
    {
        "title": "Catadioptric camera calibration using geometric invariants",
        "authors": [
            "Xianghua Ying",
            "Zhanyi Hu"
        ],
        "abstract": "Central catadioptric cameras are imaging devices that use mirrors to enhance the field of view while preserving a single effective viewpoint. In this paper, we propose a novel method for the calibration of central catadioptric cameras using geometric invariants. Lines in space are projected into conics in the catadioptric image plane as well as spheres in space. We proved that the projection of a line can provide three invariants whereas the projection of a sphere can provide two. From these invariants, constraint equations for the intrinsic parameters of catadioptric camera are derived. Therefore, there are two variants of this novel method. The first one uses the projections of lines and the second one uses the projections of spheres. In general, the projections of two lines or three spheres are sufficient to achieve the catadioptric camera calibration. One important observation in this paper is that the method based on the projections of spheres is more robust and has higher accuracy than that using the projections of lines. The performances of our method are demonstrated by the results of simulations and experiments with real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238647",
        "reference_list": [
            {
                "year": "2001",
                "id": 16
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Robot vision systems",
                "Mirrors",
                "Computer vision",
                "Layout",
                "Laboratories",
                "Pattern recognition",
                "Automation",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "computational geometry",
                "calibration",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "catadioptric camera calibration",
                "geometric invariants",
                "imaging devices",
                "mirrors",
                "field of view",
                "single effective viewpoint",
                "central catadioptric cameras",
                "conics",
                "catadioptric image plane",
                "spheres",
                "constraint equations",
                "intrinsic parameters",
                "real images",
                "computer vision",
                "robot navigation",
                "virtual reality",
                "image-based rendering"
            ]
        },
        "id": 176,
        "cited_by": [
            {
                "year": "2005",
                "id": 200
            }
        ]
    },
    {
        "title": "Paracatadioptric camera calibration using lines",
        "authors": [
            "Barreto",
            "Araujo"
        ],
        "abstract": "Paracatadioptric sensors combine a parabolic shaped mirror and a camera inducing an orthographic projection. Such a configuration provides a wide field of view while keeping a single effective viewpoint. Previous work in central catadioptric sensors proved that a line projects into a conic curve and that three line images are enough to calibrate the system. However the estimation of the conic curves where lines are mapped is hard to accomplish. In general only a small arc of the conic is visible in the image and conventional conic fitting techniques are unable to correctly estimate the curve. The present work shows that a set of conic curves corresponds to paracatadioptric line images if, and only if, certain properties are verified. These properties are used to constraint the search space and correctly estimate the curves. The accurate estimation of a minimum of three line images allows the complete calibration of the paracatadioptric camera. If the camera is skewless and the aspect ratio is known then the conic fitting problem is solved naturally by an eigensystem. For the general situation the conic curves are estimated using non-linear optimization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238648",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 10,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Mirrors",
                "Sensor systems",
                "Robot vision systems",
                "Image sensors",
                "Robot sensing systems",
                "Curve fitting",
                "Computer vision",
                "Focusing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "computational geometry",
                "calibration",
                "curve fitting",
                "cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "paracatadioptric camera calibration",
                "paracatadioptric sensors",
                "parabolic shaped mirror",
                "orthographic projection",
                "field of view",
                "single effective viewpoint",
                "central catadioptric sensors",
                "line projection",
                "conic curve",
                "conic fitting techniques",
                "paracatadioptric line images",
                "aspect ratio",
                "eigensystem",
                "nonlinear optimization",
                "omnidirectional vision",
                "computer vision",
                "mapping model",
                "image formation model",
                "calibration algorithm"
            ]
        },
        "id": 177,
        "cited_by": [
            {
                "year": "2009",
                "id": 153
            }
        ]
    },
    {
        "title": "Multiple-cue illumination estimation in textured scenes",
        "authors": [
            "Yuanzhen Li",
            "Lin",
            "Hanqing Lu",
            "Heung-Yeung Shum"
        ],
        "abstract": "In this paper, we present a method that integrates cues from shading, shadow and specular reflections for estimating directional illumination in a textured scene. Texture poses a problem for lighting estimation, since texture edges can be mistaken for changes in illumination condition, and unknown variations in albedo make reflectance model fitting impractical. Unlike previous works which all assume known or uniform reflectance, our method can deal with the effects of textures by capitalizing on physical consistencies that exist among the lighting cues. Since scene textures do not exhibit such coherence, we use this property to minimize the influence of texture on illumination direction estimation. For the recovered light source directions, a technique for estimating their intensities in the presence of texture is also proposed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238649",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 0,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Layout",
                "Optical reflection",
                "Reflectivity",
                "Light sources",
                "Computer graphics",
                "Equations",
                "Image edge detection",
                "Asia",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image texture",
                "edge detection",
                "spatial variables measurement",
                "lighting",
                "light sources",
                "albedo"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple-cue illumination estimation",
                "textured scenes",
                "integrates cue",
                "shading",
                "shadow",
                "specular reflections",
                "directional illumination",
                "lighting estimation",
                "texture edges",
                "albedo",
                "reflectance model fitting",
                "uniform reflectance",
                "physical consistencies",
                "illumination direction estimation",
                "light source directions",
                "image variation",
                "computer vision"
            ]
        },
        "id": 178,
        "cited_by": [
            {
                "year": "2005",
                "id": 34
            },
            {
                "year": "2005",
                "id": 212
            }
        ]
    },
    {
        "title": "A novel approach for texture shape recovery",
        "authors": [
            "Jing Wang",
            "Dana"
        ],
        "abstract": "In vision and graphics, there is a sustained interest in capturing accurate 3D shape with various scanning devices. However, the resulting geometric representation is only part of the story. Surface texture of real objects is also an important component of the representation and fine-scale surface geometry such as surface markings, roughness, and imprints, are essential in highly realistic rendering and accurate prediction. We present a novel approach for measuring the fine-scale surface shape of specular surfaces using a curved mirror to view multiple angles in a single image. A distinguishing aspect of our method is that it is designed for specular surfaces, unlike many methods (e.g. laser scanning) which cannot handle highly specular objects. Also, the spatial resolution is very high so that it can resolve very small surface details that are beyond the resolution of standard devices. Furthermore, our approach incorporates the simultaneous use of a bidirectional texture measurement method, so that spatially varying bidirectional reflectance is measured at the same time as surface shape.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238650",
        "reference_list": [
            {
                "year": "2001",
                "id": 166
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 2,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Rough surfaces",
                "Surface roughness",
                "Shape measurement",
                "Surface texture",
                "Spatial resolution",
                "Time measurement",
                "Graphics",
                "Geometry",
                "Rendering (computer graphics)",
                "Mirrors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "image reconstruction",
                "image texture",
                "computational geometry",
                "rendering (computer graphics)",
                "realistic images",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "texture shape recovery",
                "computer vision",
                "computer graphics",
                "3D shape",
                "scanning devices",
                "geometric representation",
                "surface texture",
                "real objects",
                "fine-scale surface geometry",
                "surface markings",
                "surface roughness",
                "realistic rendering",
                "accurate prediction",
                "fine-scale surface shape",
                "specular surfaces",
                "curved mirror",
                "multiple angles",
                "laser scanning",
                "spatial resolution",
                "bidirectional texture measurement",
                "bidirectional reflectance"
            ]
        },
        "id": 179,
        "cited_by": []
    },
    {
        "title": "Polarization-based transparent surface modeling from two views",
        "authors": [
            "Miyazaki",
            "Kagesawa",
            "Ikeuchi"
        ],
        "abstract": "In this paper, we propose a novel method to recover the surface shape of transparent objects. The degree of polarization of the light reflected from the object surface depends on the reflection angle which, in turn, depends on the object's surface normal; thus, by measuring the degree of polarization, we are able to calculate the surface normal of the object. However, degree of polarization and surface normal does not correspond one-to-one, making us to analyze two polarization images taken from two different view in order to solve the ambiguity. A parabolic curve will be a strong clue to correspond a point in one image to a point in the other image, where both points represent the same point on object surface. By comparing the degree of polarization at such corresponding points, the true surface normal can be determined.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238651",
        "reference_list": [
            {
                "year": "2003",
                "id": 129
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical polarization",
                "Shape measurement",
                "Image analysis",
                "Optical reflection",
                "Glass",
                "Computer vision",
                "Surface waves",
                "Optical refraction",
                "Genetic algorithms",
                "Information analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "image representation",
                "curve fitting",
                "object detection",
                "polarisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "polarization images",
                "transparent surface modeling",
                "surface shape",
                "reflected light",
                "object surface",
                "reflection angle",
                "parabolic curve",
                "computer vision",
                "opaque object",
                "realistic image",
                "3D shape",
                "reflected image",
                "genetic algorithm",
                "surface normal"
            ]
        },
        "id": 180,
        "cited_by": [
            {
                "year": "2005",
                "id": 184
            },
            {
                "year": "2003",
                "id": 129
            }
        ]
    },
    {
        "title": "A class of photometric invariants: separating material from shape and illumination",
        "authors": [
            "Narasimhan",
            "Visvanathan Ramesh",
            "Nayar"
        ],
        "abstract": "We derive a new class of photometric invariants that can be used for a variety of vision tasks including lighting invariant material segmentation, change detection and tracking, as well as material invariant shape recognition. The key idea is the formulation of a scene radiance model for the class of \"separable\" BRDFs, that can be decomposed into material related terms and object shape and lighting related terms. All the proposed invariants are simple rational functions of the appearance parameters (say, material or shape and lighting). The invariants in this class differ from one another in the number and type of image measurements they require. Most of the invariants in this class need changes in illumination or object position between image acquisitions. The invariants can handle large changes in lighting which pose problems for most existing vision algorithms. We demonstrate the power of these invariants using scenes with complex shapes, materials, textures, shadows and specularities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238652",
        "reference_list": [
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 6,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Shape",
                "Lighting",
                "Layout",
                "Geometry",
                "Material properties",
                "Optical reflection",
                "Reflectivity",
                "Computer science",
                "Parameter estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "image recognition",
                "image texture",
                "lighting",
                "optical tracking",
                "shape measurement",
                "position measurement"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photometric invariants",
                "material separation",
                "illumination",
                "computer vision",
                "lighting",
                "invariant material segmentation",
                "change detection",
                "tracking",
                "invariant shape recognition",
                "scene radiance model",
                "separable BRDF",
                "object shape",
                "rational functions",
                "appearance parameters",
                "image measurements",
                "object position",
                "image acquisitions",
                "vision algorithms",
                "textures",
                "shadows",
                "specularities"
            ]
        },
        "id": 181,
        "cited_by": [
            {
                "year": "2017",
                "id": 240
            },
            {
                "year": "2005",
                "id": 224
            }
        ]
    },
    {
        "title": "Towards direct recovery of shape and motion parameters from image sequences",
        "authors": [
            "Benoit",
            "Ferrie"
        ],
        "abstract": "A novel procedure is presented to construct image-domain filters (receptive fields) that directly recover local motion and shape parameters. These receptive fields are derived from training on image deformations that best discriminate between different shape and motion parameters. Beginning with the construction of 1D receptive fields that detect local surface shape and motion parameters within cross sections, we show how the recovered shape and motion model parameters are sufficient to produce local estimates of time to collision. In general, filter pairs (receptive fields) can be synthesized to perform or detect specific image deformations. At the heart of the method is the use of a matrix to represent image deformation correspondence between individual pixels of two views of a surface. The image correspondence matrix can be decomposed using singular value decomposition to yield a pair of corresponding receptive fields that detect image changes due to the deformation of interest.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238653",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image sequences",
                "Matrix decomposition",
                "Layout",
                "Deformable models",
                "Optical surface waves",
                "Optical computing",
                "Nonlinear optics",
                "Optical filters",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "image sequences",
                "singular value decomposition",
                "filtering theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape parameter recovery",
                "motion parameter recovery",
                "image sequences",
                "image-domain filters",
                "local motion recovery",
                "shape parameters",
                "image deformations",
                "1D receptive fields",
                "local surface shape",
                "cross sections",
                "model parameters",
                "local estimates",
                "filter pairs",
                "image representation",
                "image correspondence matrix",
                "singular value decomposition",
                "image changes",
                "feature point extraction",
                "computer vision"
            ]
        },
        "id": 182,
        "cited_by": []
    },
    {
        "title": "Real-time simultaneous localisation and mapping with a single camera",
        "authors": [
            "Davison"
        ],
        "abstract": "Ego-motion estimation for an agile single camera moving through general, unknown scenes becomes a much more challenging problem when real-time performance is required rather than under the off-line processing conditions under which most successful structure from motion work has been achieved. This task of estimating camera motion from measurements of a continuously expanding set of self-mapped visual features is one of a class of problems known as Simultaneous Localisation and Mapping (SLAM) in the robotics community, and we argue that such real-time mapping research, despite rarely being camera-based, is more relevant here than off-line structure from motion methods due to the more fundamental emphasis placed on propagation of uncertainty. We present a top-down Bayesian framework for single-camera localisation via mapping of a sparse set of natural features using motion modelling and an information-guided active measurement strategy, in particular addressing the difficult issue of real-time feature initialisation via a factored sampling approach. Real-time handling of uncertainty permits robust localisation via the creating and active measurement of a sparse map of landmarks such that regions can be re-visited after periods of neglect and localisation can continue through periods when few features are visible. Results are presented of real-time localisation for a hand-waved camera with very sparse prior scene knowledge and all processing carried out on a desktop PC.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238654",
        "reference_list": [],
        "citation": {
            "ieee": 530,
            "other": 294,
            "total": 824
        },
        "keywords": {
            "IEEE Keywords": [
                "Simultaneous localization and mapping",
                "Cameras",
                "Motion estimation",
                "Layout",
                "Motion measurement",
                "Robot vision systems",
                "Bayesian methods",
                "Particle measurements",
                "Sampling methods",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "motion estimation",
                "real-time systems",
                "feature extraction",
                "video cameras",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single camera",
                "ego-motion estimation",
                "real-time performance",
                "off-line processing",
                "camera motion",
                "self-mapped visual features",
                "simultaneous localisation and mapping",
                "SLAM",
                "robotics",
                "real-time mapping research",
                "uncertainty propagation",
                "Bayesian framework",
                "single-camera localisation",
                "natural features",
                "motion modelling",
                "active measurement strategy",
                "factored sampling approach",
                "robust localisation",
                "real-time localisation",
                "hand-waved camera",
                "scene knowledge",
                "desktop PC",
                "real-time processing",
                "computer vision"
            ]
        },
        "id": 183,
        "cited_by": [
            {
                "year": "2013",
                "id": 58
            },
            {
                "year": "2009",
                "id": 181
            },
            {
                "year": "2009",
                "id": 197
            },
            {
                "year": "2007",
                "id": 154
            },
            {
                "year": "2007",
                "id": 258
            },
            {
                "year": "2007",
                "id": 307
            },
            {
                "year": "2005",
                "id": 195
            }
        ]
    },
    {
        "title": "Binocular Helmholtz stereopsis",
        "authors": [
            "Zickler",
            "Ho",
            "Kriegman",
            "Ponce",
            "Belhumeur"
        ],
        "abstract": "Helmholtz stereopsis has been introduced recently as a surface reconstruction technique that does not assume a model of surface reflectance. In the reported formulation, correspondence was established using a rank constraint, necessitating at least three viewpoints and three pairs of images. Here, it is revealed that the fundamental Helmholtz stereopsis constraint defines a nonlinear partial differential equation, which can be solved using only two images. It is shown that, unlike conventional stereo, binocular Helmholtz stereopsis is able to establish correspondence (and thereby recover surface depth) for objects having an arbitrary and unknown BRDF and in textureless regions (i.e., regions of constant or slowly varying BRDF). An implementation and experimental results validate the method for specular surfaces with and without texture.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238655",
        "reference_list": [
            {
                "year": "2001",
                "id": 157
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 8,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Reflectivity",
                "Stereo vision",
                "Computer science",
                "Surface texture",
                "Shape",
                "Partial differential equations",
                "Light sources",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "image texture",
                "stereo image processing",
                "partial differential equations",
                "visual perception",
                "shape measurement"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "binocular Helmholtz stereopsis",
                "surface reconstruction technique",
                "surface reflectance",
                "nonlinear partial differential equation",
                "conventional stereo",
                "surface depth recovery",
                "BRDF",
                "textureless regions",
                "specular surfaces",
                "surface shape estimation",
                "bidirectional reflectance distribution function",
                "Helmholtz reciprocity"
            ]
        },
        "id": 184,
        "cited_by": [
            {
                "year": "2005",
                "id": 56
            }
        ]
    },
    {
        "title": "Camera calibration with known rotation",
        "authors": [
            "Frahm",
            "Koch"
        ],
        "abstract": "We address the problem of using external rotation information with uncalibrated video sequences. The main problem addressed is, what is the benefit of the orientation information for camera calibration? It is shown that in case of a rotating camera the camera calibration problem is linear even in the case that all intrinsic parameters vary. For arbitrarily moving cameras the calibration problem is also linear but underdetermined for the general case of varying all intrinsic parameters. However, if certain constraints are applied to the intrinsic parameters the camera calibration can be computed linearly. It is analyzed which constraints are needed for camera calibration of freely moving cameras. Furthermore we address the problem of aligning the camera data with the rotation sensor data in time. We give an approach to align these data in case of a rotating camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238656",
        "reference_list": [
            {
                "year": "2001",
                "id": 113
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 11,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Layout",
                "Robot vision systems",
                "Surveillance",
                "Computer vision",
                "Information processing",
                "Computer science",
                "Mathematics",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "image reconstruction",
                "calibration",
                "video signal processing",
                "video cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera calibration",
                "camera rotation",
                "external rotation information",
                "uncalibrated video sequences",
                "orientation information",
                "intrinsic parameters",
                "freely moving cameras",
                "camera data",
                "rotation sensor data",
                "scene analysis",
                "image sequence",
                "video camera",
                "computer vision",
                "self-calibration"
            ]
        },
        "id": 185,
        "cited_by": []
    },
    {
        "title": "Globally convergent autocalibration",
        "authors": [
            "Benedetti",
            "Busti",
            "Farenzena",
            "Fusiello"
        ],
        "abstract": "Existing autocalibration techniques use numerical optimization algorithms that are prone to the problem of local minima. To address this problem, we have developed a method where an interval branch-and-bound method is employed for numerical minimization. Thanks to the properties of interval analysis this method is guaranteed to converge to the global solution with mathematical certainty and arbitrary accuracy, and the only input information it requires from the user is a set of point correspondences and a search box. The cost function is based on the Huang-Faugeras constraint of the fundamental matrix. A recently proposed interval extension based on Bernstein polynomial forms has been investigated to speed up the search for the solution. Finally, some experimental results on synthetic images are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238657",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Computer vision",
                "Image reconstruction",
                "Equations",
                "Minimization methods",
                "Polynomials",
                "Information analysis",
                "Cost function",
                "Layout",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "calibration",
                "matrix algebra",
                "polynomials",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "global convergence",
                "autocalibration",
                "numerical optimization algorithms",
                "local minima",
                "interval branch-and-bound method",
                "numerical minimization",
                "interval analysis",
                "mathematical certainty",
                "arbitrary accuracy",
                "point correspondences",
                "search box",
                "cost function",
                "Huang-Faugeras constraint",
                "interval extension",
                "Bernstein polynomial forms",
                "synthetic images",
                "computer vision",
                "image reconstruction",
                "camera motion",
                "video sequence"
            ]
        },
        "id": 186,
        "cited_by": []
    },
    {
        "title": "Image spaces and video trajectories: using Isomap to explore video sequences",
        "authors": [
            "Pless"
        ],
        "abstract": "Dimensionality reduction techniques seek to represent a set of images as a set of points in a low dimensional space. Here we explore a video representation that considers a video as two parts - a space of possible images and a trajectory through that space. The non-linear dimensionality reduction technique of Isomap, gives, for many interesting scenes, a very low dimensional representation of the space of possible images. Analysis of the shape of the video trajectory through these image spaces gives new tools for video analysis. Experiments with natural video sequences illustrate methods for the very different tasks of classifying video clips and temporal super-resolution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238658",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 0,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Video sequences",
                "Videoconference",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "image representation",
                "video signal processing",
                "image classification",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image spaces",
                "video trajectories",
                "Isomap",
                "dimensionality reduction",
                "low dimensional space",
                "video representation",
                "shape analysis",
                "video analysis",
                "natural video sequences",
                "video clip classification",
                "temporal super-resolution",
                "computer vision",
                "image analysis"
            ]
        },
        "id": 187,
        "cited_by": [
            {
                "year": "2015",
                "id": 251
            },
            {
                "year": "2009",
                "id": 67
            }
        ]
    },
    {
        "title": "3D tracking = classification + interpolation",
        "authors": [
            "Tomasi",
            "Petrov",
            "Sastry"
        ],
        "abstract": "Hand gestures are examples of fast and complex motions. Computers fail to track these in fast video, but sleight of hand fools humans as well: what happens too quickly we just cannot see. We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images (classification), and fills the gaps in-between (interpolation). We illustrate this idea with experiments on hand motions similar to finger spelling. The penalty for a recognition failure is often small: if two configurations are confused, they are often similar to each other, and the illusion works well enough, for instance, to drive a graphics animation of the moving hand. We contribute advances in both feature design and classifier training: our image features are invariant to image scale, translation, and rotation, and we propose a classification method that combines VQPCA with discrimination trees.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238659",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 31,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Interpolation",
                "Classification tree analysis",
                "Humans",
                "Tracking",
                "Image recognition",
                "Image classification",
                "Fingers",
                "Graphics",
                "Animation",
                "Tree graphs"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing",
                "gesture recognition",
                "image motion analysis",
                "image classification",
                "optical tracking",
                "interpolation",
                "human computer interaction",
                "object recognition",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D tracking",
                "image classification",
                "interpolation",
                "hand gestures",
                "fast motions",
                "complex motions",
                "fast video",
                "image recognition",
                "2D images",
                "hand motions",
                "finger spelling",
                "graphics animation",
                "feature design",
                "classifier training",
                "image features",
                "image scale",
                "translation",
                "rotation",
                "classification method",
                "VQPCA",
                "discrimination trees",
                "user-computer interface",
                "3D trajectories",
                "3D configurations"
            ]
        },
        "id": 188,
        "cited_by": [
            {
                "year": "2011",
                "id": 233
            },
            {
                "year": "2005",
                "id": 50
            }
        ]
    },
    {
        "title": "Fusion of static and dynamic body biometrics for gait recognition",
        "authors": [
            "Liang Wang",
            "Huazhong Ning",
            "Tieniu Tan",
            "Weiming Hu"
        ],
        "abstract": "Human identification at a distance has recently gained growing interest from computer vision researchers. This paper aims to propose a visual recognition algorithm based upon fusion of static and dynamic body biometrics. For each sequence involving a walking figure, pose changes of the segmented moving silhouettes are represented as an associated sequence of complex vector configurations, and are then analyzed using the Procrustes shape analysis method to obtain a compact appearance representation, called static information of body. Also, a model-based approach is presented under a condensation framework to track the walker and to recover joint-angle trajectories of lower limbs, called dynamic information of gait. Both static and dynamic cues are respectively used for recognition using the nearest exemplar classifier. They are also effectively fused on decision level using different combination rules to improve the performance of both identification and verification. Experimental results on a dataset including 20 subjects demonstrate the validity of the proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238660",
        "reference_list": [
            {
                "year": "2001",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 10,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Biometrics",
                "Humans",
                "Legged locomotion",
                "Information analysis",
                "Shape",
                "Biological system modeling",
                "Pattern recognition",
                "Computer vision",
                "Trajectory",
                "Image sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "image segmentation",
                "image classification",
                "feature extraction",
                "gait analysis",
                "biometrics (access control)",
                "edge detection",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "static body biometrics",
                "dynamic body biometrics",
                "gait recognition",
                "human identification",
                "computer vision",
                "visual recognition algorithm",
                "walking figure",
                "pose changes",
                "segmented moving silhouettes",
                "associated sequence",
                "complex vector configurations",
                "Procrustes shape analysis",
                "compact appearance representation",
                "static information",
                "condensation framework",
                "joint-angle trajectories",
                "lower limbs",
                "dynamic information"
            ]
        },
        "id": 189,
        "cited_by": []
    },
    {
        "title": "Large-scale event detection using semi-hidden Markov models",
        "authors": [
            "Somboon Hongeng",
            "Ramakant Nevatia"
        ],
        "abstract": "We present a new approach to recognizing events in videos. We first detect and track moving objects in the scene. Based on the shape and motion properties of these objects, we infer probabilities of primitive events frame-by-frame by using Bayesian networks. Composite events, consisting of multiple primitive events, over extended periods of time are analyzed by using a hidden, semi-Markov finite state model. This results in more reliable event segmentation compared to the use of standard HMMs in noisy video sequences at the cost of some increase in computational complexity. We describe our approach to reducing this complexity. We demonstrate the effectiveness of our algorithm using both real-world and perturbed data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238661",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 0,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Large-scale systems",
                "Event detection",
                "Object detection",
                "Layout",
                "Shape",
                "Bayesian methods",
                "Hidden Markov models",
                "Video sequences",
                "Costs",
                "Computational complexity"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "image segmentation",
                "hidden Markov models",
                "video signal processing",
                "object detection",
                "belief networks",
                "computational complexity",
                "shape measurement",
                "motion measurement"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large-scale event detection",
                "semihidden Markov models",
                "event reconition",
                "videos",
                "moving object tracking",
                "object shape",
                "object motion",
                "primitive events",
                "Bayesian networks",
                "composite events",
                "semiMarkov finite state model",
                "event segmentation",
                "HMM",
                "noisy video sequences",
                "computational complexity",
                "real-world data",
                "perturbed data"
            ]
        },
        "id": 190,
        "cited_by": [
            {
                "year": "2013",
                "id": 397
            }
        ]
    },
    {
        "title": "Recognizing human action efforts: an adaptive three-mode PCA framework",
        "authors": [
            "Davis",
            "Hui Gao"
        ],
        "abstract": "We present a computational framework capable of labeling the effort of an action corresponding to the perceived level of exertion by the performer (low - high). The approach initially factorizes examples (at different efforts) of an action into its three-mode principal components to reduce the dimensionality. Then a learning phase is introduced to compute expressive-feature weights to adjust the model's estimation of effort to conform to given perceptual labels for the examples. Experiments are demonstrated recognizing the efforts of a person carrying bags of different weight and for multiple people walking at different paces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238662",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 6,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Principal component analysis",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "principal component analysis",
                "feature extraction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human action",
                "adaptive three-mode PCA framework",
                "computational framework",
                "principal component analysis",
                "dimensionality",
                "learning phase",
                "expressive-feature weights",
                "model estimation",
                "computer vision",
                "visual cues",
                "expressive features"
            ]
        },
        "id": 191,
        "cited_by": []
    },
    {
        "title": "Video Google: a text retrieval approach to object matching in videos",
        "authors": [
            "Sivic",
            "Zisserman"
        ],
        "abstract": "We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238663",
        "reference_list": [],
        "citation": {
            "ieee": 1887,
            "other": 1332,
            "total": 3219
        },
        "keywords": {
            "IEEE Keywords": [
                "Web pages",
                "Lighting",
                "Vector quantization",
                "Image databases",
                "Robots",
                "Layout",
                "Noise reduction",
                "File systems",
                "Object recognition",
                "Visual databases"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "computer vision",
                "image matching",
                "image representation",
                "video signal processing",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Video Google",
                "text retrieval approach",
                "object matching",
                "object retrieval",
                "scene retrieval",
                "invariant region descriptors",
                "partial occlusion",
                "unstable regions",
                "noise reduction",
                "vector quantization",
                "inverted file systems",
                "document rankings"
            ]
        },
        "id": 192,
        "cited_by": [
            {
                "year": "2017",
                "id": 75
            },
            {
                "year": "2017",
                "id": 152
            },
            {
                "year": "2017",
                "id": 249
            },
            {
                "year": "2017",
                "id": 285
            },
            {
                "year": "2017",
                "id": 306
            },
            {
                "year": "2017",
                "id": 364
            },
            {
                "year": "2017",
                "id": 583
            },
            {
                "year": "2015",
                "id": 10
            },
            {
                "year": "2015",
                "id": 135
            },
            {
                "year": "2015",
                "id": 159
            },
            {
                "year": "2015",
                "id": 209
            },
            {
                "year": "2015",
                "id": 222
            },
            {
                "year": "2015",
                "id": 298
            },
            {
                "year": "2015",
                "id": 301
            },
            {
                "year": "2015",
                "id": 505
            },
            {
                "year": "2013",
                "id": 31
            },
            {
                "year": "2013",
                "id": 38
            },
            {
                "year": "2013",
                "id": 111
            },
            {
                "year": "2013",
                "id": 156
            },
            {
                "year": "2013",
                "id": 174
            },
            {
                "year": "2013",
                "id": 208
            },
            {
                "year": "2013",
                "id": 211
            },
            {
                "year": "2013",
                "id": 212
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2013",
                "id": 231
            },
            {
                "year": "2013",
                "id": 266
            },
            {
                "year": "2013",
                "id": 317
            },
            {
                "year": "2013",
                "id": 333
            },
            {
                "year": "2013",
                "id": 379
            },
            {
                "year": "2013",
                "id": 426
            },
            {
                "year": "2013",
                "id": 434
            },
            {
                "year": "2013",
                "id": 436
            },
            {
                "year": "2011",
                "id": 26
            },
            {
                "year": "2011",
                "id": 39
            },
            {
                "year": "2011",
                "id": 47
            },
            {
                "year": "2011",
                "id": 84
            },
            {
                "year": "2011",
                "id": 96
            },
            {
                "year": "2011",
                "id": 103
            },
            {
                "year": "2011",
                "id": 110
            },
            {
                "year": "2011",
                "id": 143
            },
            {
                "year": "2011",
                "id": 185
            },
            {
                "year": "2011",
                "id": 188
            },
            {
                "year": "2011",
                "id": 209
            },
            {
                "year": "2011",
                "id": 222
            },
            {
                "year": "2011",
                "id": 226
            },
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2011",
                "id": 306
            },
            {
                "year": "2011",
                "id": 316
            },
            {
                "year": "2011",
                "id": 326
            },
            {
                "year": "2011",
                "id": 337
            },
            {
                "year": "2011",
                "id": 338
            },
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2009",
                "id": 78
            },
            {
                "year": "2009",
                "id": 80
            },
            {
                "year": "2009",
                "id": 114
            },
            {
                "year": "2009",
                "id": 141
            },
            {
                "year": "2009",
                "id": 247
            },
            {
                "year": "2009",
                "id": 256
            },
            {
                "year": "2009",
                "id": 273
            },
            {
                "year": "2009",
                "id": 287
            },
            {
                "year": "2009",
                "id": 303
            },
            {
                "year": "2007",
                "id": 2
            },
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2007",
                "id": 18
            },
            {
                "year": "2007",
                "id": 41
            },
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2007",
                "id": 164
            },
            {
                "year": "2007",
                "id": 198
            },
            {
                "year": "2007",
                "id": 216
            },
            {
                "year": "2007",
                "id": 222
            },
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2007",
                "id": 248
            },
            {
                "year": "2007",
                "id": 385
            },
            {
                "year": "2005",
                "id": 48
            },
            {
                "year": "2005",
                "id": 59
            },
            {
                "year": "2005",
                "id": 107
            },
            {
                "year": "2005",
                "id": 115
            },
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2005",
                "id": 237
            }
        ]
    },
    {
        "title": "Probabilistic bilinear models for appearance-based vision",
        "authors": [
            "Grimes",
            "Shon",
            "Rao"
        ],
        "abstract": "We present a probabilistic approach to learning object representations based on the \"content and style\" bilinear generative model of Tenenbaum and Freeman. In contrast to their earlier SVD-based approach, our approach models images using particle filters. We maintain separate particle filters to represent the content and style spaces, allowing us to define arbitrary weighting functions over the particles to help estimate the content/style densities. We combine this approach with a new EM-based method for learning basis vectors that describe content-style mixing. Using a particle-based representation permits good reconstruction despite reduced dimensionality, and increases storage capacity and computational efficiency. We describe how learning the distributions using particle filters allows us to efficiently compute a probabilistic \"novelty\" term. Our example application considers a dataset of faces under different lighting conditions. The system classifies faces of people it has seen before, and can identify previously unseen faces as new content. Using a probabilistic definition of novelty in conjunction with learning content-style separability provides a crucial building block for designing real-world, real-time object recognition systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238665",
        "reference_list": [
            {
                "year": "2001",
                "id": 49
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Particle filters",
                "Iterative algorithms",
                "State estimation",
                "Computer science",
                "Maintenance engineering",
                "Image reconstruction",
                "Computational efficiency",
                "Distributed computing",
                "Real time systems",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "probability",
                "singular value decomposition",
                "computer vision",
                "image reconstruction",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic bilinear models",
                "appearance-based vision",
                "probabilistic approach",
                "bilinear generative model",
                "SVD-based approach",
                "particle filters",
                "weighting functions",
                "content/style density",
                "EM-based method",
                "learning basis vectors",
                "content-style mixing",
                "particle-based representation",
                "storage capacity",
                "computational efficiency",
                "face classification",
                "content-style separability",
                "object recognition systems"
            ]
        },
        "id": 193,
        "cited_by": [
            {
                "year": "2011",
                "id": 255
            },
            {
                "year": "2007",
                "id": 377
            }
        ]
    },
    {
        "title": "Real time pattern matching using projection kernels",
        "authors": [
            "Hel-Or",
            "Hel-Or"
        ],
        "abstract": "A novel approach to pattern matching is presented, which reduces time complexity by two orders of magnitude compared to traditional approaches. The suggested approach uses an efficient projection scheme which bounds the distance between a pattern and an image window using very few operations. The projection framework is combined with a rejection scheme which allows rapid rejection of image windows that are distant from the pattern. Experiments show that the approach is effective even under very noisy conditions. The approach described here can also be used in classification schemes where the projection values serve as input features that are informative and fast to extract.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238666",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 7,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Pattern matching",
                "Kernel",
                "Euclidean distance",
                "Computer science",
                "Application software",
                "Pixel",
                "Image processing",
                "Computer vision",
                "Performance evaluation",
                "Computational complexity"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "feature extraction",
                "pattern matching",
                "computational complexity",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real time pattern matching",
                "projection kernels",
                "projection scheme",
                "image window",
                "rejection scheme",
                "classification schemes"
            ]
        },
        "id": 194,
        "cited_by": [
            {
                "year": "2005",
                "id": 112
            }
        ]
    },
    {
        "title": "Weighted and robust incremental method for subspace learning",
        "authors": [
            "Skocaj",
            "Leonardis"
        ],
        "abstract": "Visual learning is expected to be a continuous and robust process, which treats input images and pixels selectively. In this paper, we present a method for subspace learning, which takes these considerations into account. We present an incremental method, which sequentially updates the principal subspace considering weighted influence of individual images as well as individual pixels within an image. This approach is further extended to enable determination of consistencies in the input data and imputation of the values in inconsistent pixels using the previously acquired knowledge, resulting in a novel incremental, weighted and robust method for subspace learning.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238667",
        "reference_list": [
            {
                "year": "2001",
                "id": 49
            }
        ],
        "citation": {
            "ieee": 57,
            "other": 38,
            "total": 95
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Principal component analysis",
                "Pixel",
                "Layout",
                "Information science",
                "Humans",
                "Visual system",
                "Machine learning",
                "Computer vision",
                "Singular value decomposition"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "visual perception",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "incremental method",
                "subspace learning",
                "visual learning",
                "robust process",
                "principal subspace"
            ]
        },
        "id": 195,
        "cited_by": [
            {
                "year": "2009",
                "id": 209
            },
            {
                "year": "2007",
                "id": 111
            },
            {
                "year": "2005",
                "id": 143
            },
            {
                "year": "2005",
                "id": 178
            }
        ]
    },
    {
        "title": "Conditional feature sensitivity: a unifying view on active recognition and feature selection",
        "authors": [
            "Xiang Sean Zhou",
            "Comaniciu",
            "Krishnan"
        ],
        "abstract": "The objective of active recognition is to iteratively collect the next \"best\" measurements (e.g., camera angles or viewpoints), to maximally reduce ambiguities in recognition. However, existing work largely overlooked feature interaction issues. Feature selection, on the other hand, focuses on the selection of a subset of measurements for a given classification task, but is not context sensitive (i.e., the decision does not depend on the current input). This paper proposes a unified perspective through conditional feature sensitivity analysis, taking into account both current context and feature interactions. Based on different representations of the contextual uncertainties, we present three treatment models and exploit their joint power for dealing with complex feature interactions. Synthetic examples are used to systematically test the validity of the proposed models. A practical application in medical domain is illustrated using an echocardiography database with more than 2000 video segments with both subjective (from experts) and objective validations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238668",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensitivity analysis",
                "Cameras",
                "Current measurement",
                "Context modeling",
                "Power system modeling",
                "Object recognition",
                "Computer vision",
                "Goniometers",
                "System testing",
                "Echocardiography"
            ],
            "INSPEC: Controlled Indexing": [
                "active vision",
                "image classification",
                "feature extraction",
                "object recognition",
                "medical image processing",
                "sensitivity"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature sensitivity",
                "active recognition",
                "feature selection",
                "camera angles",
                "feature interaction",
                "context sensitive",
                "sensitivity analysis",
                "contextual uncertainty",
                "medical domain",
                "echocardiography database",
                "video segments"
            ]
        },
        "id": 196,
        "cited_by": []
    },
    {
        "title": "Using specularities for recognition",
        "authors": [
            "Osadchy",
            "Jacobs",
            "Ramamoorthi"
        ],
        "abstract": "Recognition systems have generally treated specular highlights as noise. We show how to use these highlights as a positive source of information that improves recognition of shiny objects. This also enables us to recognize very challenging shiny transparent objects, such as wine glasses. Specifically, we show how to find highlights that are consistent with a hypothesized pose of an object of known 3D shape. We do this using only a qualitative description of highlight formation that is consistent with most models of specular reflection, so no specific knowledge of an object's reflectance properties is needed. We first present a method that finds highlights produced by a dominant compact light source, whose position is roughly known. We then show how to estimate the lighting automatically for objects whose reflection is part specular and part Lambertian. We demonstrate this method for two classes of objects. First, we show that specular information alone can suffice to identify objects with no Lambertian reflectance, such as transparent wine glasses. Second, we use our complete system to recognize shiny objects, such as pottery.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238669",
        "reference_list": [
            {
                "year": "2001",
                "id": 156
            },
            {
                "year": "2001",
                "id": 80
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 11,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflectivity",
                "Glass",
                "Jacobian matrices",
                "Computer science",
                "Information resources",
                "Optical reflection",
                "Light sources",
                "Testing",
                "Shape measurement",
                "National electric code"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "object recognition",
                "position measurement",
                "stereo image processing",
                "reflection",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "recognition systems",
                "shiny objects",
                "transparent objects",
                "wine glass",
                "3D shape",
                "highlight formation",
                "specular reflection",
                "object reflectance property",
                "compact light source",
                "Lambertian reflection",
                "specular information",
                "pottery"
            ]
        },
        "id": 197,
        "cited_by": []
    },
    {
        "title": "Color edge detection by photometric quasi-invariants",
        "authors": [
            "van de Weijer",
            "Gevers",
            "Geusebroek"
        ],
        "abstract": "Photometric invariance is used in many computer vision applications. The advantage of photometric invariance is the robustness against shadows, shading and illumination conditions. However, the drawbacks of photometric invariance are the loss of discriminative power and the inherent instabilities caused by the nonlinear transformations to compute the invariants. In this paper, we propose a new class of derivatives which we refer to as photometric quasi-invariants. These quasi-invariants share with full invariants the nice property that they are robust against photometric edges, such as shadows or specular edges. Further, these quasi-invariants do not have the inherent instabilities of full photometric invariants. We will apply these quasi-invariant derivatives in the context of photometric invariant edge detection and classification. Experiments show that the quasi-invariant derivatives are stable and they significantly outperform the full invariant derivatives in discriminative power.",
        "ieee_link": "https://ieeexplore.ieee.org/document/1238670",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 3,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Photometry",
                "Optical reflection",
                "Robustness",
                "Computer vision",
                "Lighting",
                "Application software",
                "Intelligent systems",
                "Intelligent sensors",
                "Information systems"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "edge detection",
                "image colour analysis",
                "image classification",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color edge detection",
                "photometric quasiinvariants",
                "photometric invariance",
                "computer vision application",
                "discriminative power",
                "inherent instability",
                "nonlinear transformations",
                "photometric edges",
                "shadows",
                "specular edges",
                "shading",
                "illumination condition",
                "quasiinvariant derivatives"
            ]
        },
        "id": 198,
        "cited_by": []
    }
]