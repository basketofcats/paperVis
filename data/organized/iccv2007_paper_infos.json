[
    {
        "title": "Learning Graph Matching",
        "authors": [
            "Tiberio S. Caetano",
            "Li Cheng",
            "Quoc V. Le",
            "Alex J. Smola"
        ],
        "abstract": "As a fundamental problem in pattern recognition, graph matching has found a variety of applications in the field of computer vision. In graph matching, patterns are modeled as graphs and pattern recognition amounts to finding a correspondence between the nodes of different graphs. There are many ways in which the problem has been formulated, but most can be cast in general as a quadratic assignment problem, where a linear term in the objective function encodes node compatibility functions and a quadratic term encodes edge compatibility functions. The main research focus in this theme is about designing efficient algorithms for solving approximately the quadratic assignment problem, since it is NP-hard. In this paper, we turn our attention to the complementary problem: how to estimate compatibility functions such that the solution of the resulting graph matching problem best matches the expected solution that a human would manually provide. We present a method for learning graph matching: the training examples are pairs of graphs and the \"labels\" are matchings between pairs of graphs. We present experimental results with real image data which give evidence that learning can improve the performance of standard graph matching algorithms. In particular, it turns out that linear assignment with such a learning scheme may improve over state-of-the-art quadratic assignment relaxations. This finding suggests that for a range of problems where quadratic assignment was thought to be essential for securing good results, linear assignment, which is far more efficient, could be just sufficient if learning is performed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408838",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 16,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimal matching",
                "Pattern matching",
                "Machine learning",
                "Pattern recognition",
                "Surveillance",
                "Cameras",
                "Image databases",
                "Data mining",
                "Australia",
                "Application software"
            ]
        },
        "id": 0,
        "cited_by": []
    },
    {
        "title": "Learning Globally-Consistent Local Distance Functions for Shape-Based Image Retrieval and Classification",
        "authors": [
            "Andrea Frome",
            "Yoram Singer",
            "Fei Sha",
            "Jitendra Malik"
        ],
        "abstract": "We address the problem of visual category recognition by learning an image-to-image distance function that attempts to satisfy the following property: the distance between images from the same category should be less than the distance between images from different categories. We use patch-based feature vectors common in object recognition work as a basis for our image-to-image distance functions. Our large-margin formulation for learning the distance functions is similar to formulations used in the machine learning literature on distance metric learning, however we differ in that we learn local distance functions-a different parameterized function for every image of our training set-whereas typically a single global distance function is learned. This was a novel approach first introduced in Frome, Singer, & Malik, NIPS 2006. In that work we learned the local distance functions independently, and the outputs of these functions could not be compared at test time without the use of additional heuristics or training. Here we introduce a different approach that has the advantage that it learns distance functions that are globally consistent in that they can be directly compared for purposes of retrieval and classification. The output of the learning algorithm are weights assigned to the image features, which is intuitively appealing in the computer vision setting: some features are more salient than others, and which are more salient depends on the category, or image, being considered. We train and test using the Caltech 101 object recognition benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408839",
        "reference_list": [
            {
                "year": "2005",
                "id": 17
            }
        ],
        "citation": {
            "ieee": 121,
            "other": 77,
            "total": 198
        },
        "keywords": {
            "IEEE Keywords": [
                "Image retrieval",
                "Machine learning",
                "Image recognition",
                "Object recognition",
                "Shape",
                "Computer vision",
                "Benchmark testing",
                "Nearest neighbor searches",
                "Vectors",
                "Visualization"
            ]
        },
        "id": 1,
        "cited_by": [
            {
                "year": "2017",
                "id": 529
            },
            {
                "year": "2013",
                "id": 31
            },
            {
                "year": "2013",
                "id": 265
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 77
            },
            {
                "year": "2009",
                "id": 37
            },
            {
                "year": "2009",
                "id": 38
            },
            {
                "year": "2009",
                "id": 53
            },
            {
                "year": "2009",
                "id": 55
            },
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2009",
                "id": 76
            },
            {
                "year": "2009",
                "id": 253
            }
        ]
    },
    {
        "title": "Boosting Invariance and Efficiency in Supervised Learning",
        "authors": [
            "Andrea Vedaldi",
            "Paolo Favaro",
            "Enrico Grisan"
        ],
        "abstract": "In this paper we present a novel boosting algorithm for supervised learning that incorporates invariance to data transformations and has high generalization capabilities. While one can incorporate invariance by adding virtual samples to the data (e.g., by jittering), we adopt a much more efficient strategy and work along the lines of vicinal risk minimization and tangent distance methods. As in vicinal risk minimization, we incorporate invariance to data by applying anisotropic smoothing along the directions of invariance. Moreover, as in tangent distance methods, we provide a simple local approximation to such directions, thus obtaining an efficient computational scheme. We also show that it is possible to automatically design optimal weak classifiers by using gradient descent. To increase efficiency at run time, such optimal weak classifiers are projected on a Haar basis. This results in designing strong classifiers that are more computationally efficient than in the case of exhaustive search. For illustration and validation purposes, we demonstrate the novel algorithm both on synthetic and on real data sets that are publicly available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408840",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Supervised learning",
                "Risk management",
                "Face detection",
                "Runtime",
                "Testing",
                "Computer science",
                "Physics",
                "Anisotropic magnetoresistance",
                "Smoothing methods"
            ]
        },
        "id": 2,
        "cited_by": []
    },
    {
        "title": "Learning to Find Object Boundaries Using Motion Cues",
        "authors": [
            "Andrew Stein",
            "Derek Hoiem",
            "Martial Hebert"
        ],
        "abstract": "While great strides have been made in detecting and localizing specific objects in natural images, the bottom-up segmentation of unknown, generic objects remains a difficult challenge. We believe that occlusion can provide a strong cue for object segmentation and \"pop-out\", but detecting an object's occlusion boundaries using appearance alone is a difficult problem in itself. If the camera or the scene is moving, however, that motion provides an additional powerful indicator of occlusion. Thus, we use standard appearance cues (e.g. brightness/color gradient) in addition to motion cues that capture subtle differences in the relative surface motion (i.e. parallax) on either side of an occlusion boundary. We describe a learned local classifier and global inference approach which provide a frame-work for combining and reasoning about these appearance and motion cues to estimate which region boundaries of an initial over-segmentation correspond to object/occlusion boundaries in the scene. Through results on a dataset which contains short videos with labeled boundaries, we demonstrate the effectiveness of motion cues for this task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408841",
        "reference_list": [
            {
                "year": "2005",
                "id": 185
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2001",
                "id": 6
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 17,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Object detection",
                "Image segmentation",
                "Labeling",
                "Motion detection",
                "Object segmentation",
                "Cameras",
                "Videos",
                "Telephony",
                "Intelligent robots"
            ]
        },
        "id": 3,
        "cited_by": [
            {
                "year": "2013",
                "id": 440
            },
            {
                "year": "2009",
                "id": 71
            },
            {
                "year": "2009",
                "id": 294
            },
            {
                "year": "2007",
                "id": 144
            }
        ]
    },
    {
        "title": "COST: An Approach for Camera Selection and Multi-Object Inference Ordering in Dynamic Scenes",
        "authors": [
            "Abhinav Gupta",
            "Anurag Mittal",
            "Larry S. Davis"
        ],
        "abstract": "Development of multiple camera based vision systems for analysis of dynamic objects such as humans is challenging due to occlusions and similarity in the appearance of a person with the background and other people- visual \"confusion\". Since occlusion and confusion depends on the presence of other people in the scene, it leads to a dependency structure where there are often loops in the resulting Bayesian network. While approaches such as loopy belief propagation can be used for inference, they are computationally expensive and convergence is not guaranteed in many situations. We present a unified approach, COST, that reasons about such dependencies and yields an order for the inference of each person in a group of people and a set of cameras to be used for inferences for a person. Using the probabilistic distribution of the positions and appearances of people, COST performs visibility and confusion analysis for each part of each person and computes the amount of information that can be computed with and without more accurate estimation of the positions of other people. We present an optimization problem to select set of cameras and inference dependencies for each person which attempts to minimize the computational cost under given performance constraints. Results show the efficiency of COST in improving the performance of such systems and reducing the computational resources required.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408842",
        "reference_list": [
            {
                "year": "2005",
                "id": 89
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 10,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Costs",
                "Cameras",
                "Layout",
                "Distributed computing",
                "Machine vision",
                "Humans",
                "Bayesian methods",
                "Belief propagation",
                "Convergence",
                "Information analysis"
            ]
        },
        "id": 4,
        "cited_by": []
    },
    {
        "title": "Active Learning with Gaussian Processes for Object Categorization",
        "authors": [
            "Ashish Kapoor",
            "Kristen Grauman",
            "Raquel Urtasun",
            "Trevor Darrell"
        ],
        "abstract": "Discriminative methods for visual object category recognition are typically non-probabilistic, predicting class labels but not directly providing an estimate of uncertainty. Gaussian Processes (GPs) are powerful regression techniques with explicit uncertainty models; we show here how Gaussian Processes with covariance functions defined based on a Pyramid Match Kernel (PMK) can be used for probabilistic object category recognition. The uncertainty model provided by GPs offers confidence estimates at test points, and naturally allows for an active learning paradigm in which points are optimally selected for interactive labeling. We derive a novel active category learning method based on our probabilistic regression model, and show that a significant boost in classification performance is possible, especially when the amount of training data for a category is ultimately very small.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408844",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2001",
                "id": 69
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2005",
                "id": 52
            },
            {
                "year": "2003",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 87,
            "other": 52,
            "total": 139
        },
        "keywords": {
            "IEEE Keywords": [
                "Gaussian processes",
                "Kernel",
                "Uncertainty",
                "Labeling",
                "Learning systems",
                "Machine learning",
                "Testing",
                "Training data",
                "Computer vision",
                "Humans"
            ]
        },
        "id": 5,
        "cited_by": [
            {
                "year": "2017",
                "id": 458
            },
            {
                "year": "2015",
                "id": 316
            },
            {
                "year": "2015",
                "id": 331
            },
            {
                "year": "2013",
                "id": 26
            },
            {
                "year": "2013",
                "id": 150
            },
            {
                "year": "2013",
                "id": 374
            },
            {
                "year": "2009",
                "id": 135
            },
            {
                "year": "2009",
                "id": 248
            }
        ]
    },
    {
        "title": "Spectral Latent Variable Models for Perceptual Inference",
        "authors": [
            "Atul Kanaujia",
            "Cristian Sminchisescu",
            "Dimitris Metaxas"
        ],
        "abstract": "We propose non-linear generative models referred to as Sparse Spectral Latent Variable Models (SLVM), that combine the advantages of spectral embeddings with the ones of parametric latent variable models: (1) provide stable latent spaces that preserve global or local geometric properties of the modeled data; (2) offer low-dimensional generative models with probabilistic, bi-directional mappings between latent and ambient spaces, (3) are probabilistically consistent (i.e., reflect the data distribution, both jointly and marginally) and efficient to learn and use. We show that SLVMs compare favorably with competing methods based on PCA, GPLVM or GTM for the reconstruction of typical human motions like walking, running, pantomime or dancing in a benchmark dataset. Empirically, we observe that SLVMs are effective for the automatic 3d reconstruction of low-dimensional human motion in movies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408845",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Humans",
                "Legged locomotion",
                "Principal component analysis",
                "Image reconstruction",
                "Computer vision",
                "Bidirectional control",
                "Motion pictures",
                "Motion measurement",
                "Matrix decomposition"
            ]
        },
        "id": 6,
        "cited_by": []
    },
    {
        "title": "Metric Learning Using Iwasawa Decomposition",
        "authors": [
            "Bing Jian",
            "Baba C. Vemuri"
        ],
        "abstract": "Finding a good metric over the input space plays a fundamental role in machine learning. Most existing techniques use the Mahalanobis metric without incorporating the geometry of positive matrices and experience difficulties in the optimization procedure. In this paper we introduce the use of Iwasawa decomposition, a unique and effective parametrization of symmetric positive definite (SPD) matrices, for performing metric learning tasks. Unlike other previously employed factorizations, the use of the Iwasawa decomposition is able to reformulate the semidefinite programming (SDP) problems as smooth convex nonlinear programming (NLP) problems with much simpler constraints. We also introduce a modified Iwasawa coordinates for rank-deficient positive semidefinite (PSD) matrices which enables the unifying of the metric learning and linear dimensionality reduction. We show that the Iwasawa decomposition can be easily used in most recent proposed metric learning algorithms and have applied it to the Neighbourhood Components Analysis (NCA). The experimental results on several public domain datasets are also presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408846",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Matrix decomposition",
                "Jacobian matrices",
                "Machine learning",
                "Symmetric matrices",
                "Constraint optimization",
                "Clustering algorithms",
                "Algorithm design and analysis",
                "Machine learning algorithms",
                "Information science",
                "Geometry"
            ]
        },
        "id": 7,
        "cited_by": []
    },
    {
        "title": "DynamicBoost: Boosting Time Series Generated by Dynamical Systems",
        "authors": [
            "Rene Vidal",
            "Paolo Favaro"
        ],
        "abstract": "Boosting is a remarkably simple and flexible classification algorithm with widespread applications in computer vision. However, the application of boosting to non-Euclidean, infinite length, and time-varying data, such as videos, is not straightforward. In dynamic textures, for example, the temporal evolution of image intensities is captured by a linear dynamical system, whose parameters live in a Stiefel manifold, which is clearly non-Euclidean. In this paper, we present a novel boosting method for the recognition of visual dynamical processes. Our key contribution is the design of weak classifiers (features) that are formulated as linear dynamical systems. The main advantage of such features is that they can be applied to infinitely long sequences and that they can be efficiently computed by solving a set of Sylvester equations. We also present an application of our method to dynamic texture classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408847",
        "reference_list": [
            {
                "year": "2005",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 2,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Application software",
                "Computer vision",
                "Videos",
                "Face detection",
                "Pixel",
                "Sequences",
                "Speech recognition",
                "Classification tree analysis",
                "Physics"
            ]
        },
        "id": 8,
        "cited_by": []
    },
    {
        "title": "Task Specific Local Region Matching",
        "authors": [
            "Boris Babenko",
            "Piotr Dollar",
            "Serge Belongie"
        ],
        "abstract": "Many problems in computer vision require the knowledge of potential point correspondences between two images. The usual approach for automatically determining correspondences begins by comparing small neighborhoods of high saliency in both images. Since speed is of the essence, most current approaches for local region matching involve the computation of a feature vector that is invariant to various geometric and photometric transformations, followed by fast distance computations using standard vector norms. These algorithms include many parameters, and choosing an algorithm and setting its parameters for a given problem is more an art than a science. Furthermore, although invariance of the resulting feature space is in general desirable, there is necessarily a tradeoff between invariance and descriptiveness for any given task. In this paper we pose local region matching as a classification problem, and use powerful machine learning techniques to train a classifier that selects features from a much larger pool. Our algorithm can be trained on specific domains or tasks, and performs better than the state of the art in such cases. Since our method is an application of boosting, we refer to it as boosted region matching (BOOM).",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408848",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 4,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Photometry",
                "Robustness",
                "Computer science",
                "Knowledge engineering",
                "Art",
                "Machine learning",
                "Machine learning algorithms",
                "Boosting",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image matching",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "task specific local region matching",
                "computer vision",
                "geometric transformations",
                "photometric transformations",
                "standard vector norms",
                "feature space",
                "machine learning techniques",
                "boosted region matching"
            ]
        },
        "id": 9,
        "cited_by": [
            {
                "year": "2013",
                "id": 13
            }
        ]
    },
    {
        "title": "Action Recognition from Arbitrary Views using 3D Exemplars",
        "authors": [
            "Daniel Weinland",
            "Edmond Boyer",
            "Remi Ronfard"
        ],
        "abstract": "In this paper, we address the problem of learning compact, view-independent, realistic 3D models of human actions recorded with multiple cameras, for the purpose of recognizing those same actions from a single or few cameras, without prior knowledge about the relative orientations between the cameras and the subjects. To this aim, we propose a new framework where we model actions using three dimensional occupancy grids, built from multiple viewpoints, in an exemplar-based HMM. The novelty is, that a 3D reconstruction is not required during the recognition phase, instead learned 3D exemplars are used to produce 2D image information that is compared to the observations. Parameters that describe image projections are added as latent variables in the recognition process. In addition, the temporal Markov dependency applied to view parameters allows them to evolve during recognition as with a smoothly moving camera. The effectiveness of the framework is demonstrated with experiments on real datasets and with challenging recognition scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408849",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2001",
                "id": 110
            },
            {
                "year": "2005",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 150,
            "other": 117,
            "total": 267
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Hidden Markov models",
                "Humans",
                "Image recognition",
                "Image reconstruction",
                "Solid modeling",
                "Layout",
                "Parametric statistics",
                "Kinematics",
                "Image motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "hidden Markov models",
                "learning (artificial intelligence)",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human action recognition",
                "arbitrary views",
                "3D exemplar learning",
                "realistic 3D models",
                "3D occupancy grids",
                "exemplar-based HMM",
                "image projections",
                "temporal Markov dependency"
            ]
        },
        "id": 10,
        "cited_by": [
            {
                "year": "2013",
                "id": 75
            },
            {
                "year": "2013",
                "id": 390
            },
            {
                "year": "2013",
                "id": 396
            },
            {
                "year": "2011",
                "id": 14
            },
            {
                "year": "2011",
                "id": 72
            },
            {
                "year": "2011",
                "id": 325
            }
        ]
    },
    {
        "title": "Incremental Learning of Boosted Face Detector",
        "authors": [
            "Chang Huang",
            "Haizhou Ai",
            "Takayoshi Yamashita",
            "Shihong Lao",
            "Masato Kawade"
        ],
        "abstract": "In recent years, boosting has been successfully applied to many practical problems in pattern recognition and computer vision fields such as object detection and tracking. As boosting is an offline training process with beforehand collected data, once learned, it cannot make use of any newly arriving ones. However, an offline boosted detector is to be exploited online and inevitably there must be some special cases that are not covered by those beforehand collected training data. As a result, the inadaptable detector often performs badly in diverse and changeful environments which are ordinary for many real-life applications. To alleviate this problem, this paper proposes an incremental learning algorithm to effectively adjust a boosted strong classifier with domain-partitioning weak hypotheses to online samples, which adopts a novel approach to efficient estimation of training losses received from offline samples. By this means, the offline learned general-purpose detectors can be adapted to special online situations at a low extra cost, and still retains good generalization ability for common environments. The experiments show convincing results of our incremental learning approach on challenging face detection problems with partial occlusions and extreme illuminations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408850",
        "reference_list": [
            {
                "year": "2005",
                "id": 57
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 11,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Face detection",
                "Boosting",
                "Detectors",
                "Object detection",
                "Computer vision",
                "Lighting",
                "Pattern recognition",
                "Costs",
                "Computer science",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "incremental learning",
                "boosted face detector"
            ]
        },
        "id": 11,
        "cited_by": []
    },
    {
        "title": "Bottom-up saliency is a discriminant process",
        "authors": [
            "Dashan Gao",
            "Nuno Vasconcelos"
        ],
        "abstract": "A bottom-up visual saliency detector is proposed, following a decision-theoretic formulation of saliency, previously developed for top-down processing (object recognition) [5]. The saliency of a given location of the visual field is defined as the power of a Gabor-like feature set to discriminate between the visual appearance of 1) a neighborhood centered at that location (the center) and 2) a neighborhood that surrounds it (the surround). Discrimination is defined in an information-theoretic sense and the optimal saliency detector derived for a class of stimuli that complies with known statistical properties of natural images, so as to achieve a computationally efficient solution. The resulting saliency detector is shown to replicate the fundamental properties of the psychophysics of pre-attentive vision, including stimulus pop-out, inability to detect feature conjunctions, asymmetries with respect to feature presence vs. absence, and compliance with Weber's law. It is also shown that the detector produces better predictions of human eye fixations than two previously proposed bottom-up saliency detectors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408851",
        "reference_list": [
            {
                "year": "2005",
                "id": 230
            }
        ],
        "citation": {
            "ieee": 77,
            "other": 57,
            "total": 134
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Object recognition",
                "Psychology",
                "Computer vision",
                "Humans",
                "Object detection",
                "Biology computing",
                "Resource management",
                "Pattern recognition",
                "Visual system"
            ],
            "INSPEC: Controlled Indexing": [
                "decision theory",
                "feature extraction",
                "object detection",
                "visual perception"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bottom-up visual saliency detector",
                "decision-theoretic formulation",
                "object recognition",
                "Gabor-like feature set",
                "discriminant process",
                "statistical property",
                "Weber law",
                "human eye fixation prediction",
                "top-down image processing"
            ]
        },
        "id": 12,
        "cited_by": [
            {
                "year": "2013",
                "id": 207
            },
            {
                "year": "2011",
                "id": 281
            }
        ]
    },
    {
        "title": "Unsupervised Image Categorization and Object Localization using Topic Models and Correspondences between Images",
        "authors": [
            "David Liu",
            "Tsuhan Chen"
        ],
        "abstract": "Topic models from the text understanding literature have shown promising results in unsupervised image categorization and object localization. Categories are treated as topics, and words are formed by vector quantizing local descriptors of image patches. Limitations of topic models include their weakness in localizing objects, and the requirement of a fairly large proportion of words coming from the object. We present a new approach that employs correspondences between images to provide information about object configuration, which in turn enhances the reliability of object localization and categorization. This approach is efficient, as it requires only a small number of correspondences. We show improved categorization and localization performance on real and synthetic data. Moreover, we can push the limits of topic models when the proportion of words coming from the object is very low.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408852",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2005",
                "id": 115
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 13,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Image analysis",
                "Performance analysis",
                "Search engines",
                "Computer vision",
                "Image edge detection",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "object recognition",
                "vector quantisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised image categorization",
                "object localization",
                "topic models",
                "vector quantization",
                "local image descriptor",
                "image patches"
            ]
        },
        "id": 13,
        "cited_by": []
    },
    {
        "title": "Non-metric affinity propagation for unsupervised image categorization",
        "authors": [
            "Delbert Dueck",
            "Brendan J. Frey"
        ],
        "abstract": "Unsupervised categorization of images or image parts is often needed for image and video summarization or as a preprocessing step in supervised methods for classification, tracking and segmentation. While many metric-based techniques have been applied to this problem in the vision community, often, the most natural measures of similarity (e.g., number of matching SIFT features) between pairs of images or image parts is non-metric. Unsupervised categorization by identifying a subset of representative exemplars can be efficiently performed with the recently-proposed 'affinity propagation' algorithm. In contrast to k-centers clustering, which iteratively refines an initial randomly-chosen set of exemplars, affinity propagation simultaneously considers all data points as potential exemplars and iteratively exchanges messages between data points until a good solution emerges. When applied to the Olivetti face data set using a translation-invariant non-metric similarity, affinity propagation achieves a much lower reconstruction error and nearly halves the classification error rate, compared to state-of-the-art techniques. For the more challenging problem of unsupervised categorization of images from the CaltechlOl data set, we derived non-metric similarities between pairs of images by matching SIFT features. Affinity propagation successfully identifies meaningful categories, which provide a natural summarization of the training images and can be used to classify new input images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408853",
        "reference_list": [
            {
                "year": "2003",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 55,
            "other": 57,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Euclidean distance",
                "Clustering algorithms",
                "Machine learning",
                "Face detection",
                "Educational institutions",
                "Image segmentation",
                "Iterative algorithms",
                "Image reconstruction",
                "Error analysis",
                "Data preprocessing"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image reconstruction",
                "image segmentation",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonmetric affinity propagation",
                "unsupervised image categorization",
                "image-video summarization",
                "classification methods",
                "tracking methods",
                "segmentation methods",
                "SIFT feature matching",
                "Olivetti face data set",
                "translation-invariant nonmetric similarity",
                "error reconstruction"
            ]
        },
        "id": 14,
        "cited_by": [
            {
                "year": "2015",
                "id": 473
            },
            {
                "year": "2013",
                "id": 197
            },
            {
                "year": "2013",
                "id": 216
            }
        ]
    },
    {
        "title": "Contextual Distance for Data Perception",
        "authors": [
            "Deli Zhao",
            "Zhouchen Lin",
            "Xiaoou Tang"
        ],
        "abstract": "Structural perception of data plays a fundamental role in pattern analysis and machine learning. In this paper, we develop a new structural perception of data based on local contexts. We first identify the contextual set of a point by finding its nearest neighbors. Then the contextual distance between the point and one of its neighbors is defined by the difference between their contribution to the integrity of the geometric structure of the contextual set, which is depicted by a structural descriptor. The centroid and the coding length are introduced as the examples of descriptors of the contextual set. Furthermore, a directed graph (digraph) is built to model the asymmetry of perception. The edges of the digraph are weighted based on the contextual distances. Thus direction is brought to the undirected data. And the structural perception of data can be performed by mining the properties of the digraph. We also present the method for deriving the global digraph Laplacian from the alignment of the local digraph Laplacians. Experimental results on clustering and ranking of toy problems and real data show the superiority of asymmetric perception.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408854",
        "reference_list": [
            {
                "year": "2007",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 6,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise measurement",
                "Laplace equations",
                "Noise figure",
                "Clustering algorithms",
                "Noise robustness",
                "Asia",
                "Pattern analysis",
                "Machine learning",
                "Nearest neighbor searches",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "data analysis",
                "directed graphs",
                "Laplace equations",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contextual distance",
                "data perception",
                "pattern analysis",
                "machine learning",
                "digraph edges",
                "structural descriptor",
                "global digraph Laplacian",
                "toy problems",
                "asymmetric perception",
                "directed graph"
            ]
        },
        "id": 15,
        "cited_by": [
            {
                "year": "2007",
                "id": 256
            }
        ]
    },
    {
        "title": "Spectral Regression for Efficient Regularized Subspace Learning",
        "authors": [
            "Deng Cai",
            "Xiaofei He",
            "Jiawei Han"
        ],
        "abstract": "Subspace learning based face recognition methods have attracted considerable interests in recent years, including principal component analysis (PCA), linear discriminant analysis (LDA), locality preserving projection (LPP), neighborhood preserving embedding (NPE) and marginal Fisher analysis (MFA). However, a disadvantage of all these approaches is that their computations involve eigen- decomposition of dense matrices which is expensive in both time and memory. In this paper, we propose a novel dimensionality reduction framework, called spectral regression (SR), for efficient regularized subspace learning. SR casts the problem of learning the projective functions into a regression framework, which avoids eigen-decomposition of dense matrices. Also, with the regression based framework, different kinds of regularizes can be naturally incorporated into our algorithm which makes it more flexible. Computational analysis shows that SR has only linear-time complexity which is a huge speed up comparing to the cubic-time complexity of the ordinary approaches. Experimental results on face recognition demonstrate the effectiveness and efficiency of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408855",
        "reference_list": [
            {
                "year": "2005",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 121,
            "other": 87,
            "total": 208
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Principal component analysis",
                "Linear discriminant analysis",
                "Strontium",
                "Algorithm design and analysis",
                "Helium",
                "Scattering",
                "Costs",
                "Spectral analysis",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)",
                "principal component analysis",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spectral regression",
                "regularized subspace learning",
                "face recognition methods",
                "principal component analysis",
                "linear discriminant analysis",
                "locality preserving projection",
                "neighborhood preserving embedding",
                "marginal Fisher analysis",
                "dense matrices eigendecomposition"
            ]
        },
        "id": 16,
        "cited_by": [
            {
                "year": "2013",
                "id": 421
            },
            {
                "year": "2011",
                "id": 246
            },
            {
                "year": "2009",
                "id": 116
            },
            {
                "year": "2009",
                "id": 268
            }
        ]
    },
    {
        "title": "Semi-supervised Discriminant Analysis",
        "authors": [
            "Deng Cai",
            "Xiaofei He",
            "Jiawei Han"
        ],
        "abstract": "Linear Discriminant Analysis (LDA) has been a popular method for extracting features which preserve class separability. The projection vectors are commonly obtained by maximizing the between class covariance and simultaneously minimizing the within class covariance. In practice, when there is no sufficient training samples, the covariance matrix of each class may not be accurately estimated. In this paper, we propose a novel method, called Semi- supervised Discriminant Analysis (SDA), which makes use of both labeled and unlabeled samples. The labeled data points are used to maximize the separability between different classes and the unlabeled data points are used to estimate the intrinsic geometric structure of the data. Specifically, we aim to learn a discriminant function which is as smooth as possible on the data manifold. Experimental results on single training image face recognition and relevance feedback image retrieval demonstrate the effectiveness of our algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408856",
        "reference_list": [],
        "citation": {
            "ieee": 198,
            "other": 262,
            "total": 460
        },
        "keywords": {
            "IEEE Keywords": [
                "Linear discriminant analysis",
                "Principal component analysis",
                "Covariance matrix",
                "Face recognition",
                "Information retrieval",
                "Image retrieval",
                "Semisupervised learning",
                "Algorithm design and analysis",
                "Helium",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "covariance matrices",
                "face recognition",
                "feature extraction",
                "image retrieval",
                "relevance feedback"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semisupervised discriminant analysis",
                "linear discriminant analysis",
                "feature extraction",
                "class separability",
                "projection vectors",
                "class covariance",
                "covariance matrix",
                "geometric structure",
                "discriminant function",
                "image face recognition",
                "relevance feedback image retrieval"
            ]
        },
        "id": 17,
        "cited_by": [
            {
                "year": "2009",
                "id": 116
            }
        ]
    },
    {
        "title": "Discriminant Embedding for Local Image Descriptors",
        "authors": [
            "Gang Hua",
            "Matthew Brown",
            "Simon Winder"
        ],
        "abstract": "Invariant feature descriptors such as SIFT and GLOH have been demonstrated to be very robust for image matching and visual recognition. However, such descriptors are generally parameterised in very high dimensional spaces e.g. 128 dimensions in the case of SIFT. This limits the performance of feature matching techniques in terms of speed and scalability. Furthermore, these descriptors have traditionally been carefully hand crafted by manually tuning many parameters. In this paper, we tackle both of these problems by formulating descriptor design as a non- parametric dimensionality reduction problem. In contrast to previous approaches that use only the global statistics of the inputs, we adopt a discriminative approach. Starting from a large training set of labelled match/non-match pairs, we pursue lower dimensional embeddings that are optimised for their discriminative power. Extensive comparative experiments demonstrate that we can exceed the performance of the current state of the art techniques such as SIFT with far fewer dimensions, and with virtually no parameters to be tuned by hand.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408857",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 65,
            "other": 32,
            "total": 97
        },
        "keywords": {
            "IEEE Keywords": [
                "Image recognition",
                "Scalability",
                "Detectors",
                "Face detection",
                "Principal component analysis",
                "Face recognition",
                "Robustness",
                "Image matching",
                "Statistics",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "learning (artificial intelligence)",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local image descriptor",
                "discriminant embedding",
                "invariant feature descriptor",
                "image matching",
                "visual recognition",
                "nonparametric dimensionality reduction problem"
            ]
        },
        "id": 18,
        "cited_by": [
            {
                "year": "2009",
                "id": 268
            },
            {
                "year": "2009",
                "id": 274
            }
        ]
    },
    {
        "title": "Unsupervised Joint Alignment of Complex Images",
        "authors": [
            "Gary B. Huang",
            "Vidit Jain",
            "Erik Learned-Miller"
        ],
        "abstract": "Many recognition algorithms depend on careful positioning of an object into a canonical pose, so the position of features relative to a fixed coordinate system can be examined. Currently, this positioning is done either manually or by training a class-specialized learning algorithm with samples of the class that have been hand-labeled with parts or poses. In this paper, we describe a novel method to achieve this positioning using poorly aligned examples of a class with no additional labeling. Given a set of unaligned examplars of a class, such as faces, we automatically build an alignment mechanism, without any additional labeling of parts or poses in the data set. Using this alignment mechanism, new members of the class, such as faces resulting from a face detector, can be precisely aligned for the recognition process. Our alignment method improves performance on a face recognition task, both over unaligned images and over images aligned with a face alignment algorithm specifically developed for and trained on hand-labeled face images. We also demonstrate its use on an entirely different class of objects (cars), again without providing any information about parts or pose to the learning algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408858",
        "reference_list": [
            {
                "year": "2003",
                "id": 136
            }
        ],
        "citation": {
            "ieee": 110,
            "other": 45,
            "total": 155
        },
        "keywords": {
            "IEEE Keywords": [
                "Face detection",
                "Face recognition",
                "Object detection",
                "Detectors",
                "Labeling",
                "Pipelines",
                "Image recognition",
                "Motion detection",
                "Indoor environments",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "pose estimation",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised joint alignment",
                "image recognition algorithm",
                "canonical pose recognition",
                "class-specialized learning algorithm",
                "face detector",
                "face recognition"
            ]
        },
        "id": 19,
        "cited_by": [
            {
                "year": "2017",
                "id": 182
            },
            {
                "year": "2017",
                "id": 185
            },
            {
                "year": "2013",
                "id": 13
            },
            {
                "year": "2013",
                "id": 85
            },
            {
                "year": "2013",
                "id": 98
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2009",
                "id": 170
            }
        ]
    },
    {
        "title": "Proximity Distribution Kernels for Geometric Context in Category Recognition",
        "authors": [
            "Haibin Ling",
            "Stefano Soatto"
        ],
        "abstract": "We propose using the proximity distribution of vector- quantized local feature descriptors for object and category recognition. To this end, we introduce a novel \"proximity distribution kernel\" that naturally combines local geometric as well as photometric information from images. It satisfies Mercer's condition and can therefore be readily combined with a support vector machine to perform visual categorization in a way that is insensitive to photometric and geometric variations, while retaining significant discriminative power. In particular, it improves on the results obtained both with geometrically unconstrained \"bags of features\" approaches, as well as with over-constrained \"affine procrustes.\" Indeed, we test this approach on several challenging data sets, including Graz-01, Graz-02, and the PASCAL challenge. We registered the average performance at 91.5% on Graz-01, 82.7% on Graz-02, and 74.5% on PASCAL. Our approach is designed to enforce and exploit geometric consistency among objects in the same category; therefore, it does not improve the performance of existing algorithms on datasets where the data is already roughly aligned and scaled. Our method has the potential to be extended to more complex geometric relationships among local features, as we illustrate in the experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408859",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2005",
                "id": 107
            },
            {
                "year": "2005",
                "id": 106
            },
            {
                "year": "2005",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 41,
            "other": 26,
            "total": 67
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Photometry",
                "Computer vision",
                "Lighting",
                "Statistics",
                "Layout",
                "Shape",
                "Solid modeling",
                "Data systems",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "support vector machines",
                "vector quantisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "proximity distribution kernels",
                "geometric context",
                "category recognition",
                "vector-quantized local feature descriptors",
                "object recognition",
                "photometric information",
                "Mercer condition",
                "support vector machine",
                "visual categorization",
                "geometrically unconstrained approaches",
                "over-constrained affine procrustes",
                "Graz-01",
                "Graz-02",
                "PASCAL challenge"
            ]
        },
        "id": 20,
        "cited_by": [
            {
                "year": "2011",
                "id": 114
            },
            {
                "year": "2011",
                "id": 185
            },
            {
                "year": "2011",
                "id": 207
            },
            {
                "year": "2009",
                "id": 55
            },
            {
                "year": "2009",
                "id": 274
            }
        ]
    },
    {
        "title": "High Detection-rate Cascades for Real-Time Object Detection",
        "authors": [
            "Hamed Masnadi-Shirazi",
            "Nuno Vasconcelos"
        ],
        "abstract": "A new strategy is proposed for the design of cascaded object detectors of high detection-rate. The problem of jointly minimizing the false-positive rate and classification complexity of a cascade, given a constraint on its detection rate, is considered. It is shown that it reduces to the problem of minimizing false-positive rate given detection- rate and is, therefore, an instance of the classic problem of cost-sensitive learning. A cost-sensitive extension of boosting, denoted by asymmetric boosting, is introduced. It maintains a high detection-rate across the boosting iterations, and allows the design of cascaded detectors of high overall detection-rate. Experimental evaluation shows that, when compared to previous cascade design algorithms, the cascades produced by asymmetric boosting achieve significantly higher detection-rates, at the cost of a marginal increase in computation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408860",
        "reference_list": [
            {
                "year": "2003",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Boosting",
                "Detectors",
                "Algorithm design and analysis",
                "Costs",
                "Computational efficiency",
                "Design engineering",
                "Design methodology"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image classification",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high detection-rate cascades",
                "real-time object detection",
                "cascade classification complexity",
                "false-positive rate",
                "cost-sensitive learning",
                "asymmetric boosting",
                "cascade design algorithms"
            ]
        },
        "id": 21,
        "cited_by": [
            {
                "year": "2015",
                "id": 375
            }
        ]
    },
    {
        "title": "Graph-Cut Transducers for Relevance Feedback in Content Based Image Retrieval",
        "authors": [
            "Hichem Sahbi",
            "Jean-Yves Audibert",
            "Renaud Keriven"
        ],
        "abstract": "Closing the semantic gap in content based image retrieval (CBIR) basically requires the knowledge of the user's intention which is usually translated into a sequence of questions and answers (Q&A). The user's feedback to these questions provides a CBIR system with a partial labeling of the data and makes it possible to iteratively refine a decision rule on the unlabeled data. Training of this decision rule is referred to as transductive learning. This work is an original approach to relevance feedback (RF) based on graph-cuts. Training consists in implicitly modeling the manifold enclosing both the labeled and unlabeled dataset and finding a partition of this manifold using a min-cut. The contribution of this work is two-fold (i) this is the first comprehensive study of relevance feedback using graph cuts and (ii) our RF model exploits the structure of the data manifold by considering also the structure of the unlabeled data. Experiments conducted on generic as well as specific databases show that our graph-cut based approach is very effective, outperforms other existing methods and makes it possible to converge to almost all the images of the user's \"class of interest\" with a very small labeling effort. A demo is available through our image retrieval tool kit (IRTK).",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408861",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 7,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Transducers",
                "Feedback",
                "Image retrieval",
                "Content based retrieval",
                "Radio frequency",
                "Displays",
                "Labeling",
                "Image databases",
                "Image converters",
                "Bayesian methods"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "graph theory",
                "image retrieval",
                "learning (artificial intelligence)",
                "relevance feedback"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph-cut transducers",
                "relevance feedback",
                "content based image retrieval",
                "decision rule",
                "transductive learning",
                "data manifold",
                "image retrieval tool kit"
            ]
        },
        "id": 22,
        "cited_by": []
    },
    {
        "title": "Locally Smooth Metric Learning with Application to Image Retrieval",
        "authors": [
            "Hong Chang",
            "Dit-Yan Yeung"
        ],
        "abstract": "In this paper, we propose a novel metric learning method based on regularized moving least squares. Unlike most previous metric learning methods which learn a global Mahalanobis distance, we define locally smooth metrics using local affine transformations which are more flexible. The data set after metric learning can preserve the original topological structures. Moreover, our method is fairly efficient and may be used as a preprocessing step for various subsequent learning tasks, including classification, clustering, and nonlinear dimensionality reduction. In particular, we demonstrate that our method can boost the performance of content-based image retrieval (CBIR) tasks. Experimental results provide empirical evidence for the effectiveness of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408862",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 6,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image retrieval",
                "Learning systems",
                "Nearest neighbor searches",
                "Clustering algorithms",
                "Optimization methods",
                "Least squares methods",
                "Content based retrieval",
                "Machine learning",
                "Support vector machines",
                "Support vector machine classification"
            ],
            "INSPEC: Controlled Indexing": [
                "affine transforms",
                "content-based retrieval",
                "image retrieval",
                "learning (artificial intelligence)",
                "pattern classification",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "regularized moving least squares",
                "metric learning methods",
                "global Mahalanobis distance",
                "smooth metrics",
                "affine transformations",
                "pattern classification",
                "pattern clustering",
                "nonlinear dimensionality reduction",
                "content-based image retrieval"
            ]
        },
        "id": 23,
        "cited_by": []
    },
    {
        "title": "Scene Summarization for Online Image Collections",
        "authors": [
            "Ian Simon",
            "Noah Snavely",
            "Steven M. Seitz"
        ],
        "abstract": "We formulate the problem of scene summarization as selecting a set of images that efficiently represents the visual content of a given scene. The ideal summary presents the most interesting and important aspects of the scene with minimal redundancy. We propose a solution to this problem using multi-user image collections from the Internet. Our solution examines the distribution of images in the collection to select a set of canonical views to form the scene summary, using clustering techniques on visual features. The summaries we compute also lend themselves naturally to the browsing of image collections, and can be augmented by analyzing user-specified image tag data. We demonstrate the approach using a collection of images of the city of Rome, showing the ability to automatically decompose the images into separate scenes, and identify canonical views for each scene.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408863",
        "reference_list": [
            {
                "year": "2001",
                "id": 159
            }
        ],
        "citation": {
            "ieee": 96,
            "other": 77,
            "total": 173
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Internet",
                "Cities and towns",
                "Image analysis",
                "Statistical analysis",
                "Histograms",
                "Terminology",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image recognition",
                "Internet",
                "pattern clustering",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene summarization",
                "online image collections",
                "multiuser image collections",
                "Internet",
                "image distribution",
                "scene summary",
                "clustering techniques",
                "visual features",
                "user-specified image tag data",
                "image scenes"
            ]
        },
        "id": 24,
        "cited_by": [
            {
                "year": "2015",
                "id": 148
            },
            {
                "year": "2015",
                "id": 167
            },
            {
                "year": "2013",
                "id": 434
            },
            {
                "year": "2011",
                "id": 143
            },
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2009",
                "id": 78
            }
        ]
    },
    {
        "title": "Unsupervised Learning of Object Deformation Models",
        "authors": [
            "Iasonas Kokkinos",
            "Alan Yuille"
        ],
        "abstract": "The aim of this work is to learn generative models of object deformations in an unsupervised manner. Initially, we introduce an Expectation Maximization approach to estimate a linear basis for deformations by maximizing the likelihood of the training set under an Active Appearance Model (AAM). This approach is shown to successfully capture the global shape variations of objects like faces, cars and hands. However the AAM representation cannot deal with articulated objects, like cows and horses. We therefore extend our approach to a representation that allows for multiple parts with the relationships between them modeled by a Markov Random Field (MRF). Finally, we propose an algorithm for efficiently performing inference on part-based MRF object models by speeding up the estimation of observation potentials. We use manually collected landmarks to compare the alternative models and quantify learning performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408864",
        "reference_list": [
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 10,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Unsupervised learning",
                "Deformable models",
                "Active appearance model",
                "Statistics",
                "Shape",
                "Markov random fields",
                "Object detection",
                "Parameter estimation",
                "Image edge detection",
                "Cows"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "image reconstruction",
                "Markov processes",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised learning",
                "object deformation model",
                "expectation maximization approach",
                "active appearance model",
                "Markov random field"
            ]
        },
        "id": 25,
        "cited_by": [
            {
                "year": "2009",
                "id": 170
            }
        ]
    },
    {
        "title": "A Scalable Approach to Activity Recognition based on Object Use",
        "authors": [
            "Jianxin Wu",
            "Adebola Osuntogun",
            "Tanzeem Choudhury",
            "Matthai Philipose",
            "James M. Rehg"
        ],
        "abstract": "We propose an approach to activity recognition based on detecting and analyzing the sequence of objects that are being manipulated by the user. In domains such as cooking, where many activities involve similar actions, object-use information can be a valuable cue. In order for this approach to scale to many activities and objects, however, it is necessary to minimize the amount of human-labeled data that is required for modeling. We describe a method for automatically acquiring object models from video without any explicit human supervision. Our approach leverages sparse and noisy readings from RFID tagged objects, along with common-sense knowledge about which objects are likely to be used during a given activity, to bootstrap the learning process. We present a dynamic Bayesian network model which combines RFID and video data to jointly infer the most likely activity and object labels. We demonstrate that our approach can achieve activity recognition rates of more than 80% on a real-world dataset consisting of 16 household activities involving 33 objects with significant background clutter. We show that the combination of visual object recognition with RFID data is significantly more effective than the RFID sensor alone. Our work demonstrates that it is possible to automatically learn object models from video of household activities and employ these models for activity recognition, without requiring any explicit human labeling.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408865",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2005",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 98,
            "other": 72,
            "total": 170
        },
        "keywords": {
            "IEEE Keywords": [
                "Radiofrequency identification",
                "Humans",
                "RFID tags",
                "Bayesian methods",
                "Object recognition",
                "Data mining",
                "Information resources",
                "Character recognition",
                "Educational institutions",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "image sequences",
                "learning (artificial intelligence)",
                "object detection",
                "object recognition",
                "radiofrequency identification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "activity recognition",
                "object detection",
                "object-use information",
                "RFID tagged object",
                "dynamic Bayesian network model",
                "visual object recognition",
                "object model learning"
            ]
        },
        "id": 26,
        "cited_by": [
            {
                "year": "2013",
                "id": 408
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2009",
                "id": 248
            }
        ]
    },
    {
        "title": "Scene Modeling Using Co-Clustering",
        "authors": [
            "Jingen Liu",
            "Mubarak Shah"
        ],
        "abstract": "In this paper, we propose a novel approach for scene modeling. The proposed method is able to automatically discover the intermediate semantic concepts. We utilize Maximization of Mutual Information (MMI) co-clustering approach to discover clusters of semantic concepts, which we call intermediate concepts. Each intermediate concept corresponds to a cluster of visterms in the bag of Vis- terms (BOV) paradigm for scene classification. MMI co- clustering results in fewer but meaningful clusters. Unlike k-means which is used to cluster image patches based on their appearances in BOV, MMI co-clustering can group the visterms which are highly correlated to some concept. Unlike probabilistic latent semantic analysis (pLSA), which can be considered as one-sided soft clustering, MMI co- clustering simultaneously clusters visterms and images, so it is able to boost both clustering. In addition, the MMI co- clustering is an unsupervised method. We have extensively tested our proposed approach on two challenging datasets: the fifteen scene categories and the LSCOM dataset, and promising results are obtained.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408866",
        "reference_list": [
            {
                "year": "2003",
                "id": 49
            },
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2005",
                "id": 115
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 45,
            "other": 27,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Computer vision",
                "Mutual information",
                "Wheels",
                "Linear discriminant analysis",
                "Image retrieval",
                "Image analysis",
                "Testing",
                "Windows",
                "Clustering algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "pattern clustering",
                "probability",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene modeling",
                "MMI co-clustering approach",
                "bag of Vis-terms paradigm",
                "probabilistic latent semantic analysis",
                "unsupervised method",
                "LSCOM dataset",
                "image scene clasification"
            ]
        },
        "id": 27,
        "cited_by": [
            {
                "year": "2009",
                "id": 80
            },
            {
                "year": "2009",
                "id": 129
            }
        ]
    },
    {
        "title": "Automatic Cardiac View Classification of Echocardiogram",
        "authors": [
            "J. H. Park",
            "S. K. Zhou",
            "C. Simopoulos",
            "J. Otsuki",
            "D. Comaniciu"
        ],
        "abstract": "We propose a fully automatic system for cardiac view classification of echocardiogram. Given an echo study video sequence, the system outputs a view label among the pre-defined standard views. The system is built based on a machine learning approach that extracts knowledge from an annotated database. It characterizes three features: 1) integrating local and global evidence, 2) utilizing view specific knowledge, and 3) employing a multi-class Logit-boost algorithm. In our prototype system, we classify four standard cardiac views: apical four chamber and apical two chamber, parasternal long axis and parasternal short axis (at mid cavity). We achieve a classification accuracy over 96% both of training and test data sets and the system runs in a second in the environment of Pentium 4 PC with 3.4 GHz CPU and 1.5 G RAM.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408867",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 6,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Valves",
                "Ultrasonic imaging",
                "Motion analysis",
                "Machine learning",
                "Spatial databases",
                "Data mining",
                "Data systems",
                "Biomedical imaging",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "echocardiography",
                "image classification",
                "image sequences",
                "knowledge acquisition",
                "learning (artificial intelligence)",
                "medical image processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic cardiac view classification",
                "echocardiogram",
                "echo study video sequence",
                "machine learning",
                "knowledge extraction",
                "view specific knowledge",
                "multi-class Logit-boost algorithm",
                "apical four chamber",
                "apical two chamber",
                "parasternal long axis",
                "parasternal short axis"
            ]
        },
        "id": 28,
        "cited_by": []
    },
    {
        "title": "Video-based Face Recognition on Real-World Data",
        "authors": [
            "Johannes Stallkamp",
            "Hazim K. Ekenel",
            "Rainer Stiefelhagen"
        ],
        "abstract": "In this paper, we present the classification sub-system of a real-time video-based face identification system which recognizes people entering through the door of a laboratory. Since the subjects are not asked to cooperate with the system but are allowed to behave naturally, this application scenario poses many challenges. Continuous, uncontrolled variations of facial appearance due to illumination, pose, expression, and occlusion need to be handled to allow for successful recognition. Faces are classified by a local appearance-based face recognition algorithm. The obtained confidence scores from each classification are progressively combined to provide the identity estimate of the entire sequence. We introduce three different measures to weight the contribution of each individual frame to the overall classification decision. They are distance- to-model (DTM), distance-to-second-closest (DT2ND), and their combination. Both a k-nearest neighbor approach and a set of Gaussian mixtures are evaluated to produce individual frame scores. We have conducted closed set and open set identification experiments on a database of 41 subjects. The experimental results show that the proposed system is able to reach high correct recognition rates in a difficult scenario.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408868",
        "reference_list": [],
        "citation": {
            "ieee": 41,
            "other": 19,
            "total": 60
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Hidden Markov models",
                "Testing",
                "Lighting",
                "Humans",
                "Focusing",
                "Probability distribution",
                "Head",
                "Image retrieval",
                "Interactive systems"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "Gaussian processes",
                "image classification",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video-based face recognition",
                "face identification system",
                "facial appearance",
                "face classification",
                "distance- to-model",
                "distance-to-second-closest",
                "k-nearest neighbor",
                "Gaussian mixture",
                "closed set identification",
                "open set identification"
            ]
        },
        "id": 29,
        "cited_by": []
    },
    {
        "title": "Spatial Random Partition for Common Visual Pattern Discovery",
        "authors": [
            "Junsong Yuan",
            "Ying Wu"
        ],
        "abstract": "Automatically discovering common visual patterns from a collection of images is an interesting but yet challenging task, in part because it is computationally prohibiting. Although representing images as visual documents based on discrete visual words offers advantages in computation, the performance of these word-based methods largely depends on the quality of the visual word dictionary. This paper presents a novel approach base on spatial random partition and fast word-free image matching. Represented as a set of continuous visual primitives, each image is randomly partitioned many times to form a pool of subimages. Each subimage is queried and matched against the pool, and then common patterns can be localized by aggregating the set of matched subimages. The asymptotic property and the complexity of the proposed method are given in this paper, along with many real experiments. Both theoretical studies and experiment results show its advantages.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408869",
        "reference_list": [
            {
                "year": "2005",
                "id": 59
            },
            {
                "year": "2005",
                "id": 160
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 13,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Pattern matching",
                "Robustness",
                "Image matching",
                "Humans",
                "Eyes",
                "Image segmentation",
                "Image retrieval",
                "Content based retrieval",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "image matching",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatial random partition",
                "common visual pattern discovery",
                "common visual patterns",
                "image collection",
                "image represention",
                "visual documents",
                "discrete visual words",
                "visual word dictionary",
                "word-free image matching",
                "continuous visual primitives"
            ]
        },
        "id": 30,
        "cited_by": [
            {
                "year": "2017",
                "id": 227
            },
            {
                "year": "2009",
                "id": 273
            }
        ]
    },
    {
        "title": "Learning Multiscale Representations of Natural Scenes Using Dirichlet Processes",
        "authors": [
            "Jyri J. Kivinen",
            "Erik B. Sudderth",
            "Michael I. Jordan"
        ],
        "abstract": "We develop nonparametric Bayesian models for multiscale representations of images depicting natural scene categories. Individual features or wavelet coefficients are marginally described by Dirichlet process (DP) mixtures, yielding the heavy-tailed marginal distributions characteristic of natural images. Dependencies between features are then captured with a hidden Markov tree, and Markov chain Monte Carlo methods used to learn models whose latent state space grows in complexity as more images are observed. By truncating the potentially infinite set of hidden states, we are able to exploit efficient belief propagation methods when learning these hierarchical Dirichlet process hidden Markov trees (HDP-HMTs) from data. We show that our generative models capture interesting qualitative structure in natural scenes, and more accurately categorize novel images than models which ignore spatial relationships among features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408870",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2005",
                "id": 115
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 17,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Hidden Markov models",
                "Wavelet coefficients",
                "Statistics",
                "Computer science",
                "Tree graphs",
                "Image representation",
                "Statistical distributions",
                "Bayesian methods",
                "State-space methods"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "hidden Markov models",
                "image representation",
                "Monte Carlo methods",
                "natural scenes",
                "nonparametric statistics",
                "state-space methods",
                "wavelet transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning multiscale representations",
                "natural scenes",
                "Dirichlet processes",
                "nonparametric Bayesian models",
                "multiscale image representations",
                "wavelet coefficients",
                "natural images",
                "hidden Markov tree",
                "Markov chain Monte Carlo methods",
                "latent state space",
                "belief propagation methods"
            ]
        },
        "id": 31,
        "cited_by": []
    },
    {
        "title": "Improving Descriptors for Fast Tree Matching by Optimal Linear Projection",
        "authors": [
            "Krystian Mikolajczyk",
            "Jiri Matas"
        ],
        "abstract": "In this paper we propose to transform an image descriptor so that nearest neighbor (NN) search for correspondences becomes the optimal matching strategy under the assumption that inter-image deviations of corresponding descriptors have Gaussian distribution. The Euclidean NN in the transformed domain corresponds to the NN according to a truncated Mahalanobis metric in the original descriptor space. We provide theoretical justification for the proposed approach and show experimentally that the transformation allows a significant dimensionality reduction and improves matching performance of a state-of-the art SIFT descriptor. We observe consistent improvement in precision-recall and speed of fast matching in tree structures at the expense of little overhead for projecting the descriptors into transformed space. In the context of SIFT vs. transformed M- SIFT comparison, tree search structures are evaluated according to different criteria and query types. All search tree experiments confirm that transformed M-SIFTperforms better than the original SIFT.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408871",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 21,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Neural networks",
                "Object recognition",
                "Nearest neighbor searches",
                "Optimal matching",
                "Gaussian distribution",
                "Histograms",
                "Principal component analysis",
                "Art",
                "Tree data structures",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian distribution",
                "image matching",
                "tree data structures",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tree matching",
                "optimal linear projection",
                "image descriptor transformation",
                "nearest neighbor search",
                "inter-image deviations",
                "Gaussian distribution",
                "Euclidean NN",
                "truncated Mahalanobis metric",
                "tree search structures"
            ]
        },
        "id": 32,
        "cited_by": []
    },
    {
        "title": "What, where and who? Classifying events by scene and object recognition",
        "authors": [
            "Li-Jia Li",
            "Li Fei-Fei"
        ],
        "abstract": "We propose a first attempt to classify events in static images by integrating scene and object categorizations. We define an event in a static image as a human activity taking place in a specific environment. In this paper, we use a number of sport games such as snow boarding, rock climbing or badminton to demonstrate event classification. Our goal is to classify the event in the image as well as to provide a number of semantic labels to the objects and scene environment within the image. For example, given a rowing scene, our algorithm recognizes the event as rowing by classifying the environment as a lake and recognizing the critical objects in the image as athletes, rowing boat, water, etc. We achieve this integrative and holistic recognition through a generative graphical model. We have assembled a highly challenging database of 8 widely varied sport events. We show that our system is capable of classifying these event classes at 73.4% accuracy. While each component of the model contributes to the final recognition, using scene or objects alone cannot achieve this performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408872",
        "reference_list": [],
        "citation": {
            "ieee": 237,
            "other": 160,
            "total": 397
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Object recognition",
                "Image recognition",
                "Humans",
                "Snow",
                "Lakes",
                "Boats",
                "Graphical models",
                "Assembly",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object recognition",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "events classification",
                "object categorizations",
                "static image",
                "semantic labels",
                "integrative recognition",
                "holistic recognition",
                "generative graphical model"
            ]
        },
        "id": 33,
        "cited_by": [
            {
                "year": "2013",
                "id": 35
            },
            {
                "year": "2013",
                "id": 148
            },
            {
                "year": "2013",
                "id": 424
            }
        ]
    },
    {
        "title": "An Empirical Study of Object Category Recognition: Sequential Testing with Generalized Samples",
        "authors": [
            "Liang Lin",
            "Shaowu Peng",
            "Jake Porway",
            "Song-Chun Zhu",
            "Yongtian Wang"
        ],
        "abstract": "In this paper we present an empirical study of object category recognition using generalized samples and a set of sequential tests. We study 33 categories, each consisting of a small data set of 30 instances. To increase the amount of training data we have, we use a compositional object model to learn a representation for each category from which we select 30 additional templates with varied appearance from the training set. These samples better span the appearance space and form an augmented training set \u03a9 T of 1980 (60\u00d733) training templates. To perform recognition on a testing image, we use a set of sequential tests to project \u03a9 T into different representation spaces to narrow the number of candidate matches in \u03a9 T . We use\"graphlets\"(structural elements), as our local features and model Omega T at each stage using histograms of graphlets over categories, histograms of graphlets over object instances, histograms of pairs of graphlets over objects, shape context. Each test is increasingly computationally expensive, and by the end of the cascade we have a small candidate set remaining to use with our most powerful test, a top-down graph matching algorithm. We achieve an 81.4 % classification rate on classifying 800 testing images in 33 categories, 15.2% more accurate than a method without generalized samples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408873",
        "reference_list": [
            {
                "year": "2005",
                "id": 232
            },
            {
                "year": "2005",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Sequential analysis",
                "Testing",
                "Histograms",
                "Training data",
                "Image recognition",
                "Statistical analysis",
                "Computer science",
                "Performance evaluation",
                "Context modeling",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image classification",
                "image matching",
                "image representation",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object category recognition",
                "sequential testing",
                "generalized sample",
                "compositional object model",
                "augmented training set",
                "image recognition",
                "image representation",
                "graphlet",
                "top-down graph matching algorithm",
                "image classification"
            ]
        },
        "id": 34,
        "cited_by": []
    },
    {
        "title": "Fast Bilinear SfM with Side Information",
        "authors": [
            "Mahesh Ramachandran",
            "Ashok Veeraraghavan",
            "Rama Chellappa"
        ],
        "abstract": "We study the beneficial effect of side information on the Structure from Motion (SfM) estimation problem. The side information that we consider is measurement of a 'reference vector' and distance from fixed plane perpendicular to that reference vector. Firstly, we show that in the presence of this information, the SfM equations can be rewritten similar to a bilinear form in its unknowns. Secondly, we describe a fast iterative estimation procedure to recover the structure of both stationary scenes and moving objects that capitalizes on this information. We also provide a refinement procedure in order to tackle incomplete or noisy side information. We characterize the algorithm with respect to its reconstruction accuracy, memory requirements and stability. Finally, we describe two classes of commonly occurring real-world scenarios in which this algorithm will be effective: (a) presence of a dominant ground plane in the scene and (b) presence of an inertial measurement unit on board. Experiments using both real data and rigorous simulations show the efficacy of the algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408874",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Gravity",
                "Equations",
                "Measurement units",
                "Accelerometers",
                "Image reconstruction",
                "Automation",
                "Educational institutions",
                "Motion estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "iterative methods",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "side information",
                "reference vector",
                "iterative estimation procedure",
                "bilinear structure recovery",
                "refinement procedure",
                "image reconstruction",
                "motion estimation"
            ]
        },
        "id": 35,
        "cited_by": []
    },
    {
        "title": "Learning The Discriminative Power-Invariance Trade-Off",
        "authors": [
            "Manik Varma",
            "Debajyoti Ray"
        ],
        "abstract": "We investigate the problem of learning optimal descriptors for a given classification task. Many hand-crafted descriptors have been proposed in the literature for measuring visual similarity. Looking past initial differences, what really distinguishes one descriptor from another is the tradeoff that it achieves between discriminative power and invariance. Since this trade-off must vary from task to task, no single descriptor can be optimal in all situations. Our focus, in this paper, is on learning the optimal tradeoff for classification given a particular training set and prior constraints. The problem is posed in the kernel learning framework. We learn the optimal, domain-specific kernel as a combination of base kernels corresponding to base features which achieve different levels of trade-off (such as no invariance, rotation invariance, scale invariance, affine invariance, etc.) This leads to a convex optimisation problem with a unique global optimum which can be solved for efficiently. The method is shown to achieve state-of-the-art performance on the UIUC textures, Oxford flowers and Cal- tech 101 datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408875",
        "reference_list": [
            {
                "year": "2007",
                "id": 225
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2007",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 191,
            "other": 109,
            "total": 300
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Educational institutions",
                "Focusing",
                "Image retrieval",
                "Content based retrieval",
                "Inspection",
                "Euclidean distance",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative power-invariance trade-off",
                "learning optimal descriptors",
                "kernel learning framework",
                "convex optimisation problem",
                "global optimum"
            ]
        },
        "id": 36,
        "cited_by": [
            {
                "year": "2015",
                "id": 339
            },
            {
                "year": "2013",
                "id": 155
            },
            {
                "year": "2013",
                "id": 336
            },
            {
                "year": "2011",
                "id": 8
            },
            {
                "year": "2011",
                "id": 9
            },
            {
                "year": "2011",
                "id": 49
            },
            {
                "year": "2011",
                "id": 183
            },
            {
                "year": "2011",
                "id": 231
            },
            {
                "year": "2009",
                "id": 5
            },
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 37
            },
            {
                "year": "2009",
                "id": 55
            },
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2009",
                "id": 76
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2009",
                "id": 80
            },
            {
                "year": "2009",
                "id": 81
            },
            {
                "year": "2009",
                "id": 140
            },
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2009",
                "id": 276
            },
            {
                "year": "2007",
                "id": 37
            }
        ]
    },
    {
        "title": "Locally Invariant Fractal Features for Statistical Texture Classification",
        "authors": [
            "Manik Varma",
            "Rahul Garg"
        ],
        "abstract": "We address the problem of developing discriminative, yet invariant, features for texture classification. Texture variations due to changes in scale are amongst the hardest to handle. One of the most successful methods of dealing with such variations is based on choosing interest points and selecting their characteristic scales [Lazebnik et al. PAMI 2005]. However, selecting a characteristic scale can be unstable for many textures. Furthermore, the reliance on an interest point detector and the inability to evaluate features densely can be serious limitations. Fractals present a mathematically well founded alternative to dealing with the problem of scale. However, they have not become popular as texture features due to their lack of discriminative power. This is primarily because: (a) fractal based classification methods have avoided statistical characterisations of textures (which is essential for accurate analysis) by using global features; and (b) fractal dimension features are unable to distinguish between key texture primitives such as edges, corners and uniform regions. In this paper, we overcome these drawbacks and develop local fractal features that are evaluated densely. The features are robust as they do not depend on choosing interest points or characteristic scales. Furthermore, it is shown that the local fractal dimension is invariant to local bi-Lipschitz transformations whereas its extension is able to correctly distinguish between fundamental texture primitives. Textures are characterised statistically by modelling the full joint PDF of these features. This allows us to develop a texture classification framework which is discriminative, robust and achieves state-of-the-art performance as compared to affine invariant and fractal based methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408876",
        "reference_list": [
            {
                "year": "2005",
                "id": 208
            },
            {
                "year": "2001",
                "id": 191
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 48,
            "total": 82
        },
        "keywords": {
            "IEEE Keywords": [
                "Fractals",
                "Detectors",
                "Lighting",
                "Robustness",
                "Skin",
                "Laplace equations"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction",
                "fractals",
                "image classification",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "locally invariant fractal feature extraction",
                "statistical texture classification",
                "interest point detector",
                "local bi-Lipschitz transformation"
            ]
        },
        "id": 37,
        "cited_by": [
            {
                "year": "2011",
                "id": 154
            },
            {
                "year": "2007",
                "id": 36
            }
        ]
    },
    {
        "title": "Learning Structured Appearance Models from Captioned Images of Cluttered Scenes",
        "authors": [
            "Michael Jamieson",
            "Afsaneh Fazly",
            "Sven Dickinson",
            "Suzanne Stevenson",
            "Sven Wachsmuth"
        ],
        "abstract": "Given an unstructured collection of captioned images of cluttered scenes featuring a variety of objects, our goal is to learn both the names and appearances of the objects. Only a small number of local features within any given image are associated with a particular caption word. We describe a connected graph appearance model where vertices represent local features and edges encode spatial relationships. We use the repetition of feature neighborhoods across training images and a measure of correspondence with caption words to guide the search for meaningful feature configurations. We demonstrate improved results on a dataset to which an unstructured object model was previously applied. We also apply the new method to a more challenging collection of captioned images from the Web, detecting and annotating objects within highly cluttered realistic scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408877",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Shape",
                "Robustness",
                "Object detection",
                "Image databases",
                "Image storage",
                "Deformable models",
                "Image edge detection",
                "Space exploration",
                "Visual databases"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "learning (artificial intelligence)",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning structured appearance models",
                "captioned images",
                "connected graph appearance model",
                "feature neighborhoods",
                "training images",
                "cluttered realistic scenes"
            ]
        },
        "id": 38,
        "cited_by": [
            {
                "year": "2009",
                "id": 273
            }
        ]
    },
    {
        "title": "How Good are Local Features for Classes of Geometric Objects",
        "authors": [
            "Michael Stark",
            "Bernt Schiele"
        ],
        "abstract": "Recent work in object categorization often uses local image descriptors such as SIFT to learn and detect object categories. Such descriptors explicitly code local appearance and have shown impressive results on objects with sufficient local appearance statistics. However, many important object classes such as tools, cups and other man-made artifacts seem to require features that capture the respective shape and geometric layout of those object classes. Therefore this paper compares, on a novel data collection of 10 geometric object classes, various shape-based features with appearance-based descriptors such as SIFT. The analysis includes a direct comparison of feature statistics as well as results within standard recognition frameworks, which are partly intuitive, but sometimes surprising.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408878",
        "reference_list": [
            {
                "year": "2005",
                "id": 234
            },
            {
                "year": "2005",
                "id": 103
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 12,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Statistics",
                "Object recognition",
                "Image segmentation",
                "Performance evaluation",
                "Computer science",
                "Object detection",
                "Statistical analysis",
                "Focusing",
                "Motorcycles"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local features",
                "geometric objects",
                "object categorization",
                "geometric layout",
                "shape-based features",
                "appearance-based descriptors",
                "feature statistics",
                "geometric objects recognition"
            ]
        },
        "id": 39,
        "cited_by": [
            {
                "year": "2009",
                "id": 47
            }
        ]
    },
    {
        "title": "Contour Grouping Based on Local Symmetry",
        "authors": [
            "Nagesh Adluru",
            "Longin Jan Latecki",
            "Rolf Lakaemper",
            "Thomas Young",
            "Xiang Bai",
            "Ari Gross"
        ],
        "abstract": "The paper deals with grouping of edges to contours of shapes using only local symmetry and continuity. Shape skeletons are used to generate the search space for a version of the Markov Chain Monte Carlo approach utilizing particle filters to find the most likely skeleton. Intuitively this means that grouping of edge segments is performed by walking along the skeleton. The particle search, which is an adapted version of a successful algorithm in robot mapping, is assisted by a reference model of a shape, which is expressed as the sequence of sample points and radii of maximal skeleton disks. This model is sufficiently flexible to represent non-rigid deformations, but restrictive enough to perform well on real, noisy image data. The order of skeleton points (and their corresponding segments) found by the particles defines the grouping.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408879",
        "reference_list": [
            {
                "year": "2005",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Skeleton",
                "Shape",
                "Robots",
                "Legged locomotion",
                "Simultaneous localization and mapping",
                "Particle filters",
                "Noise shaping",
                "Image segmentation",
                "Educational institutions",
                "Monte Carlo methods"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "Markov processes",
                "Monte Carlo methods",
                "particle filtering (numerical methods)",
                "search problems",
                "SLAM (robots)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contour grouping",
                "shape skeletons",
                "search space",
                "Markov chain Monte Carlo approach",
                "particle filters",
                "robot mapping",
                "maximal skeleton disks",
                "nonrigid deformations"
            ]
        },
        "id": 40,
        "cited_by": []
    },
    {
        "title": "High-Dimensional Feature Matching: Employing the Concept of Meaningful Nearest Neighbors",
        "authors": [
            "Dusan Omercevic",
            "Ondrej Drbohlav",
            "Ales Leonardis"
        ],
        "abstract": "Matching of high-dimensional features using nearest neighbors search is an important part of image matching methods which are based on local invariant features. In this work we highlight effects pertinent to high-dimensional spaces that are significant for matching, yet have not been explicitly accounted for in previous work. In our approach, we require every nearest neighbor to be meaningful, that is, sufficiently close to a query feature such that it is an outlier to a background feature distribution. We estimate the background feature distribution from the extended neighborhood of a query feature given by its k nearest neighbors. Based on the concept of meaningful nearest neighbors, we develop a novel high-dimensional feature matching method and evaluate its performance by conducting image matching on two challenging image data sets. A superior performance in terms of accuracy is shown in comparison to several state-of-the-art approaches. Additionally, to make search for k nearest neighbors more efficient, we develop a novel approximate nearest neighbors search method based on sparse coding with an overcomplete basis set that provides a ten-fold speed-up over an exhaustive search even for high dimensional spaces and retains excellent approximation to an exact nearest neighbors search.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408880",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 18,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Nearest neighbor searches",
                "Image matching",
                "Computer vision",
                "Voting",
                "Vocabulary",
                "Image retrieval",
                "Vector quantization",
                "Information science",
                "Image coding",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "feature extraction",
                "image matching",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high-dimensional feature matching",
                "meaningful nearest neighbor",
                "image matching method",
                "local invariant feature",
                "background feature distribution estimation",
                "approximate nearest neighbors search method"
            ]
        },
        "id": 41,
        "cited_by": []
    },
    {
        "title": "Toward Reconstructing Surfaces With Arbitrary Isotropic Reflectance : A Stratified Photometric Stereo Approach",
        "authors": [
            "Neil G. Alldrin",
            "David J. Kriegman"
        ],
        "abstract": "We consider the problem of reconstructing the shape of a surface with an arbitrary, spatially varying isotropic bidirectional reflectance distribution function (BRDF), and introduce a novel, stratified photometric stereo method. By using a particular configuration of lights, it is possible to use symmetry in the image measurements resulting from BRDF isotropy to estimate at each point a plane containing the surface normal. For differentiable surfaces, this allows us to recover the isocontours of the depth map, but not the actual depth associated with each contour. The isocontour structure provides topological information about the surface (critical points, Reeb graph, etc.). By using additional cues in the image data or imposing additional constraints on the surface (e.g., shadows, specular highlights, Helmholtz reciprocity, uniform BRDF), the unknown height of each isocontour can be estimated and the metric structure is resolved. We validate this technique on real and synthetic data by successfully recovering the isocontours of the depth map from images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408881",
        "reference_list": [
            {
                "year": "2005",
                "id": 44
            },
            {
                "year": "2001",
                "id": 157
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 15,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Reflectivity",
                "Photometry",
                "Image reconstruction",
                "Stereo image processing",
                "Shape",
                "Bidirectional control",
                "Distribution functions",
                "Particle measurements",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "stereo image processing",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface reconstruction",
                "isotropic reflectance",
                "stratified photometric stereo approach",
                "isotropic bidirectional reflectance distribution function",
                "image measurements",
                "BRDF isotropy",
                "isocontours",
                "isocontour structure",
                "topological information",
                "image data"
            ]
        },
        "id": 42,
        "cited_by": [
            {
                "year": "2017",
                "id": 333
            },
            {
                "year": "2009",
                "id": 217
            }
        ]
    },
    {
        "title": "Reconstructing the Surface of Inhomogeneous Transparent Scenes by Scatter-Trace Photography",
        "authors": [
            "Nigel J. W. Morris",
            "Kiriakos N. Kutulakos"
        ],
        "abstract": "We present a new method for reconstructing the exterior surface of a complex transparent scene with inhomogeneous interior (e.g., multiple interfaces, reflective or painted interiors, etc). Our approach involves capturing images of the scene from one or more viewpoints while moving a proximal light source to a 2D or 3D set of positions. This gives a 2D (or 3D) dataset per pixel, called the scatter trace. The key idea of our approach is that even though light transport within a transparent scene's interior can be exceedingly complex, the scatter trace of each pixel has a highly-constrained geometry that (1) reveals the contribution of direct surface reflection, and (2) leads to a simple \"scatter- trace stereo\" algorithm for computing the local geometry of the exterior surface (depth and surface normals). We present 3D reconstruction results for a variety of scenes that exhibit complex light transport phenomena.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408882",
        "reference_list": [
            {
                "year": "2005",
                "id": 56
            },
            {
                "year": "2003",
                "id": 135
            },
            {
                "year": "2005",
                "id": 189
            },
            {
                "year": "2005",
                "id": 205
            },
            {
                "year": "2003",
                "id": 78
            },
            {
                "year": "2005",
                "id": 188
            },
            {
                "year": "2003",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 22,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Layout",
                "Photography",
                "Light scattering",
                "Optical reflection",
                "Image reconstruction",
                "Optical scattering",
                "Shape",
                "Reflectivity",
                "Light sources"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface reconstruction",
                "inhomogeneous transparent scenes",
                "scatter-trace photography",
                "highly-constrained geometry",
                "3D reconstruction"
            ]
        },
        "id": 43,
        "cited_by": [
            {
                "year": "2011",
                "id": 149
            }
        ]
    },
    {
        "title": "Toward a Theory of Shape from Specular Flow",
        "authors": [
            "Yair Adato",
            "Yuriy Vasilyev",
            "Ohad Ben-Shahar",
            "Todd Zickler"
        ],
        "abstract": "The image of a curved, specular (mirror-like) surface is a distorted reflection of the environment. The goal of our work is to develop a framework for recovering general shape from such distortions when the environment is neither calibrated nor known. To achieve this goal we consider far-field illumination, where the object-environment distance is relatively large, and we examine the dense specular flow that is induced on the image plane through relative object-environment motion. We show that under these very practical conditions the observed specular flow can be related to surface shape through a pair of coupled nonlinear partial differential equations. Importantly, this relationship depends only on the environment's relative motion and not its content. We examine the qualitative properties of these equations, present analytic methods for recovery of the shape in several special cases, and empirically validate our results using captured data. We also discuss the relevance to both computer vision and human perception.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408883",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 15,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Surface reconstruction",
                "Nonlinear distortion",
                "Image reconstruction",
                "Reflection",
                "Lighting",
                "Equations",
                "Humans",
                "Cameras",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "nonlinear differential equations",
                "partial differential equations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "specular flow",
                "distorted reflection",
                "shape recovery",
                "far-field illumination",
                "coupled nonlinear partial differential equation",
                "analytic method",
                "computer vision",
                "human perception"
            ]
        },
        "id": 44,
        "cited_by": [
            {
                "year": "2011",
                "id": 73
            },
            {
                "year": "2009",
                "id": 24
            }
        ]
    },
    {
        "title": "A Robust Graph-Based Method for The General Correspondence Problem Demonstrated on Image Stitching",
        "authors": [
            "Martin Bujnak",
            "Radim Sara"
        ],
        "abstract": "We pose robust matching with parametric and non-parametric constraints as the problem of finding a stable independent set (SIS) in an oriented graph whose vertices are all possible correspondences, whose edges capture the structure of the constraints and whose edge orientation represents pairwise comparison 'is better' based on correspondence quality, including the uncertainty of this comparison. We show SIS possess properties of both robustness and weak optimality. The main contribution of this paper is algorithmic speedup that results from exploiting the dependence between the standard uniqueness constraint and the parametric constraint. The general theory is demonstrated on the example of image stitching using homography model. The algorithm needs at most \ud835\udf05N 2 calls of a procedure testing if two ellipse correspondences are consistent with a general homography. The previous known SIS algorithm needed O(N 4 ) tests. Experiments show the method gives good results and is fast in practice with \ud835\udf05\u22480.3.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408884",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Polynomials",
                "Testing",
                "Robust stability",
                "Computational efficiency",
                "Cost function",
                "Uncertainty",
                "Solid modeling",
                "Image sampling",
                "Least squares approximation"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "graph theory",
                "image matching",
                "set theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image stitching",
                "robust graph-based method",
                "general correspondence problem",
                "image matching",
                "nonparametric constraint",
                "stable independent set process",
                "edge orientation",
                "homography model"
            ]
        },
        "id": 45,
        "cited_by": []
    },
    {
        "title": "Improving Numerical Accuracy of Gr\u00f6bner Basis Polynomial Equation Solvers",
        "authors": [
            "Martin Byrod",
            "Klas Josephson",
            "Kalle Astrom"
        ],
        "abstract": "This paper presents techniques for improving the numerical stability of Grobner basis solvers for polynomial equations. Recently Grobner basis methods have been used successfully to solve polynomial equations arising in global optimization e.g. three view triangulation and in many important minimal cases of structure from motion. Such methods work extremely well for problems of reasonably low degree, involving a few variables. Currently, the limiting factor in using these methods for larger and more demanding problems is numerical difficulties. In the paper we (i) show how to change basis in the quotient space R[x]/I and propose a strategy for selecting a basis which improves the conditioning of a crucial elimination step, (ii) use this technique to devise a Grobner basis with improved precision and (iii) show how solving for the eigenvalues instead of eigenvectors can be used to improve precision further while retaining the same speed. We study these methods on some of the latest reported uses of Grobner basis methods and demonstrate dramatically improved numerical precision using these new techniques making it possible to solve a larger class of problems than previously.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408885",
        "reference_list": [
            {
                "year": "2005",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 12,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Equations",
                "Optimization methods",
                "Linear matrix inequalities",
                "Linear algebra",
                "Numerical stability",
                "Eigenvalues and eigenfunctions",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "numerical stability",
                "polynomial approximation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "numerical stability",
                "Grobner basis solver",
                "polynomial equation",
                "quotient space",
                "eigenvalues"
            ]
        },
        "id": 46,
        "cited_by": [
            {
                "year": "2009",
                "id": 221
            },
            {
                "year": "2009",
                "id": 231
            }
        ]
    },
    {
        "title": "A Discrete Differential Operator for Direction-based Surface Morphometry",
        "authors": [
            "Maxime Boucher",
            "Oliver Lyttelton",
            "Sue Whitesides",
            "Alan Evans"
        ],
        "abstract": "This paper presents a novel directional morphometry method for surfaces using first order derivatives. Non-directional surface morphometry has been previously used to detect regions of cortical atrophy using brain MRI data. However, evaluating directional changes on surfaces requires computing gradients to obtain a full metric tensor. Non-directionality reduces the sensitivity of deformation-based morphometry to area-preserving deformations. By proposing a method to compute directional derivatives, this paper enables analysis of directional deformations on surfaces. Moreover, the proposed method exhibits improved numerical accuracy when evaluating mean curvature, compared to the so-called cotangent formula. The directional deformation of folding patterns was measured in two groups of surfaces and the proposed methodology allowed to defect morphological differences that were not detected using previous non-directional morphometry. The methodology uses a closed-form analytic formalism rather than numerical approximation and is readily generalizable to any application involving surface deformation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408886",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Surface morphology",
                "Brain",
                "Computer science",
                "Shape measurement",
                "Magnetic resonance imaging",
                "Turing machines",
                "Application software",
                "Computer graphics",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "computational geometry",
                "differential geometry",
                "function approximation",
                "mathematical operators",
                "medical image processing",
                "neurophysiology",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discrete differential operator",
                "direction-based surface morphometry",
                "first order derivatives",
                "closed-form analytic formalism",
                "brain MRI data",
                "discrete linear approximation",
                "differential geometry"
            ]
        },
        "id": 47,
        "cited_by": []
    },
    {
        "title": "Stable Affine Frames on Isophotes",
        "authors": [
            "Michal Perdoch",
            "Jiri Matas",
            "Stepan Obdrzalek"
        ],
        "abstract": "We propose a new affine-covariant feature, the stable affine frame (SAF). SAFs lie on the boundary of extremal regions, i.e. on isophotes. Instead of requiring the whole isophote to be stable with respect to intensity perturbation as in maximally stable extremal regions (MSERs), stability is required only locally, for the primitives constituting the three-point frames. The primitives are extracted by an affine invariant process that exploits properties of bitangents and algebraic moments. Thus, instead of using closed stable isophotes, i.e. MSERs, and detecting affine frames on them, SAFs are sought even on some unstable extremal regions. We show experimentally on standard datasets that SAFs have repeatability comparable to the best affine covariant detectors tested in the state-of-the-art report (Mikolajczyk et al., 2005) and consistently produce a significantly higher number of features per image. Moreover, the features cover images more evenly than MSERs, which facilitates robustness to occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408887",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            },
            {
                "year": "2003",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Stability",
                "Testing",
                "Object detection",
                "Computer vision",
                "Object recognition",
                "Shape",
                "Robustness",
                "Application software",
                "Signal processing"
            ],
            "INSPEC: Controlled Indexing": [
                "covariance analysis",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "affine-covariant feature detection",
                "stable affine frame",
                "intensity perturbation",
                "maximally stable extremal region",
                "algebraic moment",
                "bitangent moment",
                "stable isophotes"
            ]
        },
        "id": 48,
        "cited_by": []
    },
    {
        "title": "Penrose Pixels Super-Resolution in the Detector Layout Domain",
        "authors": [
            "Moshe Ben-Ezra",
            "Zhouchen Lin",
            "Bennett Wilburn"
        ],
        "abstract": "We present a novel approach to reconstruction based super- resolution that explicitly models the detector's pixel layout. Pixels in our model can vary in shape and size, and there may be gaps between adjacent pixels. Furthermore, their layout can be periodic as well as aperiodic, such as penrose tiling or a biological retina. We also present a new variant of the well known error back-projection super-resolution algorithm that makes use of the exact detector model in its back projection operator for better accuracy. Our method can be applied equally well to either periodic or aperiodic pixel tiling. Through analysis and extensive testing using synthetic and real images, we show that our approach outperforms existing reconstruction based algorithms for regular pixel arrays. We obtain significantly better results using aperiodic pixel layouts. As an interesting example, we apply our method to a retina-like pixel structure modeled by a centroidal Voronoi tessellation. We demonstrate that, in principle, this structure is better for super-resolution than the regular pixel array used in today's sensors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408888",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 9,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Biological system modeling",
                "Sensor arrays",
                "Image reconstruction",
                "Shape",
                "Retina",
                "Image analysis",
                "Algorithm design and analysis",
                "Testing",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "eye",
                "image reconstruction",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "penrose pixels",
                "detector layout domain",
                "image reconstruction",
                "pixel layout",
                "penrose tiling",
                "biological retina",
                "error backprojection super-resolution algorithm",
                "aperiodic pixel tiling",
                "pixel arrays",
                "retina-like pixel structure",
                "centroidal Voronoi tessellation"
            ]
        },
        "id": 49,
        "cited_by": []
    },
    {
        "title": "Dynamically consistent optical flow estimation",
        "authors": [
            "Nicolas Papadakis",
            "Thomas Corpetti",
            "Etienne Memin"
        ],
        "abstract": "In this paper, we present a framework for dynamic consistent estimation of dense motion fields over a sequence of images. The originality of the approach is to exploit recipes related to optimal control theory. This setup allows performing the estimation of an unknown state function according to a given dynamical model and to noisy and incomplete measurements. The overall process is formalized through the minimization of a global spatio-temporal cost functional w.r.t the complete sequence of motion fields. The minimization is handled considering an adjoint formulation. The resulting scheme consists in iterating a forward integration of the evolution model and a backward integration of the adjoint evolution model guided by a discrepancy measurement between the state variable and the available noisy observations. Such an approach allows us to cope with several delicate situations (such as the absence of data) which are not well managed with usual estimators.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408889",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 13,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Motion estimation",
                "Optimal control",
                "Optical filters",
                "Optical computing",
                "Time measurement",
                "State-space methods",
                "Bayesian methods",
                "Iron",
                "Performance evaluation"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "motion estimation",
                "state estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamically consistent optical flow estimation",
                "dense motion fields estimation",
                "optimal control theory",
                "state function estimation",
                "global spatiotemporal cost functional minimization",
                "motion field sequence",
                "adjoint formulation",
                "joint evolution model",
                "discrepancy measurement"
            ]
        },
        "id": 50,
        "cited_by": [
            {
                "year": "2007",
                "id": 105
            }
        ]
    },
    {
        "title": "MRF Optimization via Dual Decomposition: Message-Passing Revisited",
        "authors": [
            "Nikos Komodakis",
            "Nikos Paragios",
            "Georgios Tziritas"
        ],
        "abstract": "A new message-passing scheme for MRF optimization is proposed in this paper. This scheme inherits better theoretical properties than all other state-of-the-art message passing methods and in practice performs equally well/outperforms them. It is based on the very powerful technique of Dual Decomposition [1] and leads to an elegant and general framework for understanding/designing message-passing algorithms that can provide new insights into existing techniques. Promising experimental results and comparisons with the state of the art demonstrate the extreme theoretical and practical potentials of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408890",
        "reference_list": [],
        "citation": {
            "ieee": 59,
            "other": 40,
            "total": 99
        },
        "keywords": {
            "IEEE Keywords": [
                "Message passing",
                "Optimization methods",
                "Algorithm design and analysis",
                "Computer vision",
                "Tree graphs",
                "Belief propagation",
                "Integer linear programming"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "message passing",
                "nonlinear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "message passing scheme",
                "MRF optimization",
                "dual decomposition",
                "computer vision"
            ]
        },
        "id": 51,
        "cited_by": [
            {
                "year": "2011",
                "id": 9
            },
            {
                "year": "2011",
                "id": 40
            },
            {
                "year": "2011",
                "id": 174
            },
            {
                "year": "2011",
                "id": 210
            },
            {
                "year": "2009",
                "id": 70
            },
            {
                "year": "2009",
                "id": 298
            }
        ]
    },
    {
        "title": "Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval",
        "authors": [
            "Ondrej Chum",
            "James Philbin",
            "Josef Sivic",
            "Michael Isard",
            "Andrew Zisserman"
        ],
        "abstract": "Given a query image of an object, our objective is to retrieve all instances of that object in a large (1M+) image database. We adopt the bag-of-visual-words architecture which has proven successful in achieving high precision at low recall. Unfortunately, feature detection and quantization are noisy processes and this can result in variation in the particular visual words that appear in different images of the same object, leading to missed results. In the text retrieval literature a standard method for improving performance is query expansion. A number of the highly ranked documents from the original query are reissued as a new query. In this way, additional relevant terms can be added to the query. This is a form of blind rele- vance feedback and it can fail if 'outlier' (false positive) documents are included in the reissued query. In this paper we bring query expansion into the visual domain via two novel contributions. Firstly, strong spatial constraints between the query image and each result allow us to accurately verify each return, suppressing the false positives which typically ruin text-based query expansion. Secondly, the verified images can be used to learn a latent feature model to enable the controlled construction of expanded queries. We illustrate these ideas on the 5000 annotated image Oxford building database together with more than 1M Flickr images. We show that the precision is substantially boosted, achieving total recall in many cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408891",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 246,
            "other": 172,
            "total": 418
        },
        "keywords": {
            "IEEE Keywords": [
                "Information retrieval",
                "Image retrieval",
                "Feedback",
                "Image databases",
                "Feature extraction",
                "Vocabulary",
                "Data mining",
                "Solid modeling",
                "Geometry",
                "Data engineering"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "relevance feedback",
                "very large databases",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "total recall",
                "generative feature model",
                "object retrieval",
                "query image",
                "bag-of-visual-words architecture",
                "text retrieval literature",
                "blind relevance feedback",
                "visual domain",
                "text-based query expansion",
                "image verification"
            ]
        },
        "id": 52,
        "cited_by": [
            {
                "year": "2015",
                "id": 158
            },
            {
                "year": "2015",
                "id": 209
            },
            {
                "year": "2015",
                "id": 301
            },
            {
                "year": "2013",
                "id": 174
            },
            {
                "year": "2013",
                "id": 212
            },
            {
                "year": "2013",
                "id": 227
            },
            {
                "year": "2013",
                "id": 317
            },
            {
                "year": "2013",
                "id": 436
            },
            {
                "year": "2011",
                "id": 26
            },
            {
                "year": "2011",
                "id": 47
            },
            {
                "year": "2011",
                "id": 110
            },
            {
                "year": "2011",
                "id": 143
            },
            {
                "year": "2011",
                "id": 209
            },
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2009",
                "id": 141
            },
            {
                "year": "2009",
                "id": 197
            }
        ]
    },
    {
        "title": "Efficient Multi-View Reconstruction of Large-Scale Scenes using Interest Points, Delaunay Triangulation and Graph Cuts",
        "authors": [
            "Patrick Labatut",
            "Jean-Philippe Pons",
            "Renaud Keriven"
        ],
        "abstract": "We present a novel method to reconstruct the 3D shape of a scene from several calibrated images. Our motivation is that most existing multi-view stereovision approaches require some knowledge of the scene extent and often even of its approximate geometry (e.g. visual hull). This makes these approaches mainly suited to compact objects admitting a tight enclosing box, imaged on a simple or a known background. In contrast, our approach focuses on large-scale cluttered scenes under uncontrolled imaging conditions. It first generates a quasi-dense 3D point cloud of the scene by matching keypoints across images in a lenient manner, thus possibly retaining many false matches. Then it builds an adaptive tetrahedral decomposition of space by computing the 3D Delaunay triangulation of the 3D point set. Finally, it reconstructs the scene by labeling Delaunay tetrahedra as empty or occupied, thus generating a triangular mesh of the scene. A globally optimal label assignment, as regards photo-consistency of the output mesh and compatibility with the visibility of keypoints in input images, is efficiently found as a minimum cut solution in a graph.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408892",
        "reference_list": [
            {
                "year": "2001",
                "id": 52
            },
            {
                "year": "2005",
                "id": 45
            },
            {
                "year": "2003",
                "id": 156
            },
            {
                "year": "2003",
                "id": 76
            }
        ],
        "citation": {
            "ieee": 45,
            "other": 34,
            "total": 79
        },
        "keywords": {
            "IEEE Keywords": [
                "Large-scale systems",
                "Layout",
                "Image reconstruction",
                "Geometry",
                "Shape",
                "Image segmentation",
                "Level set",
                "Clouds",
                "Labeling",
                "Mesh generation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image matching",
                "image reconstruction",
                "mesh generation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview reconstruction",
                "large-scale scenes",
                "interest points",
                "graph cuts",
                "3D shape reconstruction",
                "calibrated images",
                "multiview stereovision",
                "large-scale cluttered scenes",
                "quasidense 3D point cloud",
                "image scene keypoint matching",
                "adaptive tetrahedral space decomposition",
                "3D Delaunay triangulation",
                "Delaunay tetrahedra",
                "triangular mesh",
                "optimal label assignment"
            ]
        },
        "id": 53,
        "cited_by": []
    },
    {
        "title": "Scale-Invariant Features on the Sphere",
        "authors": [
            "Peter Hansen",
            "Peter Corke",
            "Wageeh Boles",
            "Kostas Daniilidis"
        ],
        "abstract": "This paper considers an application of scale-invariant feature detection using scale-space analysis suitable for use with wide field of view cameras. Rather than obtain scale- space images via convolution with the Gaussian function on the image plane, we map the image to the sphere and obtain scale-space images as the solution to the heat (diffusion) equation on the sphere which is implemented in the frequency domain using spherical harmonics. The percentage correlation of scale-invariant features that may be matched between any two wide-angle images subject to change in camera pose is then compared using each of these methods. We also present a means by which the required sampling bandwidth may be determined and propose a suitable anti-aliasing filter which may be used when this bandwidth exceeds the maximum permissible due to computational requirements. The results show improved performance using scale-space images obtained as the solution of the diffusion equation on the sphere, with additional improvements observed using the anti-aliasing filter.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408893",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 10,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Cameras",
                "Equations",
                "Convolution",
                "Frequency domain analysis",
                "Image sampling",
                "Bandwidth",
                "Filters",
                "Robot vision systems",
                "Robot kinematics"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "filtering theory",
                "image sampling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale-invariant features",
                "feature detection",
                "scale-space analysis",
                "sphere",
                "scale-space images",
                "heat equation",
                "spherical harmonics",
                "percentage correlation",
                "sampling bandwidth",
                "anti-aliasing filter",
                "diffusion equation"
            ]
        },
        "id": 54,
        "cited_by": [
            {
                "year": "2011",
                "id": 202
            }
        ]
    },
    {
        "title": "Structure from Statistics - Unsupervised Activity Analysis using Suffix Trees",
        "authors": [
            "Raffay Hamid",
            "Siddhartha Maddi",
            "Aaron Bobick",
            "Irfan Essa"
        ],
        "abstract": "Models of activity structure for unconstrained environments are generally not available a priori. Recent representational approaches to this end are limited by their computational complexity, and ability to capture activity structure only up to some fixed temporal scale. In this work, we propose Suffix Trees as an activity representation to efficiently extract structure of activities by analyzing their constituent event-subsequences over multiple temporal scales. We empirically compare Suffix Trees with some of the previous approaches in terms of feature cardinality, discriminative prowess, noise sensitivity and activity-class discovery. Finally, exploiting properties of Suffix Trees, we present a novel perspective on anomalous subsequences of activities, and propose an algorithm to detect them in linear-time. We present comparative results over experimental data, collected from a kitchen environment to demonstrate the competence of our proposed framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408894",
        "reference_list": [
            {
                "year": "2001",
                "id": 114
            },
            {
                "year": "2001",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 11,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistical analysis",
                "Computational complexity",
                "Turning",
                "Educational institutions",
                "Working environment noise",
                "Surveillance",
                "Layout",
                "Functional analysis",
                "Cost function",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised activity analysis",
                "suffix trees",
                "event-subsequences",
                "multiple temporal scales",
                "feature cardinality",
                "noise sensitivity",
                "activity-class discovery"
            ]
        },
        "id": 55,
        "cited_by": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2011",
                "id": 99
            }
        ]
    },
    {
        "title": "When is a Discrete Diffusion a Scale-Space?",
        "authors": [
            "Ramunas Girdziusas",
            "Jorma Laaksonen"
        ],
        "abstract": "Necessary and sufficient conditions are discussed which state when the Euler-inspired forward diffusion in a discrete space-time is a scale-space in the sense of both the total and sign variation diminishing. We emphasize that the problem is algebraic and reduces to characterization of the elements of the generalized Laplacian so that the diffusion propagators are positive definite. As a key-product, explicit analytical expressions are found for the principal minors of the frequently-applied class of tridiagonal (Jacobi) matrices. Further generalizations are outlined by introducing novel techniques of evaluating matrix determinants.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408895",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Jacobian matrices",
                "Laplace equations",
                "Laboratories",
                "Information science",
                "Space technology",
                "Sufficient conditions",
                "Smoothing methods",
                "Noise measurement",
                "Focusing",
                "Signal resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image denoising",
                "Jacobian matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discrete diffusion",
                "discrete space-time",
                "variation diminishing",
                "generalized Laplacian",
                "tridiagonal matrices",
                "Jacobi matrices",
                "matrix determinants"
            ]
        },
        "id": 56,
        "cited_by": []
    },
    {
        "title": "Global Optimization through Searching Rotation Space and Optimal Estimation of the Essential Matrix",
        "authors": [
            "Richard I. Hartley",
            "Fredrik Kahl"
        ],
        "abstract": "This paper extends the set of problems for which a global solution can be found using modern optimization methods. In particular, the method is applied to estimation of the essential matrix, giving the first guaranteed optimal algorithm for estimating the relative pose under a geometric cost function, in this case, the L-infinity cost function. Convex optimization techniques has been shown to provide optimal solutions to many of the common problems in structure from motion. However, they do not apply to problems involving rotations. In this paper, we introduce a search method that allows such problems to be solved optimally. Apart from the essential matrix, the algorithm is applied to the camera pose problem, providing an optimal algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408896",
        "reference_list": [
            {
                "year": "2005",
                "id": 128
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 14,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Optimization methods",
                "Australia",
                "Coordinate measuring machines",
                "Cost function",
                "Noise measurement",
                "Image reconstruction",
                "Search methods",
                "Minimax techniques",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image motion analysis",
                "matrix algebra",
                "optimisation",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "global optimization",
                "optimal estimation",
                "essential matrix",
                "geometric cost function",
                "L-infinity cost function",
                "convex optimization techniques",
                "camera pose problem",
                "image motion analysis",
                "search method"
            ]
        },
        "id": 57,
        "cited_by": [
            {
                "year": "2013",
                "id": 181
            },
            {
                "year": "2009",
                "id": 12
            }
        ]
    },
    {
        "title": "A Homographic Framework for the Fusion of Multi-view Silhouettes",
        "authors": [
            "Saad M. Khan",
            "Pingkun Yan",
            "Mubarak Shah"
        ],
        "abstract": "This paper presents a purely image-based approach to fusing foreground silhouette information from multiple arbitrary views. Our approach does not require 3D constructs like camera calibration to carve out 3D voxels or project visual cones in 3D space. Using planar homographies and foreground likelihood information from a set of arbitrary views, we show that visual hull intersection can be performed in the image plane without requiring to go in 3D space. This process delivers a 2D grid of object occupancy likelihoods representing a cross-sectional slice of the object. Subsequent slices of the object are obtained by extending the process to planes parallel to a reference plane in a direction along the body of the object. We show that homographies of these new planes between views can be computed in the framework of plane to plane homologies using the homography induced by a reference plane and the vanishing point of the reference direction. Occupancy grids are stacked on top of each other, creating a three dimensional data structure that encapsulates the object shape and location. Object structure is finally segmented out by minimizing an energy functional over the surface of the object in a level sets formulation. We show the application of our method on complicated object shapes as well as cluttered environments containing multiple objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408897",
        "reference_list": [
            {
                "year": "2005",
                "id": 228
            },
            {
                "year": "2001",
                "id": 132
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 9,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Layout",
                "Shape",
                "Object recognition",
                "Image reconstruction",
                "Computer science",
                "Data structures",
                "Level set",
                "Fusion power generation"
            ],
            "INSPEC: Controlled Indexing": [
                "data structures",
                "image fusion"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "homographic framework",
                "multiview silhouettes fusion",
                "image-based approach",
                "camera calibration",
                "3D voxels",
                "visual cones",
                "planar homographies",
                "foreground likelihood information",
                "object occupancy likelihoods",
                "cross-sectional slice",
                "three dimensional data structure"
            ]
        },
        "id": 58,
        "cited_by": [
            {
                "year": "2007",
                "id": 202
            }
        ]
    },
    {
        "title": "A Layer-Based Restoration Framework for Variable-Aperture Photography",
        "authors": [
            "Samuel W. Hasinoff",
            "Kiriakos N. Kutulakos"
        ],
        "abstract": "We present variable-aperture photography, a new method for analyzing sets of images captured with different aperture settings, with all other camera parameters fixed. We show that by casting the problem in an image restoration framework, we can simultaneously account for defocus, high dynamic range exposure (HDR), and noise, all of which are confounded according to aperture. Our formulation is based on a layered decomposition of the scene that models occlusion effects in detail. Recovering such a scene representation allows us to adjust the camera parameters in post-capture, to achieve changes in focus setting or depth-of-field\u2014with all results available in HDR. Our method is designed to work with very few input images: we demonstrate results from real sequences obtained using the three-image \"aperture bracketing\" mode found on consumer digital SLR cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408898",
        "reference_list": [
            {
                "year": "2001",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 9,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Photography",
                "Apertures",
                "Cameras",
                "Image restoration",
                "Layout",
                "Image analysis",
                "Casting",
                "Dynamic range",
                "Focusing",
                "Design methodology"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image restoration",
                "image sequences",
                "photography"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "layer-based image restoration framework",
                "variable-aperture photography",
                "image sequence",
                "scene decomposition",
                "occlusion model",
                "scene representation"
            ]
        },
        "id": 59,
        "cited_by": [
            {
                "year": "2009",
                "id": 42
            }
        ]
    },
    {
        "title": "A New Convolution Kernel for Atmospheric Point Spread Function Applied to Computer Vision",
        "authors": [
            "S. Metari",
            "F. Deschenes"
        ],
        "abstract": "In this paper we introduce a new filter to approximate multiple scattering of light rays within a participating media. This filter is derived from the generalized Gaussian distribution GGD. It characterizes the Atmospheric Point Spread Function (APSF) and thus makes it possible to introduce three new approaches. First, it allows us to accurately simulate various weather conditions that induce multiple scattering including fog, haze, rain, etc. Second, it allows us to propose a new method for a cooperative and simultaneous estimation of visual cues, i.e., the identification of weather degradations and the estimation of optical thickness between two images of the same scene acquired under unknown weather conditions. Third, by combining this filter with two new sets of invariant features we recently developed, we obtain invariant features that can be used for the matching of atmospheric degraded images. The first set leads to atmospheric invariant features while the second one simultaneously provides atmospheric and geometric invariance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408899",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 8,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Convolution",
                "Kernel",
                "Computer vision",
                "Optical scattering",
                "Light scattering",
                "Degradation",
                "Optical filters",
                "Gaussian distribution",
                "Atmospheric modeling",
                "Rain"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "filtering theory",
                "Gaussian distribution",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convolution kernel",
                "atmospheric point spread function",
                "computer vision",
                "filtering theory",
                "generalized Gaussian distribution",
                "light ray scattering",
                "atmospheric degraded image matching",
                "geometric invariance",
                "feature extraction"
            ]
        },
        "id": 60,
        "cited_by": []
    },
    {
        "title": "Novel Depth Cues from Uncalibrated Near-field Lighting",
        "authors": [
            "Sanjeev J. Koppal",
            "Srinivasa G. Narasimhan"
        ],
        "abstract": "We present the first method to compute depth cues from images taken solely under uncalibrated near point lighting. A stationary scene is illuminated by a point source that is moved approximately along a line or in a plane. We observe the brightness profile at each pixel and demonstrate how to obtain three novel cues: plane-scene intersections, depth ordering and mirror symmetries. These cues are defined with respect to the line/plane in which the light source moves, and not the camera viewpoint. Plane-Scene Intersections are detected by finding those scene points that are closest to the light source path at some time instance. Depth Ordering for scenes with homogeneous BRDFs is obtained by sorting pixels according to their shortest distances from a plane containing the light source path. Mirror Symmetry pairs for scenes with homogeneous BRDFs are detected by reflecting scene points across a plane in which the light source moves. We show analytic results for Lambertian objects and demonstrate empirical evidence for a variety of other BRDFs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408900",
        "reference_list": [
            {
                "year": "2003",
                "id": 107
            },
            {
                "year": "2003",
                "id": 74
            },
            {
                "year": "2001",
                "id": 157
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Light sources",
                "Mirrors",
                "Stereo vision",
                "Cameras",
                "Brightness",
                "Photometry",
                "Robots",
                "Sorting",
                "Time measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "uncalibrated near-field lighting",
                "image depth cues",
                "uncalibrated near point lighting",
                "stationary scene",
                "plane-scene intersections",
                "depth ordering",
                "mirror symmetries",
                "camera viewpoint",
                "light source path",
                "homogeneous BRDFs",
                "pixel sorting",
                "mirror symmetry pairs",
                "Lambertian objects"
            ]
        },
        "id": 61,
        "cited_by": []
    },
    {
        "title": "A Matte-less, Variational Approach to Automatic Scene Compositing",
        "authors": [
            "Shanmuganathan Raman",
            "Subhasis Chaudhuri"
        ],
        "abstract": "In this paper, we consider the problem of compositing a scene from multiple images. Multiple images, for example, can be obtained by varying the exposure of the camera, by changing the object at focus, or by simply sampling a video sequence at arbitrary time instants. We develop this problem in an optimization framework and then adopt a variational approach to derive a generalized algorithm which will be able to solve diverse applications depending on the nature of the input images. Our approach has distinct advantages over the existing digital compositing techniques, such as alpha matting and alpha blending, which require an explicit preparation of the matte while there is no such requirement in the proposed technique. We demonstrate the usefulness of our approach through results from diverse applications in computer vision.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408901",
        "reference_list": [
            {
                "year": "2005",
                "id": 122
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Video sequences",
                "Application software",
                "Computer vision",
                "Statistics",
                "Image processing",
                "Cameras",
                "Focusing",
                "Image sampling",
                "Image generation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image colour analysis",
                "image sampling",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene compositing",
                "multiple images",
                "video sampling",
                "video sequence",
                "digital compositing",
                "alpha matting",
                "alpha blending",
                "computer vision"
            ]
        },
        "id": 62,
        "cited_by": []
    },
    {
        "title": "Coplanar Shadowgrams for Acquiring Visual Hulls of Intricate Objects",
        "authors": [
            "Shuntaro Yamazaki",
            "Srinivasa Narasimhan",
            "Simon Baker",
            "Takeo Kanade"
        ],
        "abstract": "Acquiring 3D models of intricate objects (like tree branches, bicycles and insects) is a hard problem due to severe self-occlusions, repeated thin structures and surface discontinuities. In theory, a shape-from-silhouettes (SFS) approach can overcome these difficulties and use many views to reconstruct visual hulls that are close to the actual shapes. In practice, however, SFS is highly sensitive to errors in silhouette contours and the calibration of the imaging system, and therefore not suitable for obtaining reliable shapes with a large number of views. We present a practical approach to SFS using a novel technique called coplanar shadowgram imaging, that allows us to use dozens to even hundreds of views for visual hull reconstruction. Here, a point light source is moved around an object and the shadows (silhouettes) cast onto a single background plane are observed. We characterize this imaging system in terms of image projection, reconstruction ambiguity, epipolar geometry, and shape and source recovery. The coplanarity of the shadowgrams yields novel geometric properties that are not possible in traditional multi-view camera- based imaging systems. These properties allow us to derive a robust and automatic algorithm to recover the visual hull of an object and the 3D positions of light source simultaneously, regardless of the complexity of the object. We demonstrate the acquisition of several intricate shapes with severe occlusions and thin structures, using 50 to 120 views.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408902",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 2,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image reconstruction",
                "Cameras",
                "Light sources",
                "Calibration",
                "Geometry",
                "Bicycles",
                "Insects",
                "Surface reconstruction",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image reconstruction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "intricate objects",
                "self-occlusions",
                "shape-from-silhouettes approach",
                "silhouette contours",
                "imaging system",
                "coplanar shadowgram imaging",
                "visual hulls reconstruction",
                "point light source",
                "image projection",
                "reconstruction ambiguity",
                "epipolar geometry",
                "source recovery",
                "shape recovery",
                "automatic algorithm"
            ]
        },
        "id": 63,
        "cited_by": [
            {
                "year": "2011",
                "id": 257
            },
            {
                "year": "2009",
                "id": 155
            },
            {
                "year": "2009",
                "id": 217
            }
        ]
    },
    {
        "title": "A Database and Evaluation Methodology for Optical Flow",
        "authors": [
            "Simon Baker",
            "Stefan Roth",
            "Daniel Scharstein",
            "Michael J. Black",
            "J.P. Lewis",
            "Richard Szeliski"
        ],
        "abstract": "The quantitative evaluation of optical flow algorithms by Barron et al. led to significant advances in the performance of optical flow methods. The challenges for optical flow today go beyond the datasets and evaluation methods proposed in that paper and center on problems associated with nonrigid motion, real sensor noise, complex natural scenes, and motion discontinuities. Our goal is to establish a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture; realistic synthetic sequences; high frame-rate video used to study interpolation error; and modified stereo sequences of static scenes. In addition to the average angular error used in Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and flow accuracy at motion boundaries and in textureless regions. We evaluate the performance of several well-known methods on this data to establish the current state of the art. Our database is freely available on the Web together with scripts for scoring and publication of the results at http://vision.middlebury.edu/flow/.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408903",
        "reference_list": [
            {
                "year": "2005",
                "id": 171
            }
        ],
        "citation": {
            "ieee": 201,
            "other": 138,
            "total": 339
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Image motion analysis",
                "Optical sensors",
                "Optical noise",
                "Layout",
                "Interpolation",
                "Benchmark testing",
                "Tracking",
                "Fluorescence",
                "Fluid flow measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "error analysis",
                "image sequences",
                "image texture",
                "interpolation",
                "optical tracking",
                "statistical analysis",
                "stereo image processing",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optical flow",
                "nonrigid motion",
                "optical tracking",
                "hidden fluorescent texture",
                "stereo sequence",
                "high frame-rate video",
                "average angular error",
                "absolute flow endpoint error",
                "frame interpolation error",
                "statistics",
                "image database"
            ]
        },
        "id": 64,
        "cited_by": [
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2013",
                "id": 20
            },
            {
                "year": "2013",
                "id": 83
            },
            {
                "year": "2011",
                "id": 199
            },
            {
                "year": "2011",
                "id": 304
            },
            {
                "year": "2009",
                "id": 18
            },
            {
                "year": "2009",
                "id": 43
            },
            {
                "year": "2009",
                "id": 124
            },
            {
                "year": "2009",
                "id": 200
            },
            {
                "year": "2009",
                "id": 201
            },
            {
                "year": "2009",
                "id": 206
            },
            {
                "year": "2009",
                "id": 213
            }
        ]
    },
    {
        "title": "Removing Non-Uniform Motion Blur from Images",
        "authors": [
            "Sunghyun Cho",
            "Yasuyuki Matsushita",
            "Seungyong Lee"
        ],
        "abstract": "We propose a method for removing non-uniform motion blur from multiple blurry images. Traditional methods focus on estimating a single motion blur kernel for the entire image. In contrast, we aim to restore images blurred by unknown, spatially varying motion blur kernels caused by different relative motions between the camera and the scene. Our algorithm simultaneously estimates multiple motions, motion blur kernels, and the associated image segments. We formulate the problem as a regularized energy function and solve it using an alternating optimization technique. Real- world experiments demonstrate the effectiveness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408904",
        "reference_list": [],
        "citation": {
            "ieee": 51,
            "other": 29,
            "total": 80
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Image restoration",
                "Cameras",
                "Image segmentation",
                "Kernel",
                "Layout",
                "Asia",
                "Frequency estimation",
                "Computer vision",
                "Frequency domain analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image restoration",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonuniform motion blur",
                "multiple blurry images",
                "motion blur kernel",
                "spatially varying motion blur kernels",
                "images blurred restoration",
                "associated image segments",
                "regularized energy function"
            ]
        },
        "id": 65,
        "cited_by": [
            {
                "year": "2017",
                "id": 113
            },
            {
                "year": "2011",
                "id": 58
            },
            {
                "year": "2011",
                "id": 62
            }
        ]
    },
    {
        "title": "Efficient Message Representations for Belief Propagation",
        "authors": [
            "Tianli Yu",
            "Ruei-Sung Lin",
            "Boaz Super",
            "Bei Tang"
        ],
        "abstract": "Belief propagation (BP) has been successfully used to approximate the solutions of various Markov random field (MRF) formulated energy minimization problems. However, large MRFs require a significant amount of memory to store the intermediate belief messages. We observe that these messages have redundant information due to the imposed smoothness prior. In this paper, we study the feasibility of applying compression techniques to the messages in the min-sum/max-product BP algorithm with 1D labels to improve the memory efficiency and reduce the read/write bandwidth. We articulate properties that an efficient message representation should satisfy. We investigate two common compression schemes, predictive coding and linear transform coding (PCA), and then propose a novel envelope point transform (EPT) method. Predictive coding is efficient and supports linear operations directly in the compressed domain, but it is only compatible with the L1 smoothness function. PCA has the disadvantage that it does not guarantee the preservation of the minimal label. EPT is not limited to L1 smoothness cost and allows a flexible quality vs. compression ratio tradeoff compared with predictive coding. Experiments on dense stereo reconstruction have shown that the predictive scheme and EPT can achieve 8times or more compression without significant loss of depth accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408905",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 9,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Belief propagation",
                "Predictive coding",
                "Bandwidth",
                "Convolution",
                "Embedded system",
                "Markov random fields",
                "Read-write memory",
                "Principal component analysis",
                "Costs",
                "Redundancy"
            ],
            "INSPEC: Controlled Indexing": [
                "backpropagation",
                "data compression",
                "linear codes",
                "Markov processes",
                "minimax techniques",
                "minimisation",
                "transform coding"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "belief propagation",
                "Markov random field",
                "energy minimization problems",
                "compression techniques",
                "min-sum-max-product BP algorithm",
                "message representation",
                "predictive coding",
                "linear transform coding",
                "envelope point transform method"
            ]
        },
        "id": 66,
        "cited_by": [
            {
                "year": "2011",
                "id": 198
            }
        ]
    },
    {
        "title": "Efficient Mining of Frequent and Distinctive Feature Configurations",
        "authors": [
            "Till Quack",
            "Vittorio Ferrari",
            "Bastian Leibe",
            "Luc Van Gool"
        ],
        "abstract": "We present a novel approach to automatically find spatial configurations of local features occurring frequently on instances of a given object class, and rarely on the background. The approach is based on computationally efficient data mining techniques and can find frequent configurations among tens of thousands of candidates within seconds. Based on the mined configurations we develop a method to select features which have high probability of lying on previously unseen instances of the object class. The technique is meant as an intermediate processing layer to filter the large amount of clutter features returned by low- level feature extraction, and hence to facilitate the tasks of higher-level processing stages such as object detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408906",
        "reference_list": [],
        "citation": {
            "ieee": 62,
            "other": 47,
            "total": 109
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Data mining",
                "Motorcycles",
                "Feature extraction",
                "Detectors",
                "Computer vision",
                "Filters",
                "Signal to noise ratio",
                "Algorithm design and analysis",
                "Heart"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "feature extraction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "frequent feature configurations",
                "distinctive feature configurations",
                "computationally efficient data mining techniques",
                "feature extraction",
                "object detection"
            ]
        },
        "id": 67,
        "cited_by": [
            {
                "year": "2013",
                "id": 317
            },
            {
                "year": "2011",
                "id": 96
            },
            {
                "year": "2011",
                "id": 188
            },
            {
                "year": "2011",
                "id": 207
            },
            {
                "year": "2011",
                "id": 275
            },
            {
                "year": "2009",
                "id": 118
            },
            {
                "year": "2009",
                "id": 256
            }
        ]
    },
    {
        "title": "LogCut - Efficient Graph Cut Optimization for Markov Random Fields",
        "authors": [
            "Victor Lempitsky",
            "Carsten Rother",
            "Andrew Blake"
        ],
        "abstract": "Markov Random Fields (MRFs) are ubiquitous in low- level computer vision. In this paper, we propose a new approach to the optimization of multi-labeled MRFs. Similarly to a-expansion it is based on iterative application of binary graph cut. However, the number of binary graph cuts required to compute a labelling grows only logarithmically with the size of label space, instead of linearly. We demonstrate that for applications such as optical flow, image restoration, and high resolution stereo, this gives an order of magnitude speed-up, for comparable energies. Iterations are performed by \"fusion\" of solutions, done with QPBO which is related to graph cut but can deal with non-submodularity. At convergence, the method achieves optima on a par with the best competitors, and sometimes even exceeds them.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408907",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 33,
            "total": 61
        },
        "keywords": {
            "IEEE Keywords": [
                "Markov random fields",
                "Image restoration",
                "Computer vision",
                "Stereo vision",
                "Optical computing",
                "Application software",
                "Labeling",
                "Image motion analysis",
                "Partitioning algorithms",
                "Message passing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "iterative methods",
                "Markov processes",
                "optimisation",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "logcut graph cut optimization",
                "multi labeled Markov random field",
                "computer vision",
                "iterative application",
                "binary graph cut"
            ]
        },
        "id": 68,
        "cited_by": [
            {
                "year": "2009",
                "id": 72
            },
            {
                "year": "2009",
                "id": 291
            }
        ]
    },
    {
        "title": "Consistent Correspondence between Arbitrary Manifold Surfaces",
        "authors": [
            "Huai-Yu Wu",
            "Chunhong Pan",
            "Qing Yang",
            "Songde Ma"
        ],
        "abstract": "We propose a novel framework for consistent correspondence between arbitrary manifold meshes. Different from most existing methods, our approach directly maps the connectivity of the source mesh onto the target mesh without needing to segment input meshes, thus effectively avoids dealing with unstable extreme conditions (e.g. complex boundaries or high genus). In this paper, firstly, a novel mean-value Laplacian fitting scheme is proposed, which aims at computing a shape-preserving (conformal) correspondence directly in 3D-to-3D space, efficiently avoiding local optimum caused by the nearest-point search, and achieving good results even with only a few marker points. Secondly, we introduce a vertex relocation and projection approach, which refines the initial fitting result in the way of local conformity. Each vertex of the initial result is gradually projected onto the target model's surface to ensure a complete surface match. Furthermore, we provide a fast and effective approach to automatically detect critic points in the context of consistent correspondence. By fitting these critic points that capture the important features of the target mesh, the output compatible mesh matches the target mesh's profiles quite well. Compared with previous approaches, our scheme is robust, fast, and convenient, thus suitable for common applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408908",
        "reference_list": [
            {
                "year": "2005",
                "id": 138
            },
            {
                "year": "2005",
                "id": 50
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface fitting",
                "Shape",
                "Topology",
                "Geometry",
                "Laplace equations",
                "Robustness",
                "Surface texture",
                "Mesh generation",
                "Laboratories",
                "Pattern recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "curve fitting",
                "Laplace equations",
                "mesh generation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "arbitrary manifold surfaces",
                "source mesh",
                "mean-value Laplacian fitting scheme",
                "shape-preserving correspondence",
                "nearest-point search",
                "vertex relocation",
                "projection approach",
                "surface match"
            ]
        },
        "id": 69,
        "cited_by": [
            {
                "year": "2011",
                "id": 74
            }
        ]
    },
    {
        "title": "Supervised Learning of Image Restoration with Convolutional Networks",
        "authors": [
            "Viren Jain",
            "Joseph F. Murray",
            "Fabian Roth",
            "Srinivas Turaga",
            "Valentin Zhigulin",
            "Kevin L. Briggman",
            "Moritz N. Helmstaedter",
            "Winfried Denk",
            "H. Sebastian Seung"
        ],
        "abstract": "Convolutional networks have achieved a great deal of success in high-level vision problems such as object recognition. Here we show that they can also be used as a general method for low-level image processing. As an example of our approach, convolutional networks are trained using gradient learning to solve the problem of restoring noisy or degraded images. For our training data, we have used electron microscopic images of neural circuitry with ground truth restorations provided by human experts. On this dataset, Markov random field (MRF), conditional random field (CRF), and anisotropic diffusion algorithms perform about the same as simple thresholding, but superior performance is obtained with a convolutional network containing over 34,000 adjustable parameters. When restored by this convolutional network, the images are clean enough to be used for segmentation, whereas the other approaches fail in this respect. We do not believe that convolutional networks are fundamentally superior to MRFs as a representation for image processing algorithms. On the contrary, the two approaches are closely related. But in practice, it is possible to train complex convolutional networks, while even simple MRF models are hindered by problems with Bayesian learning and inference procedures. Our results suggest that high model complexity is the single most important factor for good performance, and this is possible with convolutional networks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408909",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 54,
            "other": 44,
            "total": 98
        },
        "keywords": {
            "IEEE Keywords": [
                "Supervised learning",
                "Image restoration",
                "Image processing",
                "Object recognition",
                "Circuit noise",
                "Degradation",
                "Training data",
                "Electron microscopy",
                "Humans",
                "Markov random fields"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image restoration",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "Markov processes",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "supervised learning",
                "image restoration",
                "convolutional networks",
                "high-level vision problems",
                "object recognition",
                "low-level image processing",
                "gradient learning",
                "degraded images",
                "electron microscopic images",
                "neural circuitry",
                "Markov random field",
                "conditional random field",
                "anisotropic diffusion algorithms",
                "Bayesian learning",
                "inference procedures"
            ]
        },
        "id": 70,
        "cited_by": [
            {
                "year": "2017",
                "id": 212
            },
            {
                "year": "2015",
                "id": 35
            }
        ]
    },
    {
        "title": "Applications of parametric maxflow in computer vision",
        "authors": [
            "Vladimir Kolmogorov",
            "Yuri Boykov",
            "Carsten Rother"
        ],
        "abstract": "The maximum flow algorithm for minimizing energy functions of binary variables has become a standard tool in computer vision. In many cases, unary costs of the energy depend linearly on parameter \u03bb. In this paper we study vision applications for which it is important to solve the maxflow problem for different \u03bb's. An example is a weighting between data and regularization terms in image segmentation or stereo: it is desirable to vary it both during training (to learn \u03bb from ground truth data) and testing (to select best \u03bb using high-knowledge constraints, e.g. user input). We review algorithmic aspects of this parametric maximum flow problem previously unknown in vision, such as the ability to compute all breakpoints of \u03bb and corresponding optimal configurations infinite time. These results allow, in particular, to minimize the ratio of some geometric functional, such as flux of a vector field over length (or area). Previously, such functional were tackled with shortest path techniques applicable only in 2D. We give theoretical improvements for \"PDE cuts\" [5]. We present experimental results for image segmentation, 3D reconstruction, and the cosegmentation problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408910",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 38,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Application software",
                "Computer vision",
                "Image segmentation",
                "Testing",
                "Image reconstruction",
                "Educational institutions",
                "Costs",
                "Stereo vision",
                "Image restoration",
                "Minimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parametric maxflow",
                "computer vision",
                "maximum flow algorithm",
                "image segmentation",
                "geometric functional",
                "shortest path techniques",
                "3D reconstruction",
                "cosegmentation problem"
            ]
        },
        "id": 71,
        "cited_by": [
            {
                "year": "2015",
                "id": 178
            },
            {
                "year": "2015",
                "id": 187
            },
            {
                "year": "2013",
                "id": 106
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2009",
                "id": 36
            },
            {
                "year": "2009",
                "id": 65
            },
            {
                "year": "2009",
                "id": 96
            },
            {
                "year": "2009",
                "id": 108
            },
            {
                "year": "2009",
                "id": 298
            }
        ]
    },
    {
        "title": "On Constrained Sparse Matrix Factorization",
        "authors": [
            "Wei-Shi Zheng",
            "Stan Z. Li",
            "J. H. Lai",
            "Shengcai Liao"
        ],
        "abstract": "Various linear subspace methods can be formulated in the notion of matrix factorization in which a cost function is minimized subject to some constraints. Among them, constraints on sparseness have received much attention recently. Some popular constraints such as non-negativity, lasso penalty, and (plain) orthogonality etc have been so far applied to extract sparse features. However, little work has been done to give theoretical and experimental analyses on the differences of the impacts of different constraints within a framework. In this paper, we analyze the problem in a more general framework called Constrained Sparse Matrix Factorization (CSMF). In CSMF, a particular case called CSMF with non-negative components (CSMFnc) is further discussed. Unlike NMF, CSMFnc allows not only additive but also subtractive combinations of non-negative sparse components. It is useful to produce much sparser features than those produced by NMF and meanwhile have better reconstruction ability, achieving a trade-off between sparseness and low MSE value. Moreover, for optimization, an alternating algorithm is developed and a gentle update strategy is further proposed for handling the alternating process. Experimental analyses are performed on the Swimmer data set and CBCLface database. In particular, CSMF can successfully extract all the proper components without any ghost on Swimmer, gaining a significant improvement over the compared well-known algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408911",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Sparse matrices",
                "Principal component analysis",
                "Data mining",
                "Feature extraction",
                "Independent component analysis",
                "Subspace constraints",
                "Sun",
                "Computer vision",
                "Mathematics",
                "Automation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image representation",
                "independent component analysis",
                "learning (artificial intelligence)",
                "matrix decomposition",
                "optimisation",
                "sparse matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "constrained sparse matrix factorization",
                "linear subspace methods",
                "sparse feature extraction",
                "nonnegative components",
                "CSMF optimization",
                "object representation learning",
                "computer vision",
                "ICA"
            ]
        },
        "id": 72,
        "cited_by": []
    },
    {
        "title": "Gradient Feature Selection for Online Boosting",
        "authors": [
            "Xiaoming Liu",
            "Ting Yu"
        ],
        "abstract": "Boosting has been widely applied in computer vision, especially after Viola and Jones's seminal work. The marriage of rectangular features and integral-image- enabled fast computation makes boosting attractive for many vision applications. However, this popular way of applying boosting normally employs an exhaustive feature selection scheme from a very large hypothesis pool, which results in a less-efficient learning process. Furthermore, this poses additional constraint on applying boosting in an onine fashion, where feature re-selection is often necessary because of varying data characteristic, but yet impractical due to the huge hypothesis pool. This paper proposes a gradient-based feature selection approach. Assuming a generally trained feature set and labeled samples are given, our approach iteratively updates each feature using the gradient descent, by minimizing the weighted least square error between the estimated feature response and the true label. In addition, we integrate the gradient-based feature selection with an online boosting framework. This new online boosting algorithm not only provides an efficient way of updating the discriminative feature set, but also presents a unified objective for both feature selection and weak classifier updating. Experiments on the person detection and tracking applications demonstrate the effectiveness of our proposal.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408912",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 20,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Computer vision",
                "Least squares approximation",
                "Iterative algorithms",
                "Proposals",
                "Shape",
                "Machine learning",
                "Training data",
                "Visualization",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "estimation theory",
                "feature extraction",
                "gradient methods",
                "image classification",
                "learning (artificial intelligence)",
                "least mean squares methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online boosting",
                "gradient feature selection",
                "computer vision",
                "very large hypothesis pool",
                "iterative approach",
                "weighted least square error minimization",
                "weak classifier",
                "feature response estimation"
            ]
        },
        "id": 73,
        "cited_by": [
            {
                "year": "2017",
                "id": 146
            },
            {
                "year": "2011",
                "id": 146
            },
            {
                "year": "2009",
                "id": 170
            }
        ]
    },
    {
        "title": "A fast method to minimize L\u221eerror norm for geometric vision problems",
        "authors": [
            "Yongduek Seo",
            "Richard Hartley"
        ],
        "abstract": "Minimizing L \u221e error norm for some geometric vision problems provides global optimization using the well- developed algorithm called SOCP (second order cone programming). Because the error norm belongs to quasi- convex functions, bisection method is utilized to attain the global optimum. It tests the feasibility of the intersection of all the second order cones due to measurements, repeatedly adjusting the global error level. The computation time increases according to the size of measurement data since the number of second order cones for the feasibility test inflates correspondingly. We observe in this paper that not all the data need be included for the feasibility test because we minimize the maximum of the errors; we may use only a subset of the measurements to obtain the optimal estimate, and therefore we obtain a decreased computation time. In addition, by using L \u221e image error instead of L 2 Euclidean distance, we show that the problem is still a quasi-convex problem and can be solved by bisection method but with linear programming (LP). Our algorithm and experimental results are provided.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408913",
        "reference_list": [
            {
                "year": "2005",
                "id": 128
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 10,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Testing",
                "Iterative algorithms",
                "Optimization methods",
                "Time measurement",
                "Euclidean distance",
                "Geometry",
                "Computer errors",
                "Australia Council",
                "Cameras",
                "Size measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "estimation theory",
                "geometry",
                "linear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric vision problems",
                "global optimization",
                "second order cone programming",
                "quasi-convex functions",
                "bisection method",
                "global error level",
                "feasibility test",
                "Linfin image error",
                "L2Euclidean distance",
                "linear programming",
                "L\u221eerror norm"
            ]
        },
        "id": 74,
        "cited_by": [
            {
                "year": "2017",
                "id": 95
            }
        ]
    },
    {
        "title": "An L\u221eapproach to structure and motion problems in ID-vision",
        "authors": [
            "Kalle Astrom",
            "Olof Enquist",
            "Carl Olsson",
            "Fredrik Kahl",
            "Richard Hartley"
        ],
        "abstract": "The structure and motion problem of multiple one- dimensional projections of a two-dimensional environment is studied. One-dimensional cameras have proven useful in several different applications, most prominently for autonomous guided vehicles, but also in ordinary vision for analysing planar motion and the projection of lines. Previous results on one-dimensional vision are limited to classifying and solving minimal cases, bundle adjustment for finding local minima to the structure and motion problem and linear algorithms based on algebraic cost functions. In this paper, we present a method for finding the global minimum to the structure and motion problem using the max norm of reprojection errors. We show how the optimal solution can be computed efficiently using simple linear programming techniques. The algorithms have been tested on a variety of different scenarios, both real and synthetic, with good performance. In addition, we show how to solve the multiview triangulation problem, the camera pose problem and how to dualize the algorithm in the Carlsson duality sense, all within the same framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408914",
        "reference_list": [
            {
                "year": "2005",
                "id": 128
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 7,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Remotely operated vehicles",
                "Optimization methods",
                "Mobile robots",
                "Navigation",
                "Simultaneous localization and mapping",
                "Automotive engineering",
                "Systems engineering and theory",
                "Motion analysis",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image motion analysis",
                "linear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion problems",
                "1D-vision",
                "motion problem",
                "multiple one-dimensional projections",
                "one-dimensional cameras",
                "planar motion",
                "algebraic cost functions",
                "reprojection errors",
                "linear programming techniques",
                "multiview triangulation problem",
                "Carlsson duality sense"
            ]
        },
        "id": 75,
        "cited_by": []
    },
    {
        "title": "Blurred/Non-Blurred Image Alignment using Sparseness Prior",
        "authors": [
            "Lu Yuan",
            "Jian Sun",
            "Long Quan",
            "Heung-Yeung Shum"
        ],
        "abstract": "Aligning a pair of blurred and non-blurred images is a prerequisite for many image and video restoration and graphics applications. The traditional alignment methods such as direct and feature-based approaches cannot be used due to the presence of motion blur in one image of the pair. In this paper, we present an effective and accurate alignment approach for a blurred/non-blurred image pair. We exploit a statistical characteristic of the real blur kernel - the marginal distribution of kernel value is sparse. Using this sparseness prior, we can search the best alignment which produces the sparsest blur kernel. The search is carried out in scale space with a coarse-to-fine strategy for efficiency. Finally, we demonstrate the effectiveness of our algorithm for image deblurring, video restoration, and image matting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408915",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 3,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image restoration",
                "Cameras",
                "Motion estimation",
                "Graphics",
                "Asia",
                "Image enhancement",
                "Satellites",
                "Biomedical imaging",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image restoration",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "blurredimage alignment",
                "nonblurred image alignment",
                "image restoration",
                "video restoration",
                "graphics applications",
                "motion blur",
                "real blur kernel",
                "coarse-to-fine strategy",
                "image deblurring",
                "image matting"
            ]
        },
        "id": 76,
        "cited_by": [
            {
                "year": "2011",
                "id": 97
            }
        ]
    },
    {
        "title": "Efficient Silhouette Extraction with Dynamic Viewpoint",
        "authors": [
            "Yueting Zhuang",
            "Cheng Chen"
        ],
        "abstract": "A novel approach is proposed that extends the classical background subtraction method to extract silhouettes from videos in real time with dynamic viewpoint variation caused by camera movement. First, manifold learning is used to model the background under viewpoint variations. Then, for each new frame, the background image corresponding to the same viewpoint is synthesized on the fly by examining the local neighborhood on the manifold, and the silhouette is extracted via background subtraction. An extension is also presented to generate stabilized silhouettes at any fixed viewpoint within the training range. Experiments show that our approach can efficiently extract accurate silhouettes in complex situations while maintaining a low noise level.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408916",
        "reference_list": [
            {
                "year": "2005",
                "id": 30
            },
            {
                "year": "2001",
                "id": 40
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Cameras",
                "Layout",
                "Videos",
                "Data mining",
                "Motion estimation",
                "Artificial intelligence",
                "Optical filters",
                "Optical sensors",
                "Image motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "silhouette extraction",
                "background subtraction method",
                "video signal processing",
                "dynamic viewpoint variation",
                "camera movement",
                "manifold learning"
            ]
        },
        "id": 77,
        "cited_by": []
    },
    {
        "title": "A Multi-Image Restoration Method for Image Reconstruction from Projections",
        "authors": [
            "Yunqiang Chen",
            "Lin Cheng",
            "Tong Fang",
            "Rainer Raupach"
        ],
        "abstract": "Traditional Bayesian restoration methods depend heavily on the accuracy of underlying generative models. For the challenging streak noise generated in the procedure of reconstruction from projections, Bayesian methods do not generalize well because accurate signal/noise models are not readily available. In this paper, we reformulate the reconstruction problem into a multi-image based restoration task and demonstrate that multiple images and mutual independence analysis can be utilized to significantly improve the generalization capability of traditional Bayesian frameworks in challenging scenarios. An efficient mutual independence analysis term is designed based on the properties of independent random variables to enforce the independent noise constraint between multiple images in an energy optimization framework, which can effectively detect and correct restoration error due to inaccurate generative models. Quantitative comparisons on phantom image and experiments on clinical scans both show significant improvements in accuracy and robustness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408917",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Image reconstruction",
                "Signal restoration",
                "Bayesian methods",
                "Noise generators",
                "Image analysis",
                "Signal generators",
                "Random variables",
                "Constraint optimization",
                "Design optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image reconstruction",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiimage restoration method",
                "image reconstruction",
                "projections",
                "Bayesian methods",
                "mutual independence analysis",
                "generalization capability",
                "independent noise constraint",
                "energy optimization framework",
                "phantom image"
            ]
        },
        "id": 78,
        "cited_by": []
    },
    {
        "title": "Feature Preserving Image Smoothing Using a Continuous Mixture of Tensors",
        "authors": [
            "Ozlem Subakan",
            "Bing Jian",
            "Baba C. Vemuri",
            "C. Eduardo Vallejos"
        ],
        "abstract": "Many computer vision and image processing tasks require the preservation of local discontinuities, terminations and bifurcations. Denoising with feature preservation is a challenging task and in this paper, we present a novel technique for preserving complex oriented structures such as junctions and corners present in images. This is achieved in a two stage process namely. All image data are pre- processed to extract local orientation information using a steerable Gabor filter bank. The orientation distribution at each lattice point is then represented by a continuous mixture of Gaussians. The continuous mixture representation can be cast as the Laplace transform of the mixing density over the space of positive definite (covariance) matrices. This mixing density is assumed to be a parameterized distribution, namely, a mixture of Wisharts whose Laplace transform is evaluated in a closed form expression called the Rigaut type function, a scalar-valued function of the parameters of the Wishart distribution. Computation of the weights in the mixture Wisharts is formulated as a sparse deconvolution problem. The feature preserving denoising is then achieved via iterative convolution of the given image data with the Rigaut type function. We present experimental results on noisy data, real 2D images and 3D MRI data acquired from plant roots depicting bifurcating roots. Superior performance of our technique is depicted via comparison to the state-of-the-art anisotropic diffusion filter.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408918",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Smoothing methods",
                "Tensile stress",
                "Bifurcation",
                "Noise reduction",
                "Laplace equations",
                "Computer vision",
                "Image processing",
                "Data mining",
                "Gabor filters",
                "Lattices"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Gabor filters",
                "Gaussian processes",
                "image denoising",
                "iterative methods",
                "Laplace transforms",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image smoothing",
                "tensors",
                "computer vision",
                "image processing",
                "Gabor filter bank",
                "Gaussian mixture",
                "Laplace transform",
                "Rigaut type function",
                "scalar-valued function",
                "Wishart distribution",
                "sparse deconvolution",
                "feature preserving denoising",
                "iterative convolution",
                "image denoising"
            ]
        },
        "id": 79,
        "cited_by": []
    },
    {
        "title": "No Grouping Left Behind: From Edges to Curve Fragments",
        "authors": [
            "Amir Tamrakar",
            "Benjamin B. Kimia"
        ],
        "abstract": "We present a framework for extracting image contours based on geometric and structural consistency among edge element locations and orientations. The paper presents two contributions. First, we observe that while the traditional edge orientation operators are based on first-order derivatives, orientation as tangent of a localized curve requires third-order derivatives. We derive a numerically stable third-order edge operator and show that it outperforms current techniques. Second, we consider all discrete n-tuples of edges in a local neighborhood (7times7) and retain those that are geometrically consistent with a third-order local curve model. This results in a number of ordered discrete combinations of edges, each represented by a bundle of curves. The resulting curve bundle map is a representation of all possible local groupings from which longer contour fragments are constructed. We validate our results and show that our framework outperforms traditional approaches to contour extraction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408919",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2005",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 8,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Filters",
                "Solid modeling",
                "Shape",
                "Fingers",
                "Decision making",
                "Histograms",
                "Horses",
                "Databases",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curve fragments",
                "edges fragments",
                "image contours",
                "structural consistency",
                "geometric consistency",
                "edge element locations",
                "edge orientation operators",
                "first-order derivatives",
                "third-order edge operator",
                "ordered discrete combinations",
                "curve bundle map",
                "contour extraction"
            ]
        },
        "id": 80,
        "cited_by": [
            {
                "year": "2015",
                "id": 44
            },
            {
                "year": "2009",
                "id": 294
            }
        ]
    },
    {
        "title": "Multiscale Edge Detection and Fiber Enhancement Using Differences of Oriented Means",
        "authors": [
            "Meirav Galun",
            "Ronen Basri",
            "Achi Brandt"
        ],
        "abstract": "We present an algorithm for edge detection suitable for both natural as well as noisy images. Our method is based on efficient multiscale utilization of elongated filters measuring the difference of oriented means of various lengths and orientations, along with a theoretical estimation of the effect of noise on the response of such filters. We use a scale adaptive threshold along with a recursive decision process to reveal the significant edges of all lengths and orientations and to localize them accurately even in low-contrast and very noisy images. We further use this algorithm for fiber detection and enhancement by utilizing stochastic completion-like process from both sides of a fiber. Our algorithm relies on an efficient multiscale algorithm for computing all \"significantly different\" oriented means in an image in O(N log rho), where N is the number of pixels, and p is the length of the longest structure of interest. Experimental results on both natural and noisy images are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408920",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 16,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Filters",
                "Electron microscopy",
                "Noise measurement",
                "Noise reduction",
                "Length measurement",
                "Estimation theory",
                "Stochastic processes",
                "Reflectivity",
                "Spatial resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "image denoising",
                "image enhancement",
                "recursive estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale edge detection",
                "fiber enhancement",
                "natural images",
                "noisy images",
                "multiscale utilization",
                "elongated filters",
                "scale adaptive threshold",
                "recursive decision",
                "fiber detection",
                "image enhancement",
                "stochastic completion",
                "multiscale algorithm",
                "significantly different oriented means"
            ]
        },
        "id": 81,
        "cited_by": [
            {
                "year": "2009",
                "id": 294
            }
        ]
    },
    {
        "title": "PR: More than Meets the Eye",
        "authors": [
            "Anderson Rocha",
            "Siome Goldenstein"
        ],
        "abstract": "In this paper, we introduce a new image descriptor for broad Image Categorization, the Progressive Randomization (PR) that uses perturbations on the values of the Least Significant Bits (LSB) of images. We show that different classes of images have a distinct behavior under our methodology and that using statistical descriptors of LSB occurrences and enough training examples, the method already performs as well or better than comparable existing techniques in the literature. With few training examples PR still has good separability and its accuracy increases with the size of the training set. We validate our method using four image databases with different categories.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408921",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image databases",
                "Art",
                "Bayesian methods",
                "Histograms",
                "Discrete cosine transforms",
                "Cities and towns",
                "Higher order statistics",
                "Shape",
                "Unsupervised learning"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "statistical analysis",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image descriptor",
                "broad image categorization",
                "progressive randomization",
                "least significant bits",
                "statistical descriptors",
                "four image databases"
            ]
        },
        "id": 82,
        "cited_by": []
    },
    {
        "title": "Rotational Motion Deblurring of a Rigid Object from a Single Image",
        "authors": [
            "Qi Shan",
            "Wei Xiong",
            "Jiaya Jia"
        ],
        "abstract": "Most previous motion deblurring methods restore the degraded image assuming a shift-invariant linear blur filter. These methods are not applicable if the blur is caused by spatially variant motions. In this paper, we model the physical properties of a 2-D rigid body movement and propose a practical framework to deblur rotational motions from a single image. Our main observation is that the transparency cue of a blurred object, which represents the motion blur formation from an imaging perspective, provides sufficient information in determining the object movements. Comparatively, single image motion deblurring using pixel color/gradient information has large uncertainties in motion representation and computation. Our results are produced by minimizing a new energy function combining rotation, possible translations, and the transparency map using an iterative optimizing process. The effectiveness of our method is demonstrated using challenging image examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408922",
        "reference_list": [],
        "citation": {
            "ieee": 58,
            "other": 27,
            "total": 85
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Pixel",
                "Motion estimation",
                "Degradation",
                "Convolution",
                "Optimization methods",
                "Robustness",
                "Solids",
                "Color",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image colour analysis",
                "image motion analysis",
                "image representation",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "rotational motion deblurring",
                "motion deblurring methods",
                "degraded image restoration",
                "shift-invariant linear blur filter",
                "blurred object",
                "motion blur formation",
                "image motion deblurring",
                "pixel color/gradient information",
                "motion representation",
                "iterative optimizing process"
            ]
        },
        "id": 83,
        "cited_by": [
            {
                "year": "2017",
                "id": 112
            },
            {
                "year": "2013",
                "id": 182
            },
            {
                "year": "2011",
                "id": 58
            }
        ]
    },
    {
        "title": "Extracting Spatiotemporal Interest Points using Global Information",
        "authors": [
            "Shu-Fai Wong",
            "Roberto Cipolla"
        ],
        "abstract": "Local spatiotemporal features or interest points provide compact but descriptive representations for efficient video analysis and motion recognition. Current local feature extraction approaches involve either local filtering or entropy computation which ignore global information (e.g. large blobs of moving pixels) in video inputs. This paper presents a novel extraction method which utilises global information from each video input so that moving parts such as a moving hand can be identified and are used to select relevant interest points for a condensed representation. The proposed method involves obtaining a small set of subspace images, which can synthesise frames in the video input from their corresponding coefficient vectors, and then detecting interest points from the subspaces and the coefficient vectors. Experimental results indicate that the proposed method can yield a sparser set of interest points for motion recognition than existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408923",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2005",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 63,
            "other": 53,
            "total": 116
        },
        "keywords": {
            "IEEE Keywords": [
                "Data mining",
                "Spatiotemporal phenomena",
                "Detectors",
                "Entropy",
                "Motion detection",
                "Boosting",
                "Filtering",
                "Information analysis",
                "Motion analysis",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "filtering theory",
                "image recognition",
                "motion estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "global information",
                "local spatiotemporal feature extraction",
                "local filtering",
                "entropy computation",
                "video inputs",
                "subspace images",
                "video frame synthesis",
                "coefficient vectors",
                "motion recognition"
            ]
        },
        "id": 84,
        "cited_by": [
            {
                "year": "2011",
                "id": 225
            },
            {
                "year": "2009",
                "id": 118
            },
            {
                "year": "2009",
                "id": 248
            }
        ]
    },
    {
        "title": "Fast Automatic Heart Chamber Segmentation from 3D CT Data Using Marginal Space Learning and Steerable Features",
        "authors": [
            "Yefeng Zheng",
            "Adrian Barbu",
            "Bogdan Georgescu",
            "Michael Scheuering",
            "Dorin Comaniciu"
        ],
        "abstract": "Multi-chamber heart segmentation is a prerequisite for global quantification of the cardiac function. The complexity of cardiac anatomy, poor contrast, noise or motion artifacts makes this segmentation problem a challenging task. In this paper, we present an efficient, robust, and fully automatic segmentation method for 3D cardiac computed tomography (CT) volumes. Our approach is based on recent advances in learning discriminative object models and we exploit a large database of annotated CT volumes. We formulate the segmentation as a two step learning problem: anatomical structure localization and boundary delineation. A novel algorithm, marginal space learning (MSL), is introduced to solve the 9-dimensional similarity search problem for localizing the heart chambers. MSL reduces the number of testing hypotheses by about six orders of magnitude. We also propose to use steerable image features, which incorporate the orientation and scale information into the distribution of sampling points, thus avoiding the time-consuming volume data rotation operations. After determining the similarity transformation of the heart chambers, we estimate the 3D shape through learning-based boundary delineation. Extensive experiments on multi-chamber heart segmentation demonstrate the efficiency and robustness of the proposed approach, comparing favorably to the state-of-the-art. This is the first study reporting stable results on a large cardiac CT dataset with 323 volumes. In addition, we achieve a speed of less than eight seconds for automatic segmentation of all four chambers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408925",
        "reference_list": [],
        "citation": {
            "ieee": 25,
            "other": 50,
            "total": 75
        },
        "keywords": {
            "IEEE Keywords": [
                "Heart",
                "Computed tomography",
                "Anatomy",
                "Noise robustness",
                "Databases",
                "Anatomical structure",
                "Search problems",
                "Testing",
                "Image sampling",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "cardiology",
                "computerised tomography",
                "feature extraction",
                "image segmentation",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic heart chamber segmentation",
                "marginal space learning",
                "steerable features",
                "cardiac function",
                "cardiac anatomy",
                "3D cardiac computed tomography",
                "anatomical structure localization",
                "boundary delineation"
            ]
        },
        "id": 85,
        "cited_by": [
            {
                "year": "2009",
                "id": 101
            },
            {
                "year": "2009",
                "id": 205
            }
        ]
    },
    {
        "title": "Fast training and selection of Haar features using statistics in boosting-based face detection",
        "authors": [
            "Minh-Tri Pham",
            "Tat-Jen Cham"
        ],
        "abstract": "Training a cascade-based face detector using boosting and Haar features is computationally expensive, often requiring weeks on single CPU machines. The bottleneck is at training and selecting Haar features for a single weak classifier, currently in minutes. Traditional techniques for training a weak classifier usually run in 0(NT log N), with N examples (approximately 10,000), and T features (approximately 40,000). We present a method to train a weak classifier in time 0(Nd2 + T), where d is the number of pixels of the probed image sub-window (usually from 350 to 500), by using only the statistics of the weighted input data. Experimental results revealed a significantly reduced training time of a weak classifier to the order of seconds. In particular, this method suffers very minimal immerse in training time with very large increases in members of Haar features, enjoying a significant gain in accuracy, even with reduced training time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409038",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 19,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Face detection",
                "Boosting",
                "Detectors",
                "Pixel",
                "Sorting",
                "Computer vision",
                "Application software",
                "Surveillance",
                "Robot vision systems"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image classification",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast training",
                "Haar feature selection",
                "statistics",
                "boosting-based face detection",
                "weak classifier"
            ]
        },
        "id": 86,
        "cited_by": [
            {
                "year": "2017",
                "id": 20
            }
        ]
    },
    {
        "title": "Extracting Texels in 2.1D Natural Textures",
        "authors": [
            "Narendra Ahuja",
            "Sinisa Todorovic"
        ],
        "abstract": "This paper proposes the problem of unsupervised extraction of texture elements, called texels, which repeatedly occur in the image of a frontally viewed, homogeneous, 2.1D, planar texture, and presents a solution. 2.1D texture here means that the physical texels are thin objects lying along a surface that may partially occlude one another. The image texture is represented by the segmentation tree whose structure captures the recursive embedding of regions obtained from a multiscale image segmentation. In the segmentation tree, the texels appear as subtrees with similar structure, with nodes having similar photometric and geometric properties. A new learning algorithm is proposed for fusing these similar subtrees into a tree-union, which registers all visible texel parts, and thus represents a statistical, generative model of the complete (unoccluded) texel. The learning algorithm involves concurrent estimation of texel tree structure, as well as the probability distributions of its node properties. Texel detection and segmentation are achieved simultaneously by matching the segmentation tree of a new image with the texel model. Experiments conducted on a newly compiled dataset containing 2.1D natural textures demonstrate the validity of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408926",
        "reference_list": [
            {
                "year": "2003",
                "id": 95
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 12,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image texture",
                "Surface texture",
                "Photometry",
                "Stochastic processes",
                "Layout",
                "Shape measurement",
                "Predictive models",
                "Humans",
                "Tree data structures"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image texture",
                "learning (artificial intelligence)",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "texels extraction",
                "unsupervised extraction",
                "texture elements",
                "image texture",
                "segmentation tree",
                "multiscale image segmentation"
            ]
        },
        "id": 87,
        "cited_by": [
            {
                "year": "2017",
                "id": 541
            },
            {
                "year": "2013",
                "id": 162
            },
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2009",
                "id": 107
            },
            {
                "year": "2009",
                "id": 273
            }
        ]
    },
    {
        "title": "A Seeded Image Segmentation Framework Unifying Graph Cuts And Random Walker Which Yields A New Algorithm",
        "authors": [
            "Ali Kemal Sinop",
            "Leo Grady"
        ],
        "abstract": "In this work, we present a common framework for seeded image segmentation algorithms that yields two of the leading methods as special cases - The Graph Cuts and the Random Walker algorithms. The formulation of this common framework naturally suggests a new, third, algorithm that we develop here. Specifically, the former algorithms may be shown to minimize a certain energy with respect to either an \ud835\udcc1 1 or an \ud835\udcc1 2 norm. Here, we explore the segmentation algorithm defined by an \ud835\udcc1 \u221e norm, provide a method for the optimization and show that the resulting algorithm produces an accurate segmentation that demonstrates greater stability with respect to the number of seeds employed than either the Graph Cuts or Random Walker methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408927",
        "reference_list": [],
        "citation": {
            "ieee": 82,
            "other": 65,
            "total": 147
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Labeling",
                "Stability",
                "Joining processes",
                "Pixel",
                "Computer science",
                "Visualization",
                "Optimization methods",
                "Computer vision",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "optimisation",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph cuts",
                "random Walker method",
                "seeded image segmentation algorithms",
                "optimization method"
            ]
        },
        "id": 88,
        "cited_by": [
            {
                "year": "2011",
                "id": 46
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2009",
                "id": 59
            },
            {
                "year": "2009",
                "id": 91
            },
            {
                "year": "2009",
                "id": 93
            },
            {
                "year": "2009",
                "id": 104
            },
            {
                "year": "2009",
                "id": 108
            }
        ]
    },
    {
        "title": "Joint Affinity Propagation for Multiple View Segmentation",
        "authors": [
            "Jianxiong Xiao",
            "Jingdong Wang",
            "Ping Tan",
            "Long Quan"
        ],
        "abstract": "A joint segmentation is a simultaneous segmentation of registered 2D images and 3D points reconstructed from the multiple view images. It is fundamental in structuring the data for subsequent modeling applications. In this paper, we treat this joint segmentation as a weighted graph labeling problem. First, we construct a 3D graph for the joint 3D and 2D points using a joint similarity measure. Then, we propose a hierarchical sparse affinity propagation algorithm to automatically and jointly segment 2D images and group 3D points. Third, a semi-supervised affinity propagation algorithm is proposed to refine the automatic results with the user assistance. Finally, intensive experiments demonstrate the effectiveness of the proposed approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408928",
        "reference_list": [],
        "citation": {
            "ieee": 19,
            "other": 15,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image reconstruction",
                "Labeling",
                "Motion estimation",
                "Cameras",
                "Shape",
                "Data mining",
                "Inference algorithms",
                "Image sequences",
                "Clouds"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image reconstruction",
                "image registration",
                "image segmentation",
                "optimisation",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint affinity propagation",
                "multiple view image segmentation",
                "2D image registration",
                "3D point reconstruction",
                "weighted graph labeling problem",
                "joint segmentation",
                "joint similarity measure",
                "hierarchical sparse affinity propagation algorithm",
                "semisupervised affinity propagation algorithm",
                "interactive strategy learning",
                "user assistance",
                "optimization method"
            ]
        },
        "id": 89,
        "cited_by": [
            {
                "year": "2013",
                "id": 329
            },
            {
                "year": "2009",
                "id": 87
            }
        ]
    },
    {
        "title": "Untangling Cycles for Contour Grouping",
        "authors": [
            "Qihui Zhu",
            "Gang Song",
            "Jianbo Shi"
        ],
        "abstract": "We introduce a novel topological formulation for contour grouping. Our grouping criterion, called untangling cycles, exploits the inherent topological 1D structure of salient contours to extract them from the otherwise 2D image clutter. To define a measure for topological classification robust to clutter and broken edges, we use a graph formulation instead of the standard computational topology. The key insight is that a pronounced ID contour should have a clear ordering of edges, to which all graph edges adhere, and no long range entanglements persist. Finding the contour grouping by optimizing these topological criteria is challenging. We introduce a novel concept of circular embedding to encode this combinatorial task. Our solution leads to computing the dominant complex eigenvectors/eigenvalues of the random walk matrix of the contour grouping graph. We demonstrate major improvements over state-of-the-art approaches on challenging real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408929",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2005",
                "id": 159
            },
            {
                "year": "2003",
                "id": 42
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 33,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Topology",
                "Shape",
                "Object recognition",
                "Image edge detection",
                "Robustness",
                "Measurement standards",
                "Eigenvalues and eigenfunctions",
                "Visual perception",
                "Computer vision",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "eigenvalues and eigenfunctions",
                "graph theory",
                "image classification",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "untangling cycles",
                "contour grouping",
                "grouping criterion",
                "2D image clutter",
                "topological classification",
                "graph formulation",
                "combinatorial task",
                "complex eigenvectors/eigenvalues",
                "random walk matrix",
                "grouping graph"
            ]
        },
        "id": 90,
        "cited_by": [
            {
                "year": "2017",
                "id": 285
            },
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2009",
                "id": 294
            }
        ]
    },
    {
        "title": "Segmentation using Meta-texture Saliency",
        "authors": [
            "Yaser Yacoob",
            "Larry Davis"
        ],
        "abstract": "We address segmentation of an image into patches that have an underlying salient surface-roughness. Three intrinsic images are derived: reflectance, shading and meta- texture images. A constructive approach is proposed for computing a meta-texture image by preserving, equalizing and enhancing the underlying surface-roughness across color, brightness and illumination variations. We evaluate the performance on sample images and illustrate quantitatively that different patches of the same material, in an image, are normalized in their statistics despite variations in color, brightness and illumination. Finally, segmentation by line-based boundary-detection is proposed and results are provided and compared to known algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408930",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Rough surfaces",
                "Surface roughness",
                "Image segmentation",
                "Surface texture",
                "Lighting",
                "Brightness",
                "Layout",
                "Image analysis",
                "Reflectivity",
                "Hair"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image enhancement",
                "image segmentation",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "meta-texture image",
                "salient surface-roughness",
                "image enhancement",
                "image patches"
            ]
        },
        "id": 91,
        "cited_by": []
    },
    {
        "title": "A Geodesic Framework for Fast Interactive Image and Video Segmentation and Matting",
        "authors": [
            "Xue Bai",
            "Guillermo Sapiro"
        ],
        "abstract": "An interactive framework for soft segmentation and matting of natural images and videos is presented in this paper. The proposed technique is based on the optimal, linear time, computation of weighted geodesic distances to the user-provided scribbles, from which the whole data is automatically segmented. The weights are based on spatial and/or temporal gradients, without explicit optical flow or any advanced and often computationally expensive feature detectors. These could be naturally added to the proposed framework as well if desired, in the form of weights in the geodesic distances. A localized refinement step follows this fast segmentation in order to accurately compute the corresponding matte function. Additional constraints into the distance definition permit to efficiently handle occlusions such as people or objects crossing each other in a video sequence. The presentation of the framework is complemented with numerous and diverse examples, including extraction of moving foreground from dynamic background, and comparisons with the recent literature.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408931",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2005",
                "id": 122
            },
            {
                "year": "2003",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 88,
            "other": 36,
            "total": 124
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Geophysics computing",
                "Image motion analysis",
                "Optical computing",
                "Computer vision",
                "Video sequences",
                "Image processing",
                "Labeling",
                "Image reconstruction",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image segmentation",
                "image sequences",
                "natural scenes",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast interactive image segmentation",
                "fast interactive video segmentation",
                "fast interactive image matting",
                "fast interactive video matting",
                "soft segmentation",
                "natural images",
                "weighted geodesic distances",
                "user-provided scribbles",
                "spatial gradients",
                "temporal gradients",
                "video sequence",
                "moving foreground extraction",
                "dynamic background"
            ]
        },
        "id": 92,
        "cited_by": [
            {
                "year": "2013",
                "id": 449
            },
            {
                "year": "2011",
                "id": 46
            },
            {
                "year": "2011",
                "id": 56
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2009",
                "id": 17
            },
            {
                "year": "2009",
                "id": 93
            },
            {
                "year": "2009",
                "id": 99
            },
            {
                "year": "2009",
                "id": 100
            },
            {
                "year": "2009",
                "id": 113
            }
        ]
    },
    {
        "title": "A Rank Minimization Approach to Video Inpainting",
        "authors": [
            "Tao Ding",
            "Mario Sznaier",
            "Octavia I. Camps"
        ],
        "abstract": "This paper addresses the problem of video inpainting, that is seamlessly reconstructing missing portions in a set of video frames. We propose to solve this problem proceeding as follows: (i) finding a set of descriptors that encapsulate the information necessary to reconstruct a frame, (ii) finding an optimal estimate of the value of these descriptors for the missing/corrupted frames, and (iii) using the estimated values to reconstruct the frames. The main result of the paper shows that the optimal descriptor estimates can be efficiently obtained by minimizing the rank of a matrix directly constructed from the available data, leading to a simple, computationally attractive, dynamic inpainting algorithm that optimizes the use of spatio/temporal information. Moreover, contrary to most currently available techniques, the method can handle non-periodic target motions, non-stationary backgrounds and moving cameras. These results are illustrated with several examples, including reconstructing dynamic textures and object disocclusion in cases involving both moving targets and camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408932",
        "reference_list": [
            {
                "year": "2003",
                "id": 138
            },
            {
                "year": "2003",
                "id": 41
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 7,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Cameras",
                "Iterative algorithms",
                "Video sequences",
                "Image restoration",
                "Heuristic algorithms",
                "Target tracking",
                "Linear systems",
                "Motion estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image reconstruction",
                "image sequences",
                "matrix algebra",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "matrix rank minimization approach",
                "dynamic video inpainting algorithm",
                "video frame reconstruction",
                "optimal descriptor estimate",
                "nonperiodic target motion"
            ]
        },
        "id": 93,
        "cited_by": []
    },
    {
        "title": "Multi-View Stereo for Community Photo Collections",
        "authors": [
            "Michael Goesele",
            "Noah Snavely",
            "Brian Curless",
            "Hugues Hoppe",
            "Steven M. Seitz"
        ],
        "abstract": "We present a multi-view stereo algorithm that addresses the extreme changes in lighting, scale, clutter, and other effects in large online community photo collections. Our idea is to intelligently choose images to match, both at a per-view and per-pixel level. We show that such adaptive view selection enables robust performance even with dramatic appearance variability. The stereo matching technique takes as input sparse 3D points reconstructed from structure-from-motion methods and iteratively grows surfaces from these points. Optimizing for surface normals within a photoconsistency measure significantly improves the matching results. While the focus of our approach is to estimate high-quality depth maps, we also show examples of merging the resulting depth maps into compelling scene reconstructions. We demonstrate our algorithm on standard multi-view stereo datasets and on casually acquired photo collections of famous scenes gathered from the Internet.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408933",
        "reference_list": [
            {
                "year": "2003",
                "id": 136
            }
        ],
        "citation": {
            "ieee": 159,
            "other": 116,
            "total": 275
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Surface reconstruction",
                "Layout",
                "Stereo image processing",
                "Internet",
                "Robustness",
                "Cameras",
                "Iterative algorithms",
                "Image sampling",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "image reconstruction",
                "image resolution",
                "Internet",
                "photography",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview stereo image matching algorithm",
                "large online community photo collection",
                "adaptive view selection",
                "image reconstruction",
                "structure-from-motion method",
                "photoconsistency measure",
                "high-quality depth map estimation",
                "Internet photo sharing sites"
            ]
        },
        "id": 94,
        "cited_by": [
            {
                "year": "2015",
                "id": 149
            },
            {
                "year": "2015",
                "id": 448
            },
            {
                "year": "2013",
                "id": 234
            },
            {
                "year": "2011",
                "id": 111
            },
            {
                "year": "2009",
                "id": 21
            },
            {
                "year": "2009",
                "id": 21
            },
            {
                "year": "2009",
                "id": 241
            },
            {
                "year": "2009",
                "id": 246
            }
        ]
    },
    {
        "title": "Webcam Synopsis: Peeking Around the World",
        "authors": [
            "Yael Pritch",
            "Alex Rav-Acha",
            "Avital Gutman",
            "Shmuel Peleg"
        ],
        "abstract": "The world is covered with millions of Webcams, many transmit everything in their field of view over the Internet 24 hours a day. A Web search finds public webcams in airports, intersections, classrooms, parks, shops, ski resorts, and more. Even more private surveillance cameras cover many private and public facilities. Webcams are an endless resource, but most of the video broadcast will be of little interest due to lack of activity. We propose to generate a short video that will be a synopsis of an endless video streams, generated by webcams or surveillance cameras. We would like to address queries like \"I would like to watch in one minute the highlights of this camera broadcast during the past day\". The process includes two major phases: (i) An online conversion of the video stream into a database of objects and activities (rather than frames), (ii) A response phase, generating the video synopsis as a response to the user's query. To include maximum information in a short synopsis we simultaneously show activities that may have happened at different times. The synopsis video can also be used as an index into the original video stream.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408934",
        "reference_list": [
            {
                "year": "2005",
                "id": 59
            },
            {
                "year": "2005",
                "id": 134
            }
        ],
        "citation": {
            "ieee": 65,
            "other": 27,
            "total": 92
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Streaming media",
                "Surveillance",
                "Multimedia communication",
                "Broadcasting",
                "Internet",
                "Web search",
                "Airports",
                "Watches",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "digital video broadcasting",
                "video cameras",
                "video signal processing",
                "video streaming",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Webcam video synopsis",
                "surveillance camera",
                "video broadcasting",
                "video streaming"
            ]
        },
        "id": 95,
        "cited_by": [
            {
                "year": "2017",
                "id": 385
            },
            {
                "year": "2009",
                "id": 149
            }
        ]
    },
    {
        "title": "BRDF Acquisition with Basis Illumination",
        "authors": [
            "Abhijeet Ghosh",
            "Shruthi Achutha",
            "Wolfgang Heidrich",
            "Matthew O'Toole"
        ],
        "abstract": "Realistic descriptions of surface reflectance have long been a topic of interest in both computer vision and computer graphics research. In this paper, we describe a novel and fast approach for the acquisition of bidirectional reflectance distribution functions (BRDFs). We develop a novel theory for directly measuring BRDFs in a basis representation by projecting incident light as a sequence of basis functions from a spherical zone of directions. We derive an orthonormal basis over spherical zones that is ideally suited for this task. BRDF values outside the zonal directions are extrapolated by re-projecting the zonal measurements into a spherical harmonics basis, or by fitting analytical reflection models to the data. We verify this approach with a compact optical setup that requires no moving parts and only a small number of image measurements. Using this approach, a BRDF can be measured in just a few minutes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408935",
        "reference_list": [
            {
                "year": "2001",
                "id": 166
            },
            {
                "year": "2003",
                "id": 105
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 18,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Reflectivity",
                "Optical filters",
                "Analytical models",
                "Optical noise",
                "Light sources",
                "Computer vision",
                "Computer graphics",
                "Bidirectional control",
                "Distribution functions"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "computer vision",
                "extrapolation",
                "reflectivity"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "BRDF acquisition",
                "bidirectional reflectance distribution functions",
                "basis illumination",
                "surface reflectance",
                "computer vision",
                "computer graphics",
                "orthonormal basis",
                "spherical zones",
                "extrapolation",
                "spherical harmonics basis",
                "analytical reflection models",
                "optical setup"
            ]
        },
        "id": 96,
        "cited_by": []
    },
    {
        "title": "Coupled Detection and Trajectory Estimation for Multi-Object Tracking",
        "authors": [
            "Bastian Leibe",
            "Konrad Schindler",
            "Luc Van Gool"
        ],
        "abstract": "We present a novel approach for multi-object tracking which considers object detection and spacetime trajectory estimation as a coupled optimization problem. It is formulated in a hypothesis selection framework and builds upon a state-of-the-art pedestrian detector. At each time instant, it searches for the globally optimal set of spacetime trajectories which provides the best explanation for the current image and for all evidence collected so far, while satisfying the constraints that no two objects may occupy the same physical space, nor explain the same image pixels at any point in time. Successful trajectory hypotheses are fed back to guide object detection in future frames. The optimization procedure is kept efficient through incremental computation and conservative hypothesis pruning. The resulting approach can initialize automatically and track a large and varying number of persons over long periods and through complex scenes with clutter, occlusions, and large-scale background changes. Also, the global optimization framework allows our system to recover from mismatches and temporarily lost tracks. We demonstrate the feasibility of the proposed approach on several challenging video sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408936",
        "reference_list": [],
        "citation": {
            "ieee": 103,
            "other": 63,
            "total": 166
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Object detection",
                "Target tracking",
                "Detectors",
                "Predictive models",
                "Pixel",
                "Layout",
                "Large-scale systems",
                "Video sequences",
                "Filters"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "image sequences",
                "tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coupled detection",
                "trajectory estimation",
                "multiobject tracking",
                "spacetime trajectory estimation",
                "coupled optimization problem",
                "image pixels",
                "conservative hypothesis pruning",
                "video sequences"
            ]
        },
        "id": 97,
        "cited_by": [
            {
                "year": "2017",
                "id": 268
            },
            {
                "year": "2015",
                "id": 340
            },
            {
                "year": "2013",
                "id": 130
            },
            {
                "year": "2013",
                "id": 351
            },
            {
                "year": "2011",
                "id": 156
            },
            {
                "year": "2011",
                "id": 314
            },
            {
                "year": "2009",
                "id": 194
            },
            {
                "year": "2009",
                "id": 199
            }
        ]
    },
    {
        "title": "Stochastic Adaptive Tracking In A Camera Network",
        "authors": [
            "Bi Song",
            "Amit K. Roy-Chowdhury"
        ],
        "abstract": "We present a novel stochastic, adaptive strategy for tracking multiple people in a large network of video cameras. Similarities between features (appearance and biometrics) observed at different cameras are continuously adapted and the stochastically optimal path for each person computed. The following are the major contributions of the proposed approach. First, we consider situations where the feature similarities are uncertain and treat them as random variables. We show how the distributions of these random variables can be learned and how to compute the tracks in a stochastically optimal manner. Second, we consider the possibility of long-term interdependence of the features over space and time. This allows us to adoptively evolve the feature correspondences by observing the system performance over a time window, and correct for errors in the similarity computations. Third, we show that the above two conditions can be addressed by treating the issue of tracking in a camera network as an optimization problem in a stochastic adaptive system. We show results on data collected by a large camera network. The proposed approach is particularly suitable for distributed processing over the entire network.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408937",
        "reference_list": [
            {
                "year": "2003",
                "id": 125
            },
            {
                "year": "2005",
                "id": 240
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 15,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Cameras",
                "Random variables",
                "Biometrics",
                "Distributed computing",
                "System performance",
                "Error correction",
                "Stochastic systems",
                "Adaptive systems",
                "Distributed processing"
            ],
            "INSPEC: Controlled Indexing": [
                "image sensors",
                "stochastic processes",
                "target tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stochastic adaptive tracking",
                "camera network",
                "video cameras",
                "random variables",
                "multiple people tracking"
            ]
        },
        "id": 98,
        "cited_by": []
    },
    {
        "title": "Probabilistic Fusion Tracking Using Mixture Kernel-Based Bayesian Filtering",
        "authors": [
            "Bohyung Han",
            "Seong-Wook Joo",
            "Larry S. Davis"
        ],
        "abstract": "Even though sensor fusion techniques based on particle filters have been applied to object tracking, their implementations have been limited to combining measurements from multiple sensors by the simple product of individual likelihoods. Therefore, the number of observations is increased as many times as the number of sensors, and the combined observation may become unreliable through blind integration of sensor observations\u2014especially if some sensors are too noisy and non-discriminative. We describe a methodology to model interactions between multiple sensors and to estimate the current state by using a mixture of Bayesian filters\u2014one filter for each sensor, where each filter makes a different level of contribution to estimate the combined posterior in a reliable manner. In this framework, an adaptive particle arrangement system is constructed in which each particle is allocated to only one of the sensors for observation and a different number of samples is assigned to each sensor using prior distribution and partial observations. We apply this technique to visual tracking in logical and physical sensor fusion frameworks, and demonstrate its effectiveness through tracking results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408938",
        "reference_list": [
            {
                "year": "2001",
                "id": 109
            },
            {
                "year": "2001",
                "id": 100
            },
            {
                "year": "2003",
                "id": 146
            },
            {
                "year": "2001",
                "id": 107
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 12,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Sensor fusion",
                "Sensor phenomena and characterization",
                "State estimation",
                "Sensor systems",
                "Filtering",
                "Particle filters",
                "Particle tracking",
                "Particle measurements",
                "Adaptive systems"
            ],
            "INSPEC: Controlled Indexing": [
                "array signal processing",
                "Bayes methods",
                "particle filtering (numerical methods)",
                "sensor fusion",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic fusion tracking",
                "mixture kernel-based Bayesian filtering",
                "sensor fusion techniques",
                "multiple sensors",
                "particle filters",
                "object tracking",
                "blind integration",
                "adaptive particle arrangement system",
                "visual tracking"
            ]
        },
        "id": 99,
        "cited_by": []
    },
    {
        "title": "Non-rigid Photometric Stereo with Colored Lights",
        "authors": [
            "Carlos Hernandez",
            "George Vogiatzis",
            "Gabriel J. Brostow",
            "Bjorn Stenger",
            "Roberto Cipolla"
        ],
        "abstract": "We present an algorithm and the associated capture methodology to acquire and track the detailed 3D shape, bends, and wrinkles of deforming surfaces. Moving 3D data has been difficult to obtain by methods that rely on known surface features, structured light, or silhouettes. Multispec- tral photometric stereo is an attractive alternative because it can recover a dense normal field from an un-textured surface. We show how to capture such data and register it over time to generate a single deforming surface. Experiments were performed on video sequences of un- textured cloth, filmed under spatially separated red, green, and blue light sources. Our first finding is that using zero- depth-silhouettes as the initial boundary condition already produces rather smoothly varying per-frame reconstructions with high detail. Second, when these 3D reconstructions are augmented with 2D optical flow, one can register the first frame's reconstruction to every subsequent frame.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408939",
        "reference_list": [
            {
                "year": "2005",
                "id": 44
            },
            {
                "year": "2005",
                "id": 213
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 36,
            "total": 68
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Optical mixing",
                "Optical variables control",
                "Light sources",
                "Image motion analysis",
                "Calibration",
                "Cameras",
                "Image reconstruction",
                "Stereo vision",
                "Surface reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image reconstruction",
                "image sequences",
                "image texture",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonrigid photometric stereo",
                "colored lights",
                "3D shape",
                "surface features",
                "structured light",
                "multispectral photometric stereo",
                "single deforming surface",
                "video sequences",
                "untextured cloth",
                "3D reconstructions"
            ]
        },
        "id": 100,
        "cited_by": [
            {
                "year": "2017",
                "id": 566
            },
            {
                "year": "2015",
                "id": 94
            },
            {
                "year": "2015",
                "id": 388
            },
            {
                "year": "2013",
                "id": 419
            },
            {
                "year": "2011",
                "id": 140
            },
            {
                "year": "2011",
                "id": 149
            },
            {
                "year": "2011",
                "id": 277
            },
            {
                "year": "2009",
                "id": 219
            }
        ]
    },
    {
        "title": "Detection and Tracking of Multiple Humans with Extensive Pose Articulation",
        "authors": [
            "Li Zhang",
            "Bo Wu",
            "Ram Nevatia"
        ],
        "abstract": "We describe a method for detecting and tracking humans. Different from most of the previous work, we focus on humans with extensive pose articulations, under situations where there is typically only a single camera, multiple humans are present and the image resolution is low. In our method pose clusters are learned from an embedded silhouette manifold. A set of object detectors, each of which corresponds to one pose cluster, are trained based on a novel Object-Weighted Appearance Model. A probabilistic pose-based transition model is used to track multiple objects within a sliding window buffer, making use of the detection responses. The track segments in the sliding windows are connected sequentially into full trajectories. Experiments on a set of challenging surveillance videos are presented; these show good performance of our approach compared to standard pedestrian detectors, under difficult conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408940",
        "reference_list": [
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 12,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Cameras",
                "Detectors",
                "Shape",
                "Surveillance",
                "Object detection",
                "Videos",
                "Intelligent robots",
                "Image resolution",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "learning (artificial intelligence)",
                "object detection",
                "pattern clustering",
                "pose estimation",
                "probability",
                "video signal processing",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple human detection",
                "multiple human tracking",
                "extensive pose articulation",
                "image resolution",
                "embedded silhouette manifold learning",
                "object detection",
                "object-weighted appearance model",
                "probabilistic pose-based transition model",
                "sliding window buffer",
                "surveillance video",
                "pedestrian detector",
                "pattern clustering"
            ]
        },
        "id": 101,
        "cited_by": []
    },
    {
        "title": "Hierarchical Model-Based Human Motion Tracking Via Unscented Kalman Filter",
        "authors": [
            "GuoJun Liu",
            "XiangLong Tang",
            "JianHua Huang",
            "JiaFeng Liu",
            "Da Sun"
        ],
        "abstract": "This paper presents a computer vision system for tracking high-speed non-rigid skaters over a large playing area in short track speeding skating competitions. The outputs of the tracking system are spatio-temporal trajectories of the players which can be further processed and analyzed by sport experts. Given very fast and non-smooth camera motions to capture highly complex and dynamic scenes of skating, tracking amorphous skaters should be a challenging task. We propose a new method of (1) automatically computing the transformation matrices to map each frame of the imagery to the globally consistent model of the rink and (2) incorporating the hierarchical model based on the contextual knowledge and multiple cues into the unscented Kalman filter to improve the tracking performance when occlusion occurs. Experimental results show that the proposed algorithm is very efficient and effective on video recorded live by the authors in the world short track speed skating championships.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408941",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 1,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Target tracking",
                "Cameras",
                "Layout",
                "Context modeling",
                "Computer vision",
                "Motion analysis",
                "Large-scale systems",
                "Robustness",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Kalman filters",
                "matrix algebra",
                "spatiotemporal phenomena"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical model-based human motion tracking",
                "unscented Kalman filter",
                "computer vision system",
                "spatio-temporal trajectory",
                "transformation matrices"
            ]
        },
        "id": 102,
        "cited_by": []
    },
    {
        "title": "Game-Theoretic Multiple Target Tracking",
        "authors": [
            "Ming Yang",
            "Ting Yu",
            "Ying Wu"
        ],
        "abstract": "Video-based multiple target tracking (MTT) is a challenging task when similar targets are present in close vicinity. Because their visual observations are mixed and difficult to segment, their motions have to be estimated jointly. Most existing approaches perform this joint motion estimation in a centralized fashion and involve searching a rather high dimensional space, and thus leading to quite complicated joint trackers. This paper brings a new view to MTT from a game-theoretic perspective, bridging the joint motion estimation and the Nash equilibrium of a game. Instead of designing a centralized tracker, MTT is decentralized and a set of individual trackers is used, each of which tries to maximize its visual evidence for explaining its motion as well as generates interferences to others. Modelling this competition behavior, a special game is designed so that the difficult joint motion estimation is achieved at the Nash Equilibrium of this game where no individual tracker has incentives to change its motion estimate. This paper substantializes this novel idea in a solid case study where individual trackers are kernel-based trackers. An efficient best response updating procedure is designed to find the Nash equilibrium. The powerfulness of this game-theoretic MTT is shown by promising results on difficult real videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408942",
        "reference_list": [
            {
                "year": "2001",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 11,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Motion estimation",
                "Nash equilibrium",
                "Games",
                "Interference",
                "Solids",
                "Video surveillance",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "game-theoretic multiple target tracking",
                "visual observations",
                "joint motion estimation"
            ]
        },
        "id": 103,
        "cited_by": []
    },
    {
        "title": "Conditional State Space Models for Discriminative Motion Estimation",
        "authors": [
            "Minyoung Kim",
            "Vladimir Pavlovic"
        ],
        "abstract": "We consider the problem of predicting a sequence of real-valued multivariate states from a given measurement sequence. Its typical application in computer vision is the task of motion estimation. State Space Models are widely used generative probabilistic models for the problem. Instead of jointly modeling states and measurements, we propose a novel discriminative undirected graphical model which conditions the states on the measurements while exploiting the sequential structure of the problem. The major benefits of this approach are: (1) It focuses on the ultimate prediction task while avoiding probably unnecessary effort in modeling the measurement density, (2) It relaxes generative models' assumption that the measurements are independent given the states, and (3) The proposed inference algorithm takes linear time in the measurement dimension as opposed to the cubic time for Kalman filtering, which allows us to incorporate large numbers of measurement features. We show that the parameter learning can be cast as an instance of convex optimization. We also provide efficient convex optimization methods based on theorems from linear algebra. The performance of the proposed model is evaluated on both synthetic data and the human body pose estimation from silhouette videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408943",
        "reference_list": [
            {
                "year": "2005",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "State-space methods",
                "Motion estimation",
                "Density measurement",
                "Time measurement",
                "Predictive models",
                "Application software",
                "Computer vision",
                "Graphical models",
                "Inference algorithms",
                "Filtering algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "image sequences",
                "Kalman filters",
                "linear algebra",
                "motion estimation",
                "pose estimation",
                "state-space methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "conditional state space models",
                "discriminative motion estimation",
                "real-valued multivariate states",
                "computer vision",
                "discriminative undirected graphical model",
                "measurement density",
                "inference algorithm",
                "measurement dimension",
                "Kalman filtering",
                "convex optimization",
                "linear algebra",
                "human body pose estimation",
                "silhouette videos"
            ]
        },
        "id": 104,
        "cited_by": []
    },
    {
        "title": "Variational optimal control technique for the tracking of deformable objects",
        "authors": [
            "Nicolas Papadakis",
            "Etienne Memin"
        ],
        "abstract": "In this paper, a new framework for the tracking of closed curves is described. The proposed approach, formalized through an optimal control technique, enables a continuous tracking along an image sequence of a deformable curve. The associated minimization process consists in a forward integration of a dynamical model followed by a backward integration of an adjoint dynamics. This latter pde includes a term related to the discrepancy between the state variables evolution law and discrete noisy measurements of the system. The closed curves are represented through an implicit surface.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408944",
        "reference_list": [
            {
                "year": "2007",
                "id": 50
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimal control",
                "Shape",
                "Level set",
                "Image sequences",
                "Noise shaping",
                "Stochastic processes",
                "State-space methods",
                "Tracking",
                "Iron",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image sequences",
                "object detection",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational optimal control technique",
                "deformable object tracking",
                "closed curve tracking",
                "image sequence"
            ]
        },
        "id": 105,
        "cited_by": []
    },
    {
        "title": "Joint Feature Tracking and Radiometric Calibration from Auto-Exposure Video",
        "authors": [
            "Seon Joo Kim",
            "Jan-Michael Frahm",
            "Marc Pollefeys"
        ],
        "abstract": "To capture the full brightness range of natural scenes, cameras automatically adjust the exposure value which causes the brightness of scene points to change from frame to frame. Given such a video sequence, we introduce a new method for tracking features and estimating the radiometric response function of the camera and the exposure difference between frames simultaneously. We model the global and nonlinear process that is responsible for the changes in image brightness rather than adapting to the changes locally and linearly which makes our tracking more robust to the change in brightness. The radiometric response function and the exposure difference between frames are also estimated in the process which enables further video processing algorithms to deal with the varying brightness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408945",
        "reference_list": [
            {
                "year": "2001",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 9,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Radiometry",
                "Calibration",
                "Brightness",
                "Layout",
                "Karhunen-Loeve transforms",
                "Cameras",
                "Video sequences",
                "Robustness",
                "Computer science",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "feature extraction",
                "image sequences",
                "optical tracking",
                "video cameras",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint feature tracking",
                "radiometric calibration",
                "auto-exposure video sequence",
                "natural scene",
                "radiometric response function estimation",
                "nonlinear process",
                "image brightness",
                "video processing algorithm"
            ]
        },
        "id": 106,
        "cited_by": []
    },
    {
        "title": "Real-time Body Tracking Using a Gaussian Process Latent Variable Model",
        "authors": [
            "Shaobo Hou",
            "Aphrodite Galata",
            "Fabrice Caillette",
            "Neil Thacker",
            "Paul Bromiley"
        ],
        "abstract": "In this paper, we present a tracking framework for capturing articulated human motions in real-time, without the need for attaching markers onto the subject's body. This is achieved by first obtaining a low dimensional representation of the training motion data, using a nonlinear dimensionality reduction technique called back-constrained GPLVM. A prior dynamics model is then learnt from this low dimensional representation by partitioning the motion sequences into elementary movements using an unsupervised EM clustering algorithm. The temporal dependencies between these elementary movements are efficiently captured by a Variable Length Markov Model. The learnt dynamics model is used to bias the propagation of candidate pose feature vectors in the low dimensional space. By combining this with an efficient volumetric reconstruction algorithm, our framework can quickly evaluate each candidate pose against image evidence captured from multiple views. We present results that show our system can accurately track complex structured activities such as ballet dancing in real-time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408946",
        "reference_list": [
            {
                "year": "2003",
                "id": 64
            },
            {
                "year": "2003",
                "id": 85
            },
            {
                "year": "2005",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 17,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Gaussian processes",
                "Humans",
                "Hidden Markov models",
                "Principal component analysis",
                "Motion estimation",
                "Particle tracking",
                "Motion analysis",
                "Filtering",
                "Biological system modeling",
                "Particle filters"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image motion analysis",
                "image reconstruction",
                "image sequences",
                "Markov processes",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time body tracking",
                "Gaussian process latent variable model",
                "human motions",
                "low dimensional representation",
                "training motion data",
                "nonlinear dimensionality reduction technique",
                "backconstrained GPLVM",
                "motion sequences",
                "unsupervised EM clustering algorithm",
                "variable length Markov model",
                "pose feature vectors",
                "volumetric reconstruction algorithm"
            ]
        },
        "id": 107,
        "cited_by": [
            {
                "year": "2009",
                "id": 180
            }
        ]
    },
    {
        "title": "Non-Linear Beam Model for Tracking Large Deformations",
        "authors": [
            "Slobodan Ilic",
            "Pascal Fua"
        ],
        "abstract": "In this paper we investigate physics-based plane beam model, frequently used in mechanical and civil engineering, to track large non-linear deformations in images. Such models do not only contribute to robust and precise tracking, in the presence of clutter and partial occlusions, but also allow to compute the forces that produce observed deformations. We verify the correctness of the recovered forces by using them in a simulation and compare the results to the original image displacements. We apply this method to track deformations of the pole vault, the rat whiskers and the car antenna.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408947",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Physics computing",
                "Laboratories",
                "Computer vision",
                "Robustness",
                "Computational modeling",
                "Civil engineering",
                "Design engineering",
                "Material properties",
                "Inverse problems"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonlinear beam model",
                "large image deformation tracking",
                "physics-based plane beam model",
                "robust tracking",
                "precise tracking",
                "image clutter",
                "partial occlusions",
                "image displacements",
                "pole vault",
                "rat whiskers",
                "car antenna"
            ]
        },
        "id": 108,
        "cited_by": []
    },
    {
        "title": "Robust Object Trackinng wvith Regional Affine Invariant Features",
        "authors": [
            "Son Tran",
            "Larry Davis"
        ],
        "abstract": "We present a tracking algorithm based on motion analysis of regional affine invariant image features. The tracked object is represented with a probabilistic occupancy map. Using this map as support, regional features are detected and probabilistically matched across frames. The motion of pixels is then established based on the feature motion. The object occupancy map is in turn updated according to the pixel motion consistency. We describe experiments to measure the sensitivities of our approach to inaccuracy in initialization, and compare it with other approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408948",
        "reference_list": [
            {
                "year": "2005",
                "id": 194
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Tracking",
                "Computer vision",
                "Noise shaping",
                "Shape",
                "Degradation",
                "Gaussian processes",
                "Kernel",
                "Educational institutions",
                "Motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image motion analysis",
                "image resolution",
                "object detection",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust object tracking algorithm",
                "regional affine invariant features",
                "motion analysis",
                "feature detection",
                "pixel motion consistency"
            ]
        },
        "id": 109,
        "cited_by": []
    },
    {
        "title": "Interactive Offline Tracking for Color Objects",
        "authors": [
            "Yichen Wei",
            "Jian Sun",
            "Xiaoou Tang",
            "Heung-Yeung Shum"
        ],
        "abstract": "In this paper, we present an interactive offline tracking system for generic color objects. The system achieves 60- 100 fps on a 320 times 240 video. The user can therefore easily refine the tracking result in an interactive way. To fully exploit user input and reduce user interaction, the tracking problem is addressed in a global optimization framework. The optimization is efficiently performed through three steps. First, from user's input we train a fast object detector that locates candidate objects in the video based on proposed features called boosted color bin. Second, we exploit the temporal coherence to generate multiple object trajectories based on a global best-first strategy. Last, an optimal object path is found by dynamic programming.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408949",
        "reference_list": [
            {
                "year": "2003",
                "id": 46
            },
            {
                "year": "2005",
                "id": 92
            },
            {
                "year": "2003",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 13,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Video compression",
                "Object detection",
                "Target tracking",
                "Interactive systems",
                "Shape",
                "Image segmentation",
                "Detectors",
                "Application software",
                "Surveillance",
                "Trajectory"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "image colour analysis",
                "object detection",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interactive offline tracking",
                "color objects",
                "user interaction",
                "global optimization framework",
                "object detector",
                "boosted color bin",
                "temporal coherence",
                "dynamic programming"
            ]
        },
        "id": 110,
        "cited_by": [
            {
                "year": "2017",
                "id": 30
            },
            {
                "year": "2015",
                "id": 489
            },
            {
                "year": "2013",
                "id": 286
            },
            {
                "year": "2011",
                "id": 233
            }
        ]
    },
    {
        "title": "Robust Visual Tracking Based on Incremental Tensor Subspace Learning",
        "authors": [
            "Xi Li",
            "Weiming Hu",
            "Zhongfei Zhang",
            "Xiaoqin Zhang",
            "Guan Luo"
        ],
        "abstract": "Most existing subspace analysis-based tracking algorithms utilize a flattened vector to represent a target, resulting in a high dimensional data learning problem. Recently, subspace analysis is incorporated into the multilinear framework which offline constructs a representation of image ensembles using high-order tensors. This reduces spatio-temporal redundancies substantially, whereas the computational and memory cost is high. In this paper, we present an effective online tensor subspace learning algorithm which models the appearance changes of a target by incrementally learning a low-order tensor eigenspace representation through adaptively updating the sample mean and eigenbasis. Tracking then is led by the state inference within the framework in which a particle filter is used for propagating sample distributions over the time. A novel likelihood function, based on the tensor reconstruction error norm, is developed to measure the similarity between the test image and the learned tensor subspace model during the tracking. Theoretic analysis and experimental evaluations against a state-of-the-art method demonstrate the promise and effectiveness of this algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408950",
        "reference_list": [
            {
                "year": "2005",
                "id": 164
            },
            {
                "year": "2003",
                "id": 195
            }
        ],
        "citation": {
            "ieee": 46,
            "other": 28,
            "total": 74
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Tensile stress",
                "Target tracking",
                "Algorithm design and analysis",
                "Image analysis",
                "Redundancy",
                "Computational efficiency",
                "Inference algorithms",
                "Particle tracking",
                "Particle filters"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "learning (artificial intelligence)",
                "particle filtering (numerical methods)",
                "sampling methods",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust visual tracking",
                "incremental tensor subspace learning",
                "subspace analysis-based tracking algorithm",
                "data learning problem",
                "image ensemble",
                "online tensor subspace learning algorithm",
                "eigenspace representation",
                "particle filter",
                "sample distribution propagation",
                "tensor reconstruction error norm",
                "state-of-the-art method"
            ]
        },
        "id": 111,
        "cited_by": [
            {
                "year": "2017",
                "id": 62
            },
            {
                "year": "2013",
                "id": 195
            },
            {
                "year": "2011",
                "id": 146
            }
        ]
    },
    {
        "title": "Learning Motion Correlation for Tracking Articulated Human Body with a Rao-Blackwellised Particle Filter",
        "authors": [
            "Xinyu Xu",
            "Baoxin Li"
        ],
        "abstract": "Inference in 3D articulated human body tracking is challenging due to the high dimensionality and nonlinearity of the parameter-space. We propose a particle filter with Rao-Blackwellisation which marginalizes part of the state variables by exploiting the correlation between the right-side and the left-side joint Euler angles. The correlation is naturally induced by the symmetric and repetitive patterns in specific human activities. A novel algorithm is proposed to learn the correlation from the training data using partial least square regression. The learned correlation is then used as motion prior in designing the Rao-Blackwellised particle filter, which estimates only one group of state variables using the Monte Carlo method, leaving the other group being exactly computed through an analytical filter that utilizes the learned motion correlation. We evaluate the effectiveness of the motion correlation for 3D articulated human body tracking. The accuracy of the proposed 3D tracker is quantitatively assessed based on the distance between the true and the estimated marker positions. Extensive experiments with multi-camera walking sequences from the HumanEva-I/II data set show that (i) the proposed tracker achieves significantly lower estimation error than both the annealed particle filter and the standard particle filter; and (ii) the learned motion correlation generalizes well to motion performed by subjects other than the training subject.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408951",
        "reference_list": [
            {
                "year": "2005",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 7,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Particle tracking",
                "Humans",
                "Particle filters",
                "Motion analysis",
                "Joints",
                "Training data",
                "Least squares methods",
                "Motion estimation",
                "State estimation",
                "Legged locomotion"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "least squares approximations",
                "Monte Carlo methods",
                "motion estimation",
                "particle filtering (numerical methods)",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion correlation",
                "articulated human body tracking",
                "Rao-Blackwellised particle filter",
                "Euler angle",
                "partial least square regression",
                "Monte Carlo method",
                "position estimation",
                "multicamera walking sequence"
            ]
        },
        "id": 112,
        "cited_by": []
    },
    {
        "title": "Variational Particle Filter for Multi-Object Tracking",
        "authors": [
            "Yonggang Jin",
            "Farzin Mokhtarian"
        ],
        "abstract": "The paper proposes an edge-based multi-object tracking framework which deals with tracking multiple objects with occlusions using a variational particle filter. Object is modelled by a mixture of a non-parametric contour model and a non-parametric edge model using kernel density estimation. Visual tracking with a mixture model is formulated as a Bayesian incomplete data problem, where measurements in an image are associated with a generative model which is a mixture of mixture models including object models and a clutter model and unobservable associations of measurements to densities in the generative model are regarded as missing data. A likelihood for tracking multiple objects jointly with an exclusion principle is presented, where it is assumed that one measurement can only be generated from one density and one density can generate multiple measurements and it significantly reduces the complexity of enumerating all feasible events. To address the curse of dimensionality in tracking multiple objects jointly, a variational particle filter (VPF) is proposed for multi-object tracking, where the proposal distribution is based on the approximated posterior from variational inference rather than using the prior as the proposal distribution in sampling importance resampling (SIR) particle filter. With the variational particle filter, the number of particles needed for multi-object tracking can be significantly reduced. Experimental results in challenging sequences demonstrate the robust performance of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408952",
        "reference_list": [
            {
                "year": "2001",
                "id": 122
            },
            {
                "year": "2001",
                "id": 108
            },
            {
                "year": "2003",
                "id": 146
            },
            {
                "year": "2003",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 6,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Particle filters",
                "Particle tracking",
                "Density measurement",
                "Proposals",
                "Kernel",
                "Bayesian methods",
                "Sampling methods",
                "Speech processing",
                "Signal processing",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "computer vision",
                "edge detection",
                "particle filtering (numerical methods)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational particle filter",
                "multi-object tracking",
                "occlusions",
                "nonparametric contour model",
                "kernel density estimation",
                "visual tracking",
                "Bayesian incomplete data problem",
                "generative model",
                "clutter model",
                "sampling importance resampling particle filter"
            ]
        },
        "id": 113,
        "cited_by": []
    },
    {
        "title": "Exploiting Occluding Contours for Real-Time 3D Tracking: A Unified Approach",
        "authors": [
            "Gang Li",
            "Yanghai Tsin",
            "Yakup Genc"
        ],
        "abstract": "Model-based 3D object tracking is fast and robust using 3D edges. However, traditional edge-based approaches have difficulty handling occluding contours of curved surfaces, since they are not static model edges but change with the viewpoint. In this paper we propose a unified approach to edge-based tracking where 3D edges including occluding contours are utilized. This is achieved through an analysis of local surface differential geometry, which provides the foundation for incorporating occluding contours of curved surfaces into edge-based tracking. This approach uses a simple parametrization of both types of model edges within the same framework. The proposed method has been tested within the context of an existing edge-based tracking system. The system can track both types of model edges in a very fast and robust manner. Experimental results on both synthetic and real scenes are provided, which confirm that occluding contours improve real-time 3D tracking performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408953",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Robustness",
                "Cameras",
                "Geometry",
                "Layout",
                "Rendering (computer graphics)",
                "System testing",
                "Robotic assembly",
                "Augmented reality",
                "Visual servoing"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "curve fitting",
                "differential geometry",
                "edge detection",
                "object detection",
                "optical tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based 3D object tracking",
                "contour occlusion",
                "curved surface",
                "edge-based tracking system",
                "local surface differential geometry"
            ]
        },
        "id": 114,
        "cited_by": [
            {
                "year": "2015",
                "id": 249
            },
            {
                "year": "2009",
                "id": 12
            }
        ]
    },
    {
        "title": "Co-Tracking Using Semi-Supervised Support Vector Machines",
        "authors": [
            "Feng Tang",
            "Shane Brennan",
            "Qi Zhao",
            "Hai Tao"
        ],
        "abstract": "This paper treats tracking as a foreground/background classification problem and proposes an online semi- supervised learning framework. Initialized with a small number of labeled samples, semi-supervised learning treats each new sample as unlabeled data. Classification of new data and updating of the classifier are achieved simultaneously in a co-training framework. The object is represented using independent features and an online support vector machine (SVM) is built for each feature. The predictions from different features are fused by combining the confidence map from each classifier using a classifier weighting method which creates a final classifier that performs better than any classifier based on a single feature. The semi-supervised learning approach then uses the output of the combined confidence map to generate new samples and update the SVMs online. With this approach, the tracker gains increasing knowledge of the object and background and continually improves itself over time. Compared to other discriminative trackers, the online semi-supervised learning approach improves each individual classifier using the information from other features, thus leading to a more robust tracker. Experiments show that this framework performs better than state-of-the-art tracking algorithms on challenging sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408954",
        "reference_list": [
            {
                "year": "2003",
                "id": 83
            }
        ],
        "citation": {
            "ieee": 109,
            "other": 71,
            "total": 180
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Semisupervised learning",
                "Support vector machine classification",
                "Machine learning",
                "Robustness",
                "Online Communities/Technical Collaboration",
                "State estimation",
                "Nearest neighbor searches",
                "Object detection",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "learning (artificial intelligence)",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semisupervised support vector machines",
                "foreground classification",
                "online semisupervised learning",
                "co-training framework",
                "object tracking",
                "computer vision",
                "background classification"
            ]
        },
        "id": 115,
        "cited_by": [
            {
                "year": "2011",
                "id": 79
            },
            {
                "year": "2011",
                "id": 146
            }
        ]
    },
    {
        "title": "Probabilistic Color and Adaptive Multi-Feature Tracking with Dynamically Switched Priority Between Cues",
        "authors": [
            "Vijay Badrinarayanan",
            "Patrick Perez",
            "Francois Le Clerc",
            "Lionel Oisel"
        ],
        "abstract": "We present a probabilistic multi-cue tracking approach constructed by employing a novel randomized template tracker and a constant color model based particle filter. Our approach is based on deriving simple binary confidence measures for each tracker which aid priority based switching between the two fundamental cues for state estimation. Thereby the state of the object is estimated from one of the two distributions associated to the cues at each tracking step. This switching also brings about interaction between the cues at irregular intervals in the form of cross sampling. Within this scheme, we tackle the important aspect of dynamic target model adaptation under randomized template tracking which, by construction, possesses the ability to adapt to changing object appearances. Further, to track the object through occlusions we interrupt sequential resampling and achieve relock using the color cue. In order to evaluate the efficacy of this scheme, we put it to test against several state of art trackers using the VIVID online evaluation program and make quantitative comparisons.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408955",
        "reference_list": [],
        "citation": {
            "ieee": 25,
            "other": 12,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Particle tracking",
                "State estimation",
                "Particle filters",
                "Testing",
                "Filtering",
                "State-space methods",
                "Research and development",
                "Sampling methods",
                "Vehicle dynamics"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image sampling",
                "particle filtering (numerical methods)",
                "probability",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic color",
                "adaptive multifeature tracking",
                "dynamically switched priority",
                "probabilistic multi-cue tracking approach",
                "randomized template tracker",
                "constant color model",
                "particle filter",
                "binary confidence measures",
                "state estimation",
                "dynamic target model adaptation",
                "randomized template tracking",
                "sequential resampling"
            ]
        },
        "id": 116,
        "cited_by": [
            {
                "year": "2015",
                "id": 345
            },
            {
                "year": "2013",
                "id": 80
            }
        ]
    },
    {
        "title": "Robust Visual Tracking Using the Time-Reversibility Constraint",
        "authors": [
            "Hao Wu",
            "Rama Chellappa",
            "Aswin C. Sankaranarayanan",
            "Shaohua Kevin Zhou"
        ],
        "abstract": "Visual tracking is a very important front-end to many vision applications. We present a new framework for robust visual tracking in this paper. Instead of just looking forward in the time domain, we incorporate both forward and backward processing of video frames using a novel time-reversibility constraint. This leads to a new minimization criterion that combines the forward and backward similarity functions and the distances of the state vectors between the forward and backward states of the tracker. The new framework reduces the possibility of the tracker getting stuck in local minima and significantly improves the tracking robustness and accuracy. Our approach is general enough to be incorporated into most of the current tracking algorithms. We illustrate the improvements due to the proposed approach for the popular KLT tracker and a search based tracker. The experimental results show that the improved KLT tracker significantly outperforms the original KLT tracker. The time-reversibility constraint used for tracking can be incorporated to improve the performance of optical flow, mean shift tracking and other algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408956",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Karhunen-Loeve transforms",
                "Target tracking",
                "Layout",
                "Particle tracking",
                "Automation",
                "Educational institutions",
                "Data systems",
                "Optical filters",
                "Image motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "minimisation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual tracking",
                "time-reversibility constraint",
                "video frame forward processing",
                "video frame backward processing",
                "minimization criterion",
                "state vectors",
                "KLT tracker"
            ]
        },
        "id": 117,
        "cited_by": []
    },
    {
        "title": "Uninitialized, globally optimal, graph-based rectilinear shape segmentation - the opposing metrics method",
        "authors": [
            "Ali Kemal Sinop",
            "Leo Grady"
        ],
        "abstract": "We present a new approach for the incorporation of shape information into a segmentation algorithm. Unlike previous approaches to the problem, our method requires no initialization, is non-iterative and finds a steady-state (i.e., global optimum) solution. In the present work, we are specifically focused on the segmentation of rectilinear shapes. The key idea is to use the fact that certain shape classes optimize the ratio of specific metrics, which can be expressed as graph Laplacian matrices applied to indicator vectors. We show that a relaxation of the binary formulation of this problem allows a global solution via generalized eigenvectors. The approach is tested on both synthetic examples and natural images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408957",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2003",
                "id": 42
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape measurement",
                "Iterative algorithms",
                "Iterative methods",
                "Distortion measurement",
                "Computer science",
                "Visualization",
                "Steady-state",
                "Laplace equations",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "graph theory",
                "image segmentation",
                "Laplace equations",
                "matrix algebra",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "rectilinear shape segmentation",
                "opposing metrics",
                "graph Laplacian matrix",
                "indicator vectors",
                "eigenvector",
                "synthetic image",
                "natural image"
            ]
        },
        "id": 118,
        "cited_by": []
    },
    {
        "title": "Normalized Cuts Revisited: A Reformulation for Segmentation with Linear Grouping Constraints",
        "authors": [
            "Anders P. Eriksson",
            "Carl Olsson",
            "Fredrik Kahl"
        ],
        "abstract": "Indisputably Normalized Cuts is one of the most popular segmentation algorithms in computer vision. It has been applied to a wide range of segmentation tasks with great success. A number of extensions to this approach have also been proposed, ones that can deal with multiple classes or that can incorporate a priori information in the form of grouping constraints. However, what is common for all these suggested methods is that they are noticeably limited and can only address segmentation problems on a very specific form. In this paper, we present a reformulation of Normalized Cut segmentation that in a unified way can handle all types of linear equality constraints for an arbitrary number of classes. This is done by restating the problem and showing how linear constraints can be enforced exactly through duality. This allows us to add group priors, for example, that certain pixels should belong to a given class. In addition, it provides a principled way to perform multi-class segmentation for tasks like interactive segmentation. The method has been tested on real data with convincing results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408958",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 42
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 10,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Computer vision",
                "Testing",
                "Minimization methods",
                "Partitioning algorithms",
                "Lagrangian functions",
                "Image converters"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "normalized cuts",
                "image segmentation",
                "linear grouping constraints",
                "computer vision",
                "linear equality constraints",
                "linear constraints",
                "multiclass segmentation"
            ]
        },
        "id": 119,
        "cited_by": []
    },
    {
        "title": "Variational Segmentation using Fuzzy Region Competition and Local Non-Parametric Probability Density Functions",
        "authors": [
            "Benoit Mory",
            "Roberto Ardon",
            "Jean-Philippe Thiran"
        ],
        "abstract": "We describe a novel variational segmentation algorithm designed to split an image in two regions based on their intensity distributions. A functional is proposed to integrate the unknown probability density functions of both regions within the optimization process. The method simultaneously performs segmentation and non-parametric density estimation. It does not make any assumption on the underlying distributions, hence it is flexible and can be applied to a wide range of applications. Although a boundary evolution scheme may be used to minimize the functional, we choose to consider an alternative formulation with a membership function. The latter has the advantage of being convex in each variable, so that the minimization is faster and less sensitive to initial conditions. Finally, to improve the accuracy and the robustness to low-frequency artifacts, we present an extension for the more general case of local space-varying probability densities. The approach readily extends to vectorial images and 3D volumes, and we show several results on synthetic and photographic images, as well as on 3D medical data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408959",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 12,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Probability density function",
                "Image segmentation",
                "Parametric statistics",
                "Biomedical imaging",
                "Signal processing algorithms",
                "Robustness",
                "Fuzzy systems",
                "Signal design",
                "Algorithm design and analysis",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "fuzzy set theory",
                "image segmentation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational segmentation",
                "fuzzy region competition",
                "local nonparametric probability density functions",
                "intensity distributions",
                "boundary evolution scheme",
                "local space-varying probability",
                "vectorial images",
                "photographic images",
                "synthetic images"
            ]
        },
        "id": 120,
        "cited_by": [
            {
                "year": "2009",
                "id": 100
            }
        ]
    },
    {
        "title": "Hierarchical Semantics of Objects (hSOs)",
        "authors": [
            "Devi Parikh",
            "Tsuhan Chen"
        ],
        "abstract": "We introduce hSOs: hierarchical semantics of objects. An hSO is learnt from a collection of images taken from a particular scene category. The hSO captures the interactions between the objects that tend to co-occur in the scene, and hence are potentially semantically related. Such relationships are typically hierarchical. For example, in a collection of images taken in a living room scene, the TV, DVD player and coffee-table co-occur frequently. The TV and the DVD player are more closely related to each other than the coffee table, and this can be learnt from the fact that the two are located at similar relative locations across images, while the coffee table is somewhat arbitrarily placed. The goal of this paper is to learn this hierarchy that characterizes the scene. The proposed approach, being entirely unsupervised, can detect the parts of the images that belong to the foreground objects, cluster these parts to represent objects, and provide an understanding of the scene by hierarchically clustering these objects in a semantically meaningful way - all from a collection of unlabeled images of a particular scene category. In addition to providing the semantic layout of the scene, learnt hSOs can have several useful applications such as compact scene representation for scene category classification and providing context for enhanced object detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408960",
        "reference_list": [
            {
                "year": "2005",
                "id": 115
            },
            {
                "year": "2001",
                "id": 103
            },
            {
                "year": "2005",
                "id": 168
            },
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 1,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Keyboards",
                "Computerized monitoring",
                "Object detection",
                "TV",
                "DVD",
                "Object recognition",
                "Humans",
                "Application software",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object detection",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Hierarchical Semantics of Objects",
                "particular scene category",
                "image collections",
                "object clustering",
                "compact scene representation",
                "scene category classification",
                "enhanced object detection"
            ]
        },
        "id": 121,
        "cited_by": []
    },
    {
        "title": "Temporal Segmentation of Facial Behavior",
        "authors": [
            "Fernando De la Torre",
            "Joan Campoy",
            "Zara Ambadar",
            "Jeffrey F. Cohn"
        ],
        "abstract": "Temporal segmentation of facial gestures in spontaneous facial behavior recorded in real-world settings is an important, unsolved, and relatively unexplored problem in facial image analysis. Several issues contribute to the challenge of this task. These include non-frontal pose, moderate to large out-of-plane head motion, large variability in the temporal scale of facial gestures, and the exponential nature of possible facial action combinations. To address these challenges, we propose a two-step approach to temporally segment facial behavior. The first step uses spectral graph techniques to cluster shape and appearance features invariant to some geometric transformations. The second step groups the clusters into temporally coherent facial gestures. We evaluated this method in facial behavior recorded during face-to- face interactions. The video data were originally collected to answer substantive questions in psychology without concern for algorithm development. The method achieved moderate convergent validity with manual FACS (Facial Action Coding System) annotation. Further, when used to preprocess video for manual FACS annotation, the method significantly improves productivity, thus addressing the need for ground-truth data for facial image analysis. Moreover, we were also able to detect unusual facial behavior.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408961",
        "reference_list": [],
        "citation": {
            "ieee": 19,
            "other": 9,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Image segmentation",
                "Image motion analysis",
                "Shape",
                "Face detection",
                "Psychology",
                "Head",
                "Clustering algorithms",
                "Image recognition",
                "Robots"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "gesture recognition",
                "image segmentation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporal segmentation",
                "facial gestures",
                "facial image analysis",
                "spectral graph techniques",
                "shape clustering",
                "geometric transformations",
                "facial action coding system",
                "manual FACS annotation"
            ]
        },
        "id": 122,
        "cited_by": [
            {
                "year": "2015",
                "id": 497
            }
        ]
    },
    {
        "title": "Moving Object Extraction with a Hand-held Camera",
        "authors": [
            "Guofeng Zhang",
            "Jiaya Jia",
            "Wei Xiong",
            "Tien-Tsin Wong",
            "Pheng-Ann Heng",
            "Hujun Bao"
        ],
        "abstract": "This paper presents a new method to detect and accurately extract the moving object from a video sequence taken by a hand-held camera. In order to extract the high quality moving foreground, previous approaches usually assume that the background is static or through only planar-perspective transformation. In our method, based on the robust motion estimation, we are capable of handling challenging videos where the background contains complex depth and the camera undergoes unknown motions. We propose the appearance and structure consistency constraint in 3D warping to robustly model the background, which greatly improves the foreground separation even on the object boundary. The estimated dense motion field and the bi- layer segmentation result are iteratively refined where continuous and discrete optimizations are alternatively used. Experimental results of high quality moving object extraction from challenging videos demonstrate the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408963",
        "reference_list": [
            {
                "year": "2005",
                "id": 4
            },
            {
                "year": "2003",
                "id": 170
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 11,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Motion estimation",
                "Robustness",
                "Object detection",
                "Video sequences",
                "Geometry",
                "Data mining",
                "Computer vision",
                "Image motion analysis",
                "Constraint optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image segmentation",
                "image sequences",
                "motion estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object extraction",
                "hand-held camera",
                "video sequence",
                "planar-perspective transformation",
                "structure consistency constraint",
                "3D warping",
                "foreground separation",
                "object boundary",
                "dense motion field estimation",
                "continuous-discrete optimizations"
            ]
        },
        "id": 123,
        "cited_by": [
            {
                "year": "2011",
                "id": 297
            }
        ]
    },
    {
        "title": "Simultaneous Segmentation and 3D Reconstruction of Monocular Image Sequences",
        "authors": [
            "Kemal Egemen Ozden",
            "Konrad Schindler",
            "L. van Gool"
        ],
        "abstract": "When trying to extract 3D scene information and camera motion from an image sequence alone, it is often necessary to cope with independently moving objects. Recent research has unveiled some of the mathematical foundations of the problem, but a general and practical algorithm, which can handle long, realistic sequences, is still missing. In this paper, we identify the necessary parts of such an algorithm, highlight both unexplored theoretical issues and practical challenges, and propose solutions. Theoretical issues include proper handling of different situations, in which the number of independent motions changes: objects can enter the scene, objects previously moving together can split and follow independent trajectories, or independently moving objects can merge into one common motion. We derive model scoring criteria to handle these changes in the number of segments. A further theoretical issue is the resolution of the relative scale ambiguity between such changes. Practical issues include robust 3D reconstruction of freely moving foreground objects, which often have few and short feature tracks. The proposed framework simultaneously tracks features, groups them into rigidly moving segments, and reconstructs all segments in 3D. Such an online approach, as opposed to batch processing techniques, which first track features, and then perform segmentation and reconstruction, is vital in order to handle small foreground objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408964",
        "reference_list": [
            {
                "year": "2001",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 4,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image reconstruction",
                "Image sequences",
                "Layout",
                "Cameras",
                "Computer vision",
                "Robustness",
                "Error correction",
                "Data mining",
                "Motion segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image motion analysis",
                "image reconstruction",
                "image segmentation",
                "image sequences",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "3D reconstruction",
                "monocular image sequences",
                "3D scene information extraction",
                "camera motion extraction",
                "model scoring criteria",
                "feature tracking",
                "SIM framework"
            ]
        },
        "id": 124,
        "cited_by": [
            {
                "year": "2015",
                "id": 100
            },
            {
                "year": "2011",
                "id": 104
            },
            {
                "year": "2009",
                "id": 185
            }
        ]
    },
    {
        "title": "Spatially Coherent Latent Topic Model for Concurrent Segmentation and Classification of Objects and Scenes",
        "authors": [
            "Liangliang Cao",
            "Li Fei-Fei"
        ],
        "abstract": "We present a novel generative model for simultaneously recognizing and segmenting object and scene classes. Our model is inspired by the traditional bag of words representation of texts and images as well as a number of related generative models, including probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation (LDA). A major drawback of the pLSA and LDA models is the assumption that each patch in the image is independently generated given its corresponding latent topic. While such representation provides an efficient computational method, it lacks the power to describe the visually coherent images and scenes. Instead, we propose a spatially coherent latent topic model (spatial-LTM). Spatial-LTM represents an image containing objects in a hierarchical way by over-segmented image regions of homogeneous appearances and the salient image patches within the regions. Only one single latent topic is assigned to the image patches within each region, enforcing the spatial coherency of the model. This idea gives rise to the following merits of spatial-LTM: (1) spatial-LTM provides a unified representation for spatially coherent bag of words topic models; (2) spatial-LTM can simultaneously segment and classify objects, even in the case of occlusion and multiple instances; and (3) spatial-LTM can be trained either unsupervised or supervised, as well as when partial object labels are provided. We verify the success of our model in a number of segmentation and classification experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408965",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2005",
                "id": 235
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 124,
            "other": 95,
            "total": 219
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatial coherence",
                "Layout",
                "Image segmentation",
                "Linear discriminant analysis",
                "Detectors",
                "Computational efficiency",
                "Text analysis",
                "Computer vision",
                "Vocabulary",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatially coherent latent topic model",
                "concurrent segmentation",
                "object classification",
                "scene classification",
                "image representation",
                "generative models",
                "probabilistic latent semantic analysis",
                "latent dirichlet allocation",
                "over-segmented image regions",
                "salient image patches"
            ]
        },
        "id": 125,
        "cited_by": [
            {
                "year": "2015",
                "id": 240
            },
            {
                "year": "2011",
                "id": 82
            },
            {
                "year": "2011",
                "id": 228
            },
            {
                "year": "2011",
                "id": 328
            }
        ]
    },
    {
        "title": "Generalized Median Graphs: Theory and Applications",
        "authors": [
            "Lopamudra Mukherjee",
            "Vikas Singh",
            "Jiming Peng",
            "Jinhui Xu",
            "Michael J. Zeitz",
            "Ronald Berezney"
        ],
        "abstract": "We study the so-called Generalized Median graph problem where the task is to to construct a prototype (i.e., a 'model') from an input set of graphs. The problem finds applications in many vision (e.g., object recognition) and learning problems where graphs are increasingly being adopted as a representation tool. Existing techniques for this problem are evolutionary search based; in this paper, we propose a polynomial time algorithm based on a linear programming formulation. We present an additional bi-level method to obtain solutions arbitrarily close to the optimal in non-polynomial time (in worst case). Within this new framework, one can optimize edit distance functions that capture similarity by considering vertex labels as well as the graph structure simultaneously. In context of our motivating application, we discuss experiments on molecular image analysis problems - the methods will provide the basis for building a topological map of all pairs of the human chromosome.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408966",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Prototypes",
                "Biological cells",
                "Biological information theory",
                "Object recognition",
                "Buildings",
                "Humans",
                "Layout",
                "Chromosome mapping",
                "Application software",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "biology computing",
                "computational complexity",
                "computer vision",
                "evolutionary computation",
                "graph theory",
                "image representation",
                "linear programming",
                "molecular biophysics",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generalized median graph",
                "computer vision",
                "learning problem",
                "evolutionary search",
                "polynomial time algorithm",
                "linear programming formulation",
                "bi-level method",
                "edit distance function",
                "molecular image analysis"
            ]
        },
        "id": 126,
        "cited_by": []
    },
    {
        "title": "Pairwise Similarities across Images for Multiple View Rigid/Non-Rigid Segmentation and Registration",
        "authors": [
            "Luca Bertelli",
            "Marco Zuliani",
            "B.S. Manjunath"
        ],
        "abstract": "A variational approach for the background/foreground segmentation of multiple views of the same scene is presented. The main novelty is the introduction of cost functions based on pairwise similarity between pixels across different images. These cost functions are minimized within a level set framework. In addition, a warping model (rigid or non-rigid) between the emerging foregrounds in the different views is imposed, thus avoiding the introduction of a specific shape term in the cost function to handle occlusions. The thin plate spline (TPS) warping is for the first time employed within the level set framework to model non- rigid deformations. The minimization of these cost functions leads to simultaneous segmentation and registration of the different views. Examples of segmentations of a variety of objects are shown and possible applications are proposed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408967",
        "reference_list": [
            {
                "year": "2005",
                "id": 26
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Cost function",
                "Level set",
                "Layout",
                "Pixel",
                "Spline",
                "Deformable models",
                "Object recognition",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "image segmentation",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pairwise image similarity",
                "background/foreground image segmentation",
                "image registration",
                "thin plate spline warping",
                "cost function minimization"
            ]
        },
        "id": 127,
        "cited_by": []
    },
    {
        "title": "Non-Parametric Probabilistic Image Segmentation",
        "authors": [
            "Marco Andreetto",
            "Lihi Zelnik-Manor",
            "Pietro Perona"
        ],
        "abstract": "We propose a simple probabilistic generative model for image segmentation. Like other probabilistic algorithms (such as EM on a mixture of Gaussians) the proposed model is principled, provides both hard and probabilistic cluster assignments, as well as the ability to naturally incorporate prior knowledge. While previous probabilistic approaches are restricted to parametric models of clusters (e.g., Gaussians) we eliminate this limitation. The suggested approach does not make heavy assumptions on the shape of the clusters and can thus handle complex structures. Our experiments show that the suggested approach outperforms previous work on a variety of image segmentation tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408968",
        "reference_list": [
            {
                "year": "2005",
                "id": 107
            },
            {
                "year": "2005",
                "id": 38
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 4,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Clustering algorithms",
                "Shape",
                "Gaussian processes",
                "Parametric statistics",
                "Data structures",
                "Image generation",
                "Noise shaping",
                "Video sequences",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image segmentation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric probabilistic image segmentation",
                "probabilistic generative model",
                "Gaussians mixture",
                "probabilistic cluster assignments"
            ]
        },
        "id": 128,
        "cited_by": []
    },
    {
        "title": "USSR: A Unified Framework for Simultaneous Smoothing, Segmentation, and Registration of Multiple Images",
        "authors": [
            "Nicholas A. Lord",
            "Jeffrey Ho",
            "Baba C. Vemuri"
        ],
        "abstract": "Image smoothing, segmentation and registration are three key processing steps in many computer vision applications. In this paper, we present a novel framework for achieving all three seemingly disparate goals simultaneously across multiple images in a unified framework via a single variational principle. The proposed method ensures that the estimated registration is unbiased and all compositions of registration maps are compatible. The solution to the variational problem is achieved efficiently by solving a coupled system of partial differential equations over the common domain on which the registration maps are defined. The effectiveness of the proposed framework is demonstrated on sets of real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408969",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 5,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Smoothing methods",
                "Image segmentation",
                "Image reconstruction",
                "Computer vision",
                "Application software",
                "Proposals",
                "Image processing",
                "Partial differential equations",
                "Object recognition",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image registration",
                "image segmentation",
                "partial differential equations",
                "smoothing methods",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image smoothing",
                "image segmentation",
                "image registration",
                "computer vision applications",
                "single variational principle",
                "estimated registration",
                "registration maps",
                "partial differential equations",
                "unified variational framework"
            ]
        },
        "id": 129,
        "cited_by": []
    },
    {
        "title": "Capacity Scaling for Graph Cuts in Vision",
        "authors": [
            "Olivier Juan",
            "Yuri Boykov"
        ],
        "abstract": "Capacity scaling is a hierarchical approach to graph representation that can improve theoretical complexity and practical efficiency of max-flow/min-cut algorithms. Introduced by Edmonds, Karp, and Dinic in 1972, capacity scaling is well known in the combinatorial optimization community. Surprisingly, this major performance improving technique is overlooked in computer vision where graph cut methods typically solve energy minimization problems on huge N-D grids and algorithms' efficiency is a widely studied issue. Unlike some earlier hierarchical methods addressing efficiency of graph cuts in imaging, e.g. (H. Lombaert, 2005), capacity scaling preserves global optimality of the solution. This is the main motivation for our work studying capacity scaling in the context of vision. We show that capacity scaling significantly reduces non-polynomial theoretical time complexity of the max-flow algorithm in (Y. Boykov and V. Kolmorogorov, 2004) to weakly polynomial O(m2n2log(U)) where U is the largest edge weight. While (Y. Boykov and V. Kolmorogorov, 2004) is the fastest method for many applications in vision, capacity scaling gives several folds speed-ups for problems with large number of local minima. The effect is particularly strong in 3D applications with denser neighborhoods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408970",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2005",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Minimization methods",
                "Application software",
                "Costs",
                "Image edge detection",
                "Image converters",
                "Image segmentation",
                "Optimization methods",
                "Computer science",
                "Polynomials"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "capacity scaling",
                "graph cuts method",
                "graph representation",
                "max-flow/min-cut algorithms",
                "combinatorial optimization community",
                "computer vision",
                "energy minimization problems",
                "huge N-D grids"
            ]
        },
        "id": 130,
        "cited_by": []
    },
    {
        "title": "Globally Optimal Image Segmentation with an Elastic Shape Prior",
        "authors": [
            "Thomas Schoenemann",
            "Daniel Cremers"
        ],
        "abstract": "So far global optimization techniques have been developed independently for the tasks of shape matching and image segmentation. In this paper we show that both tasks can in fact be solved simultaneously using global optimization. By computing cycles of minimal ratio in a large graph spanned by the product of the input image and a shape template, we are able to compute globally optimal segmentations of the image which are similar to a familiar shape and located in places of strong gradient. The presented approach is translation-invariant and robust to local and global scaling and rotation of the given shape. We show how it can be extended to incorporate invariance to similarity transformations. The particular structure of the graph allows for run-time and memory efficient implementations. Highly parallel implementations on graphics cards allow to produce globally optimal solutions in a few seconds only.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408972",
        "reference_list": [
            {
                "year": "2001",
                "id": 67
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 13,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape measurement",
                "Runtime",
                "Robustness",
                "Computer science",
                "Graphics",
                "Statistics",
                "Level set",
                "Background noise",
                "Noise shaping"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "elastic shape prior",
                "global optimization",
                "shape matching",
                "globally optimal image segmentations",
                "strong gradient",
                "translation-invariant",
                "similarity transformations",
                "graphics cards"
            ]
        },
        "id": 131,
        "cited_by": [
            {
                "year": "2009",
                "id": 91
            }
        ]
    },
    {
        "title": "Introducing Curvature into Globally Optimal Image Segmentation: Minimum Ratio Cycles on Product Graphs",
        "authors": [
            "Thomas Schoenemann",
            "Daniel Cremers"
        ],
        "abstract": "While the majority of competitive image segmentation methods are based on energy minimization, only few allow to efficiently determine globally optimal solutions. A graph-theoretic algorithm for finding globally optimal segmentations is given by the minimum ratio cycles, first applied to segmentation by Jermyn and Ishikawa (2001). In this paper we show that the class of image segmentation problems solvable by minimum ratio cycles is significantly larger than previously considered. In particular, they allow for the introduction of higher-order regularity of the region boundary. The key idea is to introduce an extended graph representation, where each node of the graph represents an image pixel as well as the orientation of the incoming line segment. With each graph edge representing a pair of adjacent line segments, edge weights can depend on the curvature. This way arbitrary positive functions of curvature can be introduced into globally optimal segmentation by minimum ratio cycles. In numerous experiments we demonstrate that compared to length-regularity the integration of curvature-regularity will drastically improve segmentation results. Moreover, we show an interesting relation to the snakes functional: minimum ratio cycles provide a way to find one of the few cases where the snakes functional has a meaningful global minimum.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408973",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2001",
                "id": 67
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 9,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Humans",
                "Computer science",
                "Minimization methods",
                "Pixel",
                "Image generation",
                "Testing",
                "Partial differential equations",
                "Prototypes",
                "Integral equations"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "globally optimal image segmentation",
                "graph-theoretic algorithm",
                "minimum ratio cycle",
                "curvature-regularity"
            ]
        },
        "id": 132,
        "cited_by": [
            {
                "year": "2011",
                "id": 160
            },
            {
                "year": "2009",
                "id": 2
            },
            {
                "year": "2009",
                "id": 65
            }
        ]
    },
    {
        "title": "Two-View Motion Segmentation by Mixtures of Dirichlet Process with Model Selection and Outlier Removal",
        "authors": [
            "Yong-Dian Jian",
            "Chu-Song Chen"
        ],
        "abstract": "This paper presents a novel motion segmentation algorithm on the basis of mixture of Dirichlet process (MDP) models, a kind of nonparametric Bayesian framework. In contrast to previous approaches, our method consider motion segmentation and its model selection regarding to the number of motion models as an indivisible problem. The proposed algorithm can simultaneously infer the number of motion models, estimate the cluster memberships of correspondence points, and identify the outliers of input data. The key idea is to use MDP models to fully exploit the epipolar constraints before making premature decisions about the number motion models. To handle outliers efficiently, we then incorporate RANSAC within the inference process of MDP models and make them take the advantages of each other. In the experiments, we compare the proposed algorithm with naive RANSAC, GPCA and Schindler's method on both synthetic data and real image data. The experimental results show that we can handle more motions and still have satisfactory performance in the presence of various levels of noise and outlier.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408974",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 2,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Computer vision",
                "Inference algorithms",
                "Layout",
                "Sampling methods",
                "Motion estimation",
                "Labeling",
                "Information science",
                "Bayesian methods",
                "Clustering algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image motion analysis",
                "image segmentation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "two-view motion segmentation algorithm",
                "Dirichlet process model",
                "motion model selection",
                "outlier removal",
                "nonparametric Bayesian inference framework",
                "cluster membership estimation",
                "RANSAC method",
                "GPCA method",
                "Schindler method"
            ]
        },
        "id": 133,
        "cited_by": [
            {
                "year": "2009",
                "id": 185
            }
        ]
    },
    {
        "title": "Hierarchical Part-Template Matching for Human Detection and Segmentation",
        "authors": [
            "Zhe Lin",
            "Larry S. Davis",
            "David Doermann",
            "Daniel DeMenthon"
        ],
        "abstract": "Local part-based human detectors are capable of handling partial occlusions efficiently and modeling shape articulations flexibly, while global shape template-based human detectors are capable of detecting and segmenting human shapes simultaneously. We describe a Bayesian approach to human detection and segmentation combining local part-based and global template-based schemes. The approach relies on the key ideas of matching a part-template tree to images hierarchically to generate a reliable set of detection hypotheses and optimizing it under a Bayesian MAP framework through global likelihood re-evaluation and fine occlusion analysis. In addition to detection, our approach is able to obtain human shapes and poses simultaneously. We applied the approach to human detection and segmentation in crowded scenes with and without background subtraction. Experimental results show that our approach achieves good performance on images and video sequences with severe occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408975",
        "reference_list": [],
        "citation": {
            "ieee": 59,
            "other": 0,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Shape",
                "Image segmentation",
                "Bayesian methods",
                "Image edge detection",
                "Detectors",
                "Assembly",
                "Image generation",
                "Object detection",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image matching",
                "image segmentation",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical part-template matching",
                "human detection",
                "human segmentation",
                "local part-based human detectors",
                "partial occlusions",
                "shape articulations",
                "global shape template-based human detectors",
                "Bayesian approach",
                "Bayesian MAP framework",
                "global likelihood re-evaluation",
                "fine occlusion analysis",
                "background subtraction",
                "video sequences"
            ]
        },
        "id": 134,
        "cited_by": []
    },
    {
        "title": "The Joint Manifold Model for Semi-supervised Multi-valued Regression",
        "authors": [
            "Ramanan Navaratnam",
            "Andrew W. Fitzgibbon",
            "Roberto Cipolla"
        ],
        "abstract": "Many computer vision tasks may be expressed as the problem of learning a mapping between image space and a parameter space. For example, in human body pose estimation, recent research has directly modelled the mapping from image features ( z ) to joint angles ( \u03b8 ). Fitting such models requires training data in the form of labelled ( z \u03b8) pairs, from which are learned the conditional densities p( z \u03b8). Inference is then simple: given test image features z , the conditional ( z \u03b8) is immediately computed. However large amounts of training data are required to fit the models, particularly in the case where the spaces are high dimensional. We show how the use of unlabelled data\u2014samples from the marginal distributions p( z ) and p( \u03b8 )\u2014may be used to improve fitting. This is valuable because it is often significantly easier to obtain unlabelled than labelled samples. We use a Gaussian process latent variable model to learn the mapping from a shared latent low-dimensional manifold to the feature and parameter spaces. This extends existing approaches to (a) use unlabelled data, and (b) represent one-to-many mappings. Experiments on synthetic and real problems demonstrate how the use of unlabelled data improves over existing techniques. In our comparisons, we include existing approaches that are explicitly semi-supervised as well as those which implicitly make use of unlabelled examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408976",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 24,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Training data",
                "Computer vision",
                "Biological system modeling",
                "Joints",
                "Fitting",
                "Testing",
                "Gaussian processes",
                "Image resolution",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint manifold model",
                "semi-supervised multi-valued regression",
                "computer vision",
                "image space",
                "parameter space",
                "Gaussian process"
            ]
        },
        "id": 135,
        "cited_by": [
            {
                "year": "2013",
                "id": 402
            },
            {
                "year": "2011",
                "id": 52
            }
        ]
    },
    {
        "title": "Population Shape Regression From Random Design Data",
        "authors": [
            "B. C. Davis",
            "P. T. Fletcher",
            "E. Bullitt",
            "S. Joshi"
        ],
        "abstract": "Regression analysis is a powerful tool for the study of changes in a dependent variable as a function of an independent regressor variable, and in particular it is applicable to the study of anatomical growth and shape change. When the underlying process can be modeled by parameters in a Euclidean space, classical regression techniques are applicable and have been studied extensively. However, recent work suggests that attempts to describe anatomical shapes using flat Euclidean spaces undermines our ability to represent natural biological variability. In this paper we develop a method for regression analysis of general, manifold-valued data. Specifically, we extend Nadaraya-Watson kernel regression by recasting the regression problem in terms of Frechet expectation. Although this method is quite general, our driving problem is the study anatomical shape change as a function of age from random design image data. We demonstrate our method by analyzing shape change in the brain from a random design dataset of MR images of 89 healthy adults ranging in age from 22 to 79 years. To study the small scale changes in anatomy, we use the infinite dimensional manifold of diffeomorphic transformations, with an associated metric. We regress a representative anatomical shape, as a function of age, from this population.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408977",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 0,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Regression analysis",
                "Kernel",
                "Image analysis",
                "Biomedical imaging",
                "Aging",
                "Statistics",
                "Cities and towns",
                "Anatomy",
                "Extraterrestrial measurements"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain",
                "data analysis",
                "medical image processing",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "population shape regression analysis",
                "random design MR image data",
                "Euclidean space",
                "manifold-valued data analysis",
                "Nadaraya-Watson kernel regression",
                "Frechet expectation",
                "brain anatomical shape change analysis",
                "diffeomorphic transformation"
            ]
        },
        "id": 136,
        "cited_by": []
    },
    {
        "title": "Mode-seeking by Medoidshifts",
        "authors": [
            "Yaser Ajmal Sheikh",
            "Erum Arif Khan",
            "Takeo Kanade"
        ],
        "abstract": "We present a nonparametric mode-seeking algorithm, called medoidshift, based on approximating the local gradient using a weighted estimate of medoids. Like meanshift, medoidshift clustering automatically computes the number of clusters and the data does not have to be linearly separable. Unlike meanshift, the proposed algorithm does not require the definition of a mean. This property allows medoidshift to find modes even when only a distance measure between samples is defined. In this sense, the relationship between the medoidshift algorithm and the meanshift algorithm is similar to the relationship between the k-medoids and the k-means algorithms. We show that medoidshifts can also be used for incremental clustering of growing datasets by recycling previous computations. We present experimental results using medoidshift for image segmentation, incremental clustering for shot segmentation and clustering on nonlinearly separable data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408978",
        "reference_list": [],
        "citation": {
            "ieee": 39,
            "other": 36,
            "total": 75
        },
        "keywords": {
            "IEEE Keywords": [
                "Clustering algorithms",
                "Image segmentation",
                "Computer vision",
                "Application software",
                "Partitioning algorithms",
                "Robots",
                "Computer science",
                "Lakes",
                "Robotics and automation",
                "Recycling"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "gradient methods",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric mode-seeking algorithm",
                "local gradient approximation",
                "medoidshift clustering",
                "meanshift algorithm",
                "k-medoids",
                "k-means algorithms",
                "image segmentation",
                "shot segmentation"
            ]
        },
        "id": 137,
        "cited_by": [
            {
                "year": "2015",
                "id": 167
            },
            {
                "year": "2013",
                "id": 427
            },
            {
                "year": "2009",
                "id": 245
            }
        ]
    },
    {
        "title": "Half Quadratic Analysis for Mean Shift: with Extension to A Sequential Data Mode-Seeking Method",
        "authors": [
            "Xiaotong Yuan",
            "Stan Z. Li"
        ],
        "abstract": "Theoretical understanding and extension of mean shift procedure has received much attention recently. In this paper, we present a theoretical exploration and an algorithm development on mean shift. In the theory part, we point out that convex profile based mean shift can be justified from the viewpoint of half-quadratic (HQ) optimization. Such analysis facilitates the convergence study and uni-mode bandwidth selection for the latest variation, annealed mean shift. In the algorithm development part of this paper, we extend annealed mean shift inside our HQ framework to a novel method, namely adaptive mean shift (Ada-MS), to detect multiple data modes sequentially from an arbitrary starting point in linear running time. To validate the performance, we couple the investigation with two applications: image segmentation and color constancy. Extensive experiments show that the proposed method is time efficient and initialization invariant.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408979",
        "reference_list": [
            {
                "year": "2005",
                "id": 197
            },
            {
                "year": "2003",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Annealing",
                "Image segmentation",
                "Bandwidth",
                "Convergence",
                "Testing",
                "Pattern analysis",
                "Biometrics",
                "Data security",
                "National security"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sequential data mode-seeking method",
                "mean shift procedure",
                "convex profile",
                "half-quadratic optimization",
                "unimode bandwidth selection",
                "adaptive mean shift",
                "image segmentation",
                "color constancy"
            ]
        },
        "id": 138,
        "cited_by": []
    },
    {
        "title": "Deformable Template As Active Basis",
        "authors": [
            "Ying Nian Wu",
            "Zhangzhang Si",
            "Chuck Fleming",
            "Song-Chun Zhu"
        ],
        "abstract": "This article proposes an active basis model and a shared pursuit algorithm for learning deformable templates from image patches of various object categories. In our generative model, a deformable template is in the form of an active basis, which consists of a small number of Gabor wavelet elements at different locations and orientations. These elements are allowed to slightly perturb their locations and orientations before they are linearly combined to generate each individual training or testing example. The active basis model can be learned from training image patches by the shared pursuit algorithm. The algorithm selects the elements of the active basis sequentially from a dictionary of Gabor wavelets. When an element is selected at each step, the element is shared by all the training examples, in the sense that a perturbed version of this element is added to improve the encoding of each example. Our model and algorithm are developed within a probabilistic framework that naturally embraces wavelet sparse coding and random field.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408980",
        "reference_list": [],
        "citation": {
            "ieee": 24,
            "other": 12,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Pursuit algorithms",
                "Displays",
                "Image coding",
                "Testing",
                "Dictionaries",
                "Matching pursuit algorithms",
                "Encoding",
                "Machinery",
                "Statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "object recognition",
                "probability",
                "wavelet transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable template",
                "image patches",
                "object categories",
                "Gabor wavelet elements",
                "active basis model",
                "shared pursuit algorithm",
                "perturbed version",
                "encoding",
                "probabilistic framework",
                "wavelet sparse coding",
                "random field"
            ]
        },
        "id": 139,
        "cited_by": [
            {
                "year": "2009",
                "id": 117
            },
            {
                "year": "2009",
                "id": 193
            }
        ]
    },
    {
        "title": "Steerable Random Fields",
        "authors": [
            "Stefan Roth",
            "Michael J. Black"
        ],
        "abstract": "In contrast to traditional Markov random field (MRF) models, we develop a steerable random field (SRF) in which the field potentials are defined in terms of filter responses that are steered to the local image structure. In particular, we use the structure tensor to obtain derivative responses that are either aligned with, or orthogonal to, the predominant local image structure, and analyze the statistics of these steered filter responses in natural images. Clique potentials are defined over steered filter responses using a Gaussian scale mixture model and are learned from training data. The SRF model connects random field models with anisotropic regularization and provides a statistical motivation for the latter. We demonstrate that steering the random field to the local image structure improves image denoising and inpainting performance compared with traditional pairwise MRFs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408981",
        "reference_list": [
            {
                "year": "2003",
                "id": 41
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2003",
                "id": 110
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 13,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Nonlinear filters",
                "Anisotropic magnetoresistance",
                "Image restoration",
                "Statistics",
                "Markov random fields",
                "Tensile stress",
                "Training data",
                "Image denoising",
                "History",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "filtering theory",
                "Gaussian processes",
                "image denoising",
                "random processes",
                "realistic images",
                "statistical analysis",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "steerable random fields",
                "local image structure",
                "structure tensor",
                "statistics",
                "steered filter response",
                "natural image",
                "Gaussian scale mixture",
                "image denoising",
                "image inpainting"
            ]
        },
        "id": 140,
        "cited_by": [
            {
                "year": "2017",
                "id": 123
            },
            {
                "year": "2011",
                "id": 211
            }
        ]
    },
    {
        "title": "Robust Structured Light Coding for 3D Reconstruction",
        "authors": [
            "Chadi Albitar",
            "Pierre Graebling",
            "Christophe Doignon"
        ],
        "abstract": "In this paper we present a new monochromatic pattern for a robust structured light coding based on the spatial neighborhood scheme using the M-array approach. The proposed pattern is robust as it allows a high error rate characterized by an average Hamming distance higher than 6. We tackle the design problem with the definition of a small set of symbols associated to simple geometrical primitives. One of these primitives embeds the local orientation of the pattern. This is helpful while performing the search for the relevant neighborhood during the decoding process. The aim of this work is to use this pattern for the real-time 3-D reconstruction of dynamic scenes, particularly in endoscopic surgery, with fast and reliable detection and decoding stages. Ongoing results are presented to assess both the capabilities of the proposed pattern and the reliable decoding algorithm with projections onto simple 3-D scenes and onto internal structures of a pig abdomen.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408982",
        "reference_list": [],
        "citation": {
            "ieee": 37,
            "other": 34,
            "total": 71
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Decoding",
                "Layout",
                "Image reconstruction",
                "Surgery",
                "Abdomen",
                "Surface reconstruction",
                "Robot vision systems",
                "Three dimensional displays",
                "Spatial resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "decoding",
                "geometry",
                "image coding",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust structured light coding",
                "monochromatic pattern",
                "M-array approach",
                "average Hamming distance",
                "geometrical primitives",
                "decoding process",
                "real-time 3D reconstruction",
                "endoscopic surgery",
                "3D scenes"
            ]
        },
        "id": 141,
        "cited_by": []
    },
    {
        "title": "A Globally Optimal Algorithm for Robust TV-L1 Range Image Integration",
        "authors": [
            "Christopher Zach",
            "Thomas Pock",
            "Horst Bischof"
        ],
        "abstract": "Robust integration of range images is an important task for building high-quality 3D models. Since range images, and in particular range maps from stereo vision, may have a substantial amount of outliers, any integration approach aiming at high-quality models needs an increased level of robustness. Additionally, a certain level of regularization is required to obtain smooth surfaces. Computational efficiency and global convergence are further preferable properties. The contribution of this paper is a unified framework to solve all these issues. Our method is based on minimizing an energy functional consisting of a total variation (TV) regularization force and an L1 data fidelity term. We present a novel and efficient numerical scheme, which combines the duality principle for the TV term with a point-wise optimization step. We demonstrate the superior performance of our algorithm on the well-known Middlebury multi-view database and additionally on real-world multi-view images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408983",
        "reference_list": [
            {
                "year": "2003",
                "id": 171
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 39,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "TV",
                "Surface reconstruction",
                "Geometry",
                "Image reconstruction",
                "Embedded computing",
                "Computer graphics",
                "Stereo vision",
                "Computational efficiency",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "globally optimal algorithm",
                "range image integration",
                "high-quality 3D models",
                "stereo vision",
                "smooth surfaces",
                "global convergence",
                "total variation regularization",
                "data fidelity",
                "duality principle",
                "point-wise optimization",
                "Middlebury multiview database",
                "real-world multi-view images"
            ]
        },
        "id": 142,
        "cited_by": [
            {
                "year": "2015",
                "id": 242
            },
            {
                "year": "2011",
                "id": 297
            },
            {
                "year": "2007",
                "id": 385
            }
        ]
    },
    {
        "title": "Real-Time Visibility-Based Fusion of Depth Maps",
        "authors": [
            "Paul Merrell",
            "Amir Akbarzadeh",
            "Liang Wang",
            "Philippos Mordohai",
            "Jan-Michael Frahm",
            "Ruigang Yang",
            "David Nister",
            "Marc Pollefeys"
        ],
        "abstract": "We present a viewpoint-based approach for the quick fusion of multiple stereo depth maps. Our method selects depth estimates for each pixel that minimize violations of visibility constraints and thus remove errors and inconsistencies from the depth maps to produce a consistent surface. We advocate a two-stage process in which the first stage generates potentially noisy, overlapping depth maps from a set of calibrated images and the second stage fuses these depth maps to obtain an integrated surface with higher accuracy, suppressed noise, and reduced redundancy. We show that by dividing the processing into two stages we are able to achieve a very high throughput because we are able to use a computationally cheap stereo algorithm and because this architecture is amenable to hardware-accelerated (GPU) implementations. A rigorous formulation based on the notion of stability of a depth estimate is presented first. It aims to determine the validity of a depth estimate by rendering multiple depth maps into the reference view as well as rendering the reference depth map into the other views in order to detect occlusions and free- space violations. We also present an approximate alternative formulation that selects and validates only one hypothesis based on confidence. Both formulations enable us to perform video-based reconstruction at up to 25 frames per second. We show results on the multi-view stereo evaluation benchmark datasets and several outdoors video sequences. Extensive quantitative analysis is performed using an accurately surveyed model of a real building as ground truth.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408984",
        "reference_list": [],
        "citation": {
            "ieee": 107,
            "other": 59,
            "total": 166
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise reduction",
                "Noise generators",
                "Fusion power generation",
                "Fuses",
                "Redundancy",
                "Throughput",
                "Computer architecture",
                "Stability",
                "Image reconstruction",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden feature removal",
                "image fusion",
                "image reconstruction",
                "rendering (computer graphics)",
                "stereo image processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple stereo depth map rendering",
                "real-time visibility-based fusion",
                "graphical processing unit",
                "occlusion detection",
                "video-based 3D shape reconstruction"
            ]
        },
        "id": 143,
        "cited_by": [
            {
                "year": "2017",
                "id": 243
            },
            {
                "year": "2017",
                "id": 549
            },
            {
                "year": "2011",
                "id": 104
            },
            {
                "year": "2009",
                "id": 43
            },
            {
                "year": "2009",
                "id": 219
            },
            {
                "year": "2009",
                "id": 236
            }
        ]
    },
    {
        "title": "Recovering Occlusion Boundaries from a Single Image",
        "authors": [
            "Derek Hoiem",
            "Andrew N. Stein",
            "Alexei A. Efros",
            "Martial Hebert"
        ],
        "abstract": "Occlusion reasoning, necessary for tasks such as navigation and object search, is an important aspect of everyday life and a fundamental problem in computer vision. We believe that the amazing ability of humans to reason about occlusions from one image is based on an intrinsically 3D interpretation. In this paper, our goal is to recover the occlusion boundaries and depth ordering of free-standing structures in the scene. Our approach is to learn to identify and label occlusion boundaries using the traditional edge and region cues together with 3D surface and depth cues. Since some of these cues require good spatial support (i.e., a segmentation), we gradually create larger regions and use them to improve inference over the boundaries. Our experiments demonstrate the power of a scene-based approach to occlusion reasoning.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408985",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2007",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 79,
            "other": 43,
            "total": 122
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Computer vision",
                "Humans",
                "Vegetation mapping",
                "Robot vision systems",
                "Navigation",
                "Image segmentation",
                "Displays",
                "Image processing",
                "Biomedical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "inference mechanisms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "occlusion boundaries recovering",
                "single image",
                "occlusion reasoning",
                "computer vision",
                "intrinsically 3D interpretation",
                "free-standing structures",
                "3D surface",
                "scene-based approach"
            ]
        },
        "id": 144,
        "cited_by": [
            {
                "year": "2015",
                "id": 155
            },
            {
                "year": "2015",
                "id": 291
            },
            {
                "year": "2011",
                "id": 268
            },
            {
                "year": "2009",
                "id": 1
            },
            {
                "year": "2009",
                "id": 71
            },
            {
                "year": "2009",
                "id": 98
            },
            {
                "year": "2009",
                "id": 294
            }
        ]
    },
    {
        "title": "Objects in Context",
        "authors": [
            "Andrew Rabinovich",
            "Andrea Vedaldi",
            "Carolina Galleguillos",
            "Eric Wiewiora",
            "Serge Belongie"
        ],
        "abstract": "In the task of visual object categorization, semantic context can play the very important role of reducing ambiguity in objects' visual appearance. In this work we propose to incorporate semantic object context as a post-processing step into any off-the-shelf object categorization model. Using a conditional random field (CRF) framework, our approach maximizes object label agreement according to contextual relevance. We compare two sources of context: one learned from training data and another queried from Google Sets. The overall performance of the proposed framework is evaluated on the PASCAL and MSRC datasets. Our findings conclude that incorporating context into object categorization greatly improves categorization accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408986",
        "reference_list": [
            {
                "year": "2003",
                "id": 151
            }
        ],
        "citation": {
            "ieee": 217,
            "other": 133,
            "total": 350
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Context modeling",
                "Computer vision",
                "Image segmentation",
                "Psychology",
                "Computer science",
                "Training data",
                "Object recognition",
                "Statistics",
                "Electrical capacitance tomography"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image segmentation",
                "object recognition",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual object categorization",
                "semantic object context",
                "conditional random field",
                "object label agreement",
                "object recognition",
                "image segmentation",
                "image representation",
                "contextual relevance"
            ]
        },
        "id": 145,
        "cited_by": [
            {
                "year": "2017",
                "id": 125
            },
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2015",
                "id": 1
            },
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2013",
                "id": 313
            },
            {
                "year": "2011",
                "id": 1
            },
            {
                "year": "2011",
                "id": 13
            },
            {
                "year": "2011",
                "id": 39
            },
            {
                "year": "2011",
                "id": 42
            },
            {
                "year": "2011",
                "id": 64
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2009",
                "id": 87
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2009",
                "id": 119
            },
            {
                "year": "2009",
                "id": 182
            },
            {
                "year": "2009",
                "id": 254
            }
        ]
    },
    {
        "title": "3D generic object categorization, localization and pose estimation",
        "authors": [
            "Silvio Savarese",
            "Li Fei-Fei"
        ],
        "abstract": "We propose a novel and robust model to represent and learn generic 3D object categories. We aim to solve the problem of true 3D object categorization for handling arbitrary rotations and scale changes. Our approach is to capture a compact model of an object category by linking together diagnostic parts of the objects from different viewing points. We emphasize on the fact that our \"parts\" are large and discriminative regions of the objects that are composed of many local invariant features. Instead of recovering a full 3D geometry, we connect these parts through their mutual homographic transformation. The resulting model is a compact summarization of both the appearance and geometry information of the object class. We propose a framework in which learning is done via minimal supervision compared to previous works. Our results on categorization show superior performances to state-of-the-art algorithms such as (Thomas et al., 2006). Furthermore, we have compiled a new 3D object dataset that consists of 10 different object categories. We have tested our algorithm on this dataset and have obtained highly promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408987",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            }
        ],
        "citation": {
            "ieee": 136,
            "other": 78,
            "total": 214
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Solid modeling",
                "Information geometry",
                "Testing",
                "Shape",
                "Object recognition",
                "Computer science",
                "Joining processes",
                "Encoding",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)",
                "object recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D generic object categorization",
                "3D generic object localization",
                "pose estimation",
                "local invariant feature",
                "3D geometry",
                "mutual homographic transformation",
                "minimal supervision"
            ]
        },
        "id": 146,
        "cited_by": [
            {
                "year": "2013",
                "id": 36
            },
            {
                "year": "2013",
                "id": 94
            },
            {
                "year": "2013",
                "id": 217
            },
            {
                "year": "2013",
                "id": 319
            },
            {
                "year": "2013",
                "id": 436
            },
            {
                "year": "2011",
                "id": 124
            },
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2011",
                "id": 331
            }
        ]
    },
    {
        "title": "A Biologically Inspired System for Action Recognition",
        "authors": [
            "H. Jhuang",
            "T. Serre",
            "L. Wolf",
            "T. Poggio"
        ],
        "abstract": "We present a biologically-motivated system for the recognition of actions from video sequences. The approach builds on recent work on object recognition based on hierarchical feedforward architectures [25, 16, 20] and extends a neurobiological model of motion processing in the visual cortex [10]. The system consists of a hierarchy of spatio-temporal feature detectors of increasing complexity: an input sequence is first analyzed by an array of motion- direction sensitive units which, through a hierarchy of processing stages, lead to position-invariant spatio-temporal feature detectors. We experiment with different types of motion-direction sensitive units as well as different system architectures. As in [16], we find that sparse features in intermediate stages outperform dense ones and that using a simple feature selection approach leads to an efficient system that performs better with far fewer features. We test the approach on different publicly available action datasets, in all cases achieving the highest results reported to date.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408988",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2003",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 223,
            "other": 185,
            "total": 408
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Motion detection",
                "Position sensitive particle detectors",
                "Video sequences",
                "Object recognition",
                "Biological system modeling",
                "Brain modeling",
                "Sensor arrays",
                "Motion analysis",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "object recognition",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "biologically inspired system",
                "action recognition",
                "video sequences",
                "object recognition",
                "hierarchical feedforward architectures",
                "neurobiological model",
                "motion processing",
                "motion-direction sensitive units",
                "position-invariant spatio-temporal feature detectors"
            ]
        },
        "id": 147,
        "cited_by": [
            {
                "year": "2017",
                "id": 323
            },
            {
                "year": "2015",
                "id": 312
            },
            {
                "year": "2015",
                "id": 351
            },
            {
                "year": "2015",
                "id": 451
            },
            {
                "year": "2015",
                "id": 499
            },
            {
                "year": "2015",
                "id": 513
            },
            {
                "year": "2013",
                "id": 280
            },
            {
                "year": "2011",
                "id": 225
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2009",
                "id": 14
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2009",
                "id": 62
            },
            {
                "year": "2009",
                "id": 127
            },
            {
                "year": "2009",
                "id": 193
            },
            {
                "year": "2009",
                "id": 204
            },
            {
                "year": "2009",
                "id": 248
            },
            {
                "year": "2009",
                "id": 262
            }
        ]
    },
    {
        "title": "Direct Estimation of Non-Rigid Registrations with Image-Based Self-Occlusion Reasoning",
        "authors": [
            "V. Gay-Bellile",
            "A. Bartoli",
            "P. Sayd"
        ],
        "abstract": "The registration problem for images of a deforming surface has been well studied. External occlusions are usually well-handled. In 2D image-based registration, self- occlusions are more challenging. Consequently, the surface is usually assumed to be only slightly self-occluding. This paper is about image-based non-rigid registration with self-occlusion reasoning. A specific framework explicitly modeling self-occlusions is proposed. It is combined with an intensity-based, i.e. direct, data term for registration. Self-occlusions are detected as shrinking areas in the 2D warp. Experimental results on several challenging datasets show that our approach successfully registers images with self-occlusions while effectively detecting the occluded regions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408989",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 5,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Image registration",
                "Cameras",
                "Deformable models",
                "Surface texture",
                "Video sequences",
                "Erbium",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface deformation",
                "image-based nonrigid registration",
                "self-occlusion reasoning",
                "nonrigid registrations"
            ]
        },
        "id": 148,
        "cited_by": []
    },
    {
        "title": "Efficient Generic Calibration Method for General Cameras with Single Centre of Projection",
        "authors": [
            "Aubrey K. Dunne",
            "John Mallon",
            "Paul F. Whelan"
        ],
        "abstract": "Generic camera calibration is a non-parametric calibration technique that is applicable to any type of vision sensor. However, the standard generic calibration method was developed with the goal of generality and it is therefore sub-optimal for the common case of cameras with a single centre of projection (e.g. pinhole, fisheye, hyperboloidal catadioptric). This paper proposes novel improvements to the standard generic calibration method for central cameras that reduce its complexity, and improve its accuracy and robustness. Improvements are achieved by taking advantage of the geometric constraints resulting from a single centre of projection. Input data for the algorithm is acquired using active grids, the performance of which is characterised. A new linear estimation stage to the generic algorithm is proposed incorporating classical pinhole calibration techniques, and it is shown to be significantly more accurate than the linear estimation stage of the standard method. A linear method for pose estimation is also proposed and evaluated against the existing polynomial method. Distortion correction and motion reconstruction experiments are conducted with real data for a hyperboloidal catadioptric sensor for both the standard and proposed methods. Results show the accuracy and robustness of the proposed method to be superior to those of the standard method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408990",
        "reference_list": [
            {
                "year": "2005",
                "id": 200
            },
            {
                "year": "2001",
                "id": 117
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Lenses",
                "Robustness",
                "Polynomials",
                "Machine vision",
                "Sensor systems",
                "Standards development",
                "Image reconstruction",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image sensors",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generic camera calibration",
                "vision sensor",
                "active grids",
                "generic algorithm",
                "pinhole calibration techniques",
                "pose estimation",
                "polynomial method",
                "motion reconstruction",
                "hyperboloidal catadioptric sensor"
            ]
        },
        "id": 149,
        "cited_by": []
    },
    {
        "title": "Real-Time Marker-free Motion Capture from multiple cameras",
        "authors": [
            "Brice Michoud",
            "Erwan Guillou",
            "Hector Briceno",
            "Saida Bouakaz"
        ],
        "abstract": "We present a fully-automated method for real-time and marker-free 3D human motion capture. The system computes the 3D shape of the person filmed from a synchronized camera set. We obtain a robust and real-time system by using both a fast 3D shape analysis and a skin segmentation algorithm for human tracking. A skeleton-based approach facilitates the shape analysis. We are able to track fast and complex human motion in very difficult cases, like self-occlusion. Results on long video sequences with rapid and complex movements, demonstrate our approach robustness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408991",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 7,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Humans",
                "Shape",
                "Skin",
                "Robustness",
                "Real time systems",
                "Tracking",
                "Topology",
                "Kinematics",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image segmentation",
                "image sequences",
                "motion estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time marker-free motion capture",
                "multiple cameras",
                "fully-automated method",
                "real-time system",
                "3D shape analysis",
                "human tracking",
                "skin segmentation algorithm",
                "human tracking",
                "shape analysis",
                "complex human motion",
                "video sequences"
            ]
        },
        "id": 150,
        "cited_by": []
    },
    {
        "title": "High Dynamic Range Camera using Reflective Liquid Crystal",
        "authors": [
            "Hidetoshi Mannami",
            "Ryusuke Sagawa",
            "Yasuhiro Mukaigawa",
            "Tomio Echigo",
            "Yasushi Yagi"
        ],
        "abstract": "High dynamic range images (HDRIs) are needed for capturing scenes that include drastic lighting changes. This paper presents a method to improve the dynamic range of a camera by using a reflective liquid crystal. The system consists of a camera and a reflective liquid crystal placed in front of the camera. By controlling the attenuation rate of the liquid crystal, the scene radiance for each pixel is adaptively controlled. After the control, the original scene radiance is derived from the attenuation rate of the liquid crystal and the radiance obtained by the camera. A prototype system has been developed and tested for a scene that includes drastic lighting changes. The radiance of each pixel was independently controlled and the HDRIs were obtained by calculating the original scene radiance from these results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408992",
        "reference_list": [
            {
                "year": "2003",
                "id": 153
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 8,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Dynamic range",
                "Cameras",
                "Liquid crystals",
                "Layout",
                "Optical attenuators",
                "Yagi-Uda antennas",
                "Brightness",
                "Spatial resolution",
                "Liquid crystal devices",
                "Control systems"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image processing",
                "liquid crystal displays"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high dynamic range camera",
                "reflective liquid crystal",
                "high dynamic range image",
                "attenuation rate",
                "scene radiance",
                "drastic lighting change"
            ]
        },
        "id": 151,
        "cited_by": []
    },
    {
        "title": "A Restoration Framework for Correcting Photometric and Geometric Distortions in Camera-based Document Images",
        "authors": [
            "Li Zhang",
            "Andy M. Yip",
            "Chew Lim Tan"
        ],
        "abstract": "This paper presents a restoration framework for correcting both photometric and geometric distortions in camera-based images of non-planar shaped documents to facilitate human perception and machine recognition. The photometric distortions, usually perceived as shading artifacts, are corrected by separating the shading image from the reflectance image through digital inpainting and surface fitting techniques. Meanwhile, the extracted shading image is also used to recover the document's surface shape through a shape-from-shading (SFS) method with a generic formulation of the image irradiance under arbitrary illumination conditions. The recovered surface shape is then employed to correct the geometric distortions through a physically-based flattening process. Results on real document images demonstrate the performance of each sub-task and the functionality of the whole framework. OCR results on restored images also show great improvements over the original distorted images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408993",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Photometry",
                "Shape",
                "Surface fitting",
                "Cameras",
                "Image analysis",
                "Image recognition",
                "Drives",
                "Reflectivity",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "document image processing",
                "image restoration",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera-based document image restoration",
                "geometric distortion correction",
                "photometric distortion correction",
                "nonplanar shaped document",
                "human perception",
                "machine recognition",
                "digital inpainting",
                "surface fitting",
                "shape-from-shading method",
                "image irradiance",
                "arbitrary illumination condition",
                "physically-based flattening process"
            ]
        },
        "id": 152,
        "cited_by": []
    },
    {
        "title": "Multi-Camera Calibration with One-Dimensional Object under General Motions",
        "authors": [
            "L. Wang",
            "F. C. Wu",
            "Z. Y. Hu"
        ],
        "abstract": "It is well known that in order to calibrate a single camera with a one-dimensional (1D) calibration object, the object must undertake some constrained motions, in other words, it is impossible to calibrate a single camera if the object motion is of general one. For a multi-camera setup, i.e., when the number of camera is more than one, can the cameras be calibrated by a 1D object under general motions? In this work, we prove that all cameras can indeed be calibrated and a calibration algorithm is also proposed and experimentally tested. In contrast to other multi-camera calibration method, no one calibrated \"base\" camera is needed. In addition, we show that for such multi-camera cases, the minimum condition of calibration and critical motions are similar to those of calibrating a single camera with 1D calibration object.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408994",
        "reference_list": [
            {
                "year": "2005",
                "id": 41
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 16,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Geometry",
                "Laboratories",
                "Pattern recognition",
                "Automation",
                "Testing",
                "Computer vision",
                "Data mining",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multicamera calibration",
                "one-dimensional object",
                "1D object calibration",
                "calibrated base camera"
            ]
        },
        "id": 153,
        "cited_by": []
    },
    {
        "title": "Learning priors for calibrating families of stereo cameras",
        "authors": [
            "Andrew W. Fitzgibbon",
            "Duncan P. Robertson",
            "Antonio Criminisi",
            "Srikumar Ramalingam",
            "Andrew Blake"
        ],
        "abstract": "Online camera recalibration is necessary for long-term deployment of computer vision systems. Existing algorithms assume that the source of recalibration information is a set of features in a general 3D scene; and that enough features are observed that the calibration problem is well-constrained. However; these assumptions are frequently invalid outside the laboratory. Real-world scenes often lack texture, contain repeated texture, or are mostly planar, making calibration difficult or impossible. In this paper we consider the calibration of families of stereo cameras, where each camera is assumed to have parameters drawn from a common but unknown prior distribution. We show how estimation of this prior using a small-number of offline-calibrated cameras (e.g. from the same production line) allows online calibration of additional cameras using a small number of point correspondences; and that using the estimated prior significantly increases the accuracy and robustness of stereo camera calibration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408843",
        "reference_list": [
            {
                "year": "2003",
                "id": 183
            },
            {
                "year": "2001",
                "id": 117
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Layout",
                "Robot vision systems",
                "Stereo vision",
                "Production",
                "Robustness",
                "Geometry",
                "Bayesian methods",
                "Computer vision"
            ]
        },
        "id": 154,
        "cited_by": []
    },
    {
        "title": "Geolocating Static Cameras",
        "authors": [
            "Nathan Jacobs",
            "Scott Satkin",
            "Nathaniel Roman",
            "Richard Speyer",
            "Robert Pless"
        ],
        "abstract": "A key problem in widely distributed camera networks is locating the cameras. This paper considers three scenarios for camera localization: localizing a camera in an unknown environment, adding a new camera in a region with many other cameras, and localizing a camera by finding correlations with satellite imagery. We find that simple summary statistics (the time course of principal component coefficients) are sufficient to geolocate cameras without determining correspondences between cameras or explicitly reasoning about weather in the scene. We present results from a database of images from 538 cameras collected over the course of a year. We find that for cameras that remain stationary and for which we have accurate image times- tamps, we can localize most cameras to within 50 miles of the known location. In addition, we demonstrate the use of a distributed camera network in the construction a map of weather conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408995",
        "reference_list": [
            {
                "year": "2005",
                "id": 240
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 25,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Satellites",
                "Layout",
                "Calibration",
                "Sun",
                "Jacobian matrices",
                "Computer science",
                "Statistical distributions",
                "Image databases",
                "Surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image processing",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "static camera geolocation",
                "distributed camera networks",
                "camera localization",
                "satellite imagery",
                "principal component coefficients"
            ]
        },
        "id": 155,
        "cited_by": [
            {
                "year": "2015",
                "id": 76
            },
            {
                "year": "2015",
                "id": 112
            },
            {
                "year": "2015",
                "id": 442
            }
        ]
    },
    {
        "title": "Variational Stereo Vision with Sharp Discontinuities and Occlusion Handling",
        "authors": [
            "Rami Ben-Ari",
            "Nir Sochen"
        ],
        "abstract": "This paper addresses the problem of correspondence establishment in binocular stereo vision. We suggest a novel variational approach that considers both the discontinuities and occlusions. It deals with color images as well as gray levels. The proposed method divides the image domain into the visible and occluded regions where each region is handled differently. The depth discontinuities in the visible domain are preserved by use of the total variation term in conjunction with the Mumford-Shah framework. In addition to the dense disparity and the occlusion maps, our method also provides a discontinuity function revealing the location of the boundaries in the disparity map. We evaluate our method on data sets from Middlebury site showing superior performance in comparison to the state of the art variational technique.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408996",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 8,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Stereo vision",
                "Layout",
                "TV",
                "Robustness",
                "Color",
                "Cameras",
                "Smoothing methods",
                "Mathematics",
                "Belief propagation",
                "Dynamic programming"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image colour analysis",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational stereo vision",
                "sharp discontinuities",
                "occlusion handling",
                "binocular stereo vision",
                "color images",
                "gray levels",
                "image domain",
                "depth discontinuities",
                "visible domain",
                "Mumford-Shah framework",
                "dense disparity",
                "occlusion maps",
                "discontinuity function"
            ]
        },
        "id": 156,
        "cited_by": [
            {
                "year": "2013",
                "id": 25
            }
        ]
    },
    {
        "title": "Multi-View Stereo via Graph Cuts on the Dual of an Adaptive Tetrahedral Mesh",
        "authors": [
            "Sudipta N. Sinha",
            "Philippos Mordohai",
            "Marc Pollefeys"
        ],
        "abstract": "We formulate multi-view 3D shape reconstruction as the computation of a minimum cut on the dual graph of a semi- regular, multi-resolution, tetrahedral mesh. Our method does not assume that the surface lies within a finite band around the visual hull or any other base surface. Instead, it uses photo-consistency to guide the adaptive subdivision of a coarse mesh of the bounding volume. This generates a multi-resolution volumetric mesh that is densely tesselated in the parts likely to contain the unknown surface. The graph-cut on the dual graph of this tetrahedral mesh produces a minimum cut corresponding to a triangulated surface that minimizes a global surface cost functional. Our method makes no assumptions about topology and can recover deep concavities when enough cameras observe them. Our formulation also allows silhouette constraints to be enforced during the graph-cut step to counter its inherent bias for producing minimal surfaces. Local shape refinement via surface deformation is used to recover details in the reconstructed surface. Reconstructions of the Multi- View Stereo Evaluation benchmark datasets and other real datasets show the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408997",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2005",
                "id": 45
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 19,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Shape",
                "Stereo image processing",
                "Topology",
                "Cost function",
                "Refining",
                "Robustness",
                "Computer science",
                "Mesh generation"
            ],
            "INSPEC: Controlled Indexing": [
                "graphs",
                "image reconstruction",
                "mesh generation",
                "minimisation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview stereo",
                "graph cuts",
                "adaptive tetrahedral mesh",
                "3D shape reconstruction",
                "minimization",
                "topology",
                "concavities",
                "surface deformation"
            ]
        },
        "id": 157,
        "cited_by": []
    },
    {
        "title": "Reconstructing High Quality Face-Surfaces using Model Based Stereo",
        "authors": [
            "Brian Amberg",
            "Andrew Blake",
            "Andrew Fitzgibbon",
            "Sami Romdhani",
            "Thomas Vetter"
        ],
        "abstract": "We present a novel model based stereo system, which accurately extracts the 3D shape and pose of faces from multiple images taken simultaneously. Extracting the 3D shape from images is important in areas such as pose-invariant face recognition and image manipulation. The method is based on a 3D morphable face model learned from a database of facial scans. The use of a strong face prior allows us to extract high precision surfaces from stereo data of faces, where traditional correlation based stereo methods fail because of the mostly textureless input images. The method uses two or more uncalibrated images of arbitrary baseline, estimating calibration and shape simultaneously. Results using two and three input images are presented. We replace the lighting and albedo estimation of a monocular method with the use of stereo information, making the system more accurate and robust. We evaluate the method using ground truth data and the standard PIE image dataset. A comparison with the state of the art monocular system shows that the new method has a significantly higher accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408998",
        "reference_list": [
            {
                "year": "2005",
                "id": 65
            },
            {
                "year": "2001",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 18,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image reconstruction",
                "Data mining",
                "Calibration",
                "Robustness",
                "Costs",
                "Surface reconstruction",
                "Image edge detection",
                "Stereo image processing",
                "Surface fitting"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image reconstruction",
                "pose estimation",
                "stereo image processing",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D shape extraction",
                "pose-invariant face recognition",
                "image manipulation",
                "3D morphable face model",
                "albedo estimation",
                "stereo image processing",
                "PIE image dataset",
                "state of the art monocular system",
                "image reconstruction",
                "facial scan database"
            ]
        },
        "id": 158,
        "cited_by": [
            {
                "year": "2015",
                "id": 435
            }
        ]
    },
    {
        "title": "Bilayer Stereo Matching",
        "authors": [
            "Dengfeng Chai",
            "Qunsheng Peng"
        ],
        "abstract": "This paper presents two novel approaches for stereo matching. First, a bitwise algorithm for stereo matching is proposed. It represents the disparity of each pixel as a binary number, treats each bit separately and determines them step by step, each step determines one bit of the disparities and involves only a single graph cut computation. It reduces the computation complexity to (9(log2 n). Second, bilayer stereo matching for scenes consisting of foreground and background is proposed. Disparity field of foreground and background layers are recovered independently, and the final disparity field is composed using the disparity field of two layers. Discontinuity between foreground and background can be preserved successfully by this schema. Experimental results show that the combination of bitwise algorithm and bilayer stereo matching can support realtime applications such as desktop-based 3D visual communication.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408999",
        "reference_list": [
            {
                "year": "2003",
                "id": 26
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Stereo vision",
                "Layout",
                "Belief propagation",
                "Bayesian methods",
                "Application software",
                "Visual communication",
                "Labeling",
                "Iterative algorithms",
                "Kernel",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "graph theory",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bilayer stereo matching",
                "bitwise algorithm",
                "single graph cut computation",
                "computation complexity",
                "disparity field",
                "desktop-based 3D visual communication"
            ]
        },
        "id": 159,
        "cited_by": []
    },
    {
        "title": "A Variational Method for Scene Flow Estimation from Stereo Sequences",
        "authors": [
            "Frederic Huguet",
            "Frederic Devernay"
        ],
        "abstract": "This paper presents a method for scene flow estimation from a calibrated stereo image sequence. The scene flow contains the 3-D displacement field of scene points, so that the 2-D optical flow can be seen as a projection of the scene flow onto the images. We propose to recover the scene flow by coupling the optical flow estimation in both cameras with dense stereo matching between the images, thus reducing the number of unknowns per image point. The use of a variational framework allows us to properly handle discontinuities in the observed surfaces and in the 3-D displacement field. Moreover our approach handles occlusions both for the optical flow and the stereo. We obtain a partial differential equations system coupling both the optical flow and the stereo, which is numerically solved using an original multi- resolution algorithm. Whereas previous variational methods were estimating the 3-D reconstruction at time t and the scene flow separately, our method jointly estimates both in a single optimization. We present numerical results on synthetic data with ground truth information, and we also compare the accuracy of the scene flow projected in one camera with a state-of-the-art single-camera optical flow computation method. Results are also presented on a real stereo sequence with large motion and stereo discontinuities. Source code and sample data are available for the evaluation of the algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409000",
        "reference_list": [
            {
                "year": "2003",
                "id": 79
            }
        ],
        "citation": {
            "ieee": 80,
            "other": 51,
            "total": 131
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image motion analysis",
                "Optical coupling",
                "Cameras",
                "Optical computing",
                "Image sequences",
                "Partial differential equations",
                "Three dimensional displays",
                "Optimization methods",
                "Data flow computing"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image sequences",
                "partial differential equations",
                "stereo image processing",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational method",
                "scene flow estimation",
                "calibrated stereo image sequence",
                "2D optical flow",
                "optical flow estimation",
                "dense stereo matching",
                "partial differential equations system",
                "state-of-the-art single-camera optical flow computation method",
                "stereo discontinuities"
            ]
        },
        "id": 160,
        "cited_by": [
            {
                "year": "2017",
                "id": 271
            },
            {
                "year": "2015",
                "id": 390
            },
            {
                "year": "2015",
                "id": 493
            },
            {
                "year": "2013",
                "id": 171
            },
            {
                "year": "2011",
                "id": 163
            },
            {
                "year": "2011",
                "id": 230
            },
            {
                "year": "2011",
                "id": 249
            },
            {
                "year": "2011",
                "id": 291
            }
        ]
    },
    {
        "title": "Relative Epipolar Motion of Tracked Features for Correspondence in Binocular Stereo",
        "authors": [
            "Hao Du",
            "Danping Zou",
            "Yan Qiu Chen"
        ],
        "abstract": "Most 3D reconstruction solutions focus on surfaces, and there has not been much research attention paid to the problem of reconstructing 3D scenes made up of large numbers of particles, while the ability to reconstruct such dynamic scenes is potentially very useful in many areas such as colony behavior research and visual modeling. This paper proposes an approach - relative epipolar motion (REM) - towards solving the correspondence problem in stereopsis by utilizing the motion clue. It matches feature trajectories instead of the features themselves as used by existing methods. The proposed method has the following new capabilities: (1) it supports reconstructing dynamic 3D scenes of large number of undistinguishable drifting particles; (2) It is applicable to correspondence establishment for dynamic surfaces made up of repetitive textures; (3) It offers an alternative way to project structured light in active mode for deforming surface reconstruction. Experiment results on both simulated and real-world scenes demonstrate its effectiveness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409001",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 5,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Surface reconstruction",
                "Surface texture",
                "Trajectory",
                "Biological system modeling",
                "Particle tracking",
                "Computer science",
                "Marine animals",
                "Educational institutions",
                "Birds"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image reconstruction",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relative epipolar motion",
                "binocular stereo",
                "3D reconstruction solutions",
                "dynamic 3D scenes",
                "colony behavior research",
                "visual modeling",
                "stereopsis",
                "drifting particles",
                "surface reconstruction"
            ]
        },
        "id": 161,
        "cited_by": [
            {
                "year": "2009",
                "id": 202
            }
        ]
    },
    {
        "title": "Stereo Matching with the Distinctive Similarity Measure",
        "authors": [
            "Kuk-Jin Yoon",
            "In So Kweon"
        ],
        "abstract": "The point ambiguity owing to the ambiguous local appearances of image points is the one of the main causes making the stereo problem difficult. Under the point ambiguity, local similarity measures are easy to be ambiguous and this results in false matches in ambiguous regions. In this paper, we present the new similarity measure to resolve the point ambiguity problem based on the idea that the distinctiveness, not the interest, is the appropriate criterion for the feature selection under the point ambiguity. The proposed similarity measure named the Distinctive Similarity Measure (DSM) is essentially based on the distinctiveness of image points and the dissimilarity between them, which are both closely related to the local appearances of image points; the distinctiveness of an image point is related to the probability of a mismatch while the dissimilarity is related to the probability of a good match. We verify the efficiency of the proposed DSM by using testbed image sets. Experimental results show that the proposed DSM is very effective and can be easily used for improving the performance of existing stereo methods under the point ambiguity.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409002",
        "reference_list": [
            {
                "year": "2001",
                "id": 57
            },
            {
                "year": "2001",
                "id": 173
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 11,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Stereo vision",
                "Computer vision",
                "Testing",
                "Image segmentation",
                "Cost function",
                "Robot vision systems",
                "Robustness",
                "Layout",
                "Markov random fields",
                "Heuristic algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "probability",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo matching",
                "point ambiguity problem",
                "feature selection",
                "distinctive similarity measure",
                "probability"
            ]
        },
        "id": 162,
        "cited_by": [
            {
                "year": "2009",
                "id": 236
            }
        ]
    },
    {
        "title": "Minimizing the Reprojection Error in Surface Reconstruction from Images",
        "authors": [
            "Pau Gargallo",
            "Emmanuel Prados",
            "Peter Sturm"
        ],
        "abstract": "This paper addresses the problem of image-based surface reconstruction. The main contribution is the computation of the exact derivative of the reprojection error functional. This allows its rigorous minimization via gradient descent surface evolution. The main difficulty has been to correctly take into account the visibility changes that occur when the surface moves. A geometric and analytical study of these changes is presented and used for the computation of derivative. Our analysis shows the strong influence that the movement of the contour generators has on the reprojection error. As a consequence, during the proper minimization of the reprojection error, the contour generators of the surface are automatically moved to their correct location in the images. Therefore, current methods adding additional silhouettes or apparent contour constraints to ensure this alignment can now be understood and justified by a single criterion: the reprojection error.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409003",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2005",
                "id": 183
            },
            {
                "year": "2005",
                "id": 45
            },
            {
                "year": "2003",
                "id": 128
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 14,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Pixel",
                "Area measurement",
                "Layout",
                "Image generation",
                "Position measurement",
                "Error correction",
                "Reflectivity",
                "Sensor phenomena and characterization"
            ],
            "INSPEC: Controlled Indexing": [
                "error analysis",
                "gradient methods",
                "image reconstruction",
                "minimisation",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image-based surface reconstruction",
                "reprojection error minimization",
                "gradient descent surface evolution",
                "contour generator"
            ]
        },
        "id": 163,
        "cited_by": [
            {
                "year": "2017",
                "id": 402
            },
            {
                "year": "2015",
                "id": 85
            },
            {
                "year": "2015",
                "id": 249
            }
        ]
    },
    {
        "title": "N3M: Natural 3D Markers for Real-Time Object Detection and Pose Estimation",
        "authors": [
            "Stefan Hinterstoisser",
            "Selim Benhimane",
            "Nassir Navab"
        ],
        "abstract": "In this paper, a new approach for object detection and pose estimation is introduced. The contribution consists in the conception of entities permitting stable detection and reliable pose estimation of a given object. Thanks to a well- defined off-line learning phase, we design local and minimal subsets of feature points that have, at the same time, distinctive photometric and geometric properties. We call these entities Natural 3D Markers (N3Ms). Constraints on the selection and the distribution of the subsets coupled with a multi-level validation approach result in a detection at high frame rates and allow us to determine the precise pose of the object. The method is robust against noise, partial occlusions, background clutter and illumination changes. The experiments show its superiority to existing standard methods. The validation was carried out using simulated ground truth data. Excellent results on real data demonstrated the usefulness of this approach for many computer vision applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409004",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 7,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Photometry",
                "Runtime",
                "Noise robustness",
                "Background noise",
                "Lighting",
                "Computer vision",
                "Application software",
                "Robot vision systems",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "natural 3D markers",
                "real-time object detection",
                "pose estimation",
                "stable detection",
                "off-line learning phase",
                "feature points subsets",
                "multi-level validation approach",
                "background clutter",
                "computer vision"
            ]
        },
        "id": 164,
        "cited_by": []
    },
    {
        "title": "Shining a Light on Human Pose: On Shadows, Shading and the Estimation of Pose and Shape",
        "authors": [
            "Alexandru O. Balan",
            "Michael J. Black",
            "Horst Haussecker",
            "Leonid Sigal"
        ],
        "abstract": "Strong lighting is common in natural scenes yet is often viewed as a nuisance for object pose estimation and tracking. In human shape and pose estimation, cast shadows can be confused with foreground structure while self shadowing and shading variation on the body cause the appearance of the person to change with pose. Rather than attempt to minimize the effects of lighting and shadows, we show that strong lighting in a scene actually makes pose and shape estimation more robust. Additionally, by recovering multiple body poses we are able to automatically estimate the lighting in the scene and the albedo of the body. Our approach makes use of a detailed 3D body model, the parameters of which are directly recovered from image data. We provide a thorough exploration of human pose estimation under strong lighting conditions and show: 1. the estimation of the light source from cast shadows; 2. the estimation of the light source and the albedo of the body from multiple body poses; 3. that a point light and cast shadows on the ground plane can be treated as an additional \"shadow camera\" that improves pose and shape recovery, particularly in monocular scenes. Additionally we introduce the notion of albedo constancy which employs lighting normalized image data for matching. Our experiments with multiple subjects show that rather than causing problems, strong lighting improves human pose and shape estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409005",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 6,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Shape",
                "Layout",
                "Cameras",
                "Light sources",
                "Lighting",
                "Robustness",
                "Computer science",
                "Shadow mapping",
                "Biological system modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human pose",
                "pose estimation",
                "shape estimation",
                "strong lighting",
                "shading variation",
                "shadows",
                "light source",
                "shape recovery",
                "pose recovery",
                "monocular scenes",
                "lighting normalized image data"
            ]
        },
        "id": 165,
        "cited_by": []
    },
    {
        "title": "Cluster Boosted Tree Classifier for Multi-View, Multi-Pose Object Detection",
        "authors": [
            "Bo Wu",
            "Ram Nevatia"
        ],
        "abstract": "Detection of object of a known class is a fundamental problem of computer vision. The appearance of objects can change greatly due to illumination, view point, and articulation. For object classes with large intra-class variation, some divide-and-conquer strategy is necessary. Tree structured classifier models have been used for multi-view multi- pose object detection in previous work. This paper proposes a boosting based learning method, called Cluster Boosted Tree (CBT), to automatically construct tree structured object detectors. Instead of using predefined intra-class sub- categorization based on domain knowledge, we divide the sample space by unsupervised clustering based on discriminative image features selected by boosting algorithm. The sub-categorization information of the leaf nodes is sent back to refine their ancestors' classification functions. We compare our approach with previous related methods on several public data sets. The results show that our approach outperforms the state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409006",
        "reference_list": [
            {
                "year": "2005",
                "id": 57
            }
        ],
        "citation": {
            "ieee": 70,
            "other": 45,
            "total": 115
        },
        "keywords": {
            "IEEE Keywords": [
                "Classification tree analysis",
                "Object detection",
                "Face detection",
                "Detectors",
                "Computer vision",
                "Boosting",
                "Humans",
                "Support vector machines",
                "Intelligent robots",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "divide and conquer methods",
                "object detection",
                "pattern classification",
                "pattern clustering",
                "trees (mathematics)",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cluster boosted tree classifier",
                "multiview object detection",
                "multipose object detection",
                "computer vision",
                "intra-class variation",
                "divide-and-conquer strategy",
                "tree structured classifier models",
                "learning method",
                "unsupervised clustering",
                "discriminative image features",
                "boosting algorithm"
            ]
        },
        "id": 166,
        "cited_by": []
    },
    {
        "title": "Adaptive enhancement and noise reduction in very low light-level video",
        "authors": [
            "Henrik Malm",
            "Magnus Oskarsson",
            "Eric Warrant",
            "Petrik Clarberg",
            "Jon Hasselgren",
            "Calle Lejdfors"
        ],
        "abstract": "A general methodology for noise reduction and contrast enhancement in very noisy image data with low dynamic range is presented. Video footage recorded in very dim light is especially targeted. Smoothing kernels that automatically adapt to the local spatio-temporal intensity structure in the image sequences are constructed in order to preserve and enhance fine spatial detail and prevent motion blur. In color image data, the chromaticity is restored and demosaicing of raw RGB input data is performed simultaneously with the noise reduction. The method is very general, contains few user-defined parameters and has been developed for efficient parallel computation using a GPU. The technique has been applied to image sequences with various degrees of darkness and noise levels, and results from some of these tests, and comparisons to other methods, are presented. The present work has been inspired by research on vision in nocturnal animals, particularly the spatial and temporal visual summation that allows these animals to see in dim light.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409007",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 18,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise reduction",
                "Image sequences",
                "Animals",
                "Dynamic range",
                "Smoothing methods",
                "Kernel",
                "Color",
                "Image restoration",
                "Concurrent computing",
                "Noise level"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image denoising",
                "image enhancement",
                "image restoration",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive enhancement",
                "noise reduction",
                "very low light-level video",
                "contrast enhancement",
                "noisy image data",
                "low dynamic range",
                "video footage",
                "smoothing kernels",
                "local spatio-temporal intensity structure",
                "image sequences",
                "fine spatial detail enhancement",
                "motion blur",
                "color image data",
                "raw RGB input data restoration",
                "raw RGB input data demosaicing",
                "spatial visual summation",
                "temporal visual summation"
            ]
        },
        "id": 167,
        "cited_by": []
    },
    {
        "title": "Trajectory Rectification and Path Modeling for Video Surveillance",
        "authors": [
            "Imran N. Junejo",
            "Hassan Foroosh"
        ],
        "abstract": "Path modeling for video surveillance is an active area of research. We address the issue of Euclidean path modeling in a single camera for activity monitoring in a multi- camera video surveillance system. The paper proposes (i) a novel linear solution to auto-calibrate any camera observing pedestrians and (ii) to use these calibrated cameras to detect unusual object behavior. During the unsupervised training phase, after auto-calibrating a camera and metric rectifying the input trajectories, the input sequences are registered to the satellite imagery and prototype path models are constructed. This allows us to estimate metric information directly from the video sequences. During the testing phase, using our simple yet efficient similarity measures, we seek a relation between the input trajectories derived from a sequence and the prototype path models. We test the proposed method on synthetic as well as on real-world pedestrian sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409008",
        "reference_list": [
            {
                "year": "2005",
                "id": 242
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 10,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Video surveillance",
                "Cameras",
                "Testing",
                "Calibration",
                "Object detection",
                "Layout",
                "Satellites",
                "Prototypes",
                "Video sequences",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "object detection",
                "video cameras",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "trajectory rectification",
                "video surveillance",
                "Euclidean path modeling",
                "activity monitoring",
                "camera autocalibration",
                "object behavior",
                "object detection",
                "satellite imagery",
                "video sequence"
            ]
        },
        "id": 168,
        "cited_by": []
    },
    {
        "title": "A Variational Framework for Simultaneous Motion Estimation and Restoration of Motion-Blurred Video",
        "authors": [
            "Leah Bar",
            "Benjamin Berkels",
            "Martin Rumpf",
            "Guillermo Sapiro"
        ],
        "abstract": "The problem of motion estimation and restoration of objects in a blurred video sequence is addressed in this paper. Fast movement of the objects, together with the aperture time of the camera, result in a motion-blurred image. The direct velocity estimation from this blurred video is inaccurate. On the other hand, an accurate estimation of the velocity of the moving objects is critical for restoration of motion-blurred video. Therefore, restoration needs accurate motion estimation and vice versa, and a joint process is called for. To address this problem we derive a novel model of the blurring process and propose a Mumford-Shah type of variational framework, acting on consecutive frames, for joint object deblurring and velocity estimation. The proposed procedure distinguishes between the moving object and the background and is accurate also close to the boundary of the moving object. Experimental results both on simulated and real data show the importance of this joint estimation and its superior performance when compared to the independent estimation of motion and restoration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409009",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 14,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Image restoration",
                "Cameras",
                "Image reconstruction",
                "Apertures",
                "Deconvolution",
                "Numerical simulation",
                "Optical computing",
                "Video sequences",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "image sequences",
                "motion estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational framework",
                "motion estimation",
                "motion restoration",
                "video sequence",
                "velocity estimation",
                "motion-blurred video restoration"
            ]
        },
        "id": 169,
        "cited_by": [
            {
                "year": "2017",
                "id": 113
            },
            {
                "year": "2017",
                "id": 425
            }
        ]
    },
    {
        "title": "Non-homogeneous Content-driven Video-retargeting",
        "authors": [
            "Lior Wolf",
            "Moshe Guttmann",
            "Daniel Cohen-Or"
        ],
        "abstract": "Video retargeting is the process of transforming an existing video to fit the dimensions of an arbitrary display. A compelling retargeting aims at preserving the viewers' experience by maintaining the information content of important regions in the frame, whilst keeping their aspect ratio. An efficient algorithm for video retargeting is introduced. It consists of two stages. First, the frame is analyzed to detect the importance of each region in the frame. Then, a transformation that respects the analysis shrinks less important regions more than important ones. Our analysis is fully automatic and based on local saliency, motion detection and object detectors. The performance of the proposed algorithm is demonstrated on a variety of video sequences, and compared to the state of the art in image retargeting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409010",
        "reference_list": [],
        "citation": {
            "ieee": 161,
            "other": 104,
            "total": 265
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion detection",
                "Face detection",
                "Detectors",
                "TV",
                "Computer science",
                "Computer displays",
                "Image motion analysis",
                "Object detection",
                "Video sequences",
                "Large screen displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "content-driven video-retargeting",
                "arbitrary display",
                "motion detection",
                "video sequence",
                "image retargeting"
            ]
        },
        "id": 170,
        "cited_by": [
            {
                "year": "2017",
                "id": 478
            },
            {
                "year": "2013",
                "id": 68
            },
            {
                "year": "2011",
                "id": 191
            },
            {
                "year": "2011",
                "id": 230
            },
            {
                "year": "2009",
                "id": 17
            },
            {
                "year": "2009",
                "id": 131
            },
            {
                "year": "2009",
                "id": 286
            }
        ]
    },
    {
        "title": "Event Detection in Crowded Videos",
        "authors": [
            "Yan Ke",
            "Rahul Sukthankar",
            "Martial Hebert"
        ],
        "abstract": "Real-world actions occur often in crowded, dynamic environments. This poses a difficult challenge for current approaches to video event detection because it is difficult to segment the actor from the background due to distracting motion from other objects in the scene. We propose a technique for event recognition in crowded videos that reliably identifies actions in the presence of partial occlusion and background clutter. Our approach is based on three key ideas: (1) we efficiently match the volumetric representation of an event against oversegmented spatio-temporal video volumes; (2) we augment our shape-based features using flow; (3) rather than treating an event template as an atomic entity, we separately match by parts (both in space and time), enabling robustness against occlusions and actor variability. Our experiments on human actions, such as picking up a dropped object or waving in a crowd show reliable detection with few false positives.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409011",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2001",
                "id": 114
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2005",
                "id": 185
            },
            {
                "year": "2005",
                "id": 18
            }
        ],
        "citation": {
            "ieee": 132,
            "other": 87,
            "total": 219
        },
        "keywords": {
            "IEEE Keywords": [
                "Event detection",
                "Layout",
                "Object detection",
                "Shape",
                "Humans",
                "Video sequences",
                "Target tracking",
                "Robustness",
                "Motion detection",
                "Image motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image motion analysis",
                "image segmentation",
                "object detection",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "crowded video event detection",
                "crowded dynamic environments",
                "actor segmentation",
                "distracting motion",
                "event recognition technique",
                "partial occlusion",
                "background clutter",
                "volumetric representation",
                "oversegmented spatio-temporal video volumes",
                "shape-based features",
                "part matching"
            ]
        },
        "id": 171,
        "cited_by": [
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 610
            },
            {
                "year": "2015",
                "id": 366
            },
            {
                "year": "2013",
                "id": 280
            },
            {
                "year": "2011",
                "id": 225
            },
            {
                "year": "2009",
                "id": 14
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2009",
                "id": 127
            },
            {
                "year": "2009",
                "id": 140
            },
            {
                "year": "2009",
                "id": 193
            },
            {
                "year": "2009",
                "id": 248
            }
        ]
    },
    {
        "title": "Leveraging archival video for building face datasets",
        "authors": [
            "Deva Ramanan",
            "Simon Baker",
            "Sham Kakade"
        ],
        "abstract": "We introduce a semi-supervised method for building large, labeled datasets effaces by leveraging archival video. Specifically, we have implemented a system for labeling 11 years worth of archival footage from a television show. We have compiled a dataset of 611,770 faces, orders of magnitude larger than existing collections. It includes variation in appearance due to age, weight gain, changes in hairstyles, and other factors difficult to observe in smaller-scale collections. Face recognition in an uncontrolled setting can be difficult. We argue (and demonstrate) that there is much structure at varying timescales in the video data that make recognition much easier. At local time scales, one can use motion and tracking to group face images together - we may not know the identity, but we know a single label applies to all faces in a track. At medium time scales (say, within a scene), one can use appearance features such as hair and clothing to group tracks across shot boundaries. However, at longer timescales (say, across episodes), one can no longer use clothing as a cue. This suggests that one needs to carefully encode representations of appearance, depending on the timescale at which one intends to match. We assemble our final dataset by classifying groups of tracks in a nearest-neighbors framework. We use a face library obtained by labeling track clusters in a reference episode. We show that this classification is significantly easier when exploiting the hierarchical structure naturally present in the video sequences. From a data-collection point of view, tracking is vital because it adds non-frontal poses to our face collection. This is important because we know of no other method for collecting images of non-frontal faces \"in the wild\".",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409012",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 18,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Tracking",
                "Clothing",
                "TV",
                "Face recognition",
                "Layout",
                "Hair",
                "Assembly",
                "Libraries",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "archival video",
                "building face datasets",
                "semisupervised method",
                "face recognition",
                "nearest-neighbors framework"
            ]
        },
        "id": 172,
        "cited_by": [
            {
                "year": "2009",
                "id": 5
            }
        ]
    },
    {
        "title": "Temporally Consistent Reconstruction from Multiple Video Streams Using Enhanced Belief Propagation",
        "authors": [
            "E. Scott Larsen",
            "Philippos Mordohai",
            "Marc Pollefeys",
            "Henry Fuchs"
        ],
        "abstract": "We present an approach for 3D reconstruction from multiple video streams taken by static, synchronized and calibrated cameras that is capable of enforcing temporal consistency on the reconstruction of successive frames. Our goal is to improve the quality of the reconstruction by finding corresponding pixels in subsequent frames of the same camera using optical flow, and also to at least maintain the quality of the single time-frame reconstruction when these correspondences are wrong or cannot be found. This allows us to process scenes with fast motion, occlusions and self- occlusions where optical flow fails for large numbers of pixels. To this end, we modify the belief propagation algorithm to operate on a 3D graph that includes both spatial and temporal neighbors and to be able to discard messages from outlying neighbors. We also propose methods for introducing a bias and for suppressing noise typically observed in uniform regions. The bias encapsulates information about the background and aids in achieving a temporally consistent reconstruction and in the mitigation of occlusion related errors. We present results on publicly available real video sequences. We also present quantitative comparisons with results obtained by other researchers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409013",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 18,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Streaming media",
                "Belief propagation",
                "Image reconstruction",
                "Cameras",
                "Image motion analysis",
                "Layout",
                "Visualization",
                "Pixel",
                "Stereo vision",
                "Three dimensional TV"
            ],
            "INSPEC: Controlled Indexing": [
                "belief maintenance",
                "graph theory",
                "image denoising",
                "image reconstruction",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporally consistent 3D image reconstruction",
                "multiple video stream",
                "optical flow",
                "occlusion",
                "belief propagation",
                "3D graph",
                "noise suppression"
            ]
        },
        "id": 173,
        "cited_by": [
            {
                "year": "2017",
                "id": 326
            },
            {
                "year": "2015",
                "id": 100
            },
            {
                "year": "2015",
                "id": 148
            },
            {
                "year": "2011",
                "id": 144
            },
            {
                "year": "2009",
                "id": 201
            }
        ]
    },
    {
        "title": "Separating Parts from 2D Shapes using Relatability",
        "authors": [
            "Xiaofeng Mi",
            "Doug DeCarlo"
        ],
        "abstract": "It's often important to analyze shapes as made up of parts. But there are two ways to think of how parts fit together. We can characterize the remainder of a shape after apart is removed; here we want to cut the shape so what remains has the simplest possible structure. Alternatively, we can cut out the part so that the part itself takes on a simple shape. These cuts do not directly give rise to a segmentation of the shape; a point inside the shape may associate with the part, the remainder, neither, or both. We present a new model for reconstructing these cuts based on the differential geometry of smoothed local symmetries. The model takes into account relatability (which characterizes clean cuts) to determine part boundaries. Our approach complements and unifies existing work on part- based segmentation of shape, and can be used to construct interesting simplifications of shapes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409014",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 12,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Tail",
                "Computer vision",
                "Marine animals",
                "Application software",
                "Mathematical model",
                "Computer science",
                "Cognitive science",
                "Solid modeling",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "differential geometry",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "2D shapes",
                "shape analysis",
                "shape segmentation",
                "cut reconstruction",
                "differential geometry",
                "smoothed local symmetries",
                "shape part-based segmentation",
                "shape simplifications"
            ]
        },
        "id": 174,
        "cited_by": [
            {
                "year": "2011",
                "id": 38
            }
        ]
    },
    {
        "title": "The Best of Both Worlds: Combining 3D Deformable Models with Active Shape Models",
        "authors": [
            "Christian Vogler",
            "Zhiguo Li",
            "Atul Kanaujia",
            "Siome Goldenstein",
            "Dimitris Metaxas"
        ],
        "abstract": "Reliable 3D tracking is still a difficult task. Most parametrized 3D deformable models rely on the accurate extraction of image features for updating their parameters, and are prone to failures when the underlying feature distribution assumptions are invalid. Active Shape Models (ASMs), on the other hand, are based on learning, and thus require fewer reliable local image features than parametrized 3D models, but fail easily when they encounter a situation for which they were not trained. In this paper, we develop an integrated framework that combines the strengths of both 3D deformable models and ASMs. The 3D model governs the overall shape, orientation and location, and provides the basis for statistical inference on both the image features and the parameters. The ASMs, in contrast, provide the majority of reliable 2D image features over time, and aid in recovering from drift and total occlusions. The framework dynamically selects among different ASMs to compensate for large viewpoint changes due to head rotations. This integration allows the robust tracking effaces and the estimation of both their rigid and non- rigid motions. We demonstrate the strength of the framework in experiments that include automated 3D model fitting and facial expression tracking for a variety of applications, including sign language.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409015",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 8,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Active shape model",
                "Feature extraction",
                "Robustness",
                "Head",
                "Tracking",
                "Motion estimation",
                "Handicapped aids",
                "Videos",
                "Switches"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "statistical analysis",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D deformable models",
                "active shape models",
                "3D tracking",
                "image feature extraction",
                "parametrized 3D models",
                "statistical inference",
                "automated 3D model fitting",
                "face robust tracking",
                "facial expression tracking",
                "sign language"
            ]
        },
        "id": 175,
        "cited_by": [
            {
                "year": "2015",
                "id": 433
            }
        ]
    },
    {
        "title": "Spatio-Temporal Shape from Silhouette using Four-Dimensional Delaunay Meshing",
        "authors": [
            "Ehsan Aganj",
            "Jean-Philippe Pons",
            "Florent Segonne",
            "Renaud Keriven"
        ],
        "abstract": "We propose a novel method for computing a four-dimensional (4D) representation of the spatio-temporal visual hull of a dynamic scene, based on an extension of a recent provably correct Delaunay meshing algorithm. By considering time as an additional dimension, our approach exploits seamlessly the time coherence between different frames to produce a compact and high-quality 4D mesh representation of the visual hull. The 3D visual hull at a given time instant is easily obtained by intersecting this 4D mesh with a temporal plane, thus enabling interpolation of objects' shape between consecutive frames. In addition, our approach offers easy and extensive control over the size and quality of the output mesh as well as over its associated re- projection error. Our numerical experiments demonstrate the effectiveness and flexibility of our approach for generating compact, high-quality, time-coherent visual hull representations from real silhouette image data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409016",
        "reference_list": [
            {
                "year": "2005",
                "id": 228
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 7,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Layout",
                "Size control",
                "Error correction",
                "Quantization",
                "Interpolation",
                "Computer errors",
                "Computer vision",
                "Robustness",
                "Rendering (computer graphics)"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "interpolation",
                "mesh generation",
                "spatiotemporal phenomena"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "four-dimensional Delaunay meshing algorithm",
                "spatio-temporal visual hull",
                "image representation",
                "interpolation",
                "silhouette image data"
            ]
        },
        "id": 176,
        "cited_by": [
            {
                "year": "2017",
                "id": 326
            },
            {
                "year": "2017",
                "id": 560
            },
            {
                "year": "2009",
                "id": 84
            }
        ]
    },
    {
        "title": "A Component Based Deformable Model for Generalized Face Alignment",
        "authors": [
            "Yuchi Huang",
            "Qingshan Liu",
            "Dimitris Metaxas"
        ],
        "abstract": "This paper presents a component based deformable model for generalized face alignment, in which a novel bi-stage statistical framework is proposed to account for both local and global shape characteristics. Instead of using statistical analysis on the entire shape as in previous alignment work, we build separate Gaussian models for shape components to preserve more detailed local shape deformations. In each model of components the Markov Network is integrated to provide simple geometry constraints for our search strategy. In order to make a better description of the nonlinear interrelationships over the shape components, the Gaussian process latent variable model is adopted to obtain enough control of full range shape variations. Furthermore, we propose an illumination-robust feature to lead the local fitting of every shape point when light conditions change dramatically. Based on this approach, our system can generate optimal shape for images with exaggerated expressions and under variable illumination, as evidenced by extensive experimentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409017",
        "reference_list": [
            {
                "year": "2001",
                "id": 38
            },
            {
                "year": "2003",
                "id": 139
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 7,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Principal component analysis",
                "Markov random fields",
                "Facial animation",
                "Face detection",
                "Image reconstruction",
                "Shape control",
                "Active shape model",
                "Biological system modeling",
                "Gaussian processes"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "Gaussian processes",
                "Markov processes",
                "search problems",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "component based deformable model",
                "generalized face alignment",
                "bistage statistical framework",
                "statistical analysis",
                "Gaussian models",
                "local shape deformations",
                "Markov network",
                "geometry constraints",
                "search strategy",
                "Gaussian process latent variable model",
                "illumination-robust feature"
            ]
        },
        "id": 177,
        "cited_by": []
    },
    {
        "title": "Fast Matching of Planar Shapes in Sub-cubic Runtime",
        "authors": [
            "Frank R. Schmidt",
            "Dirk Farin",
            "Daniel Cremers"
        ],
        "abstract": "The matching of planar shapes can be cast as a problem of finding the shortest path through a graph spanned by the two shapes, where the nodes of the graph encode the local similarity of respective points on each contour. While this problem can be solved using dynamic time warping, the complete search over the initial correspondence leads to cubic runtime in the number of sample points. In this paper, we cast the shape matching problem as one of finding the shortest circular path on a torus. We propose an algorithm to determine this shortest cycle which has provably sub-cubic runtime. Numerical experiments demonstrate that the proposed algorithm provides faster shape matching than previous methods. As an application, we show that it allows to efficiently compute a clustering of a shape data base.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409018",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 17,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Runtime",
                "Clustering algorithms",
                "Computer science",
                "Image analysis",
                "Information retrieval",
                "Image retrieval",
                "Internet",
                "Dynamic programming",
                "Speech recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "edge detection",
                "graph theory",
                "image matching",
                "image sampling",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "planar shape matching problem",
                "subcubic runtime",
                "graph shortest path finding",
                "dynamic time warping",
                "sample points",
                "torus shortest circular path",
                "shape clustering"
            ]
        },
        "id": 178,
        "cited_by": [
            {
                "year": "2015",
                "id": 179
            },
            {
                "year": "2011",
                "id": 271
            }
        ]
    },
    {
        "title": "Shape and Appearance Context Modeling",
        "authors": [
            "Xiaogang Wang",
            "Gianfranco Doretto",
            "Thomas Sebastian",
            "Jens Rittscher",
            "Peter Tu"
        ],
        "abstract": "In this work we develop appearance models for computing the similarity between image regions containing deformable objects of a given class in realtime. We introduce the concept of shape and appearance context. The main idea is to model the spatial distribution of the appearance relative to each of the object parts. Estimating the model entails computing occurrence matrices. We introduce a generalization of the integral image and integral histogram frameworks, and prove that it can be used to dramatically speed up occurrence computation. We demonstrate the ability of this framework to recognize an individual walking across a network of cameras. Finally, we show that the proposed approach outperforms several other methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409019",
        "reference_list": [
            {
                "year": "2003",
                "id": 86
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 150,
            "other": 101,
            "total": 251
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Context modeling",
                "Deformable models",
                "Histograms",
                "Cameras",
                "Computer vision",
                "Dictionaries",
                "Robustness",
                "Lighting",
                "Computational complexity"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "appearance context modeling",
                "image regions",
                "shape context modeling",
                "spatial distribution",
                "occurrence matrices",
                "integral image",
                "integral histogram"
            ]
        },
        "id": 179,
        "cited_by": [
            {
                "year": "2017",
                "id": 117
            },
            {
                "year": "2017",
                "id": 200
            },
            {
                "year": "2017",
                "id": 497
            },
            {
                "year": "2015",
                "id": 357
            },
            {
                "year": "2015",
                "id": 420
            },
            {
                "year": "2015",
                "id": 522
            },
            {
                "year": "2013",
                "id": 315
            },
            {
                "year": "2013",
                "id": 393
            }
        ]
    },
    {
        "title": "Shape Reconstruction Based on Similarity in Radiance Changes under Varying Illumination",
        "authors": [
            "Imari Sato",
            "Takahiro Okabe",
            "Qiong Yu",
            "Yoichi Sato"
        ],
        "abstract": "This paper presents a technique for determining an object's shape based on the similarity of radiance changes observed at points on its surface under varying illumination. To examine the similarity, we use an observation vector that represents a sequence of pixel intensities of a point on the surface under different lighting conditions. Assuming convex objects under distant illumination and orthographic projection, we show that the similarity between two observation vectors is closely related to the similarity between the surface normals of the corresponding points. This enables us to estimate the object's surface normals solely from the similarity of radiance changes under unknown distant lighting by using dimensionality reduction. Unlike most previous shape reconstruction methods, our technique neither assumes particular reflection models nor requires reference materials. This makes our method applicable to a wide variety of objects made of different materials.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409020",
        "reference_list": [],
        "citation": {
            "ieee": 25,
            "other": 8,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Lighting",
                "Layout",
                "Surface reconstruction",
                "Photometry",
                "Optical reflection",
                "Reflectivity",
                "Calibration",
                "Reconstruction algorithms",
                "Inverse problems"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape reconstruction",
                "radiance change similarity",
                "varying illumination",
                "object shape",
                "image sequence"
            ]
        },
        "id": 180,
        "cited_by": [
            {
                "year": "2009",
                "id": 217
            }
        ]
    },
    {
        "title": "Shape from Varying Illumination and Viewpoint",
        "authors": [
            "Neel Joshi",
            "David J. Kriegman"
        ],
        "abstract": "We address the problem of reconstructing the 3-D shape of a Lambertian surface from multiple images acquired as an object rotates under distant and possibly varying illumination. Using camera projection matrices estimated from point correspondences across views, the algorithm computes a dense correspondence map by minimizing a multi-ocular photometric constraint. Once correspondence across views is established, photometric stereo is applied to estimate a surface normal field and 3-D surface. Conceptually, the algorithm merges multi-view stereo and photometric stereo and uses aspects of both methods to recover shape. The method is straightforward to implement and relies on established principles from the two stereo methods. We empirically validate the method on images of a number of objects and show that it outperforms previous methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409021",
        "reference_list": [
            {
                "year": "2005",
                "id": 213
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 11,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Lighting",
                "Photometry",
                "Cameras",
                "Stereo vision",
                "Image reconstruction",
                "Surface reconstruction",
                "Costs",
                "Iterative methods",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "lighting",
                "minimisation",
                "object recognition",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object shape reconstruction",
                "Lambertian surface",
                "varying image illumination",
                "camera projection matrices",
                "multi ocular photometric constraint minimization",
                "photometric stereo image",
                "surface normal field estimation",
                "multi view stereo image"
            ]
        },
        "id": 181,
        "cited_by": [
            {
                "year": "2015",
                "id": 376
            }
        ]
    },
    {
        "title": "A Symmetry-Based Generative Model for Shape",
        "authors": [
            "Nhon H. Trinh",
            "Benjamin B. Kimia"
        ],
        "abstract": "We propose a novel generative language for shape that is based on the shock graph: given a shock graph topology, we explore constraints on the geometry and dynamics of the shock graph branches at each point required to generate a valid shape, i.e., with no self-intersection, cusps, or crossovers. We model the shape boundary as a piece-wise smooth circular arc spline, which is dense in the space of piecewise smooth curves. Using this model we derive an independent set of parameters which generate a variety of shapes and satisfy the reconstruction constraints. We show simple examples of using this generative model as an active deformable shape and for morphing between two shapes. The results illustrate that it is possible to generate any generic shape with relatively few parameters, further reduced if prior knowledge of shape is available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409022",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Active shape model",
                "Deformable models",
                "Topology",
                "Electric shock",
                "Image reconstruction",
                "Active contours",
                "Clouds",
                "Solid modeling",
                "Geometry",
                "Spline"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image reconstruction",
                "splines (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "symmetry-based generative model",
                "shock graph topology",
                "geometry",
                "piecewise smooth circular arc spline",
                "piecewise smooth curves",
                "reconstruction constraints"
            ]
        },
        "id": 182,
        "cited_by": [
            {
                "year": "2009",
                "id": 294
            }
        ]
    },
    {
        "title": "Spherical-Homoscedastic Shapes",
        "authors": [
            "Onur C. Hamsici",
            "Aleix M. Martinez"
        ],
        "abstract": "Shape analysis requires invariance under translation, scale and rotation. Translation and scale invariance can be realized by normalizing shape vectors with respect to their mean and norm. This maps the shape feature vectors onto the surface of a hypersphere. After normalization, the shape vectors can be made rotational invariant by modelling the resulting data using complex scalar rotation invariant distributions defined on the complex hypersphere, e.g., using the complex Bingham distribution. However, the use of these distributions is hampered by the difficulty in estimating their parameters, which is shown to be very costly or impossible in most cases. The purpose of this paper is twofold. First, we show under which conditions the classification results obtained with complex Binghams are identical to those obtained with the easy-to-estimate complex Normal distribution. Second, we derive a kernel function which (intrinsically) maps the data into a space where the above conditions are satisfied and, hence, where the normal model can be successfully used. This results in a simple, low-cost algorithm for representing and classifying shapes. We demonstrate the use of this technique in several experimental results for object and face recognition. Comparisons to other statistical shape representation/classification approaches demonstrate the superiority of the proposed algorithms in classification accuracy and computational time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409023",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Parameter estimation",
                "Kernel",
                "Gaussian distribution",
                "Algorithm design and analysis",
                "Face recognition",
                "Classification algorithms",
                "Distributed computing",
                "Probability distribution",
                "Symmetric matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spherical-homoscedastic shapes",
                "shape analysis",
                "scale invariance",
                "shape vectors",
                "shape feature vectors",
                "complex scalar rotation invariant distributions",
                "complex Bingham distribution",
                "complex Normal distribution",
                "kernel function",
                "statistical shape representation"
            ]
        },
        "id": 183,
        "cited_by": [
            {
                "year": "2009",
                "id": 128
            }
        ]
    },
    {
        "title": "Shape from Focus and Defocus: Convexity, Quasiconvexity and Defocus-Invariant Textures",
        "authors": [
            "Paolo Favaro"
        ],
        "abstract": "In this paper we analyze the convexity and the quasiconvexity of shape from focus/defocus and image restoration. We show that these problems are strictly quasiconvex for a family of Bregman's divergences, and in particular for least-squares. In addition to giving novel analytical insight to these problems, this study can be readily exploited to design algorithms: One can do away with global minimizers and obtain the same optimal solution by employing simple and efficient local methods. We experimentally validate this investigation by comparing two minimization algorithms: one based on a local method (gradient-flow) and another based on a global method (graph cuts). We show that both algorithms find the global optimum. Finally, we fully characterize defocus-invariant textures, a class of textures that do not allow depth recovery. We show how to decompose textures into defocus-invariant and defocus-varying components, and how this decomposition can be used to dramatically improve depth estimates.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409024",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 5,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Focusing",
                "Image analysis",
                "Image restoration",
                "Image texture analysis",
                "Algorithm design and analysis",
                "Layout",
                "Cameras",
                "Cost function",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image restoration",
                "image texture",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "defocus shape",
                "quasiconvexity textures",
                "defocus-invariant textures",
                "image restoration",
                "Bregman divergences",
                "least-squares",
                "minimization algorithms",
                "gradient-flow method",
                "global method",
                "graph cuts",
                "global optimum",
                "depth recovery",
                "defocus-invariant components",
                "defocus-varying components"
            ]
        },
        "id": 184,
        "cited_by": []
    },
    {
        "title": "Shape Descriptors for Maximally Stable Extremal Regions",
        "authors": [
            "Per-Erik Forssen",
            "David G. Lowe"
        ],
        "abstract": "This paper introduces an affine invariant shape descriptor for maximally stable extremal regions (MSER). Affine invariant feature descriptors are normally computed by sampling the original grey-scale image in an invariant frame defined from each detected feature, but we instead use only the shape of the detected MSER itself. This has the advantage that features can be reliably matched regardless of the appearance of the surroundings of the actual region. The descriptor is computed using the scale invariant feature transform (SIFT), with the resampled MSER binary mask as input. We also show that the original MSER detector can be modified to achieve better scale invariance by detecting MSERs in a scale pyramid. We make extensive comparisons of the proposed feature against a SIFT descriptor computed on grey-scale patches, and also explore the possibility of grouping the shape descriptors into pairs to incorporate more context. While the descriptor does not perform as well on planar scenes, we demonstrate various categories of full 3D scenes where it outperforms the SIFT descriptor computed on grey-scale patches. The shape descriptor is also shown to be more robust to changes in illumination. We show that a system can achieve the best performance under a range of imaging conditions by matching both the texture and shape descriptors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409025",
        "reference_list": [
            {
                "year": "2005",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 67,
            "other": 37,
            "total": 104
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Layout",
                "Lighting",
                "Object recognition",
                "Computer vision",
                "Computer science",
                "Image sampling",
                "Detectors",
                "Robustness",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "affine transforms",
                "feature extraction",
                "image matching",
                "image sampling",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape descriptors",
                "maximally stable extremal regions",
                "affine invariant shape descriptor",
                "image sampling",
                "grey-scale image",
                "scale invariant feature transform",
                "MSER detector",
                "scale invariance",
                "SIFT descriptor",
                "grey-scale patches",
                "imaging conditions",
                "image matching",
                "image texture"
            ]
        },
        "id": 185,
        "cited_by": [
            {
                "year": "2009",
                "id": 166
            },
            {
                "year": "2009",
                "id": 256
            },
            {
                "year": "2009",
                "id": 259
            }
        ]
    },
    {
        "title": "Embedded Profile Hidden Markov Models for Shape Analysis",
        "authors": [
            "Rui Huang",
            "Vladimir Pavlovic",
            "Dimitris N. Metaxas"
        ],
        "abstract": "An ideal shape model should be both invariant to global transformations and robust to local distortions. In this paper we present a new shape modeling framework that achieves both efficiently. A shape instance is described by a curvature-based shape descriptor. A Profile Hidden Markov Model (PHMM) is then built on such descriptors to represent a class of similar shapes. PHMMs are a particular type of Hidden Markov Models (HMMs) with special states and architecture that can tolerate considerable shape contour perturbations, including rigid and non-rigid deformations, occlusions, and missing parts. The sparseness of the PHMM structure provides efficient inference and learning algorithms for shape modeling and analysis. To capture the global characteristics of a class of shapes, the PHMM parameters are further embedded into a subspace that models long term spatial dependencies. The new framework can be applied to a wide range of problems, such as shape matching/registration, classification/recognition, etc. Our experimental results demonstrate the effectiveness and robustness of this new model in these different settings.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409026",
        "reference_list": [
            {
                "year": "2005",
                "id": 121
            },
            {
                "year": "2001",
                "id": 102
            },
            {
                "year": "2005",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Shape",
                "Robustness",
                "Inference algorithms",
                "Algorithm design and analysis",
                "Application software",
                "Image retrieval",
                "Image segmentation",
                "Computer science",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden Markov models",
                "image classification",
                "image recognition",
                "image registration",
                "image retrieval",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "embedded profile hidden Markov models",
                "shape analysis",
                "local distortions",
                "global transformations",
                "shape contour perturbations",
                "rigid deformations",
                "nonrigid deformations",
                "occlusions",
                "inference algorithms",
                "learning algorithms"
            ]
        },
        "id": 186,
        "cited_by": []
    },
    {
        "title": "Robust Estimation of Albedo for Illumination-invariant Matching and Shape Recovery",
        "authors": [
            "Soma Biswas",
            "Gaurav Aggarwal",
            "Rama Chellappa"
        ],
        "abstract": "In this paper, we propose a non-stationary stochastic filtering framework for the task of albedo estimation from a single image. There are several approaches in literature for albedo estimation, but few include the errors in estimates of surface normals and light source directions to improve the albedo estimate. The proposed approach effectively utilizes the error statistics of surface normals and illumination direction for robust estimation of albedo. The albedo estimate obtained is further used to generate albedo-free normalized images for recovering the shape of an object. Illustrations and experiments are provided to show the efficacy of the approach and its application to illumination-invariant matching and shape recovery.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409027",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Shape",
                "Lighting",
                "Face recognition",
                "Light sources",
                "Computer graphics",
                "Automation",
                "Computer science",
                "Educational institutions",
                "Stochastic processes"
            ],
            "INSPEC: Controlled Indexing": [
                "lighting",
                "object detection",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "albedo estimation",
                "illumination-invariant matching",
                "shape recovery",
                "nonstationary stochastic filtering framework",
                "surface normals estimates",
                "light source directions",
                "error statistics"
            ]
        },
        "id": 187,
        "cited_by": []
    },
    {
        "title": "Ricci Flow for 3D Shape Analysis",
        "authors": [
            "Xianfeng Gu",
            "Sen Wang",
            "Junho Kim",
            "Yun Zeng",
            "Yang Wang",
            "Hong Qin",
            "Dimitris Samaras"
        ],
        "abstract": "Ricci flow is a powerful curvature flow method in geometric analysis. This work is the first application of surface Ricci flow in computer vision. We show that previous methods based on conformal geometries, such as harmonic maps and least-square conformal maps, which can only handle 3D shapes with simple topology are subsumed by our Ricci flow based method which can handle surfaces with arbitrary topology. Because the Ricci flow method is intrinsic and depends on the surface metric only, it is invariant to rigid motion, scaling, and isometric and conformal deformations. The solution to Ricci flow is unique and its computation is robust to noise. Our Ricci flow based method can convert all 3D problems into 2D domains and offers a general framework for 3D surface analysis. Large non-rigid deformations can be registered with feature constraints, hence we introduce a method that constrains Ricci flow computation using feature points and feature curves. Finally, we demonstrate the applicability of this intrinsic shape representation through standard shape analysis problems, such as 3D shape matching and registration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409028",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 5,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Topology",
                "Computer vision",
                "Application software",
                "Geometry",
                "Noise robustness",
                "Noise shaping",
                "Deformable models",
                "Computer graphics",
                "Conformal mapping"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "curve fitting",
                "image matching",
                "image reconstruction",
                "image registration",
                "image representation",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface Ricci flow method",
                "curvature flow method",
                "geometric analysis",
                "3D shape representation",
                "computer vision",
                "3D surface analysis framework",
                "image deformation",
                "feature curve",
                "3D shape matching",
                "3D shape registration"
            ]
        },
        "id": 188,
        "cited_by": [
            {
                "year": "2009",
                "id": 304
            }
        ]
    },
    {
        "title": "Fitting a Morphable Model to 3D Scans of Faces",
        "authors": [
            "Volker Blanz",
            "Kristina Scherbaum",
            "Hans-Peter Seidel"
        ],
        "abstract": "This paper presents a top-down approach to 3D data analysis by fitting a morphable model to scans of faces. In a unified framework, the algorithm optimizes shape, texture, pose and illumination simultaneously. The algorithm can be used as a core component in face recognition from scans. In an analysis-by-synthesis approach, raw scans are transformed into a PCA-based representation that is robust with respect to changes in pose and illumination. Illumination conditions are estimated in an explicit simulation that involves specular and diffuse components. The algorithm inverts the effect of shading in order to obtain the diffuse reflectance in each point of the facial surface. Our results include illumination correction, surface completion and face recognition on the FRGC database of scans.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409029",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 22,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Shape",
                "Face recognition",
                "Iterative algorithms",
                "Reflectivity",
                "Cost function",
                "Algorithm design and analysis",
                "Surface fitting",
                "Principal component analysis",
                "Data analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "data analysis",
                "face recognition",
                "image morphing",
                "image representation",
                "principal component analysis",
                "solid modelling",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "morphable model fitting",
                "3D face scan",
                "top-down approach",
                "3D data analysis",
                "face recognition",
                "analysis-by-synthesis approach",
                "PCA-based representation",
                "illumination correction",
                "surface fitting",
                "FRGC database"
            ]
        },
        "id": 189,
        "cited_by": [
            {
                "year": "2011",
                "id": 74
            }
        ]
    },
    {
        "title": "Modeling View and Posture Manifolds for Tracking",
        "authors": [
            "Chan-Su Lee",
            "Ahmed Elgammal"
        ],
        "abstract": "In this paper we consider modeling data lying on multiple continuous manifolds. In particular, we model the shape manifold of a person performing a motion observed from different view points along a view circle at fixed camera height. We introduce a model that ties together the body configuration (kinematics) manifold and the visual manifold (observations) in a way that facilitates tracking the 3D configuration with continuous relative view variability. The model exploits the low dimensionality nature of both the body configuration manifold and the view manifold where each of them are represented separately.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409030",
        "reference_list": [
            {
                "year": "2001",
                "id": 51
            },
            {
                "year": "2005",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 41,
            "other": 32,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Biological system modeling",
                "Humans",
                "Shape",
                "Kinematics",
                "Solid modeling",
                "Tracking",
                "Computer science",
                "Search problems",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "learning (artificial intelligence)",
                "pose estimation",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "posture manifolds",
                "multiple continuous manifolds",
                "person shape manifold modeling",
                "body configuration manifold",
                "visual manifold",
                "3D configuration tracking",
                "complex motion pose estimation",
                "learning procedure"
            ]
        },
        "id": 190,
        "cited_by": [
            {
                "year": "2013",
                "id": 275
            },
            {
                "year": "2009",
                "id": 67
            }
        ]
    },
    {
        "title": "Convex Optimization for Deformable Surface 3-D Tracking",
        "authors": [
            "Mathieu Salzmann",
            "Richard Hartley",
            "Pascal Fua"
        ],
        "abstract": "3-D shape recovery of non-rigid surfaces from 3-D to 2-D correspondences is an under-constrained problem that requires prior knowledge of the possible deformations. State-of-the-art solutions involve enforcing smoothness constraints that limit their applicability and prevent the recovery of sharply folding and creasing surfaces. Here, we propose a method that does not require such smoothness constraints. Instead, we represent surfaces as triangulated meshes and, assuming the pose in the first frame to be known, disallow large changes of edge orientation between consecutive frames, which is a generally applicable constraint when tracking surfaces in a 25 frames- per-second video sequence. We will show that tracking under these constraints can be formulated as a Second Order Cone Programming feasibility problem. This yields a convex optimization problem with stable solutions for a wide range of surfaces with very different physical properties.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409031",
        "reference_list": [
            {
                "year": "2005",
                "id": 128
            },
            {
                "year": "2005",
                "id": 52
            },
            {
                "year": "2005",
                "id": 140
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 37,
            "total": 76
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Video sequences",
                "Australia",
                "Surface reconstruction",
                "Plastics",
                "Deformable models",
                "Linearity",
                "Learning systems",
                "Manifolds",
                "Government"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image sequences",
                "mesh generation",
                "optical tracking",
                "pose estimation",
                "surface fitting",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable 3D surface tracking",
                "3D shape recovery",
                "convex optimization",
                "triangulated mesh",
                "edge orientation",
                "video sequence",
                "second order cone programming feasibility problem"
            ]
        },
        "id": 191,
        "cited_by": [
            {
                "year": "2013",
                "id": 285
            },
            {
                "year": "2011",
                "id": 170
            },
            {
                "year": "2011",
                "id": 311
            },
            {
                "year": "2009",
                "id": 232
            }
        ]
    },
    {
        "title": "Synthetic Aperture Tracking: Tracking through Occlusions",
        "authors": [
            "Neel Joshi",
            "Shai Avidan",
            "Wojciech Matusik",
            "David J. Kriegman"
        ],
        "abstract": "Occlusion is a significant challenge for many tracking algorithms. Most current methods can track through transient occlusion, but cannot handle significant extended occlusion when the object's trajectory may change significantly. We present a method to track a 3D object through significant occlusion using multiple nearby cameras (e.g., a camera array). When an occluder and object are at different depths, different parts of the object are visible or occluded in each view due to parallax. By aggregating across these views, the method can track even when any individual camera observes very little of the target object. Implementation- wise, the methods are straightforward and build upon established single-camera algorithms. They do not require explicit modeling or reconstruction of the scene and enable tracking in complex, dynamic scenes with moving cameras. Analysis of accuracy and robustness shows that these methods are successful when upwards of '70% of the object is occluded in every camera view. To the best of our knowledge, this system is the first capable of tracking in the presence of such significant occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409032",
        "reference_list": [
            {
                "year": "2005",
                "id": 89
            },
            {
                "year": "2003",
                "id": 142
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 12,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Target tracking",
                "Layout",
                "Trajectory",
                "Video sequences",
                "Photography",
                "Robustness",
                "Computer vision",
                "Application software",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "object detection",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "synthetic aperture tracking:",
                "occlusions",
                "3D object tracking",
                "parallax",
                "single-camera algorithms",
                "scene reconstruction",
                "scene modeling"
            ]
        },
        "id": 192,
        "cited_by": []
    },
    {
        "title": "Differential EMD Tracking",
        "authors": [
            "Qi Zhao",
            "Shane Brennan",
            "Hai Tao"
        ],
        "abstract": "Illumination changes cause object appearance to change drastically and many existing tracking algorithms lack the capability to handle this problem. The Earth mover's distance (EMD) is a similarity measure that is more robust against illumination changes. However, EMD is computationally expensive and we therefore propose the differential EMD (DEMD) algorithm which computes the derivative of the EMD with respect to the object location so that the EMD does not need to be computed for every location in the tracking window. The fast differential formula is derived based on the sensitivity analysis of the simplex method as applied to the EMD formula. To further reduce the computation, signatures, i.e., variable-size descriptions of distributions, are employed as an object representation. The new algorithm models local background scenes as well as foreground objects to handle scale changes in a principled way. Extensive quantitative evaluation of the proposed algorithm has been carried out using benchmark sequences and the improvement over the standard mean shift tracker is demonstrated.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409033",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 9,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Layout",
                "Photometry",
                "Sensitivity analysis",
                "Optical computing",
                "Linear programming",
                "Kernel",
                "Earth",
                "Robustness",
                "Distributed computing"
            ],
            "INSPEC: Controlled Indexing": [
                "differential equations",
                "image representation",
                "optical tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "differential EMD tracking",
                "illumination change",
                "object appearance",
                "Earth mover distance",
                "similarity measure",
                "object location",
                "fast differential formula",
                "sensitivity analysis",
                "object representation"
            ]
        },
        "id": 193,
        "cited_by": []
    },
    {
        "title": "Graph Based Discriminative Learning for Robust and Efficient Object Tracking",
        "authors": [
            "Xiaoqin Zhang",
            "Weiming Hu",
            "Steve Maybank",
            "Xi Li"
        ],
        "abstract": "Object tracking is viewed as a two-class 'one-versus-rest' classification problem, in which the sample distribution of the target is approximately Gaussian while the background samples are often multimodal. Based on these special properties, we propose a graph embedding based discriminative learning method, in which the topology structures of graphs are carefully designed to reflect the properties of the sample distributions. This method can simultaneously learn the subspace of the target and its local discriminative structure against the background. Moreover, a heuristic negative sample selection scheme is adopted to make the classification more effective. In tracking procedure, the graph based learning is embedded into a Bayesian inference framework cascaded with hierarchical motion estimation, which significantly improves the accuracy and efficiency of the localization. Furthermore, an incremental updating technique for the graphs is developed to capture the changes in both appearance and illumination. Experimental results demonstrate that, compared with two state-of-the-art methods, the proposed tracking algorithm is more efficient and effective, especially in dynamically changing and clutter scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409034",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 13,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Target tracking",
                "Lighting",
                "Inference algorithms",
                "Learning systems",
                "Bayesian methods",
                "Motion estimation",
                "Brightness",
                "Covariance matrix",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian distribution",
                "image classification",
                "learning (artificial intelligence)",
                "motion estimation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph based discriminative learning",
                "object tracking",
                "one-versus-rest classification problem",
                "Gaussian distribution",
                "graph embedding",
                "heuristic negative sample selection",
                "Bayesian inference",
                "hierarchical motion estimation",
                "incremental updating",
                "clutter scenes"
            ]
        },
        "id": 194,
        "cited_by": [
            {
                "year": "2015",
                "id": 349
            },
            {
                "year": "2013",
                "id": 195
            }
        ]
    },
    {
        "title": "Integrating Appearance and Motion Cues for Simultaneous Detection and Segmentation of Pedestrians",
        "authors": [
            "Vinay Sharma",
            "James W. Davis"
        ],
        "abstract": "We present a unified method for simultaneously acquiring both the location and the silhouette shape of people in outdoor scenes. The proposed algorithm integrates top-down and bottom-up processes in a balanced manner, employing both appearance and motion cues at different perceptual levels. Without requiring manually segmented training data, the algorithm employs a simple top-down procedure to capture the high-level cue of object familiarity. Motivated by regularities in the shape and motion characteristics of humans, interactions among low-level contour features are exploited to extract mid-level perceptual cues such as smooth continuation, common fate, and closure. A Markov random field formulation is presented that effectively combines the various cues from the top-down and bottom-up processes. The algorithm is extensively evaluated on static and moving pedestrian datasets for both detection and segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409035",
        "reference_list": [
            {
                "year": "2005",
                "id": 64
            },
            {
                "year": "2003",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion detection",
                "Shape",
                "Data mining",
                "Layout",
                "Training data",
                "Markov random fields",
                "Image segmentation",
                "Object detection",
                "Feature extraction",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image motion analysis",
                "image segmentation",
                "Markov processes",
                "object detection",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bottom-up process",
                "top-down process",
                "motion cue",
                "low-level contour feature extraction",
                "Markov random field formulation",
                "pedestrian segmentation",
                "pedestrian detection"
            ]
        },
        "id": 195,
        "cited_by": []
    },
    {
        "title": "Optimization and Learning for Registration of Moving Dynamic Textures",
        "authors": [
            "Junzhou Huang",
            "Xiaolei Huang",
            "Dimitris Metaxas"
        ],
        "abstract": "We address the problem of registering a sequence of images in a moving dynamic texture video. This involves optimization with respect to camera motion, the average image, and the dynamic texture model. This problem is highly ill-posed and almost impossible to have good solutions without priors. In this paper, we introduce powerful priors for this problem, based on two simple observations: 1) registration should simplify the dynamic texture model while preserving all useful information. It motivates us to compute a prior for the dynamic texture by marginalizing over specific dynamics in the space of all stable auto-regressive sequences; 2) the statistics of derivative filter responses in the average image can be significantly changed by registration, and better registration should lead to a sharper average image. This offers us the prior of requiring the derivative distribution of the estimated average image to be close to that learned from the input image sequence. With these priors, a new registration approach is proposed by marginalizing over the \"nuisance\" variables under a Bayesian framework. And superior motion estimation results are obtained by jointly optimizing over the registration parameters, the average image, and the dynamic texture model. Experimental results on real video sequences of moving dynamic textures show convincing performance of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409036",
        "reference_list": [
            {
                "year": "2001",
                "id": 89
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Motion estimation",
                "Video sequences",
                "Brightness",
                "Image sequences",
                "Pixel",
                "Statistical distributions",
                "Filters",
                "Bayesian methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image registration",
                "image sequences",
                "image texture",
                "optimisation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "moving dynamic textures",
                "image sequence registration",
                "moving dynamic texture video",
                "camera motion",
                "average image",
                "dynamic texture model",
                "auto-regressive sequences"
            ]
        },
        "id": 196,
        "cited_by": []
    },
    {
        "title": "Learn to Track Edges",
        "authors": [
            "Yanghai Tsin",
            "Yakup Genc",
            "Ying Zhu",
            "Visvanathan Ramesh"
        ],
        "abstract": "Reliability of a model-based edge tracker critically depends on its ability to establish correct correspondences between points on the model edges and edge pixels in an image. This is a non-trivial problem especially in the presence of large inter-frame motions and in cluttered environments. We propose an online learning approach to solving this problem. An edge pixel is represented by a descriptor composed of a small segment of intensity patterns. From training examples the algorithm utilizes the randomized forest model to learn a posteriori distribution of correspondence given the descriptor. In a new frame, the edge pixels are classified using maximum a posteriori (MAP) estimation. The proposed method is very powerful and it enables us to apply the proposed tracker to many previously impossible scenarios with unprecedented robustness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409037",
        "reference_list": [
            {
                "year": "2005",
                "id": 196
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Mathematical model",
                "Image edge detection",
                "Pixel",
                "Robustness",
                "Detectors",
                "Image segmentation",
                "Robot localization",
                "Robotic assembly",
                "Augmented reality",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "image segmentation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "edge tracking",
                "model-based edge tracker",
                "model edges",
                "image edge pixels",
                "nontrivial problem",
                "large inter-frame motions",
                "cluttered environments",
                "online learning",
                "intensity pattern segmention",
                "randomized forest model",
                "maximum a posteriori estimation"
            ]
        },
        "id": 197,
        "cited_by": [
            {
                "year": "2015",
                "id": 486
            }
        ]
    },
    {
        "title": "Vector Quantizing Feature Space with a Regular Lattice",
        "authors": [
            "Tinne Tuytelaars",
            "Cordelia Schmid"
        ],
        "abstract": "Most recent class-level object recognition systems work with visual words, i.e., vector quantized local descriptors. In this paper we examine the feasibility of a data- independent approach to construct such a visual vocabulary, where the feature space is discretized using a regular lattice. Using hashing techniques, only non-empty bins are stored, and fine-grained grids become possible in spite of the high dimensionality of typical feature spaces. Based on this representation, we can explore the structure of the feature space, and obtain state-of-the-art pixelwise classification results. In the case of image classification, we introduce a class-specific feature selection step, which takes the spatial structure of SIFT-like descriptors into account. Results are reported on the Graz02 dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408924",
        "reference_list": [
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 68,
            "other": 30,
            "total": 98
        },
        "keywords": {
            "IEEE Keywords": [
                "Lattices",
                "Vocabulary",
                "Object recognition",
                "Space exploration",
                "Image classification",
                "Pixel",
                "Table lookup",
                "Buildings",
                "Extraterrestrial phenomena",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image coding",
                "image representation",
                "object recognition",
                "vector quantisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "vector quantizing feature space",
                "regular lattice",
                "object recognition systems",
                "data-independent approach",
                "visual vocabulary",
                "hashing techniques",
                "fine-grained grids",
                "pixelwise classification results",
                "image classification",
                "feature selection step",
                "SIFT-like descriptors"
            ]
        },
        "id": 198,
        "cited_by": [
            {
                "year": "2013",
                "id": 227
            },
            {
                "year": "2009",
                "id": 80
            },
            {
                "year": "2009",
                "id": 253
            },
            {
                "year": "2009",
                "id": 256
            }
        ]
    },
    {
        "title": "Learning the Taxonomy and Models of Categories Present in Arbitrary Images",
        "authors": [
            "Narendra Ahuja",
            "Sinisa Todorovic"
        ],
        "abstract": "This paper proposes, and presents a solution to, the problem of simultaneous learning of multiple visual categories present in an arbitrary image set and their inter-category relationships. These relationships, also called their taxonomy, allow categories to be defined recursively, as spatial configurations of (simpler) subcategories each of which may be shared by many categories. Each image is represented by a segmentation tree, whose structure captures recursive embedding of image regions in a multiscale segmentation, and whose nodes contain the associated region properties. The presence of any occurring categories is reflected in the occurrence of associated, similar subtrees within the image trees. Similar subtrees across the entire image set are clustered. Each cluster corresponds to a discovered category, represented by the cluster properties. A (subcategory) cluster of small matching subtrees may occur within multiple clusters (categories) of larger matching subtrees, in different spatial relationships with subtrees from other small clusters. Such recursive embedding, grouping and intersection of clusters is captured in a directed acyclic graph (DAG) which represents the discovered taxonomy. Detection, recognition and segmentation of any of the learned categories present in a new image are simultaneously conducted by matching the segmentation tree of the new image with the learned DAG. This matching also yields a semantic explanation of the recognized category, in terms of the presence of its subcategories. Experiments with a newly compiled dataset of four-legged animals demonstrate good cross-category resolvability.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409039",
        "reference_list": [
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 7,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Taxonomy",
                "Image segmentation",
                "Image recognition",
                "Animals",
                "Leg",
                "Shape",
                "Photometry",
                "Object detection",
                "Topology",
                "Tree graphs"
            ],
            "INSPEC: Controlled Indexing": [
                "directed graphs",
                "image matching",
                "image recognition",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "arbitrary image",
                "taxonomy",
                "image tree",
                "image matching",
                "recursive embedding",
                "directed acyclic graph",
                "image detection",
                "image recognition",
                "image segmentation tree",
                "four-legged animal",
                "cross-category resolvability"
            ]
        },
        "id": 199,
        "cited_by": []
    },
    {
        "title": "Shape Priors using Manifold Learning Techniques",
        "authors": [
            "Patrick Etyngier",
            "Florent Segonne",
            "Renaud Keriven"
        ],
        "abstract": "We introduce a non-linear shape prior for the de- formable model framework that we learn from a set of shape samples using recent manifold learning techniques. We model a category of shapes as a finite dimensional manifold which we approximate using Diffusion maps, that we call the shape prior manifold. Our method computes a Delaunay triangulation of the reduced space, considered as Euclidean, and uses the resulting space partition to identify the closest neighbors of any given shape based on its Nystrom extension. Our contribution lies in three aspects. First, we propose a solution to the pre-image problem and define the projection of a shape onto the manifold. Based on closest neighbors for the Diffusion distance, we then describe a variational framework for manifold denoising. Finally, we introduce a shape prior term for the deformable framework through a non-linear energy term designed to attract a shape towards the manifold at given constant embedding. Results on shapes of cars and ventricule nuclei are presented and demonstrate the potentials of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409040",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 27,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Active shape model",
                "Principal component analysis",
                "Image segmentation",
                "Noise reduction",
                "Statistics",
                "Bayesian methods",
                "Noise shaping",
                "Geophysics computing",
                "Level set"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image denoising",
                "image segmentation",
                "learning (artificial intelligence)",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonlinear shape priors",
                "manifold learning techniques",
                "deformable model framework",
                "finite dimensional manifold",
                "diffusion maps",
                "Delaunay triangulation",
                "Euclidean space",
                "Nystrom extension",
                "pre-image problem",
                "manifold denoising variational framework",
                "image segmentation"
            ]
        },
        "id": 200,
        "cited_by": [
            {
                "year": "2011",
                "id": 284
            },
            {
                "year": "2009",
                "id": 67
            }
        ]
    },
    {
        "title": "An Invariant Large Margin Nearest Neighbour Classifier",
        "authors": [
            "M. Pawan Kumar",
            "P.H.S. Torr",
            "A. Zisserman"
        ],
        "abstract": "The k-nearest neighbour (kNN) rule is a simple and effective method for multi-way classification that is much used in Computer Vision. However, its performance depends heavily on the distance metric being employed. The recently proposed large margin nearest neighbour (LMNN) classifier [21] learns a distance metric for kNN classification and thereby improves its accuracy. Learning involves optimizing a convex problem using semidefinite programming (SDP). We extend the LMNN framework to incorporate knowledge about invariance of the data. The main contributions of our work are three fold: (i) Invariances to multivariate polynomial transformations are incorporated without explicitly adding more training data during learning - these can approximate common transformations such as rotations and affinities; (ii) the incorporation of different regularizes on the parameters being learnt; and (Hi) for all these variations, we show that the distance metric can still be obtained by solving a convex SDP problem. We call the resulting formulation invariant LMNN (lLMNN) classifier. We test our approach to learn a metric for matching (i) feature vectors from the standard Iris dataset; and (ii) faces obtained from TV video (an episode of 'Buffy the Vampire Slayer'). We compare our method with the state of the art classifiers and demonstrate improvements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409041",
        "reference_list": [
            {
                "year": "2005",
                "id": 127
            },
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 11,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Training data",
                "Computer vision",
                "Polynomials",
                "Testing",
                "Iris",
                "Face detection",
                "TV",
                "Object recognition",
                "Image recognition",
                "Information retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "image classification",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "invariant large margin nearest neighbour classifier",
                "k-nearest neighbour rule",
                "multiway classification",
                "computer vision",
                "semidefinite programming",
                "multivariate polynomial transformations",
                "feature vectors",
                "standard iris dataset"
            ]
        },
        "id": 201,
        "cited_by": [
            {
                "year": "2013",
                "id": 31
            },
            {
                "year": "2011",
                "id": 121
            },
            {
                "year": "2009",
                "id": 38
            },
            {
                "year": "2009",
                "id": 63
            }
        ]
    },
    {
        "title": "3D Model based Object Class Detection in An Arbitrary View",
        "authors": [
            "Pingkun Yan",
            "Saad M. Khan",
            "Mubarak Shah"
        ],
        "abstract": "In this paper, a novel object class detection method based on 3D object modeling is presented. Instead of using a complicated mechanism for relating multiple 2D training views, the proposed method establishes spatial connections between these views by mapping them directly to the surface of 3D model. The 3D shape of an object is reconstructed by using a homographic framework from a set of model views around the object and is represented by a volume consisting of binary slices. Features are computed in each 2D model view and mapped to the 3D shape model using the same homographic framework. To generalize the model for object class detection, features from supplemental views are also considered. A codebook is constructed from all of these features and then a 3D feature model is built. Given a 2D test image, correspondences between the 3D feature model and the testing view are identified by matching the detected features. Based on the 3D locations of the corresponding features, several hypotheses of viewing planes can be made. The one with the highest confidence is then used to detect the object using feature location matching. Performance of the proposed method has been evaluated by using the PASCAL VOC challenge dataset and promising results are demonstrated.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409042",
        "reference_list": [
            {
                "year": "2003",
                "id": 149
            },
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2007",
                "id": 58
            },
            {
                "year": "2005",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 37,
            "other": 19,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Shape",
                "Image reconstruction",
                "Computer vision",
                "Cameras",
                "Surface reconstruction",
                "Testing",
                "Computer science",
                "Solid modeling",
                "Reconstruction algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image reconstruction",
                "image representation",
                "object detection",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object class detection",
                "3D object modeling",
                "3D object shape reconstruction",
                "homographic framework",
                "object represention",
                "binary slices",
                "2D model view",
                "3D feature model"
            ]
        },
        "id": 202,
        "cited_by": [
            {
                "year": "2017",
                "id": 14
            },
            {
                "year": "2015",
                "id": 103
            },
            {
                "year": "2011",
                "id": 124
            },
            {
                "year": "2009",
                "id": 27
            },
            {
                "year": "2009",
                "id": 172
            }
        ]
    },
    {
        "title": "Dynamic Cascades for Face Detection",
        "authors": [
            "Rong Xiao",
            "Huaiyi Zhu",
            "He Sun",
            "Xiaoou Tang"
        ],
        "abstract": "In this paper, we propose a novel method, called \"dynamic cascade\", for training an efficient face detector on massive data sets. There are three key contributions. The first is a new cascade algorithm called \"dynamic cascade \", which can train cascade classifiers on massive data sets and only requires a small number of training parameters. The second is the introduction of a new kind of weak classifier, called \"Bayesian stump\", for training boost classifiers. It produces more stable boost classifiers with fewer features. Moreover, we propose a strategy for using our dynamic cascade algorithm with multiple sets of features to further improve the detection performance without significant increase in the detector's computational cost. Experimental results show that all the new techniques effectively improve the detection performance. Finally, we provide the first large standard data set for face detection, so that future researches on the topic can be compared on the same training and testing set.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409043",
        "reference_list": [],
        "citation": {
            "ieee": 36,
            "other": 21,
            "total": 57
        },
        "keywords": {
            "IEEE Keywords": [
                "Face detection",
                "Detectors",
                "Bayesian methods",
                "Robustness",
                "Heuristic algorithms",
                "Computational efficiency",
                "Helium",
                "Sun",
                "Asia",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face detection",
                "dynamic cascade algorithm",
                "Bayesian stump"
            ]
        },
        "id": 203,
        "cited_by": [
            {
                "year": "2015",
                "id": 375
            },
            {
                "year": "2013",
                "id": 93
            },
            {
                "year": "2011",
                "id": 280
            }
        ]
    },
    {
        "title": "Simultaneous Learning of Nonlinear Manifold and Dynamical Models for High-dimensional Time Series",
        "authors": [
            "Rui Li",
            "Tai-Peng Tian",
            "Stan Sclaroff"
        ],
        "abstract": "The goal of this work is to learn a parsimonious and informative representation for high-dimensional time series. Conceptually, this comprises two distinct yet tightly coupled tasks: learning a low-dimensional manifold and modeling the dynamical process. These two tasks have a complementary relationship as the temporal constraints provide valuable neighborhood information for dimensionality reduction and conversely, the low-dimensional space allows dynamics to be learnt efficiently. Solving these two tasks simultaneously allows important information to be exchanged mutually. If nonlinear models are required to capture the rich complexity of time series, then the learning problem becomes harder as the nonlinearities in both tasks are coupled. The proposed solution approximates the nonlinear manifold and dynamics using piecewise linear models. The interactions among the linear models are captured in a graphical model. By exploiting the model structure, efficient inference and learning algorithms are obtained without oversimplifying the model of the underlying dynamical process. Evaluation of the proposed framework with competing approaches is conducted in three sets of experiments: dimensionality reduction and reconstruction using synthetic time series, video synthesis using a dynamic texture database, and human motion synthesis, classification and tracking on a benchmark data set. In all experiments, the proposed approach provides superior performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409044",
        "reference_list": [
            {
                "year": "2005",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 20,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Piecewise linear approximation",
                "Piecewise linear techniques",
                "Humans",
                "Parameter estimation",
                "Kernel",
                "Couplings",
                "Tracking",
                "Computer science",
                "Graphical models",
                "Inference algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "data visualisation",
                "image representation",
                "learning (artificial intelligence)",
                "time series"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "simultaneous learning",
                "nonlinear manifold",
                "dynamical models",
                "high-dimensional time series",
                "parsimonious representation",
                "informative representation",
                "low-dimensional manifold",
                "temporal constraints",
                "valuable neighborhood information",
                "dimensionality reduction",
                "nonlinear models",
                "learning problem",
                "nonlinear dynamics",
                "piecewise linear models",
                "graphical model",
                "inference algorithms",
                "learning algorithms",
                "synthetic time series",
                "video synthesis",
                "dynamic texture database",
                "human motion synthesis",
                "classification",
                "benchmark data set"
            ]
        },
        "id": 204,
        "cited_by": [
            {
                "year": "2013",
                "id": 275
            }
        ]
    },
    {
        "title": "A probabilistic, hierarchical, and discriminant framework for rapid and accurate detection of deformable anatomic structure",
        "authors": [
            "S. Kevin Zhou",
            "F. Guo",
            "J.H. Park",
            "G. Carneiro",
            "J. Jackson",
            "M. Brendel",
            "C. Simopoulos",
            "J. Otsuki",
            "D. Comaniciu"
        ],
        "abstract": "We propose a probabilistic, hierarchical, and discriminant (PHD) framework for fast and accurate detection of deformable anatomic structures from medical images. The PHD framework has three characteristics. First, it integrates distinctive primitives of the anatomic structures at global, segmental, and landmark levels in a probabilistic manner. Second, since the configuration of the anatomic structures lies in a high-dimensional parameter space, it seeks the best configuration via a hierarchical evaluation of the detection probability that quickly prunes the search space. Finally, to separate the primitive from the background, it adopts a discriminative boosting learning implementation. We apply the PHD framework for accurately detecting various deformable anatomic structures from M- mode and Doppler echocardiograms in about a second.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409045",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 5,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Ultrasonic imaging",
                "Biomedical imaging",
                "Deformable models",
                "Heart",
                "Space exploration",
                "Echocardiography",
                "Shape",
                "Object detection",
                "Data systems",
                "Automation"
            ],
            "INSPEC: Controlled Indexing": [
                "medical image processing",
                "object detection",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Doppler echocardiograms",
                "discriminative boosting learning implementation",
                "search space",
                "detection probability",
                "high-dimensional parameter space",
                "anatomic structures",
                "medical images",
                "deformable anatomic structure detection",
                "discriminant framework",
                "hierarchical framework",
                "probabilistic framework"
            ]
        },
        "id": 205,
        "cited_by": []
    },
    {
        "title": "Chaotic Invariants for Human Action Recognition",
        "authors": [
            "Saad Ali",
            "Arslan Basharat",
            "Mubarak Shah"
        ],
        "abstract": "The paper introduces an action recognition framework that uses concepts from the theory of chaotic systems to model and analyze nonlinear dynamics of human actions. Trajectories of reference joints are used as the representation of the non-linear dynamical system that is generating the action. Each trajectory is then used to reconstruct a phase space of appropriate dimension by employing a delay-embedding scheme. The properties of the reconstructed phase space are captured in terms of dynamical and metric invariants that include Lyapunov exponent, correlation integral and correlation dimension. Finally, the action is represented by a feature vector which is a combination of these invariants over all the reference trajectories. Our contributions in this paper include :1) investigation of the appropriateness of theory of chaotic systems for human action modelling and recognition, 2) a new set of features to characterize nonlinear dynamics of human actions, 3) experimental validation of the feasibility and potential merits of carrying out action recognition using methods from theory of chaotic systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409046",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2003",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 83,
            "other": 61,
            "total": 144
        },
        "keywords": {
            "IEEE Keywords": [
                "Chaos",
                "Humans",
                "Computer vision",
                "Nonlinear dynamical systems",
                "Biological system modeling",
                "Character recognition",
                "Feature extraction",
                "Control systems",
                "Stochastic systems",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "correlation methods",
                "image recognition",
                "integral equations",
                "Lyapunov methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "chaotic invariants",
                "human action recognition",
                "chaotic system",
                "nonlinear dynamics",
                "delay-embedding",
                "Lyapunov exponent",
                "correlation integral",
                "correlation dimension",
                "feature vector",
                "human action modelling"
            ]
        },
        "id": 206,
        "cited_by": [
            {
                "year": "2011",
                "id": 179
            },
            {
                "year": "2009",
                "id": 62
            },
            {
                "year": "2009",
                "id": 249
            },
            {
                "year": "2009",
                "id": 262
            }
        ]
    },
    {
        "title": "Classification of Weakly-Labeled Data with Partial Equivalence Relations",
        "authors": [
            "Sanjiv Kumar",
            "Henry A. Rowley"
        ],
        "abstract": "In many vision problems, instead of having fully labeled training data it is easier to obtain the input in small groups, where the data in each group is constrained to be from the same class but the actual class label is not known. Such constraints give rise to partial equivalence relations. The absence of class labels prevents the use of standard discriminative methods in this scenario. On the other hand, the state-of-the-art techniques that use partial equivalence relations, e.g., relevant component analysis, learn projections that are optimal for data representation, but not discrimination. We show that this leads to poor performance in several real-world applications, especially those with high-dimensional data. In this paper, we present a novel discriminative technique for the classification of weakly-labeled data which exploits the null-space of data scatter matrices to achieve good classification accuracy. We demonstrate the superior performance of both linear and nonlinear versions of our approach on face recognition, clustering, and image retrieval tasks. Results are reported on standard datasets as well as real-world images and videos from the Web.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409047",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Training data",
                "Face recognition",
                "Image retrieval",
                "Scattering",
                "Videos",
                "Face detection",
                "Kernel",
                "Labeling",
                "Principal component analysis",
                "Information retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image representation",
                "image retrieval",
                "learning (artificial intelligence)",
                "matrix algebra",
                "pattern clustering",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weakly-labeled data classification",
                "partial equivalence relations",
                "standard discriminative method",
                "relevant component analysis",
                "data representation",
                "data scatter matrices",
                "face recognition",
                "pattern clustering",
                "image retrieval"
            ]
        },
        "id": 207,
        "cited_by": []
    },
    {
        "title": "pLSA for Sparse Arrays With Tsallis Pseudo-Additive Divergence: Noise Robustness and Algorithm",
        "authors": [
            "Tamir Hazan",
            "Roee Hardoon",
            "Amnon Shashua"
        ],
        "abstract": "We introduce the Tsallis divergence error measure in the context of pLSA matrix and tensor decompositions showing much improved performance in the presence of noise. The focus of our approach is on one hand to provide an optimization framework which extends (in the sense of a one parameter family) the Maximum Likelihood framework and on the other hand is theoretically guaranteed to provide robustness under clutter, noise and outliers in the measurement matrix under certain conditions. Specifically, the conditions under which our approach excels is when the measurement array (co-occurrences) is sparse \u2014 which happens in the application domain of \"bag of visual words\".",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409048",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise robustness",
                "Sparse matrices",
                "Tensile stress",
                "Matrix decomposition",
                "Q measurement",
                "Maximum likelihood estimation",
                "Noise measurement",
                "Entropy",
                "Energy measurement",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "array signal processing",
                "matrix algebra",
                "maximum likelihood estimation",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparse arrays",
                "Tsallis pseudo-additive divergence",
                "pLSA matrix",
                "tensor decompositions",
                "maximum likelihood framework"
            ]
        },
        "id": 208,
        "cited_by": []
    },
    {
        "title": "Discriminative Subsequence Mining for Action Classification",
        "authors": [
            "Sebastian Nowozin",
            "Gokhan Bakir",
            "Koji Tsuda"
        ],
        "abstract": "Recent approaches to action classification in videos have used sparse spatio-temporal words encoding local appearance around interesting movements. Most of these approaches use a histogram representation, discarding the temporal order among features. But this ordering information can contain important information about the action itself e.g. consider the sport disciplines of hurdle race and long jump, where the global temporal order of motions (running, jumping) is important to discriminate between the two. In this work we propose to use a sequential representation which retains this temporal order. Further, we introduce Discriminative Subsequence Mining to find optimal discriminative subsequence patterns. In combination with the LPBoost classifier, this amounts to simultaneously learning a classification function and performing feature selection in the space of all possible feature sequences. The resulting classifier linearly combines a small number of interpretable decision functions, each checking for the presence of a single discriminative pattern. The classifier is benchmarked on the KTH action classification data set and outperforms the best known results in the literature.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409049",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2005",
                "id": 21
            }
        ],
        "citation": {
            "ieee": 57,
            "other": 41,
            "total": 98
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Histograms",
                "Image recognition",
                "Robustness",
                "Itemsets",
                "Boosting",
                "Biological information theory",
                "Cybernetics",
                "Encoding",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "data mining",
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)",
                "optimisation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative subsequence mining",
                "video action classification",
                "histogram representation",
                "optimal discriminative subsequence patterns",
                "LPBoost classifier",
                "classification function learning",
                "feature selection",
                "combinatorial optimization problem",
                "computer vision"
            ]
        },
        "id": 209,
        "cited_by": [
            {
                "year": "2013",
                "id": 340
            },
            {
                "year": "2009",
                "id": 14
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2009",
                "id": 118
            }
        ]
    },
    {
        "title": "Learning Auto-Structured Regressor from Uncertain Nonnegative Labels",
        "authors": [
            "Shuicheng Yan",
            "Huan Wang",
            "Xiaoou Tang",
            "Thomas S. Huang"
        ],
        "abstract": "In this paper, we take the human age and pose estimation problems as examples to study automatic designing regressor from training samples with uncertain nonnegative labels. First, the nonnegative label is predicted as the square norm of a matrix, which is bilinearly transformed from the nonlinear mappings of the candidate kernels. Two transformation matrices are then learned for deriving such a matrix by solving a semi definite programming (SDP) problem, in which the uncertain label of each sample is expressed as two inequality constraints. The objective function of SDP controls the ranks of these two matrices, and consequently automatically determines the structure of the regressor. The whole framework for automatic designing regressor from samples with uncertain nonnegative labels has the following characteristics: 1) SDP formulation makes full use of the uncertain labels, instead of using conventional fixed labels; 2) regression with matrix norm naturally guarantees the nonnegativity of the labels, and greater prediction capability is achieved by integrating the squares of the matrix elements, which act as weak regressors; and 3) the regressor structure is automatically determined by the pursuit of simplicity, which potentially promotes the algorithmic generalization capability. Extensive experiments on two human age databases, FG-NET and Yamaha, as well as the Pointing'04 pose database, demonstrate encouraging estimation accuracy improvements over conventional regression algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409050",
        "reference_list": [],
        "citation": {
            "ieee": 68,
            "other": 44,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Linear matrix inequalities",
                "Humans",
                "Kernel",
                "Image databases",
                "Head",
                "Design engineering",
                "Asia",
                "Automatic control",
                "Algorithm design and analysis",
                "Pursuit algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "matrix algebra",
                "pose estimation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "autostructured regressor learning",
                "human age",
                "pose estimation problems",
                "automatic designing regressor",
                "uncertain nonnegative labels",
                "nonlinear mappings",
                "transformation matrices",
                "definite programming problem",
                "objective function",
                "regressor structure",
                "algorithmic generalization capability"
            ]
        },
        "id": 210,
        "cited_by": [
            {
                "year": "2013",
                "id": 358
            },
            {
                "year": "2011",
                "id": 31
            },
            {
                "year": "2009",
                "id": 255
            }
        ]
    },
    {
        "title": "Latent Model Clustering and Applications to Visual Recognition",
        "authors": [
            "Simon Polak",
            "Amnon Shashua"
        ],
        "abstract": "We consider clustering situations in which the pairwise affinity between data points depends on a latent \"context\" variable. For example, when clustering features arising from multiple object classes the affinity value between two image features depends on the object class that generated those features. We show that clustering in the context of a latent variable can be represented as a special 3D hyper- graph and introduce an algorithm for obtaining the clusters. We use the latent clustering model for an unsupervised multiple object class recognition where feature fragments are shared among multiple clusters and those in turn are shared among multiple object classes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409051",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Clustering algorithms",
                "Context modeling",
                "Application software",
                "Computer science",
                "Data engineering",
                "Random variables",
                "Tensile stress",
                "Object detection",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "graph theory",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "latent model clustering",
                "visual recognition",
                "pairwise affinity",
                "context variable",
                "clustering features",
                "image features",
                "3D hypergraph",
                "unsupervised multiple object class recognition",
                "feature fragments"
            ]
        },
        "id": 211,
        "cited_by": []
    },
    {
        "title": "Probabilistic Linear Discriminant Analysis for Inferences About Identity",
        "authors": [
            "Simon J.D. Prince",
            "James H. Elder"
        ],
        "abstract": "Many current face recognition algorithms perform badly when the lighting or pose of the probe and gallery images differ. In this paper we present a novel algorithm designed for these conditions. We describe face data as resulting from a generative model which incorporates both within-individual and between-individual variation. In recognition we calculate the likelihood that the differences between face images are entirely due to within-individual variability. We extend this to the non-linear case where an arbitrary face manifold can be described and noise is position-dependent. We also develop a \"tied\" version of the algorithm that allows explicit comparison across quite different viewing conditions. We demonstrate that our model produces state of the art results for (i) frontal face recognition (ii) face recognition under varying pose.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409052",
        "reference_list": [],
        "citation": {
            "ieee": 274,
            "other": 58,
            "total": 332
        },
        "keywords": {
            "IEEE Keywords": [
                "Linear discriminant analysis",
                "Face recognition",
                "Probes",
                "Image recognition",
                "Lighting",
                "Vectors",
                "Computer science",
                "Educational institutions",
                "Inference algorithms",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic linear discriminant analysis",
                "face recognition algorithms",
                "face images",
                "arbitrary face manifold"
            ]
        },
        "id": 212,
        "cited_by": [
            {
                "year": "2013",
                "id": 400
            }
        ]
    },
    {
        "title": "Adaptive Vocabulary Forests br Dynamic Indexing and Category Learning",
        "authors": [
            "Tom Yeh",
            "John Lee",
            "Trevor Darrell"
        ],
        "abstract": "Histogram pyramid representations computed from a vocabulary tree of visual words have proven valuable for a range of image indexing and recognition tasks; however, they have only used a single, fixed partition of feature space. We present a new efficient algorithm to incrementally compute set-of-trees (forest) vocabulary representations, and show that they improve recognition and indexing performance in methods which use histogram pyramids. Our algorithm incrementally adapts a vocabulary forest with an Inverted file system at the leaf nodes and automatically keeps existing histogram pyramid database entries up-to-date in a forward filesystem. It is possible not only to apply vocabulary tree indexing algorithms directly, but also to compute pyramid match kernel values efficiently. On dynamic recognition tasks where categories or objects under consideration may change over time, we show that adaptive vocabularies offer significant performance advantages in comparison to a single, fixed vocabulary.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409053",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 13,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Vocabulary",
                "Indexing",
                "Image recognition",
                "Kernel",
                "Training data",
                "Testing",
                "Tellurium",
                "Visual databases",
                "Current measurement",
                "Grid computing"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image representation",
                "indexing",
                "vocabulary"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive vocabulary forests",
                "dynamic indexing",
                "category learning",
                "histogram pyramid representations",
                "visual words",
                "image indexing",
                "image recognition",
                "feature space",
                "compute set-of-trees forest vocabulary representations",
                "histogram pyramids",
                "inverted file system",
                "leaf nodes",
                "histogram pyramid database",
                "forward filesystem",
                "vocabulary tree indexing",
                "pyramid match kernel",
                "dynamic recognition"
            ]
        },
        "id": 213,
        "cited_by": []
    },
    {
        "title": "ClassMap: Efficient Multiclass Recognition via Embeddings",
        "authors": [
            "Vassilis Athitsos",
            "Alexandra Stefan",
            "Quan Yuan",
            "Stan Sclaroff"
        ],
        "abstract": "In many computer vision applications, such as face recognition and hand pose estimation, we need systems that can recognize a very large number of classes. Large margin classification methods, such as AdaBoost and SVMs, often provide competitive accuracy rates, but at the cost of evaluating a large number of binary classifiers. We propose an embedding-based method for efficient multiclass recognition. In our method, patterns and classes are mapped to vectors in such a way that patterns and their associated classes tend to get mapped close to each other. This way, given a test pattern, a small set of candidate classes can be identified efficiently using simple vector comparisons. In experiments with 3D hand pose recognition (2430 classes) and face recognition (535 classes), our method is between 3 and 28 times faster compared to evaluating all binary classifiers, with negligible or no loss in classification accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409054",
        "reference_list": [
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Databases",
                "Pattern recognition",
                "Shape",
                "Computer science",
                "Computer vision",
                "Handicapped aids",
                "Image recognition",
                "Application software",
                "Costs"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "image classification",
                "pose estimation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "classmap",
                "multiclass recognition",
                "computer vision",
                "face recognition",
                "hand pose estimation",
                "large margin classification methods",
                "AdaBoost",
                "SVM",
                "competitive accuracy rates",
                "binary classifiers",
                "embedding-based method"
            ]
        },
        "id": 214,
        "cited_by": [
            {
                "year": "2009",
                "id": 142
            }
        ]
    },
    {
        "title": "People-LDA: Anchoring Topics to People using Face Recognition",
        "authors": [
            "Vidit Jain",
            "Erik Learned-Miller",
            "Andrew McCallum"
        ],
        "abstract": "Topic models have recently emerged as powerful tools for modeling topical trends in documents. Often the resulting topics are broad and generic, associating large groups of people and issues that are loosely related. In many cases, it may be desirable to influence the direction in which topic models develop. In this paper, we explore the idea of centering topics around people. In particular, given a large corpus of images featuring collections of people and associated captions, it seems natural to extract topics specifically focussed on each person. What words are most associated with George Bush? Which with Condoleezza Rice? Since people play such an important role in life, it is natural to anchor one topic to each person. In this paper, we present People-LDA, which uses the coherence efface images in news captions to guide the development of topics. In particular, we show how topics can be refined to be more closely related to a single person (like George Bush) rather than describing groups of people in a related area (like politics). To do this we introduce a new graphical model that tightly couples images and captions through a modern face recognizer. In addition to producing topics that are people specific (using images as a guiding force), the model also performs excellent soft clustering efface images, using the language model to boost performance. We present a variety of experiments comparing our method to recent developments in topic modeling and joint image-language modeling, showing that our model has lower perplexity for face identification than competing models and produces more refined topics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409055",
        "reference_list": [
            {
                "year": "2005",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 5,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Graphical models",
                "Image recognition",
                "Refining",
                "Focusing",
                "Coherence"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "people-LDA",
                "joint image-language modeling",
                "face identification"
            ]
        },
        "id": 215,
        "cited_by": [
            {
                "year": "2009",
                "id": 63
            }
        ]
    },
    {
        "title": "Depth Information by Stage Classification",
        "authors": [
            "Vladimir Nedovic",
            "Arnold W.M. Smeulders",
            "Andre Redert",
            "Jan-Mark Geusebroek"
        ],
        "abstract": "Recently, methods for estimating 3D scene geometry or absolute scene depth information from 2D image content have been proposed. However, general applicability of these methods in depth estimation may not be realizable, as inconsistencies may be introduced due to a large variety of possible pictorial content. We identify scene categorization as the first step towards efficient and robust depth estimation from single images. To that end, we describe a limited number of typical 3D scene geometries, called stages, each having a unique depth pattern and thus providing a specific context for stage objects. This type of scene information narrows down the possibilities with respect to individual objects' locations, scales and identities. We show how these stage types can be efficiently learned and how they can lead to robust extraction of depth information. Our results indicate that stages without much variation and object clutter can be detected robustly, with up to 60% success rate.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409056",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2005",
                "id": 115
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 5,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Shape",
                "Robustness",
                "Statistics",
                "Information geometry",
                "Solid modeling",
                "Image reconstruction",
                "Humans",
                "Intelligent systems",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D scene geometry estimation",
                "absolute scene depth information",
                "2D image content",
                "scene categorization",
                "stage classification",
                "feature extraction"
            ]
        },
        "id": 216,
        "cited_by": [
            {
                "year": "2009",
                "id": 224
            },
            {
                "year": "2009",
                "id": 237
            }
        ]
    },
    {
        "title": "Real-time Accurate Object Detection using Multiple Resolutions",
        "authors": [
            "Wei Zhang",
            "Gregory Zelinsky",
            "Dimitris Samaras"
        ],
        "abstract": "We propose a multi-resolution framework inspired by human visual search for general object detection. Different resolutions are represented using a coarse-to-fine feature hierarchy. During detection, the lower resolution features are initially used to reject the majority of negative windows at relatively low cost, leaving a relatively small number of windows to be processed in higher resolutions. This enables the use of computationally more expensive higher resolution features to achieve high detection accuracy. We applied this framework on Histograms of Oriented Gradient (HOG) features for object detection. Our multi-resolution detector produced better performance for pedestrian detection than state-of-the-art methods (Dalal and Triggs, 2005), and was faster during both training and testing. Testing our method on motorbikes and cars from the VOC database revealed similar improvements in both speed and accuracy, suggesting that our approach is suitable for realtime general object detection applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409057",
        "reference_list": [],
        "citation": {
            "ieee": 38,
            "other": 27,
            "total": 65
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Testing",
                "Humans",
                "Costs",
                "Histograms",
                "Computer vision",
                "Detectors",
                "Motorcycles",
                "Object oriented databases",
                "Spatial databases"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image resolution",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object detection",
                "image resolution",
                "human visual search",
                "coarse-to-fine feature hierarchy",
                "histograms of oriented gradient"
            ]
        },
        "id": 217,
        "cited_by": [
            {
                "year": "2009",
                "id": 3
            }
        ]
    },
    {
        "title": "Efficient Feature Extraction for Image Classification",
        "authors": [
            "Wei Zhang",
            "Xiangyang Xue",
            "Zichen Sun",
            "Yue-Fei Guo",
            "Mingmin Chi",
            "Hong Lu"
        ],
        "abstract": "In many image classification applications, input feature space is often high-dimensional and dimensionality reduction is necessary to alleviate the curse of dimensionality or to reduce the cost of computation. In this paper, we extract discriminant features for image classification by learning a low-dimensional embedding from finite labeled samples. In the new feature space, intra-class compactness and extra-class separability are achieved simultaneously. Target dimensionality of the embedding is selected by spectral analysis. Our method is designed suitable for data with both uni- and multi-modal class distributions. We also develop its two-dimensional variant which makes use of the matrix representation of images. Experimental results on three real image datasets demonstrate the efficacy of our method compared to the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409058",
        "reference_list": [
            {
                "year": "2005",
                "id": 158
            },
            {
                "year": "2003",
                "id": 51
            },
            {
                "year": "2005",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Image classification",
                "Spectral analysis",
                "Covariance matrix",
                "Principal component analysis",
                "Computational efficiency",
                "Data mining",
                "Scattering",
                "Training data",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image representation",
                "image sampling",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature extraction",
                "image classification",
                "cost reduction",
                "finite labeled samples",
                "spectral analysis",
                "multimodal class distributions",
                "two-dimensional variant",
                "image matrix representation",
                "image datasets"
            ]
        },
        "id": 218,
        "cited_by": []
    },
    {
        "title": "A Divide-and-Conquer Approach to 3D Object Reconstruction from Line Drawings",
        "authors": [
            "Yu Chen",
            "Jianzhuang Liu",
            "Xiaoou Tang"
        ],
        "abstract": "3D object reconstruction from a single 2D line drawing is an important problem in both computer vision and graphics. Many methods have been put forward to solve this problem, but they usually fail when the geometric structure of a 3D object becomes complex. In this paper, a novel approach based on a divide-and-conquer strategy is proposed to handle 3D reconstruction of complex manifold objects from single 2D line drawings. The approach consists of three steps: 1) dividing a complex line drawing into multiple simpler line drawings based on the result efface identification; 2) reconstructing the 3D shapes from these simpler line drawings; and 3) merging the 3D shapes into one complete object represented by the original line drawing. A number of examples are given to show that our approach can handle 3D reconstruction of more complex objects than previous methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409059",
        "reference_list": [
            {
                "year": "2005",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Shape",
                "Engineering drawings",
                "Computer vision",
                "Computer graphics",
                "Object recognition",
                "Merging",
                "Geometry",
                "Databases",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "divide and conquer methods",
                "image reconstruction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "divide-and-conquer approach",
                "3D object reconstruction",
                "line drawings",
                "computer vision",
                "geometric structure",
                "complex manifold objects",
                "face identification"
            ]
        },
        "id": 219,
        "cited_by": []
    },
    {
        "title": "Hierarchical Ensemble of Global and Local Classifiers for Face Recognition",
        "authors": [
            "Yu Su",
            "Shiguang Shan",
            "Xilin Chen",
            "Wen Gao"
        ],
        "abstract": "In the literature of psychophysics and neurophysiology, many studies have shown that both global and local features are crucial for face representation and recognition. This paper proposes a novel face recognition method which combines both global and local discriminative features. In this method, global features are extracted from whole face images by Fourier transform and local features are extracted from some spatially partitioned image patches by Gabor wavelet transform. After this, multiple classifiers are obtained by applying Fisher Discriminant Analysis on global Fourier features and local patches of Gabor features. All these classifiers are combined to form a hierarchical ensemble by sum rule. We evaluated the proposed method using Face Recognition Grand Challenge (FRGC) experimental protocols and database known as the largest data sets available. Experimental results on FRGC version 2.0 data set have shown that the proposed method achieves a verification rate of 86%, while the best reported was 76%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409060",
        "reference_list": [
            {
                "year": "2005",
                "id": 101
            },
            {
                "year": "2001",
                "id": 198
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 9,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Feature extraction",
                "Principal component analysis",
                "Psychology",
                "Neurophysiology",
                "Fourier transforms",
                "Lighting",
                "Face detection",
                "Linear discriminant analysis",
                "Discrete Fourier transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "Fourier transforms",
                "image classification",
                "statistical analysis",
                "wavelet transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "face representation",
                "local discriminative feature extraction",
                "global discriminative feature extraction",
                "Fourier transform",
                "spatially partitioned image patch",
                "Gabor wavelet transform",
                "image classifier",
                "Fisher discriminant analysis",
                "hierarchical ensemble"
            ]
        },
        "id": 220,
        "cited_by": []
    },
    {
        "title": "Noise Robust Spectral Clustering",
        "authors": [
            "Zhenguo Li",
            "Jianzhuang Liu",
            "Shifeng Chen",
            "Xiaoou Tang"
        ],
        "abstract": "This paper aims to introduce the robustness against noise into the spectral clustering algorithm. First, we propose a warping model to map the data into a new space on the basis of regularization. During the warping, each point spreads smoothly its spatial information to other points. After the warping, empirical studies show that the clusters become relatively compact and well separated, including the noise cluster that is formed by the noise points. In this new space, the number of clusters can be estimated by eigenvalue analysis. We further apply the spectral mapping to the data to obtain a low-dimensional data representation. Finally, the K-means algorithm is used to perform clustering. The proposed method is superior to previous spectral clustering methods in that (i) it is robust against noise because the noise points are grouped into one new cluster; (ii) the number of clusters and the parameters of the algorithm are determined automatically. Experimental results on synthetic and real data have demonstrated this superiority.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409061",
        "reference_list": [
            {
                "year": "2003",
                "id": 42
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 10,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise robustness",
                "Clustering algorithms",
                "Machine learning algorithms",
                "Laplace equations",
                "Asia",
                "Eigenvalues and eigenfunctions",
                "Clustering methods",
                "Computer vision",
                "Machine learning",
                "Noise reduction"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "pattern clustering",
                "spectral analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "noise robust spectral clustering",
                "spectral clustering algorithm",
                "warping model",
                "eigenvalue analysis",
                "spectral mapping",
                "low-dimensional data representation",
                "k-means algorithm"
            ]
        },
        "id": 221,
        "cited_by": [
            {
                "year": "2009",
                "id": 53
            }
        ]
    },
    {
        "title": "Ten-fold Improvement in Visual Odometry Using Landmark Matching",
        "authors": [
            "Zhiwei Zhu",
            "Taragay Oskiper",
            "Supun Samarasekera",
            "Rakesh Kumar",
            "Harpreet S. Sawhney"
        ],
        "abstract": "Our goal is to create a visual odometry system for robots and wearable systems such that localization accuracies of centimeters can be obtained for hundreds of meters of distance traveled. Existing systems have achieved approximately a 1% to 5% localization error rate whereas our proposed system achieves close to 0.1% error rate, a ten-fold reduction. Traditional visual odometry systems drift over time as the frame-to-frame errors accumulate. In this paper, we propose to improve visual odometry using visual landmarks in the scene. First, a dynamic local landmark tracking technique is proposed to track a set of local landmarks across image frames and select an optimal set of tracked local landmarks for pose computation. As a result, the error associated with each pose computation is minimized to reduce the drift significantly. Second, a global landmark based drift correction technique is proposed to recognize previously visited locations and use them to correct drift accumulated during motion. At each visited location along the route, a set of distinctive visual landmarks is automatically extracted and inserted into a landmark database dynamically. We integrate the landmark based approach into a navigation system with 2 stereo pairs and a low-cost inertial measurement unit (IMU) for increased robustness. We demonstrate that a real-time visual odometry system using local and global landmarks can precisely locate a user within 1 meter over 1000 meters in unknown indoor/outdoor environments with challenging situations such as climbing stairs, opening doors, moving foreground objects etc..",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409062",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2003",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 6,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Navigation",
                "Layout",
                "Error analysis",
                "Stereo vision",
                "Cameras",
                "Sensor systems",
                "Error correction",
                "Robots",
                "Visual databases",
                "Measurement units"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "pose estimation",
                "robot vision",
                "tracking",
                "wearable computers"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robot visual odometry system",
                "visual landmark matching",
                "wearable systems",
                "dynamic local landmark tracking technique",
                "landmark database",
                "pose computation"
            ]
        },
        "id": 222,
        "cited_by": []
    },
    {
        "title": "Limits of Learning-Based Superresolution Algorithms",
        "authors": [
            "Zhouchen Lin",
            "Junfeng He",
            "Xiaoou Tang",
            "Chi-Keung Tang"
        ],
        "abstract": "Learning-based superresolution (SR) are popular SR techniques that use application dependent priors to infer the missing details in low resolution images (LRIs). However, their performance still deteriorates quickly when the magnification factor is moderately large. This leads us to an important problem: \"Do limits of learning-based SR algorithms exist?\" In this paper, we attempt to shed some light on this problem when the SR algorithms are designed for general natural images (GNIs). We first define an expected risk for the SR algorithms that is based on the root mean squared error between the superresolved images and the ground truth images. Then utilizing the statistics of GNIs, we derive a closed form estimate of the lower bound of the expected risk. The lower bound can be computed by sampling real images. By computing the curve of the lower bound w.r.t. the magnification factor, we can estimate the limits of learning-based SR algorithms, at which the lower bound of expected risk exceeds a relatively large threshold. We also investigate the sufficient number of samples to guarantee an accurate estimation of the lower bound.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409063",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 9,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Strontium",
                "Image resolution",
                "Markov random fields",
                "Frequency",
                "Signal resolution",
                "Algorithm design and analysis",
                "Asia",
                "Statistics",
                "Image sampling",
                "Signal processing"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "image sampling",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning-based superresolution algorithms",
                "low resolution images",
                "general natural images",
                "root mean squared error",
                "ground truth images",
                "image sampling"
            ]
        },
        "id": 223,
        "cited_by": []
    },
    {
        "title": "Exploiting Object Hierarchy: Combining Models from Different Category Levels",
        "authors": [
            "Alon Zweig",
            "Daphna Weinshall"
        ],
        "abstract": "We investigated the computational properties of natural object hierarchy in the context of constellation object class models, and its utility for object class recognition. We first observed an interesting computational property of the object hierarchy: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels (e.g., closed-frame vehicles or vehicles) exhibit higher recall but lower precision when compared with the class specific level (e.g., bus). These inherent differences suggest that combining object classifiers from different hierarchical levels into a single classifier may improve classification, as it appears like these models capture different aspects of the object. We describe a method to combine these classifiers, and analyze the conditions under which improvement can be guaranteed. When given a small sample of a new object class, we describe a method to transfer knowledge across the tree hierarchy, between related objects. Finally, we describe extensive experiments using object hierarchies obtained from publicly available datasets, and show that the combined classifiers significantly improve recognition results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409064",
        "reference_list": [
            {
                "year": "2005",
                "id": 230
            },
            {
                "year": "2005",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 42,
            "other": 17,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Classification tree analysis",
                "Boosting",
                "Training data",
                "Context modeling",
                "Vehicles",
                "Humans",
                "Object recognition",
                "Computer science",
                "Databases",
                "Cognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object hierarchy",
                "category levels",
                "constellation object",
                "object class recognition",
                "object classifiers",
                "knowledge transfer"
            ]
        },
        "id": 224,
        "cited_by": [
            {
                "year": "2015",
                "id": 129
            },
            {
                "year": "2015",
                "id": 223
            },
            {
                "year": "2015",
                "id": 305
            },
            {
                "year": "2013",
                "id": 33
            },
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 286
            },
            {
                "year": "2009",
                "id": 47
            }
        ]
    },
    {
        "title": "Support Kernel Machines for Object Recognition",
        "authors": [
            "Ankita Kumar",
            "Cristian Sminchisescu"
        ],
        "abstract": "Kernel classifiers based on Support Vector Machines (SVM) have recently achieved state-of-the art results on several popular datasets like Caltech or Pascal. This was possible by combining the advantages of SVM - convexity and the availability of efficient optimizers, with 'hyperkernels' - linear combinations of kernels computed at multiple levels of image encoding. The use of hyperkernels faces the challenge of choosing the kernel weights, the use of possibly irrelevant, poorly performing kernels, and an increased number of parameters that can lead to overfitting. In this paper we advocate the transition from SVMs to Support Kernel Machines (SKM) - models that estimate both the parameters of a sparse linear combination of kernels, and the parameters of a discriminative classifier. We exploit recent kernel learning techniques, not previously used in computer vision, that show how learning SKMs can be formulated as a convex optimization problem, which can be solved efficiently using Sequential Minimal Optimization. We study kernel learning for several multi-level image encodings for supervised object recognition and report competitive results on several datasets, including INRIA pedestrian, Caltech 101 and the newly created Caltech 256.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409065",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2005",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 14,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Object recognition",
                "Support vector machines",
                "Image coding",
                "Histograms",
                "Support vector machine classification",
                "Art",
                "Availability",
                "Parameter estimation",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image classification",
                "image coding",
                "learning (artificial intelligence)",
                "minimisation",
                "object recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "supervised object recognition",
                "support kernel machine learning",
                "support vector machine",
                "image encoding",
                "sparse linear kernel combination",
                "convex optimization problem",
                "sequential minimal optimization"
            ]
        },
        "id": 225,
        "cited_by": [
            {
                "year": "2011",
                "id": 231
            },
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 55
            },
            {
                "year": "2009",
                "id": 248
            },
            {
                "year": "2007",
                "id": 36
            }
        ]
    },
    {
        "title": "Image Classification using Random Forests and Ferns",
        "authors": [
            "Anna Bosch",
            "Andrew Zisserman",
            "Xavier Munoz"
        ],
        "abstract": "We explore the problem of classifying images by the object categories they contain in the case of a large number of object categories. To this end we combine three ingredients: (i) shape and appearance representations that support spatial pyramid matching over a region of interest. This generalizes the representation of Lazebnik et al., (2006) from an image to a region of interest (ROI), and from appearance (visual words) alone to appearance and local shape (edge distributions); (ii) automatic selection of the regions of interest in training. This provides a method of inhibiting background clutter and adding invariance to the object instance 's position; and (iii) the use of random forests (and random ferns) as a multi-way classifier. The advantage of such classifiers (over multi-way SVM for example) is the ease of training and testing. Results are reported for classification of the Caltech-101 and Caltech-256 data sets. We compare the performance of the random forest/ferns classifier with a benchmark multi-way SVM classifier. It is shown that selecting the ROI adds about 5% to the performance and, together with the other improvements, the result is about a 10% improvement over the state of the art for Caltech-256.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409066",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 316,
            "other": 281,
            "total": 597
        },
        "keywords": {
            "IEEE Keywords": [
                "Image classification",
                "Shape",
                "Layout",
                "Computer vision",
                "Support vector machines",
                "Support vector machine classification",
                "Image representation",
                "Benchmark testing",
                "Turning"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image classification",
                "object categories",
                "spatial pyramid matching",
                "region of interest",
                "visual words",
                "edge distributions",
                "automatic selection",
                "background clutter",
                "random forest classifier",
                "random fern classifier"
            ]
        },
        "id": 226,
        "cited_by": [
            {
                "year": "2017",
                "id": 232
            },
            {
                "year": "2017",
                "id": 569
            },
            {
                "year": "2015",
                "id": 163
            },
            {
                "year": "2015",
                "id": 199
            },
            {
                "year": "2015",
                "id": 458
            },
            {
                "year": "2013",
                "id": 204
            },
            {
                "year": "2013",
                "id": 258
            },
            {
                "year": "2013",
                "id": 326
            },
            {
                "year": "2013",
                "id": 357
            },
            {
                "year": "2013",
                "id": 392
            },
            {
                "year": "2011",
                "id": 184
            },
            {
                "year": "2011",
                "id": 188
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2009",
                "id": 55
            },
            {
                "year": "2009",
                "id": 64
            },
            {
                "year": "2009",
                "id": 80
            },
            {
                "year": "2009",
                "id": 81
            }
        ]
    },
    {
        "title": "Multilinear Projection for Appearance-Based Recognition in the Tensor Framework",
        "authors": [
            "M. Alex O. Vasilescu",
            "Demetri Terzopoulos"
        ],
        "abstract": "Numerical multilinear (tensor) algebra is a principled mathematical approach to disentangling and explicitly and parsimoniously representing the essential factors or modes of image formation, among them illumination, scene geometry, and imaging, thereby dramatically improving the performance of appearance-based recognition. Generalizing concepts from linear (matrix) algebra, we define the identity tensor and the pseudo-inverse tensor and we employ them to develop a multilinear projection algorithm, which is natural for performing recognition in the tensor algebraic framework. Our multilinear projection algorithm simultaneously projects an unlabeled test image into multiple constituent mode spaces spanned by learned, mode-specific basis sets in order to infer its mode labels. Multilinear projection is applied to unconstrained facial image recognition, where the mode labels are person identity, viewpoint, illumination, etc.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409067",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 6,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Image recognition",
                "Lighting",
                "Principal component analysis",
                "Algebra",
                "Image analysis",
                "Projection algorithms",
                "Face recognition",
                "Layout",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "matrix algebra",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "numerical multilinear algebra",
                "image formation",
                "appearance-based image recognition",
                "pseudo-inverse tensor",
                "unconstrained facial image recognition"
            ]
        },
        "id": 227,
        "cited_by": []
    },
    {
        "title": "Eyeblink-based Anti-Spoofing in Face Recognition from a Generic Webcamera",
        "authors": [
            "Gang Pan",
            "Lin Sun",
            "Zhaohui Wu",
            "Shihong Lao"
        ],
        "abstract": "We present a real-time liveness detection approach against photograph spoofing in face recognition, by recognizing spontaneous eyeblinks, which is a non-intrusive manner. The approach requires no extra hardware except for a generic webcamera. Eyeblink sequences often have a complex underlying structure. We formulate blink detection as inference in an undirected conditional graphical framework, and are able to learn a compact and efficient observation and transition potentials from data. For purpose of quick and accurate recognition of the blink behavior, eye closity, an easily-computed discriminative measure derived from the adaptive boosting algorithm, is developed, and then smoothly embedded into the conditional model. An extensive set of experiments are presented to show effectiveness of our approach and how it outperforms the cascaded Adaboost and HMM in task of eyeblink detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409068",
        "reference_list": [],
        "citation": {
            "ieee": 109,
            "other": 69,
            "total": 178
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Face detection",
                "Humans",
                "Fingerprint recognition",
                "Hardware",
                "Cameras",
                "Mouth",
                "Head",
                "Hidden Markov models",
                "Authentication"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "hidden Markov models",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "eyeblink-based antispoofing",
                "face recognition",
                "generic Webcamera",
                "real-time liveness detection approach",
                "blink detection",
                "undirected conditional graphical framework",
                "adaptive boosting algorithm",
                "cascaded Adaboost"
            ]
        },
        "id": 228,
        "cited_by": []
    },
    {
        "title": "A Study of Face Recognition as People Age",
        "authors": [
            "Haibin Ling",
            "Stefano Soatto",
            "Narayanan Ramanathan",
            "David W. Jacobs"
        ],
        "abstract": "In this paper we study face recognition across ages within a real passport photo verification task. First, we propose using the gradient orientation pyramid for this task. Discarding the gradient magnitude and utilizing hierarchical techniques, we found that the new descriptor yields a robust and discriminative representation. With the proposed descriptor, we model face verification as a two-class problem and use a support vector machine as a classifier. The approach is applied to two passport data sets containing more than 1,800 image pairs from each person with large age differences. Although simple, our approach outperforms previously tested Bayesian technique and other descriptors, including the intensity difference and gradient with magnitude. In addition, it works as well as two commercial systems. Second, for the first time, we empirically study how age differences affect recognition performance. Our experiments show that, although the aging process adds difficulty to the recognition task, it does not surpass illumination or expression as a confounding factor.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409069",
        "reference_list": [
            {
                "year": "2005",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 57,
            "other": 30,
            "total": 87
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Aging",
                "Robustness",
                "Support vector machines",
                "Testing",
                "Computer science",
                "Educational institutions",
                "Bayesian methods",
                "Lighting",
                "Jacobian matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "passport photo verification task",
                "gradient orientation pyramid",
                "gradient magnitude",
                "hierarchical techniques",
                "discriminative representation",
                "face verification",
                "support vector machine",
                "Bayesian technique",
                "intensity difference"
            ]
        },
        "id": 229,
        "cited_by": [
            {
                "year": "2009",
                "id": 46
            }
        ]
    },
    {
        "title": "A General Discriminant Model for Color Face Recognition",
        "authors": [
            "Jian Yang",
            "Chengjun Liu"
        ],
        "abstract": "This paper presents a general discriminant model (GDM) for color face recognition. The GDM model involves two sets of variables: a set of color component combination coefficients for color image representation and a set of projection basis vectors for image discrimination. An iterative whitening-maximization (IWM) algorithm is designed to find the optimal solution of the model. The proposed algorithm is further extended to generate three color components (like the three color components of RGB color images) for further improving the face recognition performance. Experiments using the face recognition grand challenge (FRGC) database and the biometric experimentation environment (BEE) system show the effectiveness of the proposed model and algorithm. In particular, for the most challenging FRGC version 2 Experiment 4, which contains 12,776 training images, 16,028 controlled target images, and 8,014 uncontrolled query images, the proposed method achieves the face verification rate (ROC III) of 74.91% at the false accept rate of 0.1%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409070",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 6,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Color",
                "Image recognition",
                "Iterative algorithms",
                "Image databases",
                "Face detection",
                "Gray-scale",
                "Computer science",
                "Algorithm design and analysis",
                "Biometrics"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image colour analysis",
                "image representation",
                "iterative methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "general discriminant model",
                "color face recognition",
                "color image representation",
                "projection basis vectors",
                "image discrimination",
                "iterative whitening-maximization",
                "face recognition grand challenge",
                "biometric experimentation environment system"
            ]
        },
        "id": 230,
        "cited_by": []
    },
    {
        "title": "Mixture-of-Parts Pictorial Structures for Objects with Variable Part Sets",
        "authors": [
            "Robin Hess",
            "Alan Fern",
            "Eric Mortensen"
        ],
        "abstract": "For many multi-part object classes, the set of parts can vary not only in location but also in type. For example, player formations in American football involve various subsets of player types, and the spatial constraints among players depend largely upon which subset of player types constitutes the formation. In this work, we study the problem of localizing and classifying the parts of such objects. Pictorial structures provide an efficient and robust mechanism for localizing object parts. Unfortunately, these models assume that each object instance involves the same set of parts, making it difficult to apply them directly in our setting. With this motivation, we introduce the mixture-of-parts pictorial structure (MoPPS) model, which is characterized by three components: a set of available parts, a set of constraints that specify legal part subsets, and a function that returns a pictorial structure for any legal part subset. MoPPS inference corresponds to jointly computing the most likely subset of parts and their positions. We propose a restricted, but useful, representation for MoPPS models that facilitates inference via branch-and-bound optimization, which we show is efficient in practice. Experiments in the challenging domain of American football show the effectiveness of the model and inference procedure.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409071",
        "reference_list": [
            {
                "year": "2003",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Law",
                "Legal factors",
                "Humans",
                "Robustness",
                "Object recognition",
                "Deformable models",
                "Costs",
                "Computer science",
                "Arm",
                "Leg"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mixture-of-parts pictorial structures",
                "variable part sets",
                "multipart object classes",
                "player formations",
                "American football",
                "branch-and-bound optimization"
            ]
        },
        "id": 231,
        "cited_by": []
    },
    {
        "title": "Interactive Search for Image Categories by Mental Matching",
        "authors": [
            "Marin Ferecatu",
            "Donald Geman"
        ],
        "abstract": "Traditional image retrieval methods require a \"query image\" to initiate a search for members of an image category. However, when the image database is unstructured, and when the category is semantic and resides only in the mind of the user, there is no obvious way to begin (the \"page zero \" problem). We propose a new mathematical framework for relevance feedback based on mental matching and starting from a random sample of images. At each iteration the user declares which of several displayed images is closest to his category; performance is measured by the number of iterations necessary to display an instance. Our core contribution is a Bayesian formulation which scales to large databases with no semantic annotation. The two key components are a response model which accounts for the user's subjective perception of similarity and a display algorithm which seeks to maximize the flow of information. Experiments with real users and a database with 20,000 images demonstrate the efficiency of the search process.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409072",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Image databases",
                "Feedback",
                "Image retrieval",
                "Displays",
                "Visual databases",
                "Bayesian methods",
                "Content based retrieval",
                "Spatial databases",
                "Mathematics",
                "Statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image matching",
                "image retrieval",
                "image sampling",
                "iterative methods",
                "relevance feedback",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image retrieval",
                "image category",
                "image database",
                "mathematical framework",
                "relevance feedback",
                "mental matching",
                "image sampling",
                "iterative method",
                "Bayesian formulation",
                "interactive search"
            ]
        },
        "id": 232,
        "cited_by": [
            {
                "year": "2017",
                "id": 153
            },
            {
                "year": "2013",
                "id": 37
            },
            {
                "year": "2013",
                "id": 92
            }
        ]
    },
    {
        "title": "Human Pose Estimation using Motion Exemplars",
        "authors": [
            "Alireza Fathi",
            "Greg Mori"
        ],
        "abstract": "We present a motion exemplar approach for finding body configuration in monocular videos. A motion correlation technique is employed to measure the motion similarity at various space-time locations between the input video and stored video templates. These observations are used to predict the conditional state, distributions of exemplars and joint positions. Exemplar sequence selection and joint position estimation are then solved with approximate inference using Gibbs sampling and gradient ascent. The presented approach is able to find joint positions accurately for people with textured clothing. Results are presented on a dataset containing slow, fast and incline walk videos of various people from different view angles. The results demonstrate an overall improvement compared to previous methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409073",
        "reference_list": [
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 11,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Motion estimation",
                "Clothing",
                "Kinematics",
                "State estimation",
                "Birth disorders",
                "Board of Directors",
                "Motion detection",
                "Tracking",
                "Image sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sampling",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human pose estimation",
                "motion exemplars",
                "monocular videos",
                "body configuration",
                "motion correlation technique",
                "motion similarity",
                "space-time locations",
                "video templates",
                "exemplar sequence selection",
                "joint position estimation",
                "Gibbs sampling",
                "gradient ascent",
                "incline walk videos"
            ]
        },
        "id": 233,
        "cited_by": []
    },
    {
        "title": "Correspondence Transfer for the Registration of Multimodal Images",
        "authors": [
            "Zhao Yi",
            "Stefano Soatto"
        ],
        "abstract": "Gene expression data provide information on the location where certain genes are active; in order for this to be useful, such a location must be registered to an anatomical atlas. Because gene expression maps are considerably different from each other - they display the expression of different genes - and from the anatomical atlas, this problem is currently addressed either manually by trained experts, or by neglecting all image information and only using the pre-segmented boundaries. In this manuscript we concentrate on data discrepancy measures that take into account image information when this is present in both the target and template images. We exploit such \"bi-lateral\" structures to drive the correspondence process in regions where the intensity information is inconsistent, analogously to a \"motion in- painting\" task. Although no ground truth can be established, and prior information clearly plays a key role, we show that our model achieves desirable results on subjective tests validated by expert subjects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409074",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Gene expression",
                "Layout",
                "Imaging phantoms",
                "Solid modeling",
                "Computer science",
                "Displays",
                "Testing",
                "Mutual information",
                "Biomedical imaging",
                "Anatomical structure"
            ],
            "INSPEC: Controlled Indexing": [
                "genetics",
                "image motion analysis",
                "image registration",
                "image segmentation",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "correspondence transfer",
                "multimodal image registration",
                "gene expression data",
                "presegmented boundaries",
                "image information",
                "image templates",
                "motion in-painting task"
            ]
        },
        "id": 234,
        "cited_by": [
            {
                "year": "2011",
                "id": 323
            }
        ]
    },
    {
        "title": "Fast Crowd Segmentation Using Shape Indexing",
        "authors": [
            "Lan Dong",
            "Vasu Parameswaran",
            "Visvanathan Ramesh",
            "Imad Zoghlami"
        ],
        "abstract": "This paper presents a fast, accurate, and novel method for the problem of estimating the number of humans and their positions from background differenced images obtained from a single camera where inter-human occlusion is significant. The problem is challenging firstly because the state space formed by the number, positions, and articulations of people is large. Secondly, in spite of many advances in background maintenance and change detection, background differencing remains a noisy and imprecise process, and its output is far from ideal: holes, fill-ins, irregular boundaries etc. pose additional challenges for our \"mid- level\" problem of segmenting it to localize humans. We propose a novel example-based algorithm which maps the global shape feature by Fourier descriptors to various configurations of humans directly. We use locally weighted averaging to interpolate for the best possible candidate configuration. The inherent ambiguity resulting from the lack of depth and layer information in the background difference images is mitigated by the use of dynamic programming, which finds the trajectory in state space that best explains the evolution of the projected shapes. The key components of our solution are simple and fast. We demonstrate the accuracy and speed of our approach on real image sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409075",
        "reference_list": [
            {
                "year": "2001",
                "id": 122
            },
            {
                "year": "2001",
                "id": 108
            },
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 23,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Indexing",
                "Humans",
                "State-space methods",
                "Image segmentation",
                "Cameras",
                "Background noise",
                "Noise shaping",
                "Noise level",
                "Change detection algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "Fourier analysis",
                "hidden feature removal",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast crowd segmentation",
                "shape indexing",
                "background differenced images",
                "interhuman occlusion",
                "background maintenance",
                "change detection",
                "Fourier descriptors",
                "dynamic programming",
                "image sequences"
            ]
        },
        "id": 235,
        "cited_by": [
            {
                "year": "2009",
                "id": 188
            }
        ]
    },
    {
        "title": "Rock, Paper, and Scissors: extrinsic vs. intrinsic similarity of non-rigid shapes",
        "authors": [
            "Alexander M. Bronstein",
            "Michael M. Bronstein",
            "Ron Kimmel"
        ],
        "abstract": "This paper explores similarity criteria between non-rigid shapes. Broadly speaking, such criteria are divided into intrinsic and extrinsic, the first referring to the metric structure of the objects and the latter to the geometry of the shapes in the Euclidean space. Both criteria have their advantages and disadvantages; extrinsic similarity is sensitive to non-rigid deformations of the shapes, while intrinsic similarity is sensitive to topological noise. Here, we present an approach unifying both criteria in a single distance. Numerical results demonstrate the robustness of our approach in cases where using only extrinsic or intrinsic criteria fail.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409076",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 9,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Fingers",
                "Pattern recognition",
                "Iterative closest point algorithm",
                "Space technology",
                "Geometry",
                "Computer science",
                "Noise shaping",
                "Noise robustness",
                "Terminology"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "intrinsic similarity",
                "extrinsic similarity",
                "nonrigid shapes",
                "objects metric structure",
                "Euclidean space"
            ]
        },
        "id": 236,
        "cited_by": []
    },
    {
        "title": "The 3D-3D Registration Problem Revisited",
        "authors": [
            "Hongdong Li",
            "Richard Hartley"
        ],
        "abstract": "We describe a new framework for globally solving the 3D-3D registration problem with unknown point correspondences. This problem is significant as it is frequently encountered in many applications. Existing methods are not fully satisfactory, mainly due to the risk of local minima. Our framework is grounded on the Lipschitz global optimization theory. It achieves a guaranteed global optimality without any initialization. By exploiting the special structure of the problem itself and of the 3D rotation space SO(3), we propose a box-and-ball algorithm, which solves the problem efficiently. The main idea of the work can be applied to many other problems as well.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409077",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 18,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Iterative algorithms",
                "Iterative closest point algorithm",
                "Australia",
                "Principal component analysis",
                "Computer vision",
                "Application software",
                "Object recognition",
                "Graphics",
                "Biomedical imaging",
                "Medical robotics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image registration",
                "octrees",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D-3D registration problem",
                "local minima risk",
                "Lipschitz global optimization theory",
                "3D rotation space",
                "octree box-and-ball algorithm",
                "computer vision"
            ]
        },
        "id": 237,
        "cited_by": [
            {
                "year": "2017",
                "id": 0
            },
            {
                "year": "2015",
                "id": 479
            },
            {
                "year": "2013",
                "id": 181
            },
            {
                "year": "2009",
                "id": 137
            },
            {
                "year": "2009",
                "id": 165
            },
            {
                "year": "2009",
                "id": 166
            }
        ]
    },
    {
        "title": "Non-Rigid Image Registration using a Hierarchical Partition of Unity Finite Element Method",
        "authors": [
            "Sherif Makram-Ebeid",
            "Oudom Somphone"
        ],
        "abstract": "We use a hierarchical partition of unity finite element method (H-PUFEM) to represent and analyse the non-rigid deformation fields involved in multidimensional image registration. We make use of the Ritz-Galerkin direct variational method to solve non-rigid image registration problems with various deformation constraints. In this method, we directly seek a set of parameters that minimizes the objective function. We thereby avoid the loss of information that may occur when an Euler-Lagrange formulation is used. Experiments are conducted to demonstrate the advantages of our approach when registering synthetic images having little of or no localizing features. As a special case, conformal mapping problems can be accurately solved in this manner. We also illustrate our approach with an application to cardiac magnetic resonance temporal sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409078",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Image registration",
                "Finite element methods",
                "Biomedical imaging",
                "Capacitive sensors",
                "Finite difference methods",
                "Image analysis",
                "Multidimensional systems",
                "Conformal mapping",
                "Magnetic resonance",
                "Magnetic field measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "finite element analysis",
                "Galerkin method",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonrigid image registration",
                "hierarchical partition",
                "unity finite element method",
                "nonrigid deformation fields",
                "multidimensional image registration",
                "Ritz-Galerkin direct variational method",
                "objective function",
                "Euler-Lagrange formulation",
                "synthetic images",
                "cardiac magnetic resonance temporal sequences"
            ]
        },
        "id": 238,
        "cited_by": []
    },
    {
        "title": "Optimizing Image Registration by Mutually Exclusive Scale Components",
        "authors": [
            "Terrence Chen",
            "Thomas S. Huang"
        ],
        "abstract": "Local optimum has been one of the most difficult problems in image registration notwithstanding the extensive research effort that has been put into solving it. Local optimums occur when a portion of patterns in the floating image coincide with a portion of patterns in the reference image even though the two images are not entirely matched. Existing hierarchical or multi-scale methods suffer from this problem mainly because some redundant information that causes local optimums appears in multiple scales. We propose to avoid it by decomposing an image into several mutually exclusive scale components so that minimal redundant information is present. Our method is evaluated and compared with existing methods using high resolution satellite imagery where thousands of local optimum traps are hidden. We show that our method has significant improvement over existing solutions in both robustness and efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409079",
        "reference_list": [
            {
                "year": "2001",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Image registration",
                "Automatic logic units",
                "Machine vision",
                "Biomedical imaging",
                "Focusing",
                "Laplace equations",
                "Tires"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image registration optimization",
                "mutually exclusive multiscale component",
                "image decomposition",
                "high resolution satellite imagery"
            ]
        },
        "id": 239,
        "cited_by": []
    },
    {
        "title": "Gradient Intensity-Based Registration of Multi-Modal Images of the Brain",
        "authors": [
            "Ramtin Shams",
            "Rodney A. Kennedy",
            "Parastoo Sadeghi",
            "Richard Hartley"
        ],
        "abstract": "We present a fast and accurate framework for registration of multi-modal volumetric images based on decoupled estimation of registration parameters utilizing spatial information in the form of 'gradient intensity'. We introduce gradient intensity as a measure of spatial strength of an image in a given direction and show that it can be used to determine the rotational misalignment independent of translation between the images. The rotation parameters are obtained by maximizing the mutual information of 2D gradient intensity matrices obtained from 3D images, hence reducing the dimensionality of the problem and improving efficiency. The rotation parameters along with estimations of translation are then used to initialize an optimization step over a conventional pixel intensity-based method to achieve sub-voxel accuracy. Our optimization algorithm converges quickly and is less subject to the common problem of misregistration due to local extrema. Experiments show that our method significantly improves the robustness, performance and efficiency of registration compared to conventional pixel intensity-based methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409080",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 0,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Cost function",
                "Optimization methods",
                "Australia",
                "Parameter estimation",
                "Mutual information",
                "Robustness",
                "Shape measurement",
                "Land use planning",
                "Image resolution",
                "Rotation measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "brain",
                "gradient methods",
                "image registration",
                "optimisation",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gradient intensity-based registration",
                "multimodal images",
                "brain",
                "decoupled estimation",
                "registration parameters",
                "rotational misalignment",
                "2D gradient intensity matrices",
                "optimization algorithm",
                "pixel intensity-based methods"
            ]
        },
        "id": 240,
        "cited_by": []
    },
    {
        "title": "3-D Metric Reconstruction and Registration of Images of Near-planar Surfaces",
        "authors": [
            "Tae Eun Choe",
            "Gerard Medioni"
        ],
        "abstract": "In this study, we address the problem of 3-D dense metric reconstruction and registration from multiple images, given that the observed surface is nearly planar. This is difficult, as classical methods work well only if the scene is truly planar (mosaicing) or the scene has certain significant depth variations (classical Structure-from- Motion (SfM)). One domain in which this problem occurs is image analysis of the retinal fundus. Our approach is to first assume planarity, and perform 2-D global registration. A first bundle adjustment is applied to find the camera positions in metric space. We then select two images and compute the epipolar geometry between them using plane+parallax approach. These images are matched to generate a dense disparity map using mutual information. A second bundle adjustment is applied to transform the disparity map into a dense metric depth map, fixing the 2 camera positions. A third bundle adjustment is performed to refine both camera positions and a 3-D structure. All images are back-projected to the 3-D structure for the final registration. The entire process is fully automatic. In addition, a clear definition of \"near-planarity \" is provided. 3-D reconstruction is shown visually. The method is general, and can be applied to other domains, as shown in the experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409081",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            },
            {
                "year": "2005",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Surface reconstruction",
                "Cameras",
                "Layout",
                "Image motion analysis",
                "Retina",
                "Extraterrestrial measurements",
                "Computational geometry",
                "Mutual information",
                "Three dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image reconstruction",
                "image registration",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D metric reconstruction",
                "images registration",
                "near-planar surfaces",
                "mosaicing",
                "structure-from-motion",
                "image analysis",
                "retinal fundus",
                "bundle adjustment",
                "epipolar geometry",
                "dense disparity map",
                "dense metric depth map"
            ]
        },
        "id": 241,
        "cited_by": []
    },
    {
        "title": "What Can Casual Walkers Tell Us About A 3D Scene?",
        "authors": [
            "Diego Rother",
            "Kedar A. Patwardhan",
            "Guillermo Sapiro"
        ],
        "abstract": "An approach for incremental learning of a 3D scene from a single static video camera is presented in this paper. In particular, we exploit the presence of casual people walking in the scene to infer relative depth, learn shadows, and segment the critical ground structure. Considering that this type of video data is so ubiquitous, this work provides an important step towards 3D scene analysis from single cameras in readily available ordinary videos and movies. On-line 3D scene learning, as presented here, is very important for applications such as scene analysis, foreground refinement, tracking, biometrics, automated camera collaboration, activity analysis, identification, and real-time computer-graphics applications. The main contributions of this work are then two-fold. First, we use the people in the scene to continuously learn and update the 3D scene parameters using an incremental robust (L 1 ) error minimization. Secondly, models of shadows in the scene are learned using a statistical framework. A symbiotic relationship between the shadow model and the estimated scene geometry is exploited towards incremental mutual improvement. We illustrate the effectiveness of the proposed framework with applications in foreground refinement, automatic segmentation as well as relative depth mapping of the floor/ground, and estimation of 3D trajectories of people in the scene.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409082",
        "reference_list": [
            {
                "year": "2005",
                "id": 242
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 1,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Image analysis",
                "Application software",
                "Legged locomotion",
                "Motion pictures",
                "Biometrics",
                "Collaborative work",
                "Computer applications",
                "Pervasive computing"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "learning (artificial intelligence)",
                "minimisation",
                "statistical analysis",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "incremental learning",
                "static video camera",
                "casual walking people",
                "online 3D scene learning",
                "error minimization",
                "statistical framework",
                "shadow model",
                "scene geometry",
                "3D information extraction"
            ]
        },
        "id": 242,
        "cited_by": []
    },
    {
        "title": "Geometric Integrability and Consistency of 3D Point Clouds",
        "authors": [
            "George Kamberov",
            "Gerda Kamberova"
        ],
        "abstract": "Numerous applications processing 3D point data will gain from the ability to estimate reliably normals and differential geometric properties. Normal estimates are notoriously noisy, the errors propagate and may lead to flawed, inaccurate, and inconsistent curvature estimates. Frankot-Chellappa introduced the use of integrability constraints in normal estimation. Their approach deals with graphs z = f(x,y)- We present a newly discovered general orientability constraint (GOC) for 3D point clouds sampled from general surfaces, not just graphs. It provides a tool to quantify the confidence in the estimation of normals, topology, and geometry from a point cloud. Furthermore, similarly to the Frankot-Chellappa constraint, the GOC can be used directly to extract the topology and the geometry of the manifolds underlying 3D point clouds. As an illustration we describe an automatic Cloud-to-Geometry pipeline which exploits the GOC.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409083",
        "reference_list": [
            {
                "year": "2001",
                "id": 164
            },
            {
                "year": "2005",
                "id": 165
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Clouds",
                "Surface treatment",
                "Surface reconstruction",
                "Gaussian processes",
                "Pipelines",
                "Topology",
                "Computational geometry",
                "Data mining",
                "Polynomials",
                "Level set"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric integrability",
                "3D point clouds",
                "3D point data",
                "differential geometric properties",
                "general orientability constraint",
                "automatic cloud-to-geometry pipeline"
            ]
        },
        "id": 243,
        "cited_by": []
    },
    {
        "title": "Scale-Dependent 3D Geometric Features",
        "authors": [
            "John Novatnack",
            "Ko Nishino"
        ],
        "abstract": "Three-dimensional geometric data play fundamental roles in many computer vision applications. However, their scale-dependent nature, i.e. the relative variation in the spatial extents of local geometric structures, is often overlooked. In this paper we present a comprehensive framework for exploiting this 3D geometric scale variability. Specifically, we focus on detecting scale-dependent geometric features on triangular mesh models of arbitrary topology. The key idea of our approach is to analyze the geometric scale variability of a given 3D model in the scale-space of a dense and regular 2D representation of its surface geometry encoded by the surface normals. We derive novel corner and edge detectors, as well as an automatic scale selection method, that acts upon this representation to detect salient geometric features and determine their intrinsic scales. We evaluate the effectiveness and robustness of our method on a number of models of different topology. The results show that the resulting scale-dependent geometric feature set provides a reliable basis for constructing a rich but concise representation of the geometric structure at hand.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409084",
        "reference_list": [],
        "citation": {
            "ieee": 38,
            "other": 22,
            "total": 60
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Computer vision",
                "Geometry",
                "Topology",
                "Kernel",
                "Image edge detection",
                "Detectors",
                "Robustness",
                "Application software",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "edge detection",
                "feature extraction",
                "image representation",
                "mesh generation",
                "solid modelling",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale-dependent 3D geometric feature detection",
                "computer vision application",
                "triangular mesh model",
                "arbitrary topology",
                "2D representation",
                "edge detector",
                "automatic scale selection method"
            ]
        },
        "id": 244,
        "cited_by": [
            {
                "year": "2015",
                "id": 18
            },
            {
                "year": "2015",
                "id": 382
            },
            {
                "year": "2013",
                "id": 448
            }
        ]
    },
    {
        "title": "Out-of-Core Bundle Adjustment for Large-Scale 3D Reconstruction",
        "authors": [
            "Kai Ni",
            "Drew Steedly",
            "Frank Dellaert"
        ],
        "abstract": "Large-scale 3D reconstruction has recently received much attention from the computer vision community. Bundle adjustment is a key component of 3D reconstruction problems. However, traditional bundle adjustment algorithms require a considerable amount of memory and computational resources. In this paper, we present an extremely efficient, inherently out-of-core bundle adjustment algorithm. We decouple the original problem into several submaps that have their own local coordinate systems and can be optimized in parallel. A key contribution to our algorithm is making as much progress towards optimizing the global non-linear cost function as possible using the fragments of the reconstruction that are currently in core memory. This allows us to converge with very few global sweeps (often only two) through the entire reconstruction. We present experimental results on large-scale 3D reconstruction datasets, both synthetic and real.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409085",
        "reference_list": [],
        "citation": {
            "ieee": 45,
            "other": 27,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Large-scale systems",
                "Image reconstruction",
                "Layout",
                "Cameras",
                "Computer vision",
                "Cost function",
                "Earth",
                "Educational institutions",
                "Concurrent computing",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D reconstruction",
                "computer vision",
                "out-of-core bundle adjustment",
                "submaps",
                "nonlinear cost function",
                "image reconstruction"
            ]
        },
        "id": 245,
        "cited_by": [
            {
                "year": "2017",
                "id": 3
            },
            {
                "year": "2015",
                "id": 232
            },
            {
                "year": "2011",
                "id": 37
            }
        ]
    },
    {
        "title": "On the Differential Geometry of 3D Flow Patterns: Generalized Helicoids and Diffusion MRI Analysis",
        "authors": [
            "Peter Savadjiev",
            "Steven W. Zucker",
            "Kaleem Siddiqi"
        ],
        "abstract": "Configurations of dense locally parallel 3D curves occur in medical imaging, computer vision and graphics. Examples include white matter fibre tracts, textures, fur and hair. We develop a differential geometric characterization of such structures by considering the local behaviour of the associated 3D frame field, leading to the associated tangential, normal and bi-normal curvature functions. Using results from the theory of generalized minimal surfaces we adopt a generalized helicoid model as an osculating object and develop the connection between its parameters and these curvature functions. These developments allow for the construction of parametrized 3D vector fields (sampled osculating objects) to locally approximate these patterns. We apply these results to the analysis of diffusion MRI data via a type of 3D streamline flow. Experimental results on data from a human brain demonstrate the advantages of incorporating the full differential geometry.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409086",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 8,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Magnetic resonance imaging",
                "Pattern analysis",
                "Tensile stress",
                "Computer science",
                "Hair",
                "Humans",
                "Computational geometry",
                "Biomedical imaging",
                "Computer vision",
                "Computer graphics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "computer vision",
                "differential geometry",
                "magnetic resonance imaging",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "differential geometry",
                "flow patterns",
                "generalized Helicoids",
                "diffusion MRI analysis",
                "computer vision",
                "medical imaging",
                "computer graphics",
                "streamline flow",
                "bi-normal curvature functions"
            ]
        },
        "id": 246,
        "cited_by": []
    },
    {
        "title": "Efficient optimization for L\u221e-problems using pseudoconvexity",
        "authors": [
            "Carl Olsson",
            "Anders P. Eriksson",
            "Fredrik Kahl"
        ],
        "abstract": "In this paper we consider the problem of solving geometric reconstruction problems with the L \u221e -norm. Previous work has shown that globally optimal solutions can be computed reliably for a series of such problems. The methods for computing the solutions have relied on the property of quasiconvexity. For quasiconvex problems, checking if there exists a solution below a certain objective value can be posed as a convex feasibility problem. To solve the L \u221e -problem one typically employs a bisection algorithm, generating a sequence of convex problems. In this paper we present more efficient ways of computing the solutions. We derive necessary and sufficient conditions for a global optimum. A key property is that of pseudoconvexity, which is a stronger condition than quasiconvexity. The results open up the possibility of using local optimization methods for more efficient computations. We present two such algorithms. The first one is an interior point method that uses the KKT conditions and the second one is similar to the bisection method in the sense it solves a sequence of SOCP problems. Results are presented and compared to the standard bisection algorithm on real data for various problems and scenarios with improved performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409087",
        "reference_list": [
            {
                "year": "2005",
                "id": 128
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 10,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Sufficient conditions",
                "Optimization methods",
                "Motion estimation",
                "Motion detection",
                "Uncertainty",
                "Computer vision",
                "Computer errors",
                "Closed-form solution",
                "Iterative methods"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image reconstruction",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pseudoconvexity",
                "geometric reconstruction problems",
                "convex feasibility problem",
                "local optimization methods",
                "standard bisection algorithm",
                "L\u221e-problem"
            ]
        },
        "id": 247,
        "cited_by": [
            {
                "year": "2017",
                "id": 95
            },
            {
                "year": "2015",
                "id": 88
            },
            {
                "year": "2013",
                "id": 59
            }
        ]
    },
    {
        "title": "Illumination and Affine- Invariant Point Matching using an Ordinal Approach",
        "authors": [
            "Raj Gupta",
            "Anurag Mittal"
        ],
        "abstract": "We present an approach for illumination and affine-invariant point matching using ordinal features. Ordinal measures for matching only consider the order between pixels and not the absolute intensity values which enables them to be invariant to a monotonic change in intensities. The utility of such measures has been demonstrated in the past for point matching for some applications such as background subtraction and stereo matching. However, invariance of such methods to geometric transformations has been limited leading to their inapplicability for more generic matching applications such as object recognition, wide-baseline stereo matching, mosaicing or tracking points on moving objects. In this paper, we extend such methods for use in such applications. The method is invariant to an affine transformation in the patch, which makes it applicable to a variety of applications. At the same time, our method is robust to different types of noise processes possible in a real scene. Experiments indicate favorable performance when compared to other state-of-the-art methods for affine-invariant point matching.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409088",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Stereo vision",
                "Noise robustness",
                "Cameras",
                "Object recognition",
                "Application software",
                "Image storage",
                "Image recognition",
                "Layout",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "affine transforms",
                "feature extraction",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "illumination",
                "affine-invariant point matching",
                "ordinal features",
                "affine transformation",
                "real scene"
            ]
        },
        "id": 248,
        "cited_by": []
    },
    {
        "title": "Detecting Illumination in Images",
        "authors": [
            "Graham Finlayson",
            "Cl\u00e9ment Fredembach",
            "Mark S. Drew"
        ],
        "abstract": "In this paper we present a surprisingly simple yet powerful method for detecting illumination\u2014determining which pixels are lit by different lights\u2014in images. Our method is based on the chromagenic camera, which takes two pictures of each scene: one is captured as normal and the other through a coloured filter. Previous research has shown that the relationship between the colours, the RGBs, in the filtered and unfiltered images depends strongly on the colour of the light and this can be used to estimate the colour of the illuminant. While chromagenic illuminant estimation often works well it can and does fail and so is not itself a direct solution to the illuminant detection problem. In this paper we dispense with the goal of illumination estimation and seek only to use the chromagenic effect to find out which parts of a scene are illuminated by the same lights. The simplest implementation of our idea involves a combinatorial search. We precompute a dictionary of possible illuminant relations\u2014that might map RGBs to filtered counterparts\u2014from which we select a small number \ud835\udc5a corresponding to the number of distinct lights we think might be present. Each pixel, or region, is assigned the relation from this \ud835\udc5a-set that best maps filtered to unfiltered RGB. All \ud835\udc5a-sets are tried in turn and the one that has the minimum prediction error over all is found. At the end of this search process each pixel or region is assigned an integer between 1 and \ud835\udc5a indicating which of the m lights are thought to have illuminated the region. Our simple search algorithm is possible when \ud835\udc5a = 2 (and \ud835\udc5a = 3) and for this case we present experiments that show our method does a remarkable job in detecting illumination in images: if the 2 lights are shadow and non- shadow, we find the shadows almost effortlessly. Compared to ground truth data, our method delivers close to optimal performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409089",
        "reference_list": [
            {
                "year": "2005",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 10,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Layout",
                "Cameras",
                "Filters",
                "Detection algorithms",
                "Pixel",
                "Dictionaries",
                "Computer vision",
                "Image processing",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "filtering theory",
                "image colour analysis",
                "lighting",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "chromagenic camera",
                "coloured filter",
                "chromagenic illuminant estimation",
                "combinatorial search",
                "image detection",
                "illuminant detection problem"
            ]
        },
        "id": 249,
        "cited_by": []
    },
    {
        "title": "Multispectral Imaging Using Multiplexed Illumination",
        "authors": [
            "Jong-Il Park",
            "Moon-Hyun Lee",
            "Michael D. Grossberg",
            "Shree K. Nayar"
        ],
        "abstract": "Many vision tasks such as scene segmentation, or the recognition of materials within a scene, become considerably easier when it is possible to measure the spectral reflectance of scene surfaces. In this paper, we present an efficient and robust approach for recovering spectral reflectance in a scene that combines the advantages of using multiple spectral sources and a multispectral camera. We have implemented a system based on this approach using a cluster of light sources with different spectra to illuminate the scene and a conventional RGB camera to acquire images. Rather than sequentially activating the sources, we have developed a novel technique to determine the optimal multiplexing sequence of spectral sources so as to minimize the number of acquired images. We use our recovered spectral measurements to recover the continuous spectral reflectance for each scene point by using a linear model for spectral reflectance. Our imaging system can produce multispectral videos of scenes at 30fps. We demonstrate the effectiveness of our system through extensive evaluation. As a demonstration, we present the results of applying data recovered by our system to material segmentation and spectral relighting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409090",
        "reference_list": [
            {
                "year": "2003",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 60,
            "other": 38,
            "total": 98
        },
        "keywords": {
            "IEEE Keywords": [
                "Multispectral imaging",
                "Lighting",
                "Layout",
                "Reflectivity",
                "Cameras",
                "Pollution measurement",
                "Videos",
                "Image reconstruction",
                "Image segmentation",
                "Light sources"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "reflectivity"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multispectral imaging",
                "multiplexed illumination",
                "scene segmentation",
                "robust approach",
                "spectral reflectance",
                "spectral sources",
                "multispectral camera",
                "light sources",
                "RGB camera",
                "multispectral videos",
                "data recovery",
                "material segmentation",
                "spectral relighting"
            ]
        },
        "id": 250,
        "cited_by": [
            {
                "year": "2017",
                "id": 494
            },
            {
                "year": "2015",
                "id": 393
            },
            {
                "year": "2013",
                "id": 56
            },
            {
                "year": "2011",
                "id": 87
            },
            {
                "year": "2009",
                "id": 22
            }
        ]
    },
    {
        "title": "Parsing Images of Architectural Scenes",
        "authors": [
            "Alexander C. Berg",
            "Floraine Grabler",
            "Jitendra Malik"
        ],
        "abstract": "We address image parsing in the setting of architectural scenes. Our goal is to parse an image into regions of various types such as sky, foliage, buildings, and street. Furthermore we parse the building regions at a finer level of detail, identifying the positions of windows, doors, and rooflines, the colors of walls, and the spatial extent of particular buildings. Recognizing these individual elements is often impossible without the context provided by the initial parsing of the image, for instance a roofline is only defined in relation to the building below and the sky above. Our approach is driven by recognition of generic classes of visual appearance, e.g. for foliage. The generic recognition results boot-strap an image specific model that provides refined estimates to use for matting, segmentation, and more detailed parsing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409091",
        "reference_list": [
            {
                "year": "2005",
                "id": 232
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2003",
                "id": 151
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 11,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Buildings",
                "Windows",
                "Image recognition",
                "Image segmentation",
                "Brightness",
                "Computer vision",
                "Object recognition",
                "Writing",
                "Motorcycles"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image parsing",
                "architectural scene",
                "building region",
                "visual appearance",
                "generic recognition",
                "image segmentation"
            ]
        },
        "id": 251,
        "cited_by": [
            {
                "year": "2009",
                "id": 212
            }
        ]
    },
    {
        "title": "Depth and Appearance for Mobile Scene Analysis",
        "authors": [
            "Andreas Ess",
            "Bastian Leibe",
            "Luc Van Gool"
        ],
        "abstract": "In this paper, we address the challenging problem of simultaneous pedestrian detection and ground-plane estimation from video while walking through a busy pedestrian zone. Our proposed system integrates robust stereo depth cues, ground-plane estimation, and appearance-based object detection in a principled fashion using a graphical model. Object-object occlusions lead to complex interactions in this model that make an exact solution computationally intractable. We therefore propose a novel iterative approach that first infers scene geometry using belief propagation and then resolves interactions between objects using a global optimization procedure. This approach leads to a robust solution in few iterations, while allowing object detection to benefit from geometry estimation and vice versa. We quantitatively evaluate the performance of our proposed approach on several challenging test sequences showing strolls through busy shopping streets. Comparisons to various baseline systems show that it outperforms both a system using no scene geometry and one just relying on structure-from-motion without dense stereo.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409092",
        "reference_list": [],
        "citation": {
            "ieee": 185,
            "other": 110,
            "total": 295
        },
        "keywords": {
            "IEEE Keywords": [
                "Image analysis",
                "Geometry",
                "Robustness",
                "Object detection",
                "Layout",
                "Legged locomotion",
                "Graphical models",
                "Iterative methods",
                "Belief propagation",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "iterative methods",
                "object recognition",
                "optimisation",
                "traffic engineering computing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mobile scene analysis",
                "pedestrian detection",
                "video ground-plane estimation",
                "pedestrian zone",
                "robust stereo depth cues",
                "appearance-based object detection",
                "object-object occlusions",
                "iterative approach",
                "scene geometry",
                "belief propagation",
                "global optimization procedure",
                "geometry estimation",
                "structure-from-motion"
            ]
        },
        "id": 252,
        "cited_by": [
            {
                "year": "2013",
                "id": 15
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2013",
                "id": 261
            },
            {
                "year": "2013",
                "id": 323
            },
            {
                "year": "2011",
                "id": 108
            },
            {
                "year": "2009",
                "id": 3
            },
            {
                "year": "2009",
                "id": 307
            }
        ]
    },
    {
        "title": "Fast Pixel/Part Selection with Sparse Eigenvectors",
        "authors": [
            "Baback Moghaddam",
            "Yair Weiss",
            "Shai Avidan"
        ],
        "abstract": "We extend the \"Sparse LDA\" algorithm of [7] with new sparsity bounds on 2-class separability and efficient partitioned matrix inverse techniques leading to 1000-fold speed-ups. This mitigates the 0(n4) scaling that has limited this algorithm's applicability to vision problems and also prioritizes the less-myopic backward elimination stage by making it faster than forward selection. Experiments include \"sparse eigenfaces\" and gender classification on FERET data as well as pixel/part selection for OCR on MNIST data using Bayesian (GP) classification. Sparse- LDA is an attractive alternative to the more demanding Automatic Relevance Determination. State-of-the-art recognition is obtained while discarding the majority of pixels in all experiments. Our sparse models also show a better fit to data in terms of the \"evidence\" or marginal likelihood.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409093",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 6,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Linear discriminant analysis",
                "Principal component analysis",
                "Input variables",
                "Independent component analysis",
                "Partitioning algorithms",
                "Sparse matrices",
                "Bayesian methods",
                "Statistics",
                "Computer vision",
                "Eigenvalues and eigenfunctions"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "eigenvalues and eigenfunctions",
                "image classification",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast pixel-part selection",
                "sparse eigenvectors",
                "backward elimination stage",
                "gender classification",
                "FERET data",
                "MNIST data",
                "Bayesian classification",
                "automatic relevance determination",
                "state-of-the-art recognition"
            ]
        },
        "id": 253,
        "cited_by": []
    },
    {
        "title": "Phase Based Modelling of Dynamic Textures",
        "authors": [
            "Bernard Ghanem",
            "Narendra Ahuja"
        ],
        "abstract": "This paper presents a model of spatiotemporal variations in a dynamic texture (DT) sequence. Most recent work on DT modelling represents images in a DT sequence as the responses of a linear dynamical system (LDS) to noise. Despite its merits, this model has limitations because it attempts to model temporal variations in pixel intensities which do not take advantage of global motion coherence. We propose a model that relates texture dynamics to the variation of the Fourier phase, which captures the relationships among the motions of all pixels (i.e. global motion) within the texture, as well as the appearance of the texture. Unlike LDS, our model does not require segmentation or cropping during the training stage, which allows it to handle DT sequences containing a static background. We test the performance of this model on recognition and synthesis of DT's. Experiments with a dataset that we have compiled demonstrate that our phase based model outperforms LDS.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409094",
        "reference_list": [
            {
                "year": "2003",
                "id": 161
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 9,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatiotemporal phenomena",
                "Optical noise",
                "Fires",
                "Clouds",
                "Motion analysis",
                "Optical computing",
                "Image motion analysis",
                "Motion estimation",
                "Physics computing",
                "Coherence"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image sequences",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "phase based modelling",
                "dynamic texture sequence",
                "spatiotemporal variations",
                "dynamic texture modelling",
                "image representation",
                "linear dynamical system",
                "global motion coherence",
                "Fourier phase",
                "static background"
            ]
        },
        "id": 254,
        "cited_by": [
            {
                "year": "2015",
                "id": 8
            },
            {
                "year": "2011",
                "id": 154
            },
            {
                "year": "2009",
                "id": 249
            }
        ]
    },
    {
        "title": "Structure from Motion with Missing Data is NP-Hard",
        "authors": [
            "David Nister",
            "Fredrik Kahl",
            "Henrik Stewenius"
        ],
        "abstract": "This paper shows that structure from motion is NP-hard for most sensible cost functions when missing data is allowed. The result provides a fundamental limitation of what is possible to achieve with any structure from motion algorithm. Even though there are recent, promising attempts to compute globally optimal solutions, there is no hope of obtaining a polynomial time algorithm unless P=NP. The proof proceeds by encoding an arbitrary Boolean formula as a structure from motion problem of polynomial size, such that the structure from motion problem has a zero cost solution if and only if the Boolean formula is satisfiable. Hence, if there was a guaranteed way to minimize the error of the relevant family of structure from motion problems in polynomial time, the NP-complete problem 3SAT could be solved in polynomial time, which would imply that P=NP The proof relies heavily on results from both structure from motion and complexity theory.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409095",
        "reference_list": [
            {
                "year": "2005",
                "id": 127
            },
            {
                "year": "2005",
                "id": 128
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Cameras",
                "Turing machines",
                "NP-complete problem",
                "Complexity theory",
                "Encoding",
                "Length measurement",
                "Size measurement",
                "Data visualization",
                "Virtual environment"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image coding",
                "image motion analysis",
                "polynomials"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "missing data",
                "NP-hard",
                "motion algorithm",
                "polynomial time algorithm",
                "encoding",
                "arbitrary Boolean formula",
                "Boolean formula",
                "NP-complete problem"
            ]
        },
        "id": 255,
        "cited_by": [
            {
                "year": "2017",
                "id": 4
            }
        ]
    },
    {
        "title": "Laplacian PCA and Its Applications",
        "authors": [
            "Deli Zhao",
            "Zhouchen Lin",
            "Xiaoou Tang"
        ],
        "abstract": "Dimensionality reduction plays a fundamental role in data processing, for which principal component analysis (PCA) is widely used. In this paper, we develop the Laplacian PCA (LPCA) algorithm which is the extension of PCA to a more general form by locally optimizing the weighted scatter. In addition to the simplicity of PCA, the benefits brought by LPCA are twofold: the strong robustness against noise and the weak metric-dependence on sample spaces. The LPCA algorithm is based on the global alignment of locally Gaussian or linear subspaces via an alignment technique borrowed from manifold learning. Based on the coding length of local samples, the weights can be determined to capture the local principal structure of data. We also give the exemplary application of LPCA to manifold learning. Manifold unfolding (non-linear dimensionality reduction) can be performed by the alignment of tangential maps which are linear transformations of tangent coordinates approximated by LPCA. The superiority of LPCA to PCA and kernel PCA is verified by the experiments on face recognition (FRGC version 2 face database) and manifold (Scherk surface) unfolding.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409096",
        "reference_list": [
            {
                "year": "2001",
                "id": 49
            },
            {
                "year": "2007",
                "id": 15
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 21,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Laplace equations",
                "Principal component analysis",
                "Scattering",
                "Noise robustness",
                "Signal processing algorithms",
                "Face recognition",
                "Clustering algorithms",
                "Kernel",
                "Linear discriminant analysis",
                "Active shape model"
            ],
            "INSPEC: Controlled Indexing": [
                "data reduction",
                "image coding",
                "image sampling",
                "Laplace equations",
                "learning (artificial intelligence)",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Laplacian PCA",
                "data processing",
                "principal component analysis",
                "manifold learning",
                "manifold unfolding",
                "nonlinear dimensionality reduction",
                "linear transformation",
                "face recognition",
                "coding length",
                "image sampling"
            ]
        },
        "id": 256,
        "cited_by": [
            {
                "year": "2007",
                "id": 15
            }
        ]
    },
    {
        "title": "Specular Highlight Detection Based on the Fresnel Reflection Coefficient",
        "authors": [
            "Elli Angelopoulou"
        ],
        "abstract": "Reliable specularity detection can affect the accuracy of further image analysis. The majority of specularity detection algorithms are based on the chromaticity of the regions of specular highlights. They assume that the color of specular highlights of dielectrics is approximately the color of the incident light. We will show how this assumption is inaccurate especially in multispectral images. We propose a new, physics-based specularity detection method, which depends on the Fresnel term of the specular highlight, instead of assumptions on chromaticity space. We compute at each pixel an approximation to the Fresnel term at various wavelengths. We then use mean-shift analysis to segment the image based on the Fresnel information. Our experiments with multispectral, as well as traditional RGB images, show improved specularity detection and higher robustness in chromaticity space noise.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409097",
        "reference_list": [
            {
                "year": "2003",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 6,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Fresnel reflection",
                "Reflectivity",
                "Detection algorithms",
                "Image segmentation",
                "Surface waves",
                "Cameras",
                "Colored noise",
                "Image color analysis",
                "Dielectrics",
                "Noise robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "Fresnel diffraction",
                "image colour analysis",
                "image resolution",
                "image segmentation",
                "reflection",
                "spectral analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "reliable specular highlight detection algorithm",
                "Fresnel reflection coefficient",
                "multispectral image analysis",
                "physics-based specularity detection method",
                "chromaticity space noise",
                "image segmentation"
            ]
        },
        "id": 257,
        "cited_by": []
    },
    {
        "title": "Monocular SLAM as a Graph of Coalesced Observations",
        "authors": [
            "Ethan Eade",
            "Tom Drummond"
        ],
        "abstract": "We present a monocular SLAM system that avoids inconsistency by coalescing observations into independent local coordinate frames, building a graph of the local frames, and optimizing the resulting graph. We choose coordinates that minimize the nonlinearity of the updates in the nodes, and suggest a heuristic measure of such nonlinearity, using it to guide our traversal of the graph. The system operates in real-time on sequences with several hundreds of landmarks while performing global graph optimization, yielding accurate and nearly consistent estimation relative to offline bundle adjustment, and considerably better consistency than EKF SLAM and FastSLAM.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409098",
        "reference_list": [
            {
                "year": "2003",
                "id": 183
            },
            {
                "year": "2005",
                "id": 196
            }
        ],
        "citation": {
            "ieee": 44,
            "other": 21,
            "total": 65
        },
        "keywords": {
            "IEEE Keywords": [
                "Simultaneous localization and mapping",
                "Recursive estimation",
                "Cameras",
                "Uncertainty",
                "State estimation",
                "Robot kinematics",
                "Particle filters",
                "Coordinate measuring machines",
                "Real time systems",
                "Yield estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "robot vision",
                "SLAM (robots)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "monocular SLAM",
                "coalesced observations graph",
                "global graph optimization"
            ]
        },
        "id": 258,
        "cited_by": []
    },
    {
        "title": "Harvesting Image Databases from the Web",
        "authors": [
            "F. Schroff",
            "A. Criminisi",
            "A. Zisserman"
        ],
        "abstract": "The objective of this work is to automatically generate a large number of images for a specified object class (for example, penguin). A multi-modal approach employing both text, meta data and visual features is used to gather many, high-quality images from the Web. Candidate images are obtained by a text based Web search querying on the object identifier (the word penguin). The Web pages and the images they contain are downloaded. The task is then to remove irrelevant images and re-rank the remainder. First, the images are re-ranked using a Bayes posterior estimator trained on the text surrounding the image and meta data features (such as the image alternative tag, image title tag, and image filename). No visual information is used at this stage. Second, the top-ranked images are used as (noisy) training data and a SVM visual classifier is learnt to improve the ranking further. The principal novelty is in combining text/meta-data and visual features in order to achieve a completely automatic ranking of the images. Examples are given for a selection of animals (e.g. camels, sharks, penguins), vehicles (cars, airplanes, bikes) and other classes (guitar, wristwatch), totalling 18 classes. The results are assessed by precision/recall curves on ground truth annotated data and by comparison to previous approaches including those of Berg et al. (on an additional six classes) and Fergus et al.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409099",
        "reference_list": [
            {
                "year": "2003",
                "id": 84
            },
            {
                "year": "2005",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 64,
            "other": 39,
            "total": 103
        },
        "keywords": {
            "IEEE Keywords": [
                "Image databases",
                "Web search",
                "Web pages",
                "Training data",
                "Support vector machines",
                "Support vector machine classification",
                "Animals",
                "Vehicles",
                "Airplanes",
                "Bicycles"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "estimation theory",
                "image classification",
                "image retrieval",
                "meta data",
                "support vector machines",
                "text analysis",
                "visual databases",
                "Web sites"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image database",
                "meta data",
                "visual features",
                "high-quality image",
                "World Wide Web",
                "text based Web search querying",
                "object identifier",
                "Web page",
                "image reranking",
                "Bayes posterior estimator",
                "image alternative tag",
                "image title tag",
                "image filename",
                "SVM visual classifier",
                "automatic image ranking"
            ]
        },
        "id": 259,
        "cited_by": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2011",
                "id": 66
            },
            {
                "year": "2009",
                "id": 5
            },
            {
                "year": "2009",
                "id": 127
            },
            {
                "year": "2009",
                "id": 251
            }
        ]
    },
    {
        "title": "Instability of Projective Reconstruction of Dynamic Scenes near Critical Configurations",
        "authors": [
            "Marina Bertolini",
            "Cristina Turrini",
            "GianMario Besana"
        ],
        "abstract": "In the context of multiple view geometry in any dimension, we compute the minimum number of views necessary for projective reconstruction of both the set of cameras and of scenes. Within a unified approach to critical configurations and their loci, the paper focuses on the case of dynamic scenes of multiple bodies traveling along parallel straight-line trajectories with constant velocities, in the framework of higher dimensional projections introduced by Shashua and Wolf. Critical loci in this case are explicitly determined. A stratification of the resulting locus in terms of fixed common velocities is presented and leveraged to show, via a number of simulated experiments, instability of the reconstruction near critical configurations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409100",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 4,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Computer science",
                "Computational geometry",
                "Solid modeling",
                "Surface treatment",
                "Computer languages",
                "Project management",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "projective reconstruction",
                "dynamic scenes"
            ]
        },
        "id": 260,
        "cited_by": []
    },
    {
        "title": "An Omnidirectional Vision Sensor with Single View and Constant Resolution",
        "authors": [
            "Hajime Nagahara",
            "Koji Yoshida",
            "Masahiko Yachida"
        ],
        "abstract": "Many omnidirectional vision sensors based on convex mirrors have been proposed for observing a 360 degrees field of view. Some have a single viewpoint using a hyperboloidal or parabolic mirror, others have a constant resolution property. Until now, there have not been any that have had both these properties. In this paper, we propose an omnidirectional vision sensor that has both properties; that is, a single viewpoint and a constant resolution. The proposed omnidirectional sensor uses two mirrors, which improves the degree of freedom of the design for satisfying each property. We discuss optimization of the design in terms of geometry and optics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409101",
        "reference_list": [
            {
                "year": "2001",
                "id": 104
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Mirrors",
                "Cameras",
                "Image resolution",
                "Robot vision systems",
                "Geometry",
                "Lenses",
                "Nonlinear optics",
                "Optical sensors",
                "Shape",
                "Design optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "image sensors",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "omnidirectional vision sensor",
                "constant resolution",
                "convex mirrors",
                "hyperboloidal mirror",
                "parabolic mirror",
                "single viewpoint"
            ]
        },
        "id": 261,
        "cited_by": []
    },
    {
        "title": "The von Kries Hypothesis and a Basis for Color Constancy",
        "authors": [
            "Hamilton Y. Chong",
            "Steven J. Gortler",
            "Todd Zickler"
        ],
        "abstract": "Color constancy is almost exclusively modeled with diagonal transforms. However, the choice of basis under which diagonal transforms are taken is traditionally ad hoc. Attempts to remedy the situation have been hindered by the fact that no joint characterization of the conditions for {sensors, illuminants, reflectances} to support diagonal color constancy has previously been achieved. In this work, we observe that the von Kries compatibility conditions are impositions only on the sensor measurements, not the physical spectra. This allows us to formulate the von Kries compatibility conditions succinctly as rank constraints on an order 3 measurement tensor. Given this, we propose an algorithm that computes a (locally) optimal choice of color basis for diagonal color constancy and compare the results against other proposed choices.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409102",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 9,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Sensor phenomena and characterization",
                "Visual system",
                "Lighting",
                "Tensile stress",
                "Material properties",
                "Layout",
                "Humans",
                "Computer vision",
                "Gain control",
                "Linear algebra"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "von Kries Hypothesis",
                "color constancy",
                "diagonal transforms",
                "order 3 measurement tensor",
                "von Kries compatibility conditions"
            ]
        },
        "id": 262,
        "cited_by": [
            {
                "year": "2015",
                "id": 33
            },
            {
                "year": "2013",
                "id": 237
            }
        ]
    },
    {
        "title": "Surface-from-Gradients with Incomplete Data for Single View Modeling",
        "authors": [
            "Heung-Sun Ng",
            "Tai-Pang Wu",
            "Chi-Keung Tang"
        ],
        "abstract": "Surface gradients are useful to surface reconstruction in single view modeling, shape-from-shading, and photometric stereo. Previous algorithms minimize a complex, nonlinear energy functional, or require dense surface gradients to perform integration to generate 3D locations, or require user-input heights to constrain the solution space, or produce severe distortion and smooth out surface details. Most single-view algorithms output a Monge patch (height-field), which may introduce further surface distortion along object silhouettes and surface orientation discontinuities. Our proposed algorithm operates on a single view of complete or incomplete data. The data can be gradients without 3D locations, or 3D locations without gradients. The output surface, which is not necessarily a height-field, preserves salient depth and orientation discontinuities. Experimental comparisons on both simple and complex data show that our method produces better surfaces with significantly less distortion and more details preserved. The implementation of our closed-form solution is very straightforward.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409103",
        "reference_list": [
            {
                "year": "2005",
                "id": 241
            },
            {
                "year": "2005",
                "id": 44
            },
            {
                "year": "2005",
                "id": 129
            },
            {
                "year": "2005",
                "id": 213
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Surface fitting",
                "Image reconstruction",
                "Nonlinear distortion",
                "Stereo vision",
                "Closed-form solution",
                "Computer vision",
                "Photometry",
                "Graphics",
                "Numerical stability"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image reconstruction",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface-from-gradients",
                "incomplete data",
                "single view modeling",
                "surface reconstruction",
                "shape-from-shading",
                "photometric stereo",
                "nonlinear energy functional",
                "dense surface gradients",
                "surface distortion",
                "object silhouettes",
                "surface orientation discontinuities",
                "3D locations",
                "closed-form solution"
            ]
        },
        "id": 263,
        "cited_by": []
    },
    {
        "title": "Visual Tracking by Affine Kernel Fitting Using Color and Object Boundary",
        "authors": [
            "Ido Leichter",
            "Michael Lindenbaum",
            "Ehud Rivlin"
        ],
        "abstract": "Kernel-based trackers aggregate image features within the support of a kernel (a mask) regardless of their spatial structure. These trackers spatially fit the kernel (usually in location and in scale) such that a function of the aggregate is optimized. We propose a kernel-based visual tracker that exploits the constancy of color and the presence of color edges along the target boundary. The tracker estimates the best affinity of a spatially aligned pair of kernels, one of which is color-related and the other of which is object boundary-related. In a sense, this work extends previous kernel-based trackers by incorporating the object boundary cue into the tracking process and by allowing the kernels to be affinely transformed instead of only translated and isotropically scaled. These two extensions make for more precise target localization. Moreover, a more accurately localized target facilitates safer updating of its reference color model, further enhancing the tracker's robustness. The improved tracking is demonstrated for several challenging image sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409104",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Target tracking",
                "Histograms",
                "Robustness",
                "Aggregates",
                "Shape",
                "Head",
                "Face",
                "Kalman filters",
                "Image sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "image colour analysis",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual tracking",
                "affine kernel fitting",
                "color boundary",
                "object boundary",
                "kernel-based trackers",
                "image features",
                "kernel-based visual tracker",
                "color edges",
                "target boundary",
                "target localization",
                "reference color model",
                "image sequences"
            ]
        },
        "id": 264,
        "cited_by": []
    },
    {
        "title": "Retrieving actions in movies",
        "authors": [
            "Ivan Laptev",
            "Patrick Perez"
        ],
        "abstract": "We address recognition and localization of human actions in realistic scenarios. In contrast to the previous work studying human actions in controlled settings, here we train and test algorithms on real movies with substantial variation of actions in terms of subject appearance, motion, surrounding scenes, viewing angles and spatio-temporal extents. We introduce a new annotated human action dataset and use it to evaluate several existing methods. We in particular focus on boosted space-time window classifiers and introduce \"keyframe priming\" that combines discriminative models of human motion and shape within an action. Keyframe priming is shown to significantly improve the performance of action detection. We present detection results for the action class \"drinking\" evaluated on two episodes of the movie \"Coffee and Cigarettes\".",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409105",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2005",
                "id": 59
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2005",
                "id": 178
            },
            {
                "year": "2005",
                "id": 21
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2005",
                "id": 94
            },
            {
                "year": "2005",
                "id": 52
            },
            {
                "year": "2005",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 124,
            "other": 94,
            "total": 218
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion pictures",
                "Humans",
                "Videos",
                "Testing",
                "Layout",
                "Shape",
                "Motion control",
                "Volcanoes",
                "Application software",
                "YouTube"
            ],
            "INSPEC: Controlled Indexing": [
                "video retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "retrieving actions",
                "movies",
                "address recognition",
                "human actions"
            ]
        },
        "id": 265,
        "cited_by": [
            {
                "year": "2017",
                "id": 383
            },
            {
                "year": "2017",
                "id": 462
            },
            {
                "year": "2017",
                "id": 463
            },
            {
                "year": "2015",
                "id": 353
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2013",
                "id": 172
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2009",
                "id": 31
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2009",
                "id": 118
            },
            {
                "year": "2009",
                "id": 121
            },
            {
                "year": "2009",
                "id": 186
            },
            {
                "year": "2009",
                "id": 191
            }
        ]
    },
    {
        "title": "A Nonlinear Discriminative Approach to AAM Fitting",
        "authors": [
            "Jason Saragih",
            "Roland Goecke"
        ],
        "abstract": "The Active Appearance Model (AAM) is a powerful generative method for modeling and registering deformable visual objects. Most methods for AAM fitting utilize a linear parameter update model in an iterative framework. Despite its popularity, the scope of this approach is severely restricted, both in fitting accuracy and capture range, due to the simplicity of the linear update models used. In this paper, we present an new AAM fitting formulation, which utilizes a nonlinear update model. To motivate our approach, we compare its performance against two popular fitting methods on two publicly available face databases, in which this formulation boasts significant performance improvements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409106",
        "reference_list": [
            {
                "year": "2003",
                "id": 47
            },
            {
                "year": "2005",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 62,
            "other": 22,
            "total": 84
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Shape",
                "Fitting",
                "Error correction",
                "Power generation",
                "Deformable models",
                "Cost function",
                "Parametric statistics",
                "Principal component analysis",
                "Power engineering and energy"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convergence of numerical methods",
                "image registration",
                "iterative methods",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonlinear discriminative approach",
                "AAM fitting formulation",
                "active appearance model",
                "deformable visual object modeling",
                "deformable visual object registration",
                "linear parameter update model",
                "iterative framework",
                "convergence rates",
                "boosting procedure"
            ]
        },
        "id": 266,
        "cited_by": [
            {
                "year": "2017",
                "id": 496
            },
            {
                "year": "2013",
                "id": 73
            },
            {
                "year": "2009",
                "id": 128
            },
            {
                "year": "2009",
                "id": 132
            },
            {
                "year": "2009",
                "id": 289
            }
        ]
    },
    {
        "title": "Using Color Compatibility for Assessing Image Realism",
        "authors": [
            "Jean-Francois Lalonde",
            "Alexei A. Efros"
        ],
        "abstract": "Why does placing an object from one photograph into another often make the colors of that object suddenly look wrong? One possibility is that humans prefer distributions of colors that are often found in nature; that is, we find pleasing these color combinations that we see often. Another possibility is that humans simply prefer colors to be consistent within an image, regardless of what they are. In this paper, we explore some of these issues by studying the color statistics of a large dataset of natural images, and by looking at differences in color distribution in realistic and unrealistic images. We apply our findings to two problems: 1) classifying composite images into realistic vs. non- realistic, and 2) recoloring image regions for realistic compositing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409107",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 17,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Layout",
                "Statistical distributions",
                "Art",
                "Color",
                "Painting",
                "Computer science",
                "Geometry",
                "Lighting",
                "Physics"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image colour analysis",
                "statistical analysis",
                "very large databases",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color compatibility",
                "image realism assessment",
                "color statistics",
                "large natural image dataset",
                "color distribution",
                "realistic images",
                "unrealistic images",
                "composite image classification",
                "image region recoloring"
            ]
        },
        "id": 267,
        "cited_by": [
            {
                "year": "2015",
                "id": 440
            }
        ]
    },
    {
        "title": "Correspondence labelling for wide-timeframe free-form surface matching",
        "authors": [
            "Jonathan Starck",
            "Adrian Hilton"
        ],
        "abstract": "This paper addresses the problem of estimating dense correspondence between arbitrary frames from captured sequences of shape and appearance for surfaces undergoing free-form deformation. Previous techniques require either a prior model, limiting the range of surface deformations, or frame-to-frame surface tracking which suffers from stabilisation problems over complete motion sequences and does not provide correspondence between sequences. The primary contribution of this paper is the introduction of a system for wide-timeframe surface matching without the requirement for a prior model or tracking. Deformation- invariant surface matching is formulated as a locally isometric mapping at a discrete set of surface points. A set of feature descriptors are presented that are invariant to isometric deformations and a novel MAP-MRF framework is presented to label sparse-to-dense surface correspondence, preserving the relative distribution of surface features while allowing for changes in surface topology. Performance is evaluated on challenging data from a moving person with loose clothing. Ground-truth feature correspondences are manually marked and the recall-accuracy characteristic is quantified in matching. Results demonstrate an improved performance compared to non-rigid point-pattern matching using robust matching and graph-matching using relaxation labelling, with successful matching achieved across wide variations in human body pose and surface topology.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409108",
        "reference_list": [
            {
                "year": "2005",
                "id": 191
            },
            {
                "year": "2003",
                "id": 120
            },
            {
                "year": "2005",
                "id": 181
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 22,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Surface fitting",
                "Topology",
                "Deformable models",
                "Tracking",
                "Humans",
                "Robustness",
                "Speech processing",
                "Signal processing",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "image sequences",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "wide-timeframe free-form surface matching",
                "image sequence",
                "frame-to-frame surface tracking",
                "image motion analysis",
                "deformation- invariant surface matching",
                "isometric mapping",
                "MAP-MRF framework",
                "sparse-to-dense surface correspondence",
                "robust matching",
                "graph-matching",
                "non-rigid point-pattern matching",
                "human body pose"
            ]
        },
        "id": 268,
        "cited_by": [
            {
                "year": "2013",
                "id": 328
            },
            {
                "year": "2009",
                "id": 123
            }
        ]
    },
    {
        "title": "Using High-Level Visual Information for Color Constancy",
        "authors": [
            "Joost van de Weijer",
            "Cordelia Schmid",
            "Jakob Verbeek"
        ],
        "abstract": "We propose to use high-level visual information to improve illuminant estimation. Several illuminant estimation approaches are applied to compute a set of possible illuminants. For each of them an illuminant color corrected image is evaluated on the likelihood of its semantic content: is the grass green, the road grey, and the sky blue, in correspondence with our prior knowledge of the world. The illuminant resulting in the most likely semantic composition of the image is selected as the illuminant color. To evaluate the likelihood of the semantic content, we apply probabilistic latent semantic analysis. The image is modelled as a mixture of semantic classes, such as sky, grass, road, and building. The class description is based on texture, position and color information. Experiments show that the use of high-level information improves illuminant estimation over a purely bottom-up approach. Furthermore, the proposed method is shown to significantly improve semantic class recognition performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409109",
        "reference_list": [],
        "citation": {
            "ieee": 32,
            "other": 21,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Reflectivity",
                "Roads",
                "Light sources",
                "Computer vision",
                "Layout",
                "Color",
                "Application software",
                "Image databases",
                "Statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high-level visual information",
                "color constancy",
                "illuminant estimation",
                "illuminant color corrected image",
                "semantic image composition",
                "probabilistic latent semantic analysis"
            ]
        },
        "id": 269,
        "cited_by": [
            {
                "year": "2017",
                "id": 572
            },
            {
                "year": "2013",
                "id": 115
            },
            {
                "year": "2009",
                "id": 224
            }
        ]
    },
    {
        "title": "Signals on Pencils of Lines",
        "authors": [
            "Justin Domke",
            "Yiannis Aloimonos"
        ],
        "abstract": "This paper proposes the \"epipolar pencil transformation\" (EPT). This is a tool for comparing the signals in different images, with no use of feature detection, yet taking advantage of the constraints given by epipolar geometry. The idea is to develop a descriptor for each point, summarizing the signals on the pencil of lines intersecting at that point. To compute the EPT, first find compact descriptors for each line, then combine these appropriately for each pencil. Given the EPT for two images, computing the epipolar geometry reduces to a closest pairs problem- select one pencil from each set such that the L1 distance (in descriptor space) is minimized. By this reduction to a high dimensional closest pairs problem, recent advances in computational geometry can be used to efficiently identify the best global solution. This technique is robust, as each potential solution is evaluated by comparing the signals for all the lines passing through the two hypothesized epipoles. At the same time, as the closest pairs algorithm performs a global search, the solution is not distracted by local minima. The EPT is used here both for the problem of two-view rigid motion, and many-view place recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409110",
        "reference_list": [
            {
                "year": "2003",
                "id": 37
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Computational geometry",
                "Automation",
                "Computer science",
                "Computer vision",
                "Robustness",
                "Information geometry",
                "Image converters",
                "Costs",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "edge detection",
                "feature extraction",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pencils of lines",
                "epipolar pencil transformation",
                "feature detection",
                "epipolar geometry",
                "computational geometry",
                "image signal comparison",
                "two-view rigid motion",
                "many-view place recognition"
            ]
        },
        "id": 270,
        "cited_by": []
    },
    {
        "title": "Deformable Image Mosaicing for Optical Biopsy",
        "authors": [
            "Kevin Loewke",
            "David Camarillo",
            "Kenneth Salisbury",
            "Sebastian Thrun"
        ],
        "abstract": "Traditional image mosaicing usually relies on rigid image transformations. In many medical applications, however, tissue deformation during image acquisition or 3D parallax effects may require nonrigid transformations in the mosaicing process. This paper presents a new method that integrates deformable surface models into the image mosaicing algorithms. Our approach has two main contributions. First, we present a global alignment algorithm to efficiently deal with accumulated image registration errors. Second, we introduce a local alignment algorithm to accommodate local scene deformations. These two problems are integrated into a single optimization problem that simultaneously recovers the motion of the camera as well as the structure of the scene. Our approach is demonstrated on simulations, images from a hand-held digital camera, and microscopic images acquired with a micro-endoscope.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409111",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical optical imaging",
                "Biopsy",
                "Layout",
                "Optical imaging",
                "Image registration",
                "Medical services",
                "Biomedical equipment",
                "Microscopy",
                "Biomedical imaging",
                "Endoscopes"
            ],
            "INSPEC: Controlled Indexing": [
                "biological tissues",
                "biomechanics",
                "biomedical optical imaging",
                "deformation",
                "endoscopes",
                "image registration",
                "medical image processing",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable image mosaicing",
                "optical biopsy",
                "rigid image transformations",
                "tissue deformation",
                "image acquisition",
                "3D parallax effects",
                "image registration",
                "local alignment algorithm",
                "optimization",
                "hand-held digital camera",
                "microendoscope"
            ]
        },
        "id": 271,
        "cited_by": []
    },
    {
        "title": "On the Extraction of Curve Skeletons using Gradient Vector Flow",
        "authors": [
            "M. Sabry Hassouna",
            "Aly A. Farag"
        ],
        "abstract": "In this paper, we propose a new variational framework for computing continuous curve skeletons from discrete objects that are suitable for structural shape representation. We have derived a new energy function, which is proportional to some medialness function, such that the minimum cost path between any two medial voxels in the shape is a curve skeleton. We have employed two different medialness functions; the Euclidean distance field and a variant of the magnitude of the gradient vector flow (GVF), resulting in two different energy functions. The first energy controls the identification of the shape topological nodes from which curve skeletons start, while the second one controls the extraction of curve skeletons. The accuracy and robustness of the proposed framework are validated both quantitatively and qualitatively against competing techniques as well as several 3D shapes of different complexity.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409112",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 9,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Skeleton",
                "Computer vision",
                "Shape control",
                "Noise robustness",
                "Image processing",
                "Laboratories",
                "Structural shapes",
                "Cost function",
                "Euclidean distance",
                "Machine intelligence"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "feature extraction",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curve skeletons extraction",
                "gradient vector flow",
                "discrete objects",
                "energy function",
                "medialness function",
                "Euclidean distance field",
                "energy functions",
                "shape topological nodes"
            ]
        },
        "id": 272,
        "cited_by": []
    },
    {
        "title": "Plane-based self-calibration of radial distortion",
        "authors": [
            "Jean-Philippe Tardif",
            "Peter Sturm",
            "Sebastien Roy"
        ],
        "abstract": "We present an algorithm for plane-based self-calibration of cameras with radially symmetric distortions given a set of sparse feature matches in at least two views. The projection function of such cameras can be seen as a projection with a pinhole camera, followed by a non-parametric displacement of the image points in the direction of the distortion center. The displacement is a function of the points' distance to the center. Thus, the generated distortion is radially symmetric. Regular cameras, fish-eyes as well as the most popular central catadioptric devices can be described by such a model. Our approach recovers a distortion function consistent with all the views, or estimates one for each view if they are taken by different cameras. We consider a least squares algebraic solution for computing the homography between two views that is valid for rectified (undistorted) point correspondences. We observe that the terms of the function are bilinear in the unknowns of the homography and the distortion coefficient associated to each point. Our contribution is to approximate this non-convex problem by a convex one. To do so, we replace the bilinear terms by a set of new variables and obtain a linear least squares problem. We show that like the distortion coefficients, these variables are subject to monotonicity constraints. Thus, the approximate problem is a convex quadratic program. We show that solving it is sufficient for accurately estimating the distortion parameters. We validate our approach on simulated data as well as on fish-eye and catadioptric cameras. We also compare our solution to three state-of-the-art algorithms and show similar performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409113",
        "reference_list": [
            {
                "year": "2005",
                "id": 239
            },
            {
                "year": "2005",
                "id": 15
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 2,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Nonlinear distortion",
                "Cameras",
                "Optical distortion",
                "Least squares methods",
                "Least squares approximation",
                "Nonlinear optics",
                "Parameter estimation",
                "Lenses",
                "Geometrical optics",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "distortion",
                "feature extraction",
                "image matching",
                "image sensors",
                "least squares approximations",
                "nonparametric statistics",
                "parameter estimation",
                "quadratic programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "plane-based self-calibration",
                "radially symmetric distortions",
                "sparse feature matches",
                "camera projection function",
                "pinhole camera",
                "nonparametric image point displacement",
                "least squares algebraic solution",
                "convex quadratic program",
                "distortion parameter estimation"
            ]
        },
        "id": 273,
        "cited_by": [
            {
                "year": "2013",
                "id": 236
            }
        ]
    },
    {
        "title": "Globally Optimal Affine and Metric Upgrades in Stratified Autocalibration",
        "authors": [
            "Manmohan Chandraker",
            "Sameer Agarwal",
            "David Kriegman",
            "Serge Belongie"
        ],
        "abstract": "We present a practical, stratified autocalibration algorithm with theoretical guarantees of global optimality. Given a projective reconstruction, the first stage of the algorithm upgrades it to affine by estimating the position of the plane at infinity. The plane at infinity is computed by globally minimizing a least squares formulation of the modulus constraints. In the second stage, the algorithm upgrades this affine reconstruction to a metric one by globally minimizing the infinite homography relation to compute the dual image of the absolute conic (DIAC). The positive semidefiniteness of the DIAC is explicitly enforced as part of the optimization process, rather than as a post-processing step. For each stage, we construct and minimize tight convex relaxations of the highly non-convex objective functions in a branch and bound optimization framework. We exploit the problem structure to restrict the search space for the DIAC and the plane at infinity to a small, fixed number of branching dimensions, independent of the number of views. Experimental evidence of the accuracy, speed and scalability of our algorithm is presented on synthetic and real data. MATLAB code for the implementation is made available to the community.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409114",
        "reference_list": [
            {
                "year": "2005",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 5,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "H infinity control",
                "Thyristors",
                "Image reconstruction",
                "Minimization methods",
                "Least squares methods",
                "Scalability",
                "MATLAB",
                "Parameter estimation",
                "Cameras",
                "Calibration"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "least squares approximations",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "globally optimal affine-metric upgrades",
                "stratified autocalibration algorithm",
                "least squares formulation",
                "modulus constraints",
                "infinite homography relation",
                "dual image of the absolute conic",
                "post-processing process",
                "optimization process",
                "nonconvex objective functions",
                "problem structure",
                "MATLAB code"
            ]
        },
        "id": 274,
        "cited_by": [
            {
                "year": "2009",
                "id": 11
            }
        ]
    },
    {
        "title": "Real-Time SLAM Relocalisation",
        "authors": [
            "Brian Williams",
            "Georg Klein",
            "Ian Reid"
        ],
        "abstract": "Monocular SLAM has the potential to turn inexpensive cameras into powerful pose sensors for applications such as robotics and augmented reality. However, current implementations lack the robustness required to be useful outside laboratory conditions: blur, sudden motion and occlusion all cause tracking to fail and corrupt the map. Here we present a system which automatically detects and recovers from tracking failure while preserving map integrity. By extending recent advances in keypoint recognition the system can quickly resume tracking - i.e. within a single frame time of 33 ms - using any of the features previously stored in the map. Extensive tests show that the system can reliably generate maps for long sequences even in the presence of frequent tracking failure.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409115",
        "reference_list": [
            {
                "year": "2005",
                "id": 196
            }
        ],
        "citation": {
            "ieee": 63,
            "other": 51,
            "total": 114
        },
        "keywords": {
            "IEEE Keywords": [
                "Simultaneous localization and mapping",
                "Robot vision systems",
                "Cameras",
                "Robot sensing systems",
                "Robotics and automation",
                "Augmented reality",
                "Robustness",
                "Laboratories",
                "Tracking",
                "Resumes"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image restoration",
                "image sequences",
                "mobile robots",
                "optical tracking",
                "robot vision",
                "SLAM (robots)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time monocular SLAM relocalisation",
                "automatic tracking failure detection",
                "automatic tracking failure recovery",
                "image recognition",
                "image sequence"
            ]
        },
        "id": 275,
        "cited_by": [
            {
                "year": "2011",
                "id": 152
            },
            {
                "year": "2011",
                "id": 298
            },
            {
                "year": "2009",
                "id": 197
            }
        ]
    },
    {
        "title": "Accurate Non-Iterative \ud835\udc42(\ud835\udc5b) Solution to the P\ud835\udc5bP Problem",
        "authors": [
            "Francesc Moreno-Noguer",
            "Vincent Lepetit",
            "Pascal Fua"
        ],
        "abstract": "We propose a non-iterative solution to the PnP problem-the estimation of the pose of a calibrated camera from n 3D-to-2D point correspondences\u2014whose computational complexity grows linearly with \ud835\udc5b<. This is in contrast to state-of-the-art methods that are \ud835\udc42(\ud835\udc5b 5 ) or even \ud835\udc42(\ud835\udc5b 8 ), without being more accurate. Our method is applicable for all \ud835\udc5b\u22654 and handles properly both planar and non-planar configurations. Our central idea is to express the \ud835\udc5b 3D points as a weighted sum of four virtual control points. The problem then reduces to estimating the coordinates of these control points in the camera referential, which can be done in \ud835\udc42(\ud835\udc5b) time by expressing these coordinates as weighted sum of the eigenvectors of a 12 \u00d7 12 matrix and solving a small constant number of quadratic equations to pick the right weights. The advantages of our method are demonstrated by thorough testing on both synthetic and real-data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409116",
        "reference_list": [],
        "citation": {
            "ieee": 75,
            "other": 58,
            "total": 133
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Iterative methods",
                "Computer vision",
                "Equations",
                "Robot vision systems",
                "Computational complexity",
                "Centralized control",
                "Transmission line matrix methods",
                "Application software",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "eigenvalues and eigenfunctions",
                "matrix algebra",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "PnP problem",
                "noniterative solution",
                "3D-to-2D point correspondences",
                "eigenvectors",
                "pose estimation",
                "camera calibration",
                "computational complexity"
            ]
        },
        "id": 276,
        "cited_by": [
            {
                "year": "2015",
                "id": 226
            },
            {
                "year": "2013",
                "id": 351
            },
            {
                "year": "2011",
                "id": 262
            }
        ]
    },
    {
        "title": "Random Walk and Front Propagation on Watershed Adjacency Graphs for Multilabel Image Segmentation",
        "authors": [
            "Christophe Chefd'hotel",
            "Alexis Sebbane"
        ],
        "abstract": "The watershed partition of an image often results in over-segmentation. This well-known phenomenon is due to variations of intensity that do not correspond to object boundaries and produce spurious local minima in the image gradient magnitude. Filtering minima or merging watershed regions is then necessary to obtain a higher-level description of the data. In this paper, we propose new solutions to this problem by applying two interactive multilabel partitioning techniques to the adjacency graph of the watershed regions. In our first approach, the partition is derived from the probability that a \"random walker\" starting at an arbitrary node, first reaches a node with a pre-assigned label. In the second approach, we compute a geodesic partition of the graph using competing wavefronts starting at prescribed nodes. Both methods are based on existing segmentation techniques previously implemented on image lattices. Using a watershed adjacency graph greatly reduces their memory footprint and computational cost. We demonstrate the practicality and versatility of this approach with several experiments on 2D and 3D datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409117",
        "reference_list": [
            {
                "year": "2003",
                "id": 43
            },
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Partitioning algorithms",
                "Merging",
                "Lattices",
                "Geophysics computing",
                "Computational efficiency",
                "Labeling",
                "Visualization",
                "Filtering",
                "Computed tomography"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "probability",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "random walk",
                "front propagation",
                "watershed adjacency graph",
                "multilabel image segmentation",
                "interactive multilabel partitioning",
                "watershed region",
                "probability",
                "image lattice"
            ]
        },
        "id": 277,
        "cited_by": []
    },
    {
        "title": "Interactive Left Ventricular Segmentation in Cardiac Images",
        "authors": [
            "Wendeson da Silva Oliveira",
            "Angela Vilhena Dias"
        ],
        "abstract": "This work presents a technique to perform segmentation of the left ventricle in images of the human myocardium. Segmentation is one of the first steps of the image analysis. Edge-based segmentation provides the detection of region boundaries, but the contours are only based on local computations, and is often sensitive to local variations in intensity, noise and physical artifacts. In interactive methods, problems like these fatigue the user and require many interventions at the contour delineation process. To reduce these problems, we propose the addition of two new features in the live-wire method: a region intensity and a proximity feature. Experimental results show that this alternative approach can achieve accurate edge localization and improved efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409118",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image edge detection",
                "Deformable models",
                "Topology",
                "Shape",
                "Myocardium",
                "Physics computing",
                "Fatigue",
                "Active contours",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "cardiology",
                "image segmentation",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interactive left ventricular segmentation",
                "cardiac images",
                "human myocardium",
                "image analysis",
                "edge-based segmentation",
                "contour delineation process",
                "live-wire method",
                "region intensity",
                "proximity feature",
                "edge localization"
            ]
        },
        "id": 278,
        "cited_by": []
    },
    {
        "title": "Quadratic Markovian Probability Fields for Image Binary Segmentation",
        "authors": [
            "Mariano Rivera",
            "Pedro P. Mayorga"
        ],
        "abstract": "We present a Markov random field model for image binary segmentation that computes the probability that each pixel belongs to a given class. We show that the computation of a real valued field has noticeable computational and performance advantages with respect to the computation of binary valued field; the proposed energy function is efficiently minimized with standard fast linear order algorithms as conjugate gradient or multigrid Gauss-Seidel schemes. By providing a good initial guesses as starting point we avoid to construct from scratch a new solution, accelerating the computational process, and allow us to naturally implement efficient multigrid algorithms. For applications with limited computational time, a good partial solution can be obtained by stopping the iterations even if the global optimum is not yet reached. We present a meticulous comparison with state of the art methods: graph cut, random walker and GMMF The algorithms' performance are compared using a cross-validation procedure and an automatics algorithm for learning the parameter set.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409119",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Markov random fields",
                "Application software",
                "Iterative algorithms",
                "Gaussian processes",
                "Computer applications",
                "Image generation",
                "Pixel",
                "Acceleration",
                "Image motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "conjugate gradient methods",
                "graph theory",
                "image segmentation",
                "iterative methods",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "quadratic Markovian probability",
                "image binary segmentation",
                "Markov random field model",
                "conjugate gradient method",
                "multigrid Gauss-Seidel scheme",
                "graph cut method",
                "random walker method",
                "GMMF method"
            ]
        },
        "id": 279,
        "cited_by": []
    },
    {
        "title": "Computing the \u03b1-channel with probabilistic segmentation for image colorization",
        "authors": [
            "Oscar Dalmau-Cedeno",
            "Mariano Rivera",
            "Pedro P. Mayorga"
        ],
        "abstract": "We propose a gray scale image colorization method based on a Bayesian segmentation framework in which the classes are established from scribbles made by a user on the image. These scribbles can be considered as a multimap (multilabels map) that defines the boundary conditions of a probability measure field to be computed in each pixel. The components of such a probability measure field express the degree of belonging of each pixel to spatially smooth classes. In a first step we obtain the probability measure field by computing the global minima of a positive definite quadratic cost function with linear constraints. Then color is introduced in a second step through a pixelwise operation. The computed probabilities (memberships) are used for defining the weights of a simple linear combination of user provided colors associated to each class. An advantage of our method is that it allows us to re-colorize part or the whole image in an easy way, without need of recomputing the memberships (or /sp alpha/-channels).",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409120",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Humans",
                "Pixel",
                "Bayesian methods",
                "Boundary conditions",
                "Cost function",
                "Gray-scale",
                "DVD",
                "Color",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image colour analysis",
                "image resolution",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic segmentation",
                "gray scale image colorization method",
                "Bayesian segmentation framework",
                "boundary conditions",
                "probability measure field",
                "quadratic cost function",
                "linear constraints",
                "pixelwise operation"
            ]
        },
        "id": 280,
        "cited_by": []
    },
    {
        "title": "Enabling Users to Guide the Design of Robust Model Fitting Algorithms",
        "authors": [
            "Matthias Wimmer",
            "Freek Stulp",
            "Bernd Radig"
        ],
        "abstract": "Model-based image interpretation extracts high-level information from images using a priori knowledge about the object of interest. The computational challenge in model fitting is to determine the model parameters that best match a given image, which corresponds to finding the global optimum of the objective function. When it comes to the robustness and accuracy of fitting models to specific images, humans still outperform state- of-the-art model fitting systems. Therefore, we propose a method in which non-experts can guide the process of designing model fitting algorithms. In particular, this paper demonstrates how to obtain robust objective functions for face model fitting applications, by learning their calculation rules from example images annotated by humans. We evaluate the obtained function using a publicly available image database and compare it to a recent state-of-the-art approach in terms of accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409121",
        "reference_list": [
            {
                "year": "2001",
                "id": 199
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Algorithm design and analysis",
                "Robustness",
                "Humans",
                "Data mining",
                "Computational modeling",
                "Process design",
                "Face",
                "Image databases",
                "Layout",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image processing",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust model fitting algorithms",
                "model-based image interpretation",
                "information extraction",
                "objective function",
                "image database"
            ]
        },
        "id": 281,
        "cited_by": []
    },
    {
        "title": "Soylent Grid: it's Made of People",
        "authors": [
            "Stephan Steinbach",
            "Vincent Rabaud",
            "Serge Belongie"
        ],
        "abstract": "The ground truth labeling of an image dataset is a task that often requires a large amount of human time and labor. We present an infrastructure for distributed human labeling that can exploit the modularity of common vision problems involving segmentation and recognition. We present the different elements of this infrastructure in detail, in particular the different vision human computational tasks (HCTs) and machine computable tasks (MCTs). We also discuss the impact of such a system on Internet security vs. the current state of the art. Finally, we present our prototype implementation of such a system, named soylent grid, on typical problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409122",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Humans",
                "Optical character recognition software",
                "Computer vision",
                "Image segmentation",
                "Detectors",
                "Testing",
                "Distributed computing",
                "Computer networks",
                "Data flow computing"
            ],
            "INSPEC: Controlled Indexing": [
                "grid computing",
                "image recognition",
                "image segmentation",
                "Internet",
                "security of data"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Soylent Grid",
                "ground truth labeling",
                "image dataset",
                "distributed human labeling",
                "image segmentation",
                "image recognition",
                "human computational tasks",
                "machine computable tasks",
                "Internet security"
            ]
        },
        "id": 282,
        "cited_by": []
    },
    {
        "title": "An Interactive Approach to Pose-Assisted and Appearance-based Segmentation of Humans",
        "authors": [
            "Zhe Lin",
            "Larry S. Davis",
            "David Doermann",
            "Daniel DeMenthon"
        ],
        "abstract": "An interactive human segmentation approach is described. Given regions of interest provided by users, the approach iteratively estimates segmentation via a generalized EM algorithm. Specifically, it encodes both spatial and color information in a nonparametric kernel density estimator, and incorporates local MRF constraints and global pose inferences to propagate beliefs over image space iteratively to determine a coherent segmentation. This ensures the segmented humans resemble the shapes of human poses. Additionally, a layered occlusion model and a probabilistic occlusion reasoning method are proposed to handle segmentation of multiple humans in occlusion. The approach is tested on a wide variety of images containing single or multiple occluded humans, and the segmentation performance is evaluated quantitatively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409123",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 49
            },
            {
                "year": "2005",
                "id": 97
            },
            {
                "year": "2005",
                "id": 58
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 10,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Shape",
                "Image segmentation",
                "Object segmentation",
                "Kernel",
                "Educational institutions",
                "Inference algorithms",
                "Iterative algorithms",
                "Testing",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "hidden feature removal",
                "image segmentation",
                "inference mechanisms",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interactive estimation approach",
                "pose-assisted human segmentation",
                "appearance-based human segmentation",
                "EM algorithm",
                "nonparametric kernel density estimator",
                "layered occlusion model",
                "probabilistic occlusion reasoning method"
            ]
        },
        "id": 283,
        "cited_by": []
    },
    {
        "title": "Assisted Video Object Labeling By Joint Tracking of Regions and Keypoints",
        "authors": [
            "Julien Fauqueur",
            "Gabriel Brostow",
            "Roberto Cipolla"
        ],
        "abstract": "Manual labeling of objects in videos is a tedious task. We present an approach which automatically propagates the labels from a single frame to the next ones. We tackle the challenging problem of tracking segmented regions by combining keypoint tracking with an advanced multiple region matching strategy, based on inclusion similarity and connected regions. We ran experiments on a 101 frame driving video sequence for which we produced the corresponding hand- labeled groundtruth. We make this valuable dataset available for the research community. We show our technique can accommodate variations in segmentation (and correct them), even in presence of multiple independent motions and partial occlusion. Results show that most of the labeled pixels can be correctly propagated even after a hundred frames. The performance of this automatic propagation mechanism over many frames can greatly reduce the user effort in the task of video object labeling.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409124",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Image segmentation",
                "Video sequences",
                "Object recognition",
                "Humans",
                "Robustness",
                "Computer vision",
                "Radio access networks",
                "Biomedical imaging",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image segmentation",
                "image sequences",
                "object detection",
                "tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "assisted video object labeling",
                "joint segmented region tracking",
                "keypoint tracking",
                "multiple region matching strategy",
                "video sequence"
            ]
        },
        "id": 284,
        "cited_by": []
    },
    {
        "title": "Interacting with Projected Media on Deformable Surfaces",
        "authors": [
            "Fitriani",
            "Wooi-Boon Goh"
        ],
        "abstract": "This paper presents a novel human-computer interface for projector/camera-based applications that uses a deformable interaction surface. We discuss its design and implementation within the context of a radically different approach for controlling home appliances by pressing virtual buttons that are projected on soft deformable surfaces such as a sofa pillow. Effective real-time computer vision algorithms for implementing pointing and selection action detection for such an interface are discussed. Experimental results highlight the parameters and factors that have significant effect on the overall performance of such an interface.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409125",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer interfaces",
                "Fingers",
                "Application software",
                "Pressing",
                "Lighting control",
                "Event detection",
                "Home appliances",
                "Computer vision",
                "Humans",
                "DVD"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "domestic appliances",
                "home automation",
                "home computing",
                "human computer interaction",
                "optical projectors",
                "virtual reality"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human-computer interface",
                "projector/camera-based application",
                "deformable interaction surface",
                "home appliance control",
                "virtual buttons",
                "soft deformable surfaces",
                "real-time computer vision algorithm"
            ]
        },
        "id": 285,
        "cited_by": []
    },
    {
        "title": "Articulated Shape Matching by Robust Alignment of Embedded Representations",
        "authors": [
            "Diana Mateus",
            "Fabio Cuzzolin",
            "Radu Horaud",
            "Edmond Boyer"
        ],
        "abstract": "In this paper we propose a general framework to solve the articulated shape matching problem, formulated as finding point-to-point correspondences between two shapes represented by 2-D or 3-D point clouds. The original point- sets are embedded in a spectral representation and the actual matching is carried out in the embedded space. We analyze the advantages of this choice as well as the reasons for which the task remains a difficult one. In particular, we show that although embedded-space matching still has intrinsic combinatorial difficulties, it can be solved by searching for an optimal orthogonal transformation that aligns the two shape embeddings. Relying on the model based clustering formalism, we propose a probabilistic formulation which casts the matching into an EM algorithm. Outliers are properly handled by the algorithm and a simple strategy is adopted to initialize it. Experiments are performed with three embedding methods (Isomap, LLE, and Laplacian embedding) and with 3-D voxelsets representing a human-motion sequence.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408833",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Shape measurement",
                "Eigenvalues and eigenfunctions",
                "Clustering algorithms",
                "Spectral analysis",
                "Clouds",
                "Laplace equations",
                "Sequences",
                "Computer vision",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "image representation",
                "image sequences",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "articulated shape matching",
                "robust alignment",
                "embedded representations",
                "3D point clouds",
                "2D point clouds",
                "spectral representation",
                "embedded-space matching",
                "optimal orthogonal transformation",
                "model based clustering formalism",
                "probabilistic formulation",
                "3D voxelsets",
                "human-motion sequence"
            ]
        },
        "id": 286,
        "cited_by": []
    },
    {
        "title": "Perspectively Invariant Normal Features",
        "authors": [
            "Kevin Koser",
            "Reinhard Koch"
        ],
        "abstract": "We extend the successful 2D robust feature concept into the third dimension in that we produce a descriptor for a reconstructed 3D surface region. The descriptor is perspectively invariant if the region can locally be approximated well by a plane. We exploit depth and texture information, which is nowadays available in real-time from video of moving cameras, from stereo systems or PMD cameras (photonic mixer devices). By computing a normal view onto the surface we still keep the descriptiveness of similarity invariant features like SIFT while achieving in- variance against perspective distortions, while descriptiveness typically suffers when using affine invariant features. Our approach can be exploited for structure-from-motion, for stereo or PMD cameras, alignment of large scale reconstructions or improved video registration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408837",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 15,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Geometrical optics",
                "Image reconstruction",
                "Feature extraction",
                "Surface texture",
                "Robustness",
                "Surface reconstruction",
                "Shape",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image motion analysis",
                "image reconstruction",
                "image registration",
                "stereo image processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D surface region reconstruction",
                "photonic mixer device cameras",
                "affine invariant feature",
                "video registration",
                "structure-from-motion method",
                "stereo image processing",
                "2D robust feature detection"
            ]
        },
        "id": 287,
        "cited_by": [
            {
                "year": "2015",
                "id": 490
            },
            {
                "year": "2013",
                "id": 350
            }
        ]
    },
    {
        "title": "Detecting and Localizing 3D Object Classes using Viewpoint Invariant Reference Frames",
        "authors": [
            "Matthew Toews",
            "Tal Arbel"
        ],
        "abstract": "In this paper, we investigate detection and localization of general 3D object classes by relating local scale-invariant features to a viewpoint invariant reference frame. This can generally be achieved by either a multi-view representation, where features and reference frame are modeled as a collection of distinct views, or by a viewpoint invariant representation, where features and reference frame are modeled independently of viewpoint. We compare multi-view and viewpoint invariant representations trained and tested on the same data, where the viewpoint invariant approach results in fewer false positive detections and higher average precision. We present a new, iterative learning algorithm to determine an optimal viewpoint invariant reference frame from training images in a data-driven manner. The learned optimal reference frame is centrally located with respect to the 3D object class and to image features in a given view, thereby minimizing reference frame localization error as predicted by theory and maintaining a consistent geometrical interpretation with respect to the underlying object class. Modeling and detection based on the optimal reference frame improves detection performance for both multiview and viewpoint invariant representations. Experimentation is performed on the class of 3D faces, using the public color FERET database for training, the CMU profile database for testing and SIFT image features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408832",
        "reference_list": [
            {
                "year": "2003",
                "id": 84
            },
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2005",
                "id": 48
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Face detection",
                "Spatial databases",
                "Testing",
                "Iterative algorithms",
                "Image databases",
                "Solid modeling",
                "Geometry",
                "Performance evaluation",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation",
                "interpolation",
                "iterative methods",
                "learning (artificial intelligence)",
                "object detection",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object class detection",
                "3D object class localization",
                "viewpoint invariant reference frame",
                "image representation",
                "iterative learning algorithm",
                "geometrical interpretation"
            ]
        },
        "id": 288,
        "cited_by": [
            {
                "year": "2013",
                "id": 355
            }
        ]
    },
    {
        "title": "3D object recognition from range images using pyramid matching",
        "authors": [
            "Xinju Li",
            "Igor Guskov"
        ],
        "abstract": "Recognition of 3D objects from different viewpoints is a difficult problem. In this paper, we propose a new method to recognize 3D range images by matching local surface descriptors. The input 3D surfaces are first converted into a set of local shape descriptors computed on surface patches defined by detected salient features. We compute the similarities between input 3D images by matching their descriptors with a pyramid kernel function. The similarity matrix of the images is used to train for classification using SVM, and new images can be recognized by comparing with the training set. The approach is evaluated on both synthetic and real 3D data with complex shapes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408829",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 18,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Kernel",
                "Image recognition",
                "Shape",
                "Computer vision",
                "Histograms",
                "Rough surfaces",
                "Surface roughness",
                "Spatial databases",
                "Image converters"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "learning (artificial intelligence)",
                "object recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object recognition",
                "3D range images",
                "local shape descriptors",
                "pyramid kernel function",
                "SVM",
                "training set"
            ],
            "Author Keywords": [
                "3D object recognition",
                "pyramid kernel function",
                "feature pairs",
                "surface descriptor"
            ]
        },
        "id": 289,
        "cited_by": []
    },
    {
        "title": "Variable Dimensional Local Shape Descriptors for Object Recognition in Range Data",
        "authors": [
            "Babak Taati",
            "Michel Bondy",
            "Piotr Jasiobedzki",
            "Michael Greenspan"
        ],
        "abstract": "We propose a new set of highly descriptive local shape descriptors (LSDs) for model-based object recognition and pose determination in input range data. Object recognition is performed in three phases: point matching, where point correspondences are established between range data and the complete model using local shape descriptors; pose recovery, where a computationally robust algorithm generates a rough alignment between the model and its instance in the scene, if such an instance is present; and pose refinement. While previously developed LSDs take a minimalist approach, in that they try to construct low dimensional and compact descriptors, we use high (up to 9) dimensional descriptors as the key to more accurate and robust point correspondence. Our strategy significantly simplifies the computational burden of the pose recovery phase by investing more time in the point matching phase. Experiments with Lidar and dense stereo range data illustrate the effectiveness of the approach by providing a higher percentage of correct matches in the candidate point matches list than a leading minimalist technique. Consequently, the number of RANSAC iterations required for recognition and pose determination is drastically smaller in our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408830",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 8,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Object recognition",
                "Layout",
                "Robustness",
                "Iterative algorithms",
                "Histograms",
                "Laser radar",
                "Solid modeling",
                "Clouds",
                "Bonding"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variable dimensional local shape descriptors",
                "model-based object recognition",
                "range data",
                "pose determination",
                "point matching",
                "pose recovery",
                "pose refinement",
                "robust point correspondence",
                "Lidar",
                "dense stereo range data",
                "RANSAC iterations"
            ]
        },
        "id": 290,
        "cited_by": []
    },
    {
        "title": "Learning 3-D Scene Structure from a Single Still Image",
        "authors": [
            "Ashutosh Saxena",
            "Min Sun",
            "Andrew Y. Ng"
        ],
        "abstract": "We consider the problem of estimating detailed 3D structure from a single still image of an unstructured environment. Our goal is to create 3D models which are both quantitatively accurate as well as visually pleasing. For each small homogeneous patch in the image, we use a Markov random field (MRF) to infer a set of \"plane parameters\" that capture both the 3D location and 3D orientation of the patch. The MRF, trained via supervised learning, models both image depth cues as well as the relationships between different parts of the image. Inference in our model is tractable, and requires only solving a convex optimization problem. Other than assuming that the environment is made up of a number of small planes, our model makes no explicit assumptions about the structure of the scene; this enables the algorithm to capture much more detailed 3D structure than does prior art (such as Saxena et ah, 2005, Delage et ah, 2005, and Hoiem et el, 2005), and also give a much richer experience in the 3D flythroughs created using image-based rendering, even for scenes with significant non-vertical structure. Using this approach, we have created qualitatively correct 3D models for 64.9% of 588 images downloaded from the Internet, as compared to Hoiem et al.'s performance of 33.1%. Further, our models are quantitatively more accurate than either Saxena et al. or Hoiem et al.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408828",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 53,
            "other": 32,
            "total": 85
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Markov random fields",
                "Supervised learning",
                "Sun",
                "Computer science",
                "Inference algorithms",
                "Art",
                "Rendering (computer graphics)",
                "Internet",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image processing",
                "learning (artificial intelligence)",
                "Markov processes",
                "random processes",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D scene structure",
                "single still image",
                "Markov random field",
                "supervised learning",
                "convex optimization problem",
                "image-based rendering"
            ]
        },
        "id": 291,
        "cited_by": [
            {
                "year": "2017",
                "id": 278
            },
            {
                "year": "2015",
                "id": 382
            },
            {
                "year": "2009",
                "id": 17
            },
            {
                "year": "2009",
                "id": 71
            },
            {
                "year": "2007",
                "id": 388
            }
        ]
    },
    {
        "title": "Depth-From-Recognition: Inferring Meta-data by Cognitive Feedback",
        "authors": [
            "Alexander Thomas",
            "Vittorio Ferrari",
            "Bastian Leibe",
            "Tinne Tuytelaars",
            "Luc Van Gool"
        ],
        "abstract": "Thanks to recent progress in category-level object recognition, we have now come to a point where these techniques have gained sufficient maturity and accuracy to succesfully feed back their output to other processes. This is what we refer to as cognitive feedback. In this paper, we study one particular form of cognitive feedback, where the ability to recognize objects of a given category is exploited to infer meta-data such as depth cues, 3D points, or object decomposition in images of previously unseen object instances. Our approach builds on the implicit shape model of Leibe and Schiele, and extends it to transfer annotations from training images to test images. Experimental results validate the viability of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408831",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 7,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Feedback",
                "Layout",
                "Shape",
                "Buildings",
                "Object recognition",
                "Image recognition",
                "Testing",
                "Object detection",
                "Feeds",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth-from-recognition",
                "metadata",
                "cognitive feedback",
                "category-level object recognition",
                "Leibe-Schiele implicit shape model"
            ]
        },
        "id": 292,
        "cited_by": []
    },
    {
        "title": "A Scene Representation Based on Multi-Modal 2D and 3D Features",
        "authors": [
            "Emre Baseski",
            "Nicolas Pugeault",
            "Sinan Kalkan",
            "Dirk Kraft",
            "Florentin Worgotter",
            "Norbert Kruger"
        ],
        "abstract": "Visually extracted 2D and 3D information have their own advantages and disadvantages that complement each other. Therefore, it is important to be able to switch between the different dimensions according to the requirements of the problem and use them together to combine the reliability of 2D information with the richness of 3D information. In this article, we use 2D and 3D information in a feature- based vision system and demonstrate their complementary properties on different applications (namely: depth prediction, scene interpretation, grasping from vision and object learning)1.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408836",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Data mining",
                "Uncertainty",
                "Switches",
                "Machine vision",
                "Image reconstruction",
                "Humans",
                "Object recognition",
                "Solid modeling",
                "Orbital robotics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene representation",
                "multi-modal 2D feature",
                "multi-modal 3D feature",
                "depth prediction",
                "scene interpretation",
                "object learning"
            ]
        },
        "id": 293,
        "cited_by": []
    },
    {
        "title": "3D Object Representation Using Transform and Scale Invariant 3D Features",
        "authors": [
            "Erdem Akagunduz",
            "Ilkay Ulusoy"
        ],
        "abstract": "An algorithm is proposed for 3D object representation using generic 3D features which are transformation and scale invariant. Descriptive 3D features and their relations are used to construct a graphical model for the object which is later trained and then used for detection purposes. Descriptive 3D features are the fundamental structures which are extracted from the surface of the 3D scanner output. This surface is described by mean and Gaussian curvature values at every data point at various scales and a scale-space search is performed in order to extract the fundamental structures and to estimate the location and the scale of each fundamental structure.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408835",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 9,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Face detection",
                "Data mining",
                "Iterative closest point algorithm",
                "Nose",
                "Layout",
                "Shape",
                "Graphical models",
                "Object recognition",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Gaussian processes",
                "image representation",
                "object detection",
                "object recognition",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object representation",
                "3D transform",
                "scale invariant 3D feature extraction",
                "object detection",
                "Gaussian curvature",
                "object recognition"
            ]
        },
        "id": 294,
        "cited_by": []
    },
    {
        "title": "A 3D Teacher for Car Detection in Aerial Images",
        "authors": [
            "Stefan Kluckner",
            "Georg Pacher",
            "Helmut Grabner",
            "Horst Bischof",
            "Joachim Bauer"
        ],
        "abstract": "This paper demonstrates how to reduce the hand labeling effort considerably by 3D information in an object detection task. In particular, we demonstrate how an efficient car detector for aerial images with minimal hand labeling effort can be build. We use an on-line boosting algorithm to incrementally improve the detection results. Initially, we train the classifier with a single positive (car) example, randomly drawn from a fixed number of given samples. When applying this detector to an image we obtain many false positive detections. We use information from a stereo matcher to detect some of these false positives (e.g. detected cars on a facade) and feed back this information to the classifier as negative updates. This improves the detector considerably, thus reducing the number of false positives. We show that we obtain similar results to hand labeling by iteratively applying this strategy. The performance of our algorithm is demonstrated on digital aerial images of urban environments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4408834",
        "reference_list": [
            {
                "year": "2003",
                "id": 83
            },
            {
                "year": "2001",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 9,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Detectors",
                "Object detection",
                "Face detection",
                "Labeling",
                "Stereo vision",
                "Machine learning algorithms",
                "Computer graphics",
                "Feeds",
                "Iterative algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object detection",
                "road vehicles",
                "stereo image processing",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D teacher",
                "car detection",
                "aerial image",
                "object detection",
                "hand labeling",
                "online boosting algorithm",
                "stereo matching"
            ]
        },
        "id": 295,
        "cited_by": []
    },
    {
        "title": "Discrete camera calibration from the information distance between pixel streams",
        "authors": [
            "Etienne Grossmann",
            "Francesco Orabona",
            "Jose Antonio Gaspar"
        ],
        "abstract": "We consider the problem of estimating the relative orientation of a number of individual photocells-or pixels-that hold fixed relative positions. The photocells measure the intensity of on a pencil of lines. We assume that the light-field thus sampled is changing, e.g. as the result of motion of the sensors and use the obtained measurements to estimate the orientations of the photocells. We explore an information-theoretic and geometric approach: based on real-world data, we build anon-parametric functional relation linking the information distance between the data streams of two photocells, and the angular separation between the photocells. Then, given data streams produced by arrays of pixels in similar conditions, we use the functional relation to estimate the angles between pixels. Finally, we embed the estimated angles in the unit 3D sphere to obtain the estimated layout of the array.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409189",
        "reference_list": [
            {
                "year": "2005",
                "id": 15
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Computer vision",
                "Optical distortion",
                "Biosensors",
                "Sensor arrays",
                "Biology computing",
                "Computational geometry",
                "Pixel",
                "Geometrical optics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image resolution",
                "image sensors",
                "photoelectric cells"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discrete camera calibration",
                "pixel streams",
                "photocells",
                "light intensity"
            ]
        },
        "id": 296,
        "cited_by": []
    },
    {
        "title": "Two Minimal Problems for Cameras with Radial Distortion",
        "authors": [
            "Zuzana Kukelova",
            "Tomas Pajdla"
        ],
        "abstract": "Epipolar geometry and relative camera pose computation for uncalibrated cameras with radial distortion has recently been formulated as a minimal problem and successfully solved in floating point arithmetics. The singularity of the fundamental matrix has been used to reduce the minimal number of points to eight. It was assumed that the cameras were not calibrated but had same distortions. In this paper we formulate two new minimal problems for estimating epipolar geometry of cameras with radial distortion. First we present a minimal algorithm for partially calibrated cameras with same radial distortion. Using the trace constraint which holds for the epipolar geometry of calibrated cameras to reduce the number of necessary points from eight to six. We demonstrate that the problem is solvable in exact rational arithmetics. Secondly, we present a minimal algorithm for uncalibrated cameras with different radial distortions. We show that the problem can be solved using nine points in two views by manipulating polynomials by a sequence of Gauss-Jordan eliminations in exact rational arithmetics. We demonstrate the algorithms on synthetic and real data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409190",
        "reference_list": [
            {
                "year": "2005",
                "id": 80
            },
            {
                "year": "2005",
                "id": 239
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 6,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Equations",
                "Polynomials",
                "Floating-point arithmetic",
                "Computational geometry",
                "Gaussian processes",
                "Pixel",
                "Cybernetics",
                "Digital cameras",
                "Calibration",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "distortion",
                "geometry",
                "image processing",
                "polynomials"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "radial distortion",
                "epipolar geometry",
                "camera pose computation",
                "trace constraint",
                "calibrated camera",
                "polynomials",
                "Gauss-Jordan elimination",
                "rational arithmetics"
            ]
        },
        "id": 297,
        "cited_by": []
    },
    {
        "title": "Estimation of the Epipole using Optical Flow at Antipodal Points",
        "authors": [
            "John Lim",
            "Nick Barnes"
        ],
        "abstract": "This paper develops an algorithm for estimating the epipole or direction of translation of a moving monocular observer. To this end, we use constraints arising from two points that are antipodal on the image sphere. The antipodal point condition is necessary for decoupling rotation from translation. One such pair of points constrains the epipole to lie on a plane, and using two pairs of points, we have two such planes. The intersection of these two planes gives an estimate of the epipole. This means we require image motion measurements at two pairs of antipodal points to obtain an estimate. Repeating this will yield a set of possible solutions and a variety of methods could be applied to obtain a robust and refined estimate from this set. One robust and simple method is chosen for illustrative purposes and results on real images are shown. With real sequences, results of below 2deg error in the estimate of the epipole can be obtained. Since antipodal points on an image sphere are required, this algorithm must use some kind of omnidirectional or large field-of-view (FOV) sensor.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409191",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 5,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Cameras",
                "Optical sensors",
                "Motion estimation",
                "Nonlinear optics",
                "Geometrical optics",
                "Motion measurement",
                "Robustness",
                "Layout",
                "Yield estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "epipole estimation",
                "optical flow",
                "antipodal points",
                "moving monocular observer",
                "image motion measurements"
            ]
        },
        "id": 298,
        "cited_by": []
    },
    {},
    {
        "title": "Non-additive Approach for Omnidirectional Image Gradient Estimation",
        "authors": [
            "Florence Jacquey",
            "Frederic Comby",
            "Olivier Strauss"
        ],
        "abstract": "The way catadioptric images are acquired implies that they present radial distortions. Therefore, classical processing may not be suitable. This statement will be illustrated by considering edge detection matter. Classical edge detectors usually consist in three steps : gradient computation, maximization and thresholding. The two lasts steps use pixels neighborhood concept. On the opposite of perspective images where pixel neighborhood is intuitive, catadioptric images present radial resolution changes. Then, the size and shape of pixel neighborhood have to be depending on pixel location. This article presents a new gradient estimation approach based on non-additive kernels. This technique is adapted to catadioptric images and also provides a natural threshold discarding the arbitrary thresholding step.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409193",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 4,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Mirrors",
                "Kernel",
                "Interpolation",
                "Geometry",
                "Image resolution",
                "Image processing",
                "Pixel",
                "Robot vision systems",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "edge detection",
                "gradient methods",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "omnidirectional image gradient estimation",
                "catadioptric images",
                "edge detection",
                "radial resolution changes",
                "nonadditive kernels",
                "thresholding step",
                "maximization",
                "omnidirectional vision"
            ]
        },
        "id": 300,
        "cited_by": []
    },
    {
        "title": "Omnidirectional Cameras as Backing-Up Aid",
        "authors": [
            "Tobias Ehlgen",
            "Markus Thom",
            "Markus Glaser"
        ],
        "abstract": "Omnidirectional cameras are well suited for maneuvering tasks due to to their large field of view. We mounted two catadioptric cameras on a vehicle and provide the driver with a bird's-eye view of the surrounding area behind the vehicle as well as the area on the left and right hand side. In order to enlarge the field of view of the bird's-eye view image, we will show an extension to the bird's-eye view image that provides a larger field of view and thus simplifies challenging maneuvering tasks. An overlay onto the resulting image shows the motion path of the vehicle while reversing. This motion path is connected to the steering angle. The driver can easily see where the vehicle will move when the steering angle is not changed throughout the movement.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409194",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 2,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Vehicle detection",
                "Vehicle driving",
                "Mirrors",
                "Image edge detection",
                "Face detection",
                "Control systems",
                "Sensor systems",
                "Acoustic devices",
                "Concrete"
            ],
            "INSPEC: Controlled Indexing": [
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "omnidirectional cameras",
                "backing-up aid",
                "catadioptric cameras",
                "vehicle motion path"
            ]
        },
        "id": 301,
        "cited_by": []
    },
    {
        "title": "Methods for space line localization from single catadioptric images: new proposals and comparisons",
        "authors": [
            "Vincenzo Caglioti",
            "Simone Gasparini",
            "Pierluigi Taddei"
        ],
        "abstract": "Line localization from a single image of a central camera is an ill-posed problem unless other constraints or apriori knowledge are exploited. Recently, it has been proved that noncentral catadioptric cameras allow space lines to be localized from a single image. In this paper we propose two novel localization algorithms. The first method exploits a pair of coplanar viewing rays to localize the space line. The second method follows a constrained non-linear minimization procedure using a suitable parametrization to represent space lines. We compare the accuracy of the proposed method w.r.t. the classical line localization algorithm and two robust variants of it. We carried out both synthetic and real experiments and evaluated the performance in localizing a set of space lines. We also propose a quality index for the viewing surfaces associated to space lines in order to better evaluate the quality of the localization. The experimental results showed the effectiveness and the accuracy of both proposed methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409195",
        "reference_list": [
            {
                "year": "2001",
                "id": 18
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Cameras",
                "Mirrors",
                "Robustness",
                "Image reconstruction",
                "Surface reconstruction",
                "Minimization methods",
                "Optical devices",
                "Manufacturing",
                "Reconstruction algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sensors",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "space line localization",
                "single catadioptric images",
                "noncentral catadioptric cameras",
                "coplanar viewing rays",
                "constrained nonlinear minimization",
                "quality index"
            ]
        },
        "id": 302,
        "cited_by": []
    },
    {
        "title": "A new Omnidirectional Stereovision Sensor",
        "authors": [
            "El Mustapha Mouaddib",
            "Gilles Dequen",
            "Laure Devendeville"
        ],
        "abstract": "This paper describes a new compact omnidirectional stereovision sensor that combines a single orthographic camera and four paraboloidal mirrors. Its geometry has been designed with the help of a stochastic optimization approach in order to minimize the 3D reconstruction error. In comparison with state-of-the-art sensors described in the literature, better results are obtained for this sensor during simulations. We will especially compare it with a classical configuration with two mirrors. Two criteria will be used: the 3D reconstruction accuracy and the field of view. We illustrate the advantages of our sensor within a framework of a simulation using a realistic environment and a ray-tracing software.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409196",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Mirrors",
                "Sensor phenomena and characterization",
                "Design optimization",
                "Calibration",
                "Optical sensors",
                "Lenses",
                "Robot vision systems",
                "Geometry",
                "Stochastic processes"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sensors",
                "optimisation",
                "stereo image processing",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "compact omnidirectional stereovision sensor",
                "orthographic camera",
                "paraboloidal mirrors",
                "stochastic optimization approach",
                "3D reconstruction error",
                "ray-tracing software"
            ]
        },
        "id": 303,
        "cited_by": []
    },
    {
        "title": "Linear solution for the pose estimation of noncentral catadioptric systems",
        "authors": [
            "Nuno Goncalves",
            "Helder Araujo"
        ],
        "abstract": "This paper presents a linear method to estimate the pose of a noncentral catadioptric system with a quadric shaped mirror in relation to a world reference frame (or local reference frame without loss of generality). The vision system is assumed to be calibrated. The method uses also as input data the structure of the scene. It is proved that any reflection point should belong to an analytical quadric that intersects the mirror quadric itself. This constraint can be written linearly in the 3D scene point coordinates (in the camera reference frame). The unknown pose screw transformation that relates camera and world reference frames can then be used in the linear model, allowing for the construction of a linear equation in the pose transformation elements. Additional constraints are used to force the estimated rotation elements to build an orthogonal matrix. Tests with simulated data and also on real images with different mirrors proved the method to be consistent and to estimate the pose accurately. However, it was also observed that the method is sensitive to noise. The results are compared with another method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409197",
        "reference_list": [
            {
                "year": "2001",
                "id": 117
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Mirrors",
                "Machine vision",
                "Robot kinematics",
                "Optical sensors",
                "Robot vision systems",
                "Polynomials",
                "Layout",
                "Optical reflection",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pose estimation",
                "noncentral catadioptric systems",
                "world reference frame",
                "vision system",
                "reflection point",
                "mirror quadric",
                "3D scene point coordinates",
                "camera reference frame",
                "linear equation",
                "pose transformation elements",
                "orthogonal matrix"
            ]
        },
        "id": 304,
        "cited_by": []
    },
    {
        "title": "Orientation and Pose recovery from Spherical Panoramas",
        "authors": [
            "Florian Kangni",
            "Robert Laganiere"
        ],
        "abstract": "This paper addresses the problem of camera pose recovery from spherical images. The 3D information is extracted from a set of panoramas sparsely distributed over a scene of interest. We present an algorithm to recover the position of omni-directional cameras in a scene using pair-wise essential matrices. First, all rotations with respect to the world frame are found using an incremental bundle adjustment procedure, thus achieving what we called cube alignment. The structure of the scene is then computed using a full bundle adjustment. During this step, the previously computed panorama orientations, used to feed the global optimization process, are further refined. Results are shown for indoor and outdoor panorama sets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409198",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Data mining",
                "Layout",
                "Navigation",
                "Global Positioning System",
                "Information technology",
                "Feeds",
                "Motion estimation",
                "Parameter estimation",
                "Interpolation"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "image sequences",
                "motion estimation",
                "optimisation",
                "video cameras"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera orientation recovery",
                "camera pose recovery",
                "spherical image panorama",
                "incremental bundle adjustment procedure",
                "cube alignment",
                "global optimization process",
                "image sequence",
                "motion parameter estimation"
            ]
        },
        "id": 305,
        "cited_by": []
    },
    {
        "title": "Catadioptric Image-based Rendering for Mobile Robot Localization",
        "authors": [
            "Hynek Bakstein",
            "Ales Leonardis"
        ],
        "abstract": "We present an approach to view-based mobile robot localization using a X-slits image based rendering (IBR) method for creating novel views from a set of input images. The input images are acquired by a non-central catadioptric sensor mounted on a robot moving on a straight line. We propose to use the IBR for column ordering only, where occlusions in the horizontal direction are modeled and the sensor can be non-central. For the column matching between a query view at an unknown position and virtual views created by IBR, we use correlation of columns.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409199",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Rendering (computer graphics)",
                "Mobile robots",
                "Robot vision systems",
                "Cameras",
                "Layout",
                "Robot sensing systems",
                "Robot localization",
                "Robustness",
                "Employment",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden feature removal",
                "image matching",
                "image sensors",
                "mobile robots",
                "rendering (computer graphics)",
                "robot vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "catadioptric x-slits image-based rendering",
                "view-based mobile robot localization",
                "catadioptric sensor",
                "horizontal direction occlusion",
                "column matching"
            ]
        },
        "id": 306,
        "cited_by": []
    },
    {
        "title": "Localizing Unordered Panoramic Images Using the Levenshtein Distance",
        "authors": [
            "Damien Michel",
            "Antonis A. Argyros",
            "Manolis I.A. Lourakis"
        ],
        "abstract": "This paper proposes a feature-based method for recovering the relative positions of the viewpoints of a set of panoramic images for which no a priori order information is available, along with certain structure information regarding the imaged environment. The proposed approach operates incrementally, employing the Levenshtein distance to deduce the spatial proximity of image viewpoints and thus determine the order in which images should be processed. The Levenshtein distance also provides matches between images, from which their underlying environment points can be recovered. Recovered points that are visible in multiple views permit the localization of more views which in turn allow the recovery of more points. The process repeats until all views have been localized. Periodic refinement of the reconstruction with the aid of bundle adjustment, distributes the reconstruction errors among images. The method is demonstrated on several unordered sets of panoramic images obtained in an indoor environment.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409200",
        "reference_list": [
            {
                "year": "2003",
                "id": 183
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Cameras",
                "Robot vision systems",
                "Mobile robots",
                "Computer science",
                "Indoor environments",
                "Motion estimation",
                "Image sequences",
                "Impedance matching",
                "Photometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "image reconstruction",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Levenshtein distance",
                "feature-based method",
                "image recovery",
                "image matching",
                "image reconstruction",
                "unordered panoramic image localization",
                "image registration",
                "computer vision"
            ]
        },
        "id": 307,
        "cited_by": []
    },
    {
        "title": "Performance Analysis and Validation of a Paracatadioptric Omnistereo System",
        "authors": [
            "Xiaojin Gong",
            "Anbumani Subramanian",
            "Christopher L. Wyatt",
            "Daniel J. Stilwell"
        ],
        "abstract": "In this paper we present a vector-based 3D localization formula for a paracatadioptric omnistereo system. Based on vector representation, the performance of this stereo system is analyzed numerically, including the maximum detectable range and the uncertainty of 3D localization, with respect to the flexible stereo configuration of the system, positions of scene points, as well as errors in correspondence matching and errors in stereo configuration. The results of performance analysis are used to guide the trajectory of an autonomous surface vehicle (ASV), which is equipped with a paracatadioptric omnidirectional camera, in a map building application.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409201",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Performance analysis",
                "Cameras",
                "Uncertainty",
                "Layout",
                "Navigation",
                "Remotely operated vehicles",
                "Mobile robots",
                "Humans",
                "Estimation error",
                "Assembly"
            ],
            "INSPEC: Controlled Indexing": [
                "error analysis",
                "image representation",
                "stereo image processing",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "performance analysis",
                "paracatadioptric omnistereo system",
                "vector representation",
                "stereo configuration",
                "autonomous surface vehicle",
                "paracatadioptric omnidirectional camera",
                "map building application"
            ]
        },
        "id": 308,
        "cited_by": []
    },
    {
        "title": "Depth Map Regeneration via Improved Graph Cuts Using a Novel Omnidirectional Stereo Sensor",
        "authors": [
            "Lei He",
            "Chuanjiang Luo",
            "Feng Zhu",
            "Yingming Hao",
            "Jinjun Ou",
            "Jing Zhou"
        ],
        "abstract": "An integrated framework mainly focusing on stereo matching has been presented in this paper to obtain dense depth maps for a mobile robot that is equipped with a novel omnidirectional stereo vision sensor that is designed to obtain height information. The vision sensor is composed of a common perspective camera and two hyperbolic mirrors, which are separately fixed inside a glass cylinder. As the separation between the two mirrors provides much enlarged baseline, the precision of the system has improved correspondingly. Nevertheless, the large disparity space and image particularities that are different from general stereo vision system result in poor performance using common methods. To satisfy the reliability requirement by mobile robot navigation, we use improved graph cuts method, in which more appropriate three-variable smootheness model is proposed for general priors corresponding to more reasonable piecewise smoothness assumption since the well-known swap move algorithm can be applied to a wider class of functions. We also show the necessary modification to handle panoramic images, including deformed matching template, adaptable template scale. Experiment shows that this proposed vision system is feasible as a practical stereo sensor for accurate depth map generation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409202",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Mirrors",
                "Cameras",
                "Stereo vision",
                "Mobile robots",
                "Robot vision systems",
                "Coaxial components",
                "Layout",
                "Glass",
                "Navigation",
                "Machine vision"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "graph theory",
                "image matching",
                "mobile robots",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth map regeneration",
                "stereo matching",
                "omnidirectional stereo vision sensor",
                "mobile robot navigation",
                "graph cut method",
                "swap move algorithm",
                "panoramic image",
                "deformed matching template",
                "adaptable template scale",
                "hyperbolic mirrors",
                "cameras"
            ]
        },
        "id": 309,
        "cited_by": []
    },
    {
        "title": "Learning Higher-order Transition Models in Medium-scale Camera Networks",
        "authors": [
            "Ryan Farrell",
            "David Doermann",
            "Larry S. Davis"
        ],
        "abstract": "We present a Bayesian framework for learning higher- order transition models in video surveillance networks. Such higher-order models describe object movement between cameras in the network and have a greater predictive power for multi-camera tracking than camera adjacency alone. These models also provide inherent resilience to camera failure, filling in gaps left by single or even multiple non-adjacent camera failures. Our approach to estimating higher-order transition models relies on the accurate assignment of camera observations to the underlying trajectories of objects moving through the network. We addresses this data association problem by gathering the observations and evaluating alternative partitions of the observation set into individual object trajectories. Searching the complete partition space is intractable, so an incremental approach is taken, iteratively adding observations and pruning unlikely partitions. Partition likelihood is determined by the evaluation of a probabilistic graphical model. When the algorithm has considered all observations, the most likely (MAP) partition is taken as the true object trajectories. From these recovered trajectories, the higher-order statistics we seek can be derived and employed for tracking. The partitioning algorithm we present is parallel in nature and can be readily extended to distributed computation in medium-scale smart camera networks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409203",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Partitioning algorithms",
                "Trajectory",
                "Bayesian methods",
                "Video surveillance",
                "Predictive models",
                "Tracking",
                "Resilience",
                "Filling",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "higher order statistics",
                "image fusion",
                "iterative methods",
                "learning (artificial intelligence)",
                "optical tracking",
                "probability",
                "video cameras",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "higher-order transition model learning",
                "medium-scale camera network",
                "Bayesian framework",
                "video surveillance network",
                "object movement",
                "multicamera tracking",
                "most likely partition likelihood",
                "incremental approach",
                "probabilistic graphical model",
                "higher-order statistics",
                "data association problem"
            ]
        },
        "id": 310,
        "cited_by": []
    },
    {
        "title": "Flexible Mirror Imaging",
        "authors": [
            "Sujit Kuthirummal",
            "Shree K. Nayar"
        ],
        "abstract": "The field of view of a traditional camera has a fixed shape. This severely restricts how scene elements can be composed into an image. We present a novel imaging system that uses a flexible mirror in conjunction with a camera to overcome this limitation. By deforming the mirror, our system can produce fields of view with a wide range of shapes and sizes. A captured image is typically a multi-perspective view of the scene with spatially varying resolution. As a result, scene objects appear distorted. To minimize these distortions, we have developed an efficient algorithm that maps a captured image to one with almost uniform resolution. To determine this mapping we need to know the shape of the mirror. For this, we have developed a simple calibration method that automatically estimates the mirror shape from its boundary, which is visible in the captured image. We present a number of examples that demonstrate that a flexible field of view imaging system can be used to compose scenes in ways that have not been possible before. This flexibility can be exploited in applications such as video surveillance and monitoring.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409204",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Mirrors",
                "Shape",
                "Layout",
                "Cameras",
                "Spatial resolution",
                "Image resolution",
                "Calibration",
                "Optical imaging",
                "Video surveillance",
                "Monitoring"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "flexible mirror imaging",
                "traditional camera",
                "imaging system",
                "spatially varying resolution",
                "captured image",
                "uniform resolution",
                "calibration"
            ]
        },
        "id": 311,
        "cited_by": []
    },
    {
        "title": "Focus in Catadioptric Imaging Systems",
        "authors": [
            "Rahul Swaminathan"
        ],
        "abstract": "Catadioptric imaging systems consisting of conventional lens based cameras and mirrors are widely used in many vision applications. Most of the prior work in this area has focused either on aspects of mirror design or the application it is being used for. In contrast, little attention has been paid to the aspect of sharp image formation. In this paper we study how reflections in curved mirrors affect image formation from the perspective of lens focus. In particular we answer the question \"How and where must the lens focus when imaging reflections in curved mirrors?\". Using caustics to model the reflections, we show that the space of infinite scene depth is compressed within a finite volume which we call the \"caustic volume\". Analysis of the caustic volume is presented for common catadioptric systems. We also present a framework to derive the \"optimal\" focal plane at which to focus the lens for sharp image formation along with experimental verification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409205",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Focusing",
                "Mirrors",
                "Lenses",
                "Optical imaging",
                "Layout",
                "Optical reflection",
                "Image analysis",
                "Cameras",
                "Image quality",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "catadioptric imaging system",
                "lens based camera",
                "sharp image formation",
                "curved mirrors",
                "lens focus",
                "imaging reflection",
                "infinite scene depth",
                "caustic volume"
            ]
        },
        "id": 312,
        "cited_by": [
            {
                "year": "2015",
                "id": 391
            }
        ]
    },
    {
        "title": "Position and radius of spheres from single off-axis catadioptric images",
        "authors": [
            "Vincenzo Caglioti",
            "Simone Gasparini"
        ],
        "abstract": "In this paper we address the problem of sphere localization from a single image taken with a noncentral catadioptric camera. We propose a method for determining both the radius and the position of an unknown sphere from a single, catadioptric image. The method can find its application in the field of robotic vision, especially in mobile robots playing soccer in RoboCup contests, in order to improve robot capabilities related to playing with a flying ball. Recently, a method for sphere reconstruction from single image taken with a noncentral, axial-symmetric, catadioptric camera has been proposed. In an axial symmetric catadioptric cameras, the pinhole of the camera is placed on the mirror axis. Though axial symmetric cameras help to simplify the geometrical treatment of the problem, they are difficult to set up since they require a precise alignment, usually hard to check. In this paper we deal with the general case of off-axis catadioptric cameras, with the camera pinhole placed in a general position w.r.t. the mirror. We devise a simple geometrical method by which we determine both the position of a sphere and its radius from its apparent image contour. Since our approach is based on coplanar viewing rays, it has a wider applicability w.r.t. the previous method, as it relaxes the constraint on camera position, i.e. it does not require a precise alignment, and the constraint on mirror axial symmetry, i.e. it can be applied to a wider class of mirrors. Some preliminary experiments both on simulated and real image are also presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409206",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Robot vision systems",
                "Mobile robots",
                "Mirrors",
                "Information analysis",
                "Data mining",
                "Image reconstruction",
                "Object detection",
                "Indexing",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "geometry",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sphere localization",
                "noncentral catadioptric camera",
                "catadioptric image",
                "off-axis catadioptric cameras",
                "camera pinhole",
                "geometrical method",
                "image contour",
                "coplanar viewing rays"
            ]
        },
        "id": 313,
        "cited_by": []
    },
    {
        "title": "A Unifying Omnidirectional Camera Model and its Applications",
        "authors": [
            "Christian Toepfer",
            "Tobias Ehlgen"
        ],
        "abstract": "Omnidirectional cameras are leaving the scientific labs to be used in market applications. Many such application areas like robotics and automotive need very accurate models of the image formation. When a car or robot motion is controlled by an omnidirectional camera system, the exact control of action depends on a reliable calibration. An unprecise calibration can cause costly or hazardous consequences for man and machine. Therefore, we developed an omnidirectional camera model that can deal with parabolic and hyperbolic mirrors in combination with distorting lenses. It has the adequate complexity to allow both, precision and robustness of the calibration process. We successfully applied this model to our driver assistance systems of future light and heavy trucks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409207",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Robot vision systems",
                "Calibration",
                "Motion control",
                "Control systems",
                "Automotive engineering",
                "Robot motion",
                "Robot control",
                "Mirrors",
                "Lenses"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image sensors",
                "mirrors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unifying omnidirectional camera model",
                "image formation",
                "reliable calibration",
                "parabolic mirrors",
                "hyperbolic mirrors",
                "distorting lenses",
                "driver assistance systems"
            ]
        },
        "id": 314,
        "cited_by": []
    },
    {
        "title": "Rectangle Extraction in Catadioptric Images",
        "authors": [
            "Jean-Charles Bazin",
            "Inso Kweon",
            "Cedric Demonceaux",
            "Pascal Vasseur"
        ],
        "abstract": "Nowadays, robotic systems are more and more equipped with catadioptric cameras. However several problems associated to catadioptric vision have been studied only slightly. Especially algorithms for detecting rectangles in catadioptric images have not yet been developed whereas it is required in diverse applications such as building extraction in aerial images. We show that working in the equivalent sphere provides an appropriate framework to detect lines, parallelism, orthogonality and therefore rectangles. Finally, we present experimental results on synthesized and real data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409208",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Mirrors",
                "Robot vision systems",
                "Data mining",
                "Lenses",
                "Focusing",
                "Image sensors",
                "Image edge detection",
                "Image enhancement",
                "Face detection"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "robot vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "catadioptric image extraction",
                "robotic systems",
                "catadioptric cameras",
                "catadioptric vision",
                "building extraction"
            ]
        },
        "id": 315,
        "cited_by": []
    },
    {
        "title": "Circle-Marker Detection Method for Omnidirectional Images and its Application to Robot Positioning",
        "authors": [
            "Yoshihiko Mochizuki",
            "Atsushi Imiya",
            "Akihiko Torii"
        ],
        "abstract": "Introduction In this paper, we develop an algorithm for the detection of circles from an image captured by a monocular omnidirectional camera system. We assume that an image captured by an omnidirectional camera system is normalised to a spherical image, the image on the unit sphere. Using this geometrical property of the omnidirectional images, we introduce a method for marker-based positioning and navigation of autonomous mobile robots which mounts a monocular omnidirectional camera system. We first clarify the geometric properties of the spherical image of a circle marker placed on the ground plane, and we show that, for the detection of a plane from circle markers, we are required to capture at least two coplanar circle markers. We prove that the image of a circle on the spherical image is a fourth-order algebraic curve, which is the intersection of a sphere and an oblique elliptic cone. For the detection of marker images on the spherical images, we introduce a method for transforming the detection of this fourth-order algebraic curve to the detection of a spatial conic, which is quadric. Second, we develop a voting method for the extraction of images of planar circle markers on a spherical image, using the spatial-quadric detection strategy. Finally, using the assumption for the geometrical configuration of the camera system and the circle markers on the plane on which the robot moves, we derive a positioning algorithm. This positioning method for the robot mounting a monocular omnidirectional camera system allows us to navigate a robot using our circle-detection algorithm. We show some numerical examples both for synthetic and real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409209",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Robot vision systems",
                "Navigation",
                "Mobile robots",
                "Voting",
                "Equations",
                "Reconstruction algorithms",
                "Visual system",
                "Geometrical optics",
                "Optical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "algebra",
                "image sensors",
                "mobile robots",
                "position control"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "circle-marker detection method",
                "omnidirectional images",
                "robot positioning",
                "monocular omnidirectional camera system",
                "spherical image",
                "geometrical property",
                "marker-based positioning",
                "autonomous mobile robots",
                "coplanar circle markers",
                "fourth-order algebraic curve",
                "oblique elliptic cone",
                "spatial conic",
                "spatial-quadric detection"
            ]
        },
        "id": 316,
        "cited_by": []
    },
    {
        "title": "Contrast Enhancement from Multiple Panoramic Images",
        "authors": [
            "Irene Cheng",
            "Anup Basu"
        ],
        "abstract": "In this work we discuss an efficient strategy for combining multiple panoramic scans of the same scene to create a single higher contrast image. Prior research papers mainly consider that precise exposure times for multiple images are known and create a new high dynamic range representation for each pixel. We simply consider multiple scans of the same scene where the exposure times are not known, and no special filters or image detectors are used in the image acquisition process. In this unrestricted scenario, the problem is how to select different parts of a scene from the \"best\" image among a collection of images in order to optimize the overall clarity or contrast of a single final image. Experimental results are shown, and steps to improve results using modified contrast measures for color images are described. The results are enhanced using morphological filtering and image blending.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409210",
        "reference_list": [
            {
                "year": "2001",
                "id": 0
            },
            {
                "year": "2003",
                "id": 153
            },
            {
                "year": "2001",
                "id": 104
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Dynamic range",
                "Histograms",
                "Filters",
                "Cameras",
                "Image quality",
                "Strips",
                "Information science",
                "Pixel",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image enhancement",
                "image representation",
                "image resolution",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contrast enhancement",
                "multiple panoramic images",
                "dynamic range representation",
                "multiple scans",
                "image acquisition process"
            ]
        },
        "id": 317,
        "cited_by": []
    },
    {
        "title": "Radon Transform and Harmonical Analysis Using Lines for 3D Rotation Estimation without Correpondences from Omnidirectional Vision",
        "authors": [
            "Luis Eduardo Falcon",
            "Eduardo Bayro-Corrochano"
        ],
        "abstract": "Because of their large field of view, omnidirectional images can assist with localization tasks to robotic navigation algorithms. Since the images taken by omnidirectional sensors can be mapped to the sphere, the problem of attitude estimation of a 3D camera rotation can be treated as a problem of estimating rotations between spherical images. Usually, this rotation estimation problem has been solved using point correspondences or gradient information of the points of the images, with the respective computational time consuming of those point algorithms. We present an effective solution to the attitude estimation problem using line information of the images through the Radon transform, because line algorithms are less time consuming that those using points. In the formulation of the Radon transform we include a similarity function on the cross product of two images which assigns a weight to all lines pairs, and where this similarity function is integrated over all lines pairs that satisfy a constraint for lines. That is, we formulate the problem to obtain the Euler angles of the 3D camera rotation as a correlation of functions defined on the product of spheres S2 times S2 which are acted upon by elements of the direct product groups of rotations SO(3) timesSO(3). Because of the spherical treatment of the data, our approach utilizes the spherical fourier transform and spherical harmonics to produce a solution in the Fourier domain.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409211",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Harmonic analysis",
                "Cameras",
                "Image analysis",
                "Robot vision systems",
                "Fourier transforms",
                "Algorithm design and analysis",
                "Robot sensing systems",
                "Navigation",
                "Image sensors",
                "Mirrors"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "Fourier transforms",
                "image processing",
                "Radon transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Radon transform",
                "harmonical analysis",
                "3D rotation estimation",
                "omnidirectional vision",
                "robotic navigation algorithms",
                "omnidirectional sensors",
                "attitude estimation",
                "3D camera rotation",
                "spherical images",
                "point correspondences",
                "gradient information",
                "line information",
                "Euler angles",
                "spherical treatment",
                "spherical Fourier transform"
            ]
        },
        "id": 318,
        "cited_by": []
    },
    {
        "title": "Registration of anatomical images using geodesic paths of diffeomorphisms parameterized with stationary vector fields",
        "authors": [
            "Monica Hernandez",
            "Matias N. Bossa",
            "Salvador Olmos"
        ],
        "abstract": "Computational Anatomy aims for the study of the statistical variability in anatomical structures. Variability is encoded by the transformations existing among populations of anatomical images. These transformations are usually computed from diffeomorphic registration based on the large deformation paradigm. In this framework diffeomorphisms are usually computed as end points of paths on the Riemannian manifold of diffeomorphisms parameterized by non-stationary vector fields. Recently, an alternative parameterization based on stationary vector fields has been developed. In this article we propose to use this stationary parameterization for diffeomorphic registration. We formulate the variational problem related to this registration scenario and derive the associated Euler-Lagrange equations. We evaluate the performance of the non-stationary vs the stationary parameterizations in real and synthetic 3D-MRI datasets. Compared to the non-stationary parameterization, our proposal provides similar accuracy in terms of image matching and deformation smoothness while drastically reducing memory and time requirements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409126",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Anatomy",
                "Image matching",
                "Geophysics computing",
                "Anatomical structure",
                "Equations",
                "Statistical analysis",
                "Communications technology",
                "Proposals",
                "Information theory",
                "Space stations"
            ]
        },
        "id": 319,
        "cited_by": []
    },
    {
        "title": "Robust Image Registration using Mixtures of t-distributions",
        "authors": [
            "Demetrios Gerogiannis",
            "Christophoros Nikou",
            "Aristidis Likas"
        ],
        "abstract": "We propose a pixel similarity-based algorithm enabling accurate rigid registration between single and multimodal images presenting gross dissimilarities due to noise, missing data or outlying measures. The method relies on the partitioning of a reference image by a Student's t-mixture model (SMM). This partition is then projected onto the image to be registered. The main idea is that a t-component in the reference image corresponds to a t-component in the image to be registered. If the images are correctly registered the weighted sum of distances between the corresponding components is minimized. The use of SMM components is justified by the property that they have heavier tails than standard Gaussians, thus providing robustness to outliers. Experimental results indicate that, even in the case of images presenting low SNR or important amount of dissimilarities due to temporal changes, the proposed algorithm compares favorably to the histogram-based mutual information method that is widely used in a variety of applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409127",
        "reference_list": [
            {
                "year": "2005",
                "id": 163
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Image registration",
                "Gaussian processes",
                "Biomedical imaging",
                "Pixel",
                "Mutual information",
                "Histograms",
                "Tail",
                "Image sequences",
                "X-ray imaging"
            ]
        },
        "id": 320,
        "cited_by": []
    },
    {
        "title": "Rectified Surface Mosaics",
        "authors": [
            "Robert E. Carroll",
            "Steven M. Seitz"
        ],
        "abstract": "We approach mosaicing as a camera tracking problem within a known parameterized surface. From a video of a camera moving within a surface, we compute a mosaic representing the texture of that surface, flattened onto a planar image. Our approach works by defining a warp between images as a function of surface geometry and camera pose. Globally optimizing this warp to maximize alignment across all frames determines the camera trajectory, and the corresponding flattened mosaic image. In contrast to previous mosaicing methods which assume planar or distant scenes, or controlled camera motion, our approach enables mosaicing in cases where the camera moves unpredictably through proximal surfaces, such as in medical endoscopy applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409128",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Endoscopes",
                "Surface texture",
                "Rendering (computer graphics)",
                "Computed tomography",
                "Geometry",
                "Biomedical imaging",
                "Biomedical optical imaging",
                "Optical distortion"
            ]
        },
        "id": 321,
        "cited_by": []
    },
    {
        "title": "From Uncertainties to Statistical Model Building and Segmentation of the Left Ventricle",
        "authors": [
            "Maxime Taron",
            "Nikos Paragios",
            "Marie-Pierre Jolly"
        ],
        "abstract": "Reliable segmentation of the left ventricle is a long sought objective in medical imaging for automatic retrieval of anatomical and pathological measurements and detection of malfunctions. In this paper, we propose a novel model-constrained approach to address this task. The method is based on an implicit representation of the shape model used in a shape registration framework with a Thin Plate Spline transform to retrieve possible deformations. The main innovation of our approach resides in the use of uncertainties defined on the registered shape to augment the training set and improve the robustness of the statistical deformable model. We use ICA to reduce the dimensionality of the space of deformations and provide a good separation of the different deformable parts of the heart. Furthermore the estimation of uncertainties is also introduced in the segmentation process which is addressed in a variational framework where prior knowledge and visual support are considered. The method lead to very promising qualitative and quantitative experimental results in CT.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409129",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Uncertainty",
                "Shape",
                "Image segmentation",
                "Deformable models",
                "Biomedical imaging",
                "Image retrieval",
                "Pathology",
                "Spline",
                "Technological innovation",
                "Robustness"
            ]
        },
        "id": 322,
        "cited_by": []
    },
    {
        "title": "3D Variational Brain Tumor Segmentation using a High Dimensional Feature Set",
        "authors": [
            "Dana Cobzas",
            "Neil Birkbeck",
            "Mark Schmidt",
            "Martin Jagersand",
            "Albert Murtha"
        ],
        "abstract": "Tumor segmentation from MRI data is an important but time consuming task performed manually by medical experts. Automating this process is challenging due to the high diversity in appearance of tumor tissue, among different patients and, in many cases, similarity between tumor and normal tissue. One other challenge is how to make use of prior information about the appearance of normal brain. In this paper we propose a variational brain tumor segmentation algorithm that extends current approaches from texture segmentation by using a high dimensional feature set calculated from MRI data and registered atlases. Using manually segmented data we learn a statistical model for tumor and normal tissue. We show that using a conditional model to discriminate between normal and abnormal regions significantly improves the segmentation results compared to traditional generative models. Validation is performed by testing the method on several cancer patient MRI scans.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409130",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 24,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Neoplasms",
                "Image segmentation",
                "Magnetic resonance imaging",
                "Biomedical imaging",
                "Level set",
                "Computer science",
                "Brain",
                "Data mining",
                "Layout",
                "Shape"
            ]
        },
        "id": 323,
        "cited_by": []
    },
    {
        "title": "Using the Pn Potts model with learning methods to segment live cell images",
        "authors": [
            "Christopher Russell",
            "Dimitris Metaxas",
            "Christophe Restif",
            "Philip Torr"
        ],
        "abstract": "We present a segmentation method for live cell images, using graph cuts and learning methods. The images used here are particularly challenging because of the shared grey-level distributions of cells and background, which only differ by their textures, and the local imprecision around cell borders. We use the P n Potts model recently presented by Kohli et al. [9]: functions on higher-order cliques of pixels are included into the traditional Potts model, allowing us to account for local texture features, and to find the optimal solution efficiently. We use learning methods to define the potential functions used in the P n Potts model. We present the model and the learning methods we used, and compare our segmentation results with similar work in cytometry. While our method performs similarly, it requires little manual tuning and thus is straightforward to adapt to other images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409131",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Learning systems",
                "Labeling",
                "Computer vision",
                "Graphical models",
                "Pixel",
                "Costs",
                "Microscopy",
                "Minimization methods",
                "Markov random fields"
            ]
        },
        "id": 324,
        "cited_by": []
    },
    {
        "title": "Implicit Meshing for Finite Element Methods using Levelsets",
        "authors": [
            "Theodore Papadopoulo",
            "Sylvain Vallaghe"
        ],
        "abstract": "Finite Element methods (FEM) usually require a mesh to describe the geometric domain on which the computations are occuring. These meshes must have several properties: 1) they must approximate the geometrical domain accurately, 2) they must have good numerical properties, and 3) they must be small enough so that the computations take a reasonable amount of time. These goals are somewhat contradictory and in many cases such as biomedical images - and particularly in the case of the head -, even though the geometric domains can effectively be extracted, eg from Magnetic Resonance Images (MRI), the generation of such meshes is quite difficult. This paper describes a technique that bypasses this mesh generation step going directly from a description by levelsets of the interfaces separating the various domains to the matrix associated to the FEM method. Using the levelsets description is quite convenient as it is already used by many segmentation tools. The technique is illustrated on spherical and realistic geometries for the Electroencephalography (EEG) direct problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409132",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Finite element methods",
                "Electroencephalography",
                "Biomedical computing",
                "Biomedical imaging",
                "Magnetic heads",
                "Magnetic resonance",
                "Magnetic resonance imaging",
                "Image generation",
                "Mesh generation",
                "Geometry"
            ]
        },
        "id": 325,
        "cited_by": []
    },
    {
        "title": "Bilinear Models for Spatio-Temporal Point Distribution Analysis: Application to Extrapolation of Whole Heart Cardiac Dynamics",
        "authors": [
            "Corne Hoogendoorn",
            "Federico M. Sukno",
            "Sebastian Ordas",
            "Alejandro F. Frangi"
        ],
        "abstract": "In this work we introduce the usage of bilinear models as a means of factorising the shape variation induced by subject variability and the contraction of the human heart. We show that it is feasible to reconstruct the shape of the heart at a certain point in the cardiac cycle if we are given a small number of shapes representing the same heart at different points in the same cycle, using the bilinear model. Depending on pathology and the ratios between healthy and pathological hearts in the training set, RMS reconstruction errors measured between 1.39 and 16.58 millimetres, with a median of 6.79 and 90th percentile of 9.95 millimetres.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409133",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Extrapolation",
                "Heart",
                "Principal component analysis",
                "Biomedical imaging",
                "Humans",
                "Image reconstruction",
                "Pathology",
                "Shape measurement",
                "Spatiotemporal phenomena",
                "Image motion analysis"
            ]
        },
        "id": 326,
        "cited_by": []
    },
    {
        "title": "A Novel Image Based Verification Method for Respiratory Motion Management in Radiation Therapy",
        "authors": [
            "Ali Khamene",
            "Christian Schaller",
            "Joachim Hornegger",
            "Juan Carlos Celi",
            "Barbara Ofstad",
            "Eike Rietzel",
            "X. Allen Li",
            "An Tai",
            "John Bayouth"
        ],
        "abstract": "Precise localization of moving targets is essential to increase local control of the cancer via dose escalation while reducing the severity of normal tissue complication. Localization of targets in real time with radio-opaque marker is less favorable considering the excess radiation dose to the patient and potential complications of implantation. Various external surrogates could provide indications of the targets' positions during the breathing process. However, there is a great deal of uncertainty in the correlation between external surrogates and internal target positions/trajectory during respiratory cycles. In order to address this problem, we have developed an algorithm that automatically establishes correspondences between the fluoroscopic sequence frames taken from the patient on the day of treatment and the various phases of a 4DCT planning data set. Image based mapping/synchronization procedure is performed using an underlying Markov model established for the breathing process. The mapping procedure is formulated as an optimization process and is solved efficiently using a dynamic programming technique. Results on the phantom, synthetic, and real patient data demonstrate the effectiveness of the proposed method in coping with respiratory correlation variations. The approach could primarily be used for automatic gating interval adaptation in the gated radiotherapy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409134",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical applications of radiation",
                "Neoplasms",
                "Biomedical imaging",
                "Oncology",
                "Educational institutions",
                "Medical treatment",
                "Cancer",
                "Visualization",
                "Planning",
                "Cities and towns"
            ]
        },
        "id": 327,
        "cited_by": []
    },
    {
        "title": "Nonrigid Intraoperative Cortical Surface Tracking Using Game Theory",
        "authors": [
            "Christine DeLorenzo",
            "Xenophon Papademetris",
            "Lawrence H. Staib",
            "Kenneth P. Vives",
            "Dennis D. Spencer",
            "James S. Duncan"
        ],
        "abstract": "During neurosurgery, nonrigid brain deformation prevents preoperatively acquired images from accurately depicting the intraoperative brain. Stereo vision systems can be used to track cortical surface deformation and update preoperative brain images in conjunction with a biomechanical model. However, these stereo systems are often plagued with calibration error, which can corrupt the deformation estimation. In order to decouple the effects of camera calibration and surface deformation, a framework is needed which can solve for disparate and often competing variables. Game theory, which was developed specifically to handle decision making in this type of competitive environment, has been applied to various fields from economics to biology. In this paper, we apply game theory to cortical surface tracking and use it to infer information about the physical processes of brain deformation and image acquisition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409135",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Game theory",
                "Calibration",
                "Cameras",
                "Neurosurgery",
                "Brain modeling",
                "Deformable models",
                "Decision making",
                "Environmental economics",
                "Biomedical engineering",
                "Radiology"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "medical image processing",
                "stereo image processing",
                "surgery"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonrigid intraoperative cortical surface tracking",
                "game theory",
                "neurosurgery",
                "nonrigid brain deformation",
                "intraoperative brain",
                "stereo vision systems",
                "cortical surface deformation",
                "brain images",
                "biomechanical model",
                "deformation estimation",
                "camera calibration",
                "decision making",
                "image acquisition"
            ]
        },
        "id": 328,
        "cited_by": []
    },
    {
        "title": "Measuring Cortical Thickness Using An Image Domain Local Surface Model And Topology Preserving Segmentation",
        "authors": [
            "Sandhitsu R. Das",
            "Brian B. Avants",
            "Murray Grossman",
            "James C. Gee"
        ],
        "abstract": "We present a measure of gray matter (GM) thickness based on local surface models in the image domain. Thickness is measured by integrating GM probability maps along the white matter (WM) surface normal direction. The method is simple to implement and allows statistical tests to be performed in the gray matter volume. A novel topology preserving segmentation method is introduced that is able to accurately recover GM in deep sulci. We apply this methodology to a longitudinal study of gray matter atrophy in a patient cohort diagnosed with frontotemporal dementia (FTD) spectrum disorders. Following image-based normalization of GM thickness maps, results show significant reduction in cortical thickness in several Brodmann areas spanning temporal, parietal and frontal lobes across subjects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409136",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Thickness measurement",
                "Topology",
                "Image segmentation",
                "Atrophy",
                "Surface morphology",
                "Neuroimaging",
                "Alzheimer's disease",
                "Volume measurement",
                "Time measurement",
                "Probability"
            ],
            "INSPEC: Controlled Indexing": [
                "diseases",
                "image segmentation",
                "medical image processing",
                "neurophysiology",
                "probability",
                "statistical testing",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cortical thickness measurement",
                "image domain local surface model",
                "topology preserving segmentation",
                "gray matter thickness",
                "local surface models",
                "GM probability maps",
                "white matter surface normal direction",
                "statistical tests",
                "patient cohort diagnosis",
                "frontotemporal dementia spectrum disorders",
                "image-based normalization",
                "GM thickness maps"
            ]
        },
        "id": 329,
        "cited_by": []
    },
    {
        "title": "Cortical Folding Development Study based on Over-Complete Spherical Wavelets",
        "authors": [
            "Peng Yu",
            "Boon Thye Thomas Yeo",
            "P. Ellen Grant",
            "Bruce Fischl",
            "Polina Golland"
        ],
        "abstract": "We introduce the use of over-complete spherical wavelets for shape analysis of 2D closed surfaces. Bi-orthogonal spherical wavelets have been shown to be powerful tools in the segmentation and shape analysis of 2D closed surfaces, but unfortunately they suffer from aliasing problems and are therefore not invariant under rotations of the underlying surface parameterization. In this paper, we demonstrate the theoretical advantage of over-complete wavelets over bi-orthogonal wavelets and illustrate their utility on both synthetic and real data. In particular, we show that over-complete spherical wavelets allow us to build more stable cortical folding development models, and detect a wider array of regions of folding development in a newborn dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409137",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 6,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface waves",
                "Wavelet transforms",
                "Wavelet analysis",
                "Wavelet coefficients",
                "Biomedical imaging",
                "Shape",
                "Image analysis",
                "Frequency",
                "Image segmentation",
                "Signal processing"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "medical image processing",
                "wavelet transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cortical folding development",
                "over-complete spherical wavelets",
                "shape analysis",
                "2D closed surface",
                "biorthogonal spherical wavelets",
                "surface segmentation",
                "surface parameterization"
            ]
        },
        "id": 330,
        "cited_by": []
    },
    {
        "title": "Detecting Cortical Surface Regions in Structural MR Data",
        "authors": [
            "Biswajit Bose",
            "John Fisher",
            "Bruce Fischl",
            "Oliver Hinds",
            "Eric Grimson"
        ],
        "abstract": "We present a novel level-set method for evolving open surfaces embedded in three-dimensional volumes. We adapt the method for statistical detection and segmentation of cytoarchitectonic regions of the cortical ribbon (e.g., Brodmann areas). In addition, we incorporate an explicit interface appearance model which is oriented normal to the open surface, allowing one to model characteristics beyond voxel intensities and high gradients. We show that such models are well suited to detecting embedded cortical structures. Appearance models of the interface are used in two ways: firstly, to evolve an open surface in the normal direction for the purpose of detecting the location of the surface, and secondly, to evolve the boundary of the surface in a direction tangential to the surface in order to delineate the extent of a specific Brodmann area within the cortical ribbon. The utility of the method is demonstrated on a challenging ex-vivo structural MR dataset for detection of Brodmann area 17.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409138",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cerebral cortex",
                "Computer science",
                "Artificial intelligence",
                "Laboratories",
                "Hospitals",
                "Medical diagnostic imaging",
                "Humans",
                "Medical diagnosis",
                "Brain modeling",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain",
                "medical image processing",
                "object detection",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cortical surface detection",
                "structural MR data",
                "level-set method",
                "statistical detection",
                "cytoarchitectonic region segmentation",
                "cortical ribbon",
                "Brodmann area"
            ]
        },
        "id": 331,
        "cited_by": []
    },
    {
        "title": "3D Topology Preserving Flows for Viewpoint-Based Cortical Unfolding",
        "authors": [
            "Kelvin Rocha",
            "Ganesh Sundaramoorthi",
            "Anthony Yezzi"
        ],
        "abstract": "We present a variational method for unfolding of the cortex based on a user-chosen point of view as an alternative to more traditional global flattening methods, which incur more distortion around the region of interest. Our approach involves two novel contributions. The first is an energy function and its corresponding gradient flow to measure the average visibility of a region of interest of a surface from a given viewpoint. The second is an additional energy function and flow designed to preserve the 3D topology of the evolving surface. This latter contribution receives significant focus in this paper as it is crucial to obtain the desired unfolding effect derived from the first energy functional and flow. Without it, the resulting topology changes render the unconstrained evolution uninteresting for the purpose of cortical visualization, exploration, and inspection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409139",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Topology",
                "Image segmentation",
                "Distortion measurement",
                "Energy measurement",
                "Fluid flow measurement",
                "Active contours",
                "Biomedical imaging",
                "Visualization",
                "Kelvin",
                "Focusing"
            ],
            "INSPEC: Controlled Indexing": [
                "biology computing",
                "brain",
                "data visualisation",
                "image segmentation",
                "stereo image processing",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D topology preserving flows",
                "viewpoint-based cortical unfolding",
                "variational method",
                "cortex unfolding",
                "gradient flow",
                "surface evolution",
                "unfolding effect",
                "first energy functional",
                "topology change",
                "cortical visualization",
                "cortical exploration",
                "cortical inspection"
            ]
        },
        "id": 332,
        "cited_by": []
    },
    {
        "title": "Diffusion Tensor Estimation by Maximizing Rician Likelihood",
        "authors": [
            "Bennett Landman",
            "Pierre-Louis Bazin",
            "Jerry Prince"
        ],
        "abstract": "Diffusion tensor imaging (DTI) is widely used to characterize white matter in health and disease. Previous approaches to the estimation of diffusion tensors have either been statistically suboptimal or have used Gaussian approximations of the underlying noise structure, which is Rician in reality. This can cause quantities derived from these tensors \u2014 e.g., fractional anisotropy and apparent diffusion coefficient \u2014 to diverge from their true values, potentially leading to artifactual changes that confound clinically significant ones. This paper presents a novel maximum likelihood approach to tensor estimation, denoted Diffusion Tensor Estimation by Maximizing Rician Likelihood (DTEMRL). In contrast to previous approaches, DTEMRL considers the joint distribution of all observed data in the context of an augmented tensor model to account for variable levels of Rician noise. To improve numeric stability and prevent non-physical solutions, DTEMRL incorporates a robust characterization of positive definite tensors and a new estimator of underlying noise variance. In simulated and clinical data, mean squared error metrics show consistent and significant improvements from low clinical SNR to high SNR. DTEMRL may be readily supplemented with spatial regularization or a priori tensor distributions for Bayesian tensor estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409140",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 10,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Rician channels",
                "Diffusion tensor imaging",
                "Robust stability",
                "Signal to noise ratio",
                "Diseases",
                "Gaussian approximation",
                "Gaussian noise",
                "1f noise",
                "Anisotropic magnetoresistance"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "maximum likelihood estimation",
                "mean square error methods",
                "medical image processing",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "diffusion tensor estimation",
                "Rician likelihood maximization",
                "diffusion tensor imaging",
                "white matter",
                "Gaussian approximations",
                "fractional anisotropy",
                "apparent diffusion coefficient",
                "maximum likelihood approach",
                "noise variance",
                "mean squared error metrics",
                "spatial regularization",
                "a priori tensor distributions",
                "Bayesian tensor estimation"
            ]
        },
        "id": 333,
        "cited_by": [
            {
                "year": "2009",
                "id": 299
            }
        ]
    },
    {
        "title": "Axon radius measurements in vivo from diffusion MRI: a feasibility study",
        "authors": [
            "Daniel C. Alexander"
        ],
        "abstract": "This paper investigates the feasibility of using diffusion MRI to measure axon-cell dimensions in the white matter of live subjects. A simple geometric model of white-matter tissue provides an expression that relates the axon radius to the diffusion MRI signal. The aim is to determine the accuracy and precision with which we can estimate this potentially important new biomarker. Precision and accuracy depend critically on the acquisition protocol. The paper proposes a general strategy to optimize the experiment design of in-vivo diffusion MRI experiments. The applicability of the design optimization extends well beyond the current work to optimizing the acquisition for any model of the diffusion process. Simulation experiments and results suggest feasibility of measuring larger axon radii in vivo on modern MRI scanners using the optimized acquisition schemes, but that higher gradient strengths are required to measure smaller axons.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409141",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Nerve fibers",
                "In vivo",
                "Magnetic resonance imaging",
                "Biomarkers",
                "Design optimization",
                "Particle measurements",
                "Microstructure",
                "Solid modeling",
                "Displacement measurement",
                "Tensile stress"
            ],
            "INSPEC: Controlled Indexing": [
                "biological tissues",
                "biomedical MRI",
                "diffusion",
                "medical image processing",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "axon radius measurements",
                "axon-cell dimensions",
                "white-matter tissue",
                "diffusion MRI signal",
                "biomarker",
                "acquisition protocol",
                "in-vivo diffusion MRI",
                "design optimization",
                "optimized acquisition schemes",
                "gradient strengths"
            ]
        },
        "id": 334,
        "cited_by": []
    },
    {
        "title": "Fast Invariant Riemannian DT-MRI Regularization",
        "authors": [
            "Yaniv Gur",
            "Nir Sochen"
        ],
        "abstract": "We present regularization by invariant denoising/smoothing of Diffusion Tensor MRI (DTI). Our solution to the problem emerges from a pure geometric point of view. The image domain and the image's values are combined together and described as a (mathematical) fiber bundle. The space of all possible DT images is the space of sections of this fiber bundle. DT image is a map that attaches a three-dimensional symmetric and positive-definite (SPD) matrix to each volume element. We treat the more general space Pn of n-dimensional SPD matrices and introduce a natural GL(n)-invariant metric via the underlying algebraic structure. A metric over sections of the fiber bundle is induced then in terms of the natural metric on Pn. This turns P3 tensors, and in general Pn tensors, into a Riemannian symmetric spaces. By means of the Beltrami framework we define a GL(n)-invariant functional over the space of sections. Then, by calculus of variations we derive the invariant equations of motion. We show that by choosing the Iwasawa coordinates the analytical calculations as well as the numerical implementation become simple. These coordinates evolve with respect to the geometry of the section via the induced metric. The numerical implementation of these flows via standard finite difference schemes is straightforward. The result is a full GL(n) invariant algorithm which is at least as fast and efficient as the Log-Euclidean method. Finally, we demonstrate this framework on real DTI data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409142",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 6,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Diffusion tensor imaging",
                "Tensile stress",
                "Symmetric matrices",
                "Noise reduction",
                "Smoothing methods",
                "Magnetic resonance imaging",
                "Calculus",
                "Equations",
                "Geometry",
                "Finite difference methods"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "matrix algebra",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast invariant Riemannian",
                "invariant denoising",
                "invariant smoothing",
                "Diffusion Tensor MRI",
                "image domain",
                "mathematical fiber bundle",
                "positive-definite matrix",
                "SPD matrices",
                "Riemannian symmetric spaces",
                "Beltrami framework",
                "invariant equations",
                "Iwasawa coordinates",
                "Log-Euclidean method"
            ]
        },
        "id": 335,
        "cited_by": []
    },
    {
        "title": "A Robust Algorithm for Fiber-Bundle Atlas Construction",
        "authors": [
            "U. Ziyan",
            "M.R. Sabuncu",
            "W. Eric",
            "L. Grimson",
            "C.-F. Westin"
        ],
        "abstract": "In this paper, we demonstrate an integrated registration and clustering algorithm to compute an atlas of fiber- bundles from a set of multi-subject diffusion weighted MR images. We formulate a maximum likelihood problem which the proposed method solves using a generalized Expectation Maximization (EM) framework. Additionally, the algorithm employs an outlier rejection and denoising strategy to produce sharp probabilistic maps of certain bundles of interest. This map is potentially useful for making diffusion measurements in a common coordinate system to identify pathology related changes or developmental trends.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409143",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Clustering algorithms",
                "Diffusion tensor imaging",
                "Iterative algorithms",
                "Nerve fibers",
                "Noise reduction",
                "Pathology",
                "Tensile stress",
                "Anatomical structure",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "image denoising",
                "image registration",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fiber-bundle atlas construction",
                "registration algorithm",
                "clustering algorithm",
                "multi-subject diffusion weighted MR images",
                "maximum likelihood problem",
                "generalized expectation maximization framework",
                "outlier rejection",
                "denoising strategy",
                "sharp probabilistic maps"
            ]
        },
        "id": 336,
        "cited_by": []
    },
    {
        "title": "A reliable skin mole localization scheme",
        "authors": [
            "Taeg Sang Cho",
            "William T Freeman",
            "Hensin Tsao"
        ],
        "abstract": "Mole pattern changes are important cues in detecting melanoma at an early stage. As a first step to automatically register mole pattern changes from skin images, this paper presents a framework to detect and label moles on skin images in the presence of clutter, occlusions, and varying imaging conditions. The input image is processed with cascaded blocks to successively discard non-mole pixels. Our method first searches the entire input image for skin regions using a non-parametric skin detection scheme, and the detected skin regions are further processed using a difference of Gaussian (DoG) filter to find possible mole candidates of varying sizes. Mole candidates are classified as moles in the final stage using a trained support vector machine. To increase the mole classification accuracy, hair is removed if present on the skin image using steerable filters and a graphical model. The performance of the designed system is evaluated with 28 test images, and the experimental results demonstrate the effectiveness of the proposed mole localization scheme.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409144",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Skin",
                "Hair",
                "Filters",
                "Malignant tumors",
                "Registers",
                "Pixel",
                "Support vector machines",
                "Support vector machine classification",
                "Image processing",
                "Torso"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image classification",
                "medical image processing",
                "object detection",
                "skin",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "reliable skin mole localization scheme",
                "mole pattern changes",
                "skin images",
                "nonparametric skin detection scheme",
                "Gaussian filter difference",
                "support vector machine",
                "mole classification accuracy",
                "steerable filters"
            ]
        },
        "id": 337,
        "cited_by": []
    },
    {
        "title": "Curvature Estimation for Enhancement of Crossing Curves",
        "authors": [
            "Erik Franken",
            "Remco Duits",
            "Bart ter Haar Romeny"
        ],
        "abstract": "In this paper we describe a method for estimating curvature of elongated structures in images. The curvature estimation is performed on an invertible orientation score, which is a 3D entity obtained from a 2D image by convolution with a rotating kernel. By considering the group structure we can define left-invariant derivatives, which are essential to construct operations on the orientation score that amount to rotationally invariant operations on the corresponding image. The problem of estimating curvature of an oriented structure is stated as a minimization problem, which can be solved by eigenvector analysis of a matrix constructed from the non-symmetric Hessian matrix. The experiments show the method performs well for a wide range of curvatures and noise levels. The method clearly outperforms a related curvature estimation method by Van Ginkel et al. that tends to give estimates that are too small. We show how we can incorporate the curvature estimate in our method for coherence-enhancing diffusion in orientation scores. This method has superior performance in enhancing crossing contours, which is demonstrated on medical images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409145",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical imaging",
                "Robustness",
                "Convolution",
                "Kernel",
                "State estimation",
                "Noise level",
                "Estimation theory",
                "Yield estimation",
                "Image analysis",
                "Blood vessels"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "Hessian matrices",
                "image enhancement",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curvature estimation",
                "crossing curves enhancement",
                "2D image",
                "minimization problem",
                "eigenvector analysis",
                "nonsymmetric Hessian matrix",
                "coherence-enhancing diffusion",
                "medical images"
            ]
        },
        "id": 338,
        "cited_by": []
    },
    {
        "title": "Limited view CT reconstruction via constrained metric labeling",
        "authors": [
            "Vikas Singh",
            "Petru M. Dinu",
            "Lopamudra Mukherjee",
            "Jinhui Xu",
            "Kenneth R. Hoffmann"
        ],
        "abstract": "This paper proposes an new optimization framework for tomographic reconstruction of 3D volumes when only a limited number of projection views are available. The problem has several important clinical applications spanning coronary angiographic imaging, breast tomosynthesis and dental imaging. We first show that the limited view reconstruction problem can be formulated as a \"constrained\" version of the metric labeling problem. This lays the groundwork for a linear programming framework that brings together metric labeling classification and classical algebraic tomographic reconstruction (ART) in a unified model. If the imaged volume is known to be comprised of a finite set of attenuation coefficients, given a regular limited view reconstruction as an input, we can view it as a \"denoising\" task - where voxels must be reassigned subject to maximally maintaining consistency with the input reconstruction and the objective of ART simultaneously. The approach can reliably reconstruct volumes with several multiple contrast objects as well as the simpler binary contrast case which can be solved near-optimally in practice. We present evaluations on cone bean computed tomography, it can also be readily extended to other tomographic modalities as a viable approach for limited-view tomographic reconstruction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409146",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Image reconstruction",
                "Computed tomography",
                "Subspace constraints",
                "Breast",
                "Dentistry",
                "Linear programming",
                "Attenuation",
                "Noise reduction",
                "Maintenance"
            ],
            "INSPEC: Controlled Indexing": [
                "algebra",
                "computerised tomography",
                "image classification",
                "image denoising",
                "image reconstruction",
                "linear programming",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "CT reconstruction",
                "constrained metric labeling",
                "optimization",
                "spanning coronary angiographic imaging",
                "breast tomosynthesis",
                "dental imaging",
                "linear programming",
                "image classification",
                "algebraic tomographic reconstruction",
                "image denoising",
                "medical image processing"
            ]
        },
        "id": 339,
        "cited_by": []
    },
    {
        "title": "Dense Multiscale Motion Extraction from Cardiac Cine MR Tagging using HARP Technology",
        "authors": [
            "Luc Florack",
            "Hans van Assen",
            "Avan Suinesiaputra"
        ],
        "abstract": "We propose an operational method to extract the left ventricle (LV) systole dynamics using harmonic phase (HARP) images extracted from tagged cardiac MR sequences. Established techniques to generate HARP sequences provide independent evidence for motion extraction, in the sense that the combined linear system for scalar brightness conservation, applied to the HARP images, can be uniquely solved for a dense field of motion parameters without the need for regularization. In contrast to some of the previously proposed popular methods, no segmentation or tracking of tags over time, nor interpolation of a sparse motion field explicitly coupled to the tag pattern is required, and the problem of tag fading is bypassed. An important novelty is the incorporation of automatic local scale selection so as to obtain a robust solution, which not only yields a stable, but also a smoothly varying motion field of the (healthy) LV myocardial wall. The scheme relies on an integer parameter representing order of approximation, and allows one to simultaneously obtain a dense field of differential tensors capturing the low order differential structure of the motion field, which is useful for the computation of relevant local quantities such as strain rates and material acceleration fields. The methodology is generic and straightforward to implement, and can be generalized to 3D and, in principle, to account for higher order differential structure.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409147",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 5,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Tagging",
                "Linear systems",
                "Brightness",
                "Image segmentation",
                "Tracking",
                "Interpolation",
                "Fading",
                "Robustness",
                "Myocardium",
                "Tensile stress"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "cardiology",
                "feature extraction",
                "image motion analysis",
                "image sequences",
                "medical image processing",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion extraction",
                "cardiac cine MR tagging",
                "left ventricle systole dynamics",
                "harmonic phase image",
                "HARP sequence",
                "motion parameter",
                "automatic local scale selection",
                "myocardial wall",
                "integer parameter",
                "differential tensors",
                "strain rates",
                "material acceleration"
            ]
        },
        "id": 340,
        "cited_by": []
    },
    {
        "title": "A Variational Approach for Combined Segmentation and Estimation of Respiratory Motion in Temporal Image Sequences",
        "authors": [
            "Jan Ehrhardt",
            "Alexander Schmidt-Richberg",
            "Heinz Handels"
        ],
        "abstract": "In this paper a variational approach for the combined segmentation and registration of temporal image sequences is presented. The purpose of the proposed method is to estimate respiratory-induced organ motion in temporal CT image sequences and to segment a structure of interest simultaneously. In this model the segmentation of all images in the sequences is obtained by finding a non-linear registration to an initial segmentation in a reference image. A dense non-linear displacement field is estimated using image intensities and segmentation information in the images. Both problems (registration and segmentation) are formulated in a joint variational approach and solved simultaneously. A validation of the combined registration and segmentation approach is presented and demonstrates that the simultaneous solution of both problems improves the segmentation performance over a sequential application of the registration and segmentation steps.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409148",
        "reference_list": [
            {
                "year": "2003",
                "id": 116
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Motion estimation",
                "Image sequences",
                "Computed tomography",
                "Shape",
                "Level set",
                "Deformable models",
                "Biomedical informatics",
                "Biomedical imaging",
                "Spatiotemporal phenomena"
            ],
            "INSPEC: Controlled Indexing": [
                "computerised tomography",
                "image registration",
                "image segmentation",
                "image sequences",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational approach",
                "respiratory motion",
                "image segmentation",
                "image registration",
                "nonlinear registration",
                "temporal CT image sequences",
                "joint variational approach"
            ]
        },
        "id": 341,
        "cited_by": []
    },
    {
        "title": "Motion Analysis of Endovascular Stent-Grafts by MDL Based Registration",
        "authors": [
            "Georg Langs",
            "Nikos Paragios",
            "Rene Donner",
            "Pascal Desgranges",
            "Alain Rahmouni",
            "Hicham Kobeiter"
        ],
        "abstract": "The endovascular repair of a traumatic rupture of the thoracic aorta - that would otherwise lead to the death of the patient - is performed by delivering a stent-graft into the vessel at the rupture location. The age range of the affected patients is large and the stent-graft will stay in the body for the remaining life. The technique is relatively new, and no experience with regard to long-term effects, and durability exists. To predict long-term complications, such as ruptures or destructive interactions with surrounding tissue during the life of the patient, it is important to understand the - rather intense and constant - movement of the stent- graft during the cardiac cycle. A computed tomography with heart gating (gated CT) acquires sequences that show the region of the stent-graft at different time points. We analyze the motion of stent-grafts with a model based approach. Stent-grafts are represented as sparse sets of axis points extracted from the gated CT, and motion patterns are captured by a minimum description length based group-wise registration of the stent-graft at different time points. No parameterization or a priori definition of the topology is necessary, and highly variable elasticity properties in the data volume can by accounted for by the sparse statistical model, that captures correlations and motion components of the stent-graft. We report results for deformation models and registration accuracy for 5 patients.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409149",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion analysis",
                "Computed tomography",
                "Biomedical imaging",
                "Heart",
                "Buildings",
                "Computer graphics",
                "Pattern recognition",
                "Image processing",
                "Hospitals",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "cardiology",
                "computerised tomography",
                "image motion analysis",
                "image registration",
                "image sequences",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion analysis",
                "endovascular stent-grafts",
                "MDL based registration",
                "endovascular repair",
                "traumatic rupture",
                "thoracic aorta",
                "computed tomography",
                "heart gating",
                "gated CT",
                "sparse statistical model"
            ]
        },
        "id": 342,
        "cited_by": []
    },
    {
        "title": "Lung Nodule Growth Analysis from 3D CT Data with a Coupled Segmentation and Registration Framework",
        "authors": [
            "Yuanjie Zheng",
            "Karl Steiner",
            "Thomas Bauer",
            "Jingyi Yu",
            "Dinggang Shen",
            "Chandra Kambhamettu"
        ],
        "abstract": "In this paper we propose a new framework to simultaneously segment and register lung and tumor in serial CT data. Our method assumes nonrigid transformation on lung deformation and rigid structure on the tumor. We use the B- Spline-based nonrigid transformation to model the lung deformation while imposing rigid transformation on the tumor to preserve the volume and the shape of the tumor. In particular, we set the control points within the tumor to form a control mesh and thus assume the tumor region follows the same rigid transformation as the control mesh. For segmentation, we apply a 2D graph-cut algorithm on the 3D lung and tumor datasets. By iteratively performing segmentation and registration, our method achieves highly accurate segmentation and registration on serial CT data. Finally, since our method eliminates the possible volume variations of the tumor during registration, we can further estimate accurately the tumor growth, an important evidence in lung cancer diagnosis. Initial experiments on five sets of patients ' serial CT data show that our method is robust and reliable.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409150",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Computed tomography",
                "Lung neoplasms",
                "Cancer detection",
                "Image segmentation",
                "Shape",
                "Information analysis",
                "Biotechnology",
                "Radiology",
                "Deformable models",
                "Iterative algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "computerised tomography",
                "image registration",
                "image segmentation",
                "medical image processing",
                "patient diagnosis",
                "splines (mathematics)",
                "tumours"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "lung nodule growth analysis",
                "3D CT data",
                "segmentation framework",
                "registration framework",
                "tumor",
                "lung deformation",
                "rigid structure",
                "B-spline-based nonrigid transformation",
                "2D graph-cut algorithm",
                "lung cancer diagnosis"
            ]
        },
        "id": 343,
        "cited_by": []
    },
    {
        "title": "Coupling CRFs and Deformable Models for 3D Medical Image Segmentation",
        "authors": [
            "Gabriel Tsechpenakis",
            "Jianhua Wang",
            "Brandon Mayer",
            "Dimitris Metaxas"
        ],
        "abstract": "In this paper we present a hybrid probabilistic framework for 3D image segmentation, using Conditional Random Fields (CRFs) and implicit deformable models. Our 3D deformable model uses voxel intensity and higher scale textures as data-driven terms, while the shape is formulated implicitly using the Euclidean distance transform. The data-driven terms are used as observations in a 3D discriminative CRF, which drives the model evolution based on a simple graphical model. In this way, we solve the model evolution as a joint MAP estimation problem for the 3D label field of the CRF and the 3D shape of the deformable model. We demonstrate the performance of our approach in the estimation of the volume of the human tear menisci from images obtained with optical coherence tomography.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409151",
        "reference_list": [
            {
                "year": "2003",
                "id": 151
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Biomedical imaging",
                "Image segmentation",
                "Shape",
                "Euclidean distance",
                "Drives",
                "Graphical models",
                "Humans",
                "Biomedical optical imaging",
                "Coherence"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image segmentation",
                "maximum likelihood estimation",
                "medical image processing",
                "optical tomography",
                "probability",
                "random processes",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D medical image segmentation",
                "conditional random field",
                "3D implicit deformable model",
                "probabilistic framework",
                "voxel intensity",
                "Euclidean distance transform",
                "3D graphical model",
                "MAP estimation problem",
                "optical coherence tomography",
                "human tear menisci"
            ]
        },
        "id": 344,
        "cited_by": []
    },
    {
        "title": "Region-Based Segmentation via Non-Rigid Template Matching",
        "authors": [
            "Kinda Anna Saddi",
            "Christophe Chefd'hotel",
            "Mikael Rousson",
            "Farida Cheriet"
        ],
        "abstract": "We propose a new region segmentation method based on non-rigid template matching. We align a binary template to an image by maximizing the likelihood of intensity distributions within a region of interest and its background. The intensity model and the corresponding a posteriori distributions are estimated and updated throughout the alignment. The geometric deformation of the template is based on a fluid registration model. Unlike contour-based segmentation techniques, this registration framework allows for a global regularization of the template variations. This enables the segmentation of irregular shapes while avoiding leaks. We apply our method to the segmentation of the liver in computed tomography images, a challenging task due to the high inter-patient variability in the shape of this organ. We show that our segmentation results are equivalent or superior in accuracy to results obtained using existing techniques based on 3D shape models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409152",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Liver",
                "Deformable models",
                "Computed tomography",
                "Biomedical imaging",
                "Cost function",
                "Image registration",
                "Solid modeling",
                "Anatomical structure"
            ],
            "INSPEC: Controlled Indexing": [
                "computerised tomography",
                "image matching",
                "image registration",
                "image segmentation",
                "liver",
                "maximum likelihood estimation",
                "medical image processing",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "region segmentation method",
                "nonrigid template matching",
                "intensity distribution",
                "a posteriori distribution",
                "geometric template deformation",
                "fluid registration model",
                "computed tomography",
                "liver",
                "inter-patient variability",
                "3D shape model"
            ]
        },
        "id": 345,
        "cited_by": []
    },
    {
        "title": "Spatially Varying Classification with Localization Certainty in Level Set Segmentation",
        "authors": [
            "Jenny Folkesson",
            "Carl-Fredrik Westin"
        ],
        "abstract": "We introduce a segmentation framework which extends spatially varying classification to not only incorporate anatomical localization from shape estimation, but to also encode certainty of the localization by local shape variability. The method iterates between a classification step where a statistical classifier learned from feature selection is extended with anatomical localization features, and a shape estimation step where, given the class probability maps, shape is inferred by particle filtering using a level set shape model that accounts for local degrees of anatomical variability. The spatially varying classification is embedded in a geodesic active region framework which allows for local deviations from the inferred shape using an iteratively updated classification based region term. The method is evaluated on late gadolinium enhanced cardiac MRI and is to our knowledge the first automatic segmentation method demonstrated on this type of data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409153",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Level set",
                "Shape",
                "Image segmentation",
                "Biomedical imaging",
                "Static VAr compensators",
                "Myocardium",
                "Laboratories",
                "Mathematics",
                "Hospitals",
                "Magnetic resonance imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image segmentation",
                "particle filtering (numerical methods)",
                "probability",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "level set segmentation",
                "spatially varying classification",
                "anatomical localization",
                "shape estimation",
                "local shape variability",
                "statistical classifier",
                "feature selection",
                "class probability map",
                "particle filtering",
                "level set shape model",
                "geodesic active region"
            ]
        },
        "id": 346,
        "cited_by": []
    },
    {
        "title": "Fuzzy classification of brain MRI using a priori knowledge: weighted fuzzy C-means",
        "authors": [
            "Olivier Salvado",
            "Pierrick Bourgeat",
            "Oscar Acosta Tamayo",
            "Maria Zuluaga",
            "Sebastien Ourselin"
        ],
        "abstract": "We report in this communication a new formulation for the cost function of the well-known fuzzy C-means classification technique whereby we introduce weights. We derive the equations of this new weighted fuzzy C-means algorithm (WFCM) in the presence of additive and multiplicative bias field. We show that the weights can be designed in the same manner as prior probabilities commonly used in maximum a posteriori classifier (MAP) to introduce prior knowledge (e.g. using atlas), and increase robustness to noise (e.g. using Markov random field). Using prior probabilities of three popular MAP algorithms, we compare the performances of our proposed WFCM scheme using the simulated MRI T1W BrainWeb datasets, as well as five T1W MR patient scans. Our results show that WFCM achieves superior performances for low SNR conditions, whereas a Gaussian mixture model is desirable for high noise levels. WFCM allows rigorous comparison of fuzzy and probabilistic classifiers, and offers a framework where improvements can be shared between those two types of classifier.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409155",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Magnetic resonance imaging",
                "Cost function",
                "Markov random fields",
                "Brain modeling",
                "Image segmentation",
                "Diseases",
                "Maximum likelihood estimation",
                "Low-frequency noise",
                "Signal resolution",
                "Australia"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain",
                "fuzzy set theory",
                "Gaussian processes",
                "image classification",
                "maximum likelihood estimation",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "brain MRI",
                "weighted fuzzy C-means",
                "fuzzy C-means classification technique",
                "maximum a posteriori classifier",
                "MAP algorithms",
                "SNR",
                "Gaussian mixture model"
            ]
        },
        "id": 347,
        "cited_by": []
    },
    {
        "title": "Finding a Closed Boundary by Growing Minimal Paths from a Single Point on 2D or 3D Images",
        "authors": [
            "Fethallah Benmansour",
            "Stephane Bonneau",
            "Laurent D. Cohen"
        ],
        "abstract": "In this paper, we present a new method for segmenting closed contours and surfaces. Our work builds on a variant of the Fast Marching algorithm. First, an initial point on the desired contour is chosen by the user. Next, new keypoints are detected automatically using a front propagation approach. We assume that the desired object has a closed boundary. This a-priori knowledge on the topology is used to devise a relevant criterion for stopping the keypoint detection and front propagation. The final domain visited by the front will yield a band surrounding the object of interest. Linking pairs of neighboring keypoints with minimal paths allows us to extract a closed contour from a 2D image. Detection of a variety of objects on real images is demonstrated. Using a similar same idea, we can extract networks of minimal paths from a 3D image called Geodesic Meshing. The proposed method is applied to 3D data with promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409156",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Active contours",
                "Image segmentation",
                "Topology",
                "Data mining",
                "Deformable models",
                "Joining processes",
                "Object detection",
                "Costs",
                "Equations",
                "Image processing"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "growing minimal paths",
                "2D images",
                "3D images",
                "closed contours segmenting",
                "surface segmention",
                "fast marching algorithm",
                "keypoint detection",
                "front propagation",
                "neighboring keypoints",
                "geodesic meshing"
            ]
        },
        "id": 348,
        "cited_by": []
    },
    {
        "title": "What Data to Co-register for Computing Atlases",
        "authors": [
            "B.T. Thomas Yeo",
            "Mert Sabuncu",
            "Hartmut Mohlberg",
            "Katrin Amunts",
            "Karl Zilles",
            "Polina Golland",
            "Bruce Fischl"
        ],
        "abstract": "We argue that registration should be thought of as a means to an end, and not as a goal by itself. In particular, we consider the problem of predicting the locations of hidden labels of a test image using observable features, given a training set with both the hidden labels and observable features. For example, the hidden labels could be segmentation labels or activation regions in fMRI, while the observable features could be sulcal geometry or MR intensity. We analyze a probabilistic framework for computing an optimal atlas, and the subsequent registration of a new subject using only the observable features to optimize the hidden label alignment to the training set. We compare two approaches for co-registering training images for the atlas construction: the traditional approach of only using observable features and a novel approach of only using hidden labels. We argue that the alternative approach is superior particularly when the relationship between the hidden labels and observable features is complex and unknown. As an application, we consider the task of registering cortical folds to optimize Brodmann area localization. We show that the alignment of the Brodmann areas improves by up to 25% when using the alternative atlas compared with the traditional atlas. To the best of our knowledge, these are the most accurate Brodmann area localization results (achieved via cortical fold registration) reported to date.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409157",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Testing",
                "Biomedical imaging",
                "Image segmentation",
                "Magnetic resonance imaging",
                "Biomedical computing",
                "Psychiatry",
                "Psychology",
                "Geometry",
                "Image registration",
                "Statistical analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "image segmentation",
                "learning (artificial intelligence)",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image registration",
                "fMRI",
                "probabilistic framework",
                "optimal atlas",
                "subsequent registration",
                "training images coregistering",
                "Brodmann area localization"
            ]
        },
        "id": 349,
        "cited_by": []
    },
    {
        "title": "A Statistical Approach to Determine Symmetrical Solutions for the Registration of 3D Knee Implant Models to Sagittal Fluoroscopy Images",
        "authors": [
            "J. Hermans",
            "J. Bellemans",
            "D. Vandermeulen",
            "P. Suetens"
        ],
        "abstract": "During the registration of 3D CAD models of metallic knee implant components to single-plane sagittal fluoroscopy images, the 3D pose of each implant component is estimated by maximizing the similarity between its 2D image appearance and the observed fluoroscopy image. Because knee implant components are highly symmetrical with respect to the sagittal plane, two significantly different model poses result in 2D image projections very similar to an observed sagittal fluoroscopy image. Traditional 2D/3D registration algorithms tend to converge to one of these symmetrical poses discarding the other one. This paper presents a method which simultaneously estimates both symmetrical solutions. In order not to limit the proposed method to 3D models with an exact plane of symmetry, a completely data-driven symmetry constraint is used which is imposed to the estimated pose parameters. The algorithm is embedded in a statistical framework which is optimized using a deterministic annealing expectation-maximization approach. The validity of the method is demonstrated by registration of the tibial knee implant component to real and simulated fluoroscopy images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409158",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Knee",
                "Implants",
                "Video sequences",
                "Image converters",
                "Kinematics",
                "Image edge detection",
                "Annealing",
                "Convergence",
                "Biomedical imaging",
                "Hospitals"
            ],
            "INSPEC: Controlled Indexing": [
                "diagnostic radiography",
                "expectation-maximisation algorithm",
                "image registration",
                "medical image processing",
                "orthopaedics",
                "pose estimation",
                "prosthetics",
                "simulated annealing",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "statistical approach",
                "3D knee implant models",
                "single-plane sagittal fluoroscopy images",
                "3D CAD model registration",
                "metallic knee implant components",
                "3D pose estimation",
                "2D image appearance",
                "deterministic annealing expectation-maximization approach"
            ]
        },
        "id": 350,
        "cited_by": []
    },
    {
        "title": "Multi-start Method with Prior Learning for Image Registration",
        "authors": [
            "Gang Song",
            "Brian B. Avants",
            "James C. Gee"
        ],
        "abstract": "We propose an efficient image registration strategy that is based on learned prior distributions of transformation parameters. These priors are used to constrain a finite- time multi-start optimization method. Motivation for this approach comes from the fact that standard affine brain image registration methods, especially those based on gradient descent optimization alone, are affected by the initial search position. While global optimization methods can resolve this problem, they are are often very time consuming. Our goal is to build an explicit prior model of the gap between a typical registration solution and the solution gained by a global optimization method. We use this learned prior model to restrict randomized search in the relevant parameter space surrounding the initial solution. Global optimization in this restricted parameter space provides, in finite time, results that are superior to both gradient descent and the general multi-start strategy. The performance of our method is illustrated on a data set of 67 elderly and neurodegenerative brains. Our novel learning strategy and the associated registration method are shown to outperform other approaches. Theoretical, synthetic and real-world examples illustrate this improvement.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409159",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image registration",
                "Cost function",
                "Optimization methods",
                "Image analysis",
                "Laboratories",
                "Senior citizens",
                "Design methodology",
                "Image sampling",
                "Tellurium",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "brain",
                "gradient methods",
                "image registration",
                "learning systems",
                "medical image processing",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning",
                "brain image registration",
                "transformation parameters",
                "finite-time multistart optimization",
                "gradient descent optimization",
                "global optimization"
            ]
        },
        "id": 351,
        "cited_by": []
    },
    {
        "title": "A Unified and Efficient Approach for Free-form Deformable Registration",
        "authors": [
            "Ali Khamene",
            "Fred Azar",
            "Loren Schwarz",
            "Darko Zikic",
            "Nassir Navab",
            "Eike Rietzel"
        ],
        "abstract": "We propose a novel numerical approach for solving the free-form deformable registration problem. The central idea is to utilize the well understood techniques from variational deformable registration problems. We demonstrate that it is possible to formulate the free-form deformable registration problem as the optimization of an energy functional as in the dense deformation case. This energy functional possesses image distance and regularization terms, which are both functions of the free-form deformation control points. We then setup a semi-backward (implicit) partial differential equation that optimizes the established energy functional. In addition to being mathematically justified, this approach provides both accuracy and speed. Our evaluation on synthetic, real, two dimensional, and three dimensional data demonstrates accuracy and computational effectiveness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409160",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical imaging",
                "Displacement control",
                "Spline",
                "Image motion analysis",
                "Partial differential equations",
                "Computer vision",
                "Motion analysis",
                "Biomedical optical imaging",
                "Nonlinear optics",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "partial differential equations",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "free-form deformable registration",
                "variational deformable registration problems",
                "energy functional optimization",
                "dense deformation case",
                "image distance term",
                "image regularization term",
                "semi-backward partial differential equation"
            ]
        },
        "id": 352,
        "cited_by": []
    },
    {
        "title": "Improved FFD B-Spline Image Registration",
        "authors": [
            "Nicholas J. Tustison",
            "Brian A. Avants",
            "James C. Gee"
        ],
        "abstract": "Due to their computational efficiency and other salient properties, B-splines form the basis not only in comprising the de facto standard for curve and surface representation but also for various nonrigid registration techniques frequently employed in medical image analysis. These registration techniques fall under the rubric of Free-Form Deformation (FFD) approaches in which the object to be registered is embedded within a B-spline object. The deformation of the B-spline object represents the transformation of the registration. Representative, and often cited within the relevant community, of this class of techniques is the formulation of Rueckert et. al [7] who employed cubic splines with normalized mutual information to study breast deformation. Similar techniques from various groups provided incremental novelty in the form of disparate explicit regularization terms as well as the employment of various image metrics and tailored optimization methods. For several algorithms, the underlying gradient-based optimization retained its essential characteristics since Rueckert's incarnation. We assert that such a straightforward gradient-learning is suboptimal in certain cases and to remedy this sub-optimality, we propose a fitting-based strategy for registration in the spirit of Thirion 's Demons [14] and directly manipulated free-form deformations [2], which takes advantage of our previously developed generalized B-spline fitting algorithm [17].",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409161",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 4,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Spline",
                "Image registration",
                "Optimization methods",
                "Mutual information",
                "Laboratories",
                "Computational efficiency",
                "Biomedical imaging",
                "Image analysis",
                "Breast",
                "Employment"
            ],
            "INSPEC: Controlled Indexing": [
                "curve fitting",
                "diagnostic radiography",
                "gradient methods",
                "image reconstruction",
                "image registration",
                "image representation",
                "mammography",
                "medical image processing",
                "splines (mathematics)",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "FFD B-spline image registration",
                "curve representation",
                "surface representation",
                "medical image analysis",
                "free-form deformation approach",
                "breast deformation",
                "gradient-based optimization",
                "Rueckert incarnation",
                "B-spline fitting algorithm"
            ]
        },
        "id": 353,
        "cited_by": []
    },
    {
        "title": "Correspondence Establishment in Statistical Modeling of Shapes with Arbitrary Topology",
        "authors": [
            "Ekaterina Syrkina",
            "Miguel A. Gonzalez Ballester",
            "Gabor Szekely"
        ],
        "abstract": "Correspondence establishment is a key step in statistical shape model building. There are several automated methods for solving this problem in 3D, but they usually can only handle objects with simple topology, like that of a sphere or a disc. We propose an extension to correspondence establishment over a population based on the optimization of the minimal description length function, allowing considering objects with arbitrary topology. Instead of using a fixed structure of kernel placement on a sphere for the systematic manipulation of point landmark positions, we rely on an adaptive, hierarchical organization of surface patches. This hierarchy can be built on surfaces of arbitrary topology and the resulting patches are used as a basis for a consistent, multi-scale modification of the surfaces' parameterization, based on point distribution models. The feasibility of the approach is demonstrated on synthetic models with different topologies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409162",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Topology",
                "Kernel",
                "Optimization methods",
                "Computer vision",
                "Laboratories",
                "Covariance matrix",
                "Piecewise linear techniques",
                "Genetics",
                "Biomedical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "medical image processing",
                "optimisation",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "correspondence establishment",
                "statistical shape model building",
                "arbitrary topology",
                "optimization",
                "minimal description length function",
                "kernel placement",
                "point landmark positions",
                "medical image analysis"
            ]
        },
        "id": 354,
        "cited_by": []
    },
    {
        "title": "Type-Constrained Robust Fitting of Quadrics with Application to the 3D Morphological Characterization of Saddle-Shaped Articular Surfaces",
        "authors": [
            "Stephane Allaire",
            "Jean-Jose Jacq",
            "Valerie Burdin",
            "Christian Roux",
            "Christine Couture"
        ],
        "abstract": "The scope of this paper is the guaranteed fitting of specified types of quadratic surfaces to scattered 3D point clouds. Since we chose quadrics to account for articular surfaces of various shapes in medical images, the models thus estimated usefully extract global symmetry-related intrinsic features in human joints: centers, axes, extremal curvatures. The unified type-enforcing method is based on a constrained weighted least-squares minimization of algebraic residuals which uses a robust and bias- corrected metric. Provided that at most one quadratic constraint is involved, every step produces closed-form eigenvector solutions. In this framework, guaranteeing the occurrence of 3D primitives of certain types among this eigendecomposition is not a straightforward transcription of the priorly handled 2D case. To explore possibilities, we re-exploit a mapping to a 2D space called the quadric shape map (QSM) where the influence of any constraint on shape and type can in fact be studied visually. As a result, we provide a new enforceable quadratic constraint that practically ensures types such as hyperboloids, which helps characterize saddle-like articular surfaces. Application to a database shows how this guarantee is needed to coherently extract the center and axes of the ankle joint.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409163",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface fitting",
                "Robustness",
                "Surface morphology",
                "Shape",
                "Scattering",
                "Clouds",
                "Biomedical imaging",
                "Humans",
                "Minimization methods",
                "Visual databases"
            ],
            "INSPEC: Controlled Indexing": [
                "curve fitting",
                "eigenvalues and eigenfunctions",
                "feature extraction",
                "least squares approximations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "quadrics type-constrained robust fitting",
                "3D morphological characterization",
                "saddle-shaped articular surfaces",
                "scattered 3D point clouds",
                "symmetry-related intrinsic features",
                "type-enforcing method",
                "constrained weighted least-squares minimization",
                "algebraic residuals",
                "bias-corrected metric",
                "closed-form eigenvector solutions",
                "eigendecomposition",
                "quadric shape map"
            ]
        },
        "id": 355,
        "cited_by": []
    },
    {
        "title": "Modeling Brain Anatomy with 3D Arrangements of Curves",
        "authors": [
            "W. Mio",
            "J. C. Bowers",
            "M. K. Hurdal",
            "X. Liu"
        ],
        "abstract": "We employ 3D arrangements of curves to represent and analyze biological shapes, in particular, the anatomy of the human brain. The arrangements of curves may vary from fairly sparse - such as a collection of sulcal lines that coarsely approximates the global shape of the brain - to very dense decompositions of the cortical surface into space curves. A space of shapes of such arrangements is constructed equipped with geodesic metrics that can be used in conjunction with curve registration techniques to quantify shape resemblance or dissimilarity, as well as to identify the regions where anatomical differences are most pronounced. The metric is applied to the panellation and labeling of configurations associated with the left and right hemispheres of the brain. Examples are also given of geodesic interpolations between decompositions into space curves of surfaces representing the entire left hemisphere of the brain.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409164",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Brain modeling",
                "Anatomy",
                "Shape",
                "Surface morphology",
                "Surface reconstruction",
                "Speech",
                "Humans",
                "Extraterrestrial measurements",
                "Natural languages",
                "Diseases"
            ],
            "INSPEC: Controlled Indexing": [
                "brain",
                "image registration",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "brain anatomy",
                "curves 3D arrangements",
                "biological shapes",
                "human brain",
                "cortical surface",
                "space curves",
                "geodesic metrics",
                "curve registration techniques",
                "brain hemisphere"
            ]
        },
        "id": 356,
        "cited_by": []
    },
    {
        "title": "Diffusion Tensor Image Smoothing Using Efficient and Effective Anisotropic Filtering",
        "authors": [
            "Qing Xu",
            "Adam W. Anderson",
            "John C. Gore",
            "Zhaohua Ding"
        ],
        "abstract": "To improve the accuracy of tissue structural and architectural characterization with diffusion tensor imaging, an anisotropic smoothing algorithm is presented for reducing noise in diffusion tensor images efficiently and effectively. The presented algorithm is based on previous anisotropic diffusion filtering, which is implemented with a straightforward but inefficient explicit numerical scheme. The main contribution of this paper is to improve the performance of the previous method considerably by using unconditionally stable and second order time accurate semi-implicit scheme. Our new method needs only few or even one iteration to achieve better smoothed images than what is generated by tens of iterations of the previous method, which makes it more attractive to practical use. Experiments with simulated and in vivo data have demonstrated the advantage of our new algorithm for denoising diffusion tensor images in terms of efficiency and effectiveness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409165",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Smoothing methods",
                "Anisotropic filters",
                "Diffusion tensor imaging",
                "Anisotropic magnetoresistance",
                "Noise reduction",
                "In vivo",
                "Signal to noise ratio",
                "Filtering algorithms",
                "1f noise"
            ],
            "INSPEC: Controlled Indexing": [
                "biological tissues",
                "filtering theory",
                "image denoising",
                "medical image processing",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image smoothing",
                "anisotropic filtering",
                "tissue structural characterization",
                "tissue architectural characterization",
                "diffusion tensor imaging",
                "anisotropic smoothing",
                "image denoising"
            ]
        },
        "id": 357,
        "cited_by": []
    },
    {
        "title": "Diffusion Maps Segmentation of Magnetic Resonance Q-Ball Imaging",
        "authors": [
            "Demian Wassermann",
            "Maxime Descoteaux",
            "Rachid Deriche"
        ],
        "abstract": "We present a Diffusion Maps clustering method applied to diffusion MRI in order to segment complex white matter fiber bundles. It is well-known that diffusion tensor imaging (DTI) is restricted in complex fiber regions with crossings and this is why recent High Angular Resolution Diffusion Imaging (HARDI) such has Q-Ball Imaging (QBI) have been introduced to overcome these limitations. QBI reconstructs the diffusion orientation distribution function (ODF), a spherical function that has its maximum(a) agreeing with the underlying fiber population. In this paper, we use the ODF representation in a small set of spherical harmonic coefficients as input to the Diffusion Maps clustering method. We first show the advantage of using Diffusion Maps clustering over classical methods such as N-Cuts and Laplacian Eigenmaps. In particular, our ODF Diffusion Maps requires a smaller number of hypothesis from the input data, reduces the number of artifacts in the segmentation and automatically exhibits the number of clusters segmenting the Q-Ball image by using an adaptative scale-space parameter. We also show that our ODF Diffusion Maps clustering can reproduce published results using the diffusion tensor (DT) clustering with N-Cuts on simple synthetic images without crossings. On more complex data with crossings, we show that our method succeeds to separate fiber bundles and crossing regions whereas the DT- based methods generate artifacts and exhibit wrong number of clusters. Finally, we show results on a real brain dataset where we successfully segment the fiber bundles.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409166",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Magnetic resonance",
                "Magnetic resonance imaging",
                "Diffusion tensor imaging",
                "High-resolution imaging",
                "Clustering methods",
                "Image resolution",
                "Image reconstruction",
                "Distribution functions",
                "Laplace equations"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "image reconstruction",
                "image representation",
                "image resolution",
                "image segmentation",
                "medical image processing",
                "pattern clustering",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "diffusion maps segmentation",
                "magnetic resonance Q-ball imaging",
                "diffusion maps clustering method",
                "diffusion MRI",
                "white matter fiber bundle",
                "diffusion tensor imaging",
                "high angular resolution diffusion imaging",
                "diffusion orientation distribution function reconstruction",
                "ODF representation",
                "adaptative scale-space parameter"
            ]
        },
        "id": 358,
        "cited_by": []
    },
    {
        "title": "Locally-Constrained Region-Based Methods for DW-MRI Segmentation",
        "authors": [
            "John Melonakos",
            "Marc Niethammer",
            "Vandana Mohan",
            "Marek Kubicki",
            "James V. Miller",
            "Allen Tannenbaum"
        ],
        "abstract": "In this paper, we describe a method for segmenting fiber bundles from diffusion-weighted magnetic resonance images using a locally-constrained region based approach. From a pre-computed optimal path, the algorithm propagates outward capturing only those voxels which are locally connected to the fiber bundle. Rather than attempting to find large numbers of open curves or single fibers, which individually have questionable meaning, this method segments the full fiber bundle region. The strengths of this approach include its ease-of-use, computational speed, and applicability to a wide range of fiber bundles. In this work, we show results for segmenting the cingulum bundle. Finally, we explain how this approach and extensions thereto overcome a major problem that typical region-based flows experience when attempting to segment neural fiber bundles.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409167",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Tensile stress",
                "Biomedical imaging",
                "Magnetic resonance",
                "Biomedical computing",
                "Bayesian methods",
                "Decision making",
                "Stochastic resonance",
                "Signal to noise ratio",
                "Hospitals"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain",
                "image segmentation",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "diffusion-weighted magnetic resonance image",
                "DW-MRI segmentation",
                "locally-constrained region based approach",
                "neural fiber bundle"
            ]
        },
        "id": 359,
        "cited_by": []
    },
    {
        "title": "Exploiting peak anisotropy for tracking through complex structures",
        "authors": [
            "Kiran K Seunarine",
            "P A Cook",
            "M G Hall",
            "K V Embleton",
            "G J M Parker",
            "Daniel C Alexander"
        ],
        "abstract": "This work shows that multi-fibre reconstruction techniques, such as Persistent Angular Structure (PAS) MRI or QBall Imaging, provide much more information than just discrete fibre orientations, which is all that previous tractography algorithms exploit from them. We show that the shapes of the peaks of the functions output by multiple-fibre reconstruction algorithms reflect the underlying distribution of fibres. Furthermore, we show how to exploit this extra information to improve Probabilistic Index of Connectivity (PICo) tractography. The method uses the Bingham distribution to model the uncertainty in fibre-orientation estimates obtained from peaks in the PAS or QBall Orientation Distribution Function (ODF). The Bingham model captures anisotropy in the uncertainty, allowing the method to track through fanning and bending structures, which previous methods do not recover reliably. We devise a new calibration procedure to construct a mapping from peak shape to Bingham parameters. We test the accuracy of the calibration using a bootstrap experiment. Finally, we show that exploiting the peak shape in this way can provide improved PICo tractography results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409168",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 18,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Anisotropic magnetoresistance",
                "Image reconstruction",
                "Magnetic resonance imaging",
                "Shape",
                "Diffusion tensor imaging",
                "Biomedical imaging",
                "Uncertainty",
                "Reconstruction algorithms",
                "Calibration",
                "Streaming media"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "image reconstruction",
                "medical image processing",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "peak anisotropy",
                "complex structure tracking",
                "multifibre reconstruction",
                "persistent angular structure",
                "MRI",
                "discrete fibre orientations",
                "tractography algorithms",
                "probabilistic index of connectivity",
                "orientation distribution function",
                "bending structures",
                "Bingham parameters",
                "bootstrap experiment"
            ]
        },
        "id": 360,
        "cited_by": []
    },
    {
        "title": "Structure-Specific Statistical Mapping of White Matter Tracts using the Continuous Medial Representation",
        "authors": [
            "Paul A. Yushkevich",
            "Hui Zhang",
            "Tony J. Simon",
            "James C. Gee"
        ],
        "abstract": "This paper describes a new statistical analysis framework for diffusion-based white matter studies. The framework is based on a recent unbiased normalization algorithm for diffusion tensor images. Taking advantage of the fact that most human white matter tracts are thin sheet-like structures, this framework uses deformable medial models to represent six of the major tracts in a white matter atlas derived for a given set of images. The medial representation allows one to average tensor-based features along directions perpendicular to the tracts, thus reducing data dimensionality and accounting for errors in normalization. Unlike earlier work in the area of tract-based spatial statistics (Smith et al, 2006), this framework enables the analysis of individual white matter structures, and provides a range of possibilities for computing statistics and visualizing differences between cohorts. The framework is demonstrated in a study of white matter differences in pediatric chromosome 22q deletion syndrome.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409169",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistical analysis",
                "Skeleton",
                "Smoothing methods",
                "Solid modeling",
                "Performance analysis",
                "Tensile stress",
                "Data visualization",
                "Biological cells",
                "Anatomical structure",
                "Laboratories"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "statistical analysis",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure-specific statistical mapping",
                "continuous medial representation",
                "diffusion-based white matter studies",
                "diffusion tensor images",
                "normalization algorithm",
                "tensor-based features",
                "tract-based spatial statistics",
                "pediatric chromosome deletion syndrome"
            ]
        },
        "id": 361,
        "cited_by": []
    },
    {
        "title": "Modeling the Marginal Distributions of Complex Wavelet Coefficient Magnitudes for the Classification of Zoom-Endoscopy Images",
        "authors": [
            "Roland Kwitt",
            "Andreas Uhl"
        ],
        "abstract": "In this paper, we propose a set of new image features for the classification of zoom-endoscopy images. The feature extraction step is based on fitting a two-parameter Weibull distribution to the wavelet coefficient magnitudes of sub-bands obtained from a complex wavelet transform variant. We show, that the shape and scale parameter possess more discriminative power than the classic mean and standard deviation based features for complex subband coefficient magnitudes. Furthermore, we discuss why the commonly used Rayleigh distribution model is suboptimal in our case.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409170",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 13,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Wavelet coefficients",
                "Cancer",
                "Filters",
                "Wavelet transforms",
                "Colonic polyps",
                "Delay",
                "Colon",
                "Discrete wavelet transforms",
                "Feature extraction",
                "Lesions"
            ],
            "INSPEC: Controlled Indexing": [
                "endoscopes",
                "feature extraction",
                "medical image processing",
                "wavelet transforms",
                "Weibull distribution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "marginal distribution modeling",
                "complex wavelet coefficient magnitudes",
                "zoom-endoscopy image classification",
                "image features",
                "feature extraction",
                "Weibull distribution",
                "complex wavelet transform variant",
                "discriminative power",
                "complex subband coefficient magnitudes",
                "Rayleigh distribution model"
            ]
        },
        "id": 362,
        "cited_by": []
    },
    {
        "title": "On Detecting Subtle Pathology via Tissue Clustering of Multi-parametric Data using Affinity Propagation",
        "authors": [
            "Ragini Verma",
            "Peng Wang"
        ],
        "abstract": "We propose a novel framework for tissue abnormality characterization in normal appearing brain tissue (NABT) that is progressively deteriorating, using affinity propagation applied to multi-parametric data created using a combination of magnetic resonance (MR) protocols. While traditional tissue segmentation and clustering can reveal clusters pertaining to healthy and diseased tissue easily, a complete characterization of the effect of pathology requires the study of heterogeneity of NABT. The problem is rendered challenging by the fact that there are no training samples available for such tissue and hence classification based techniques cannot be used and neither can traditional clustering techniques since the number of clusters are not known a priori. Our framework for the automated clustering of tissue types employs a combination of a) manifold learning, that determines the underlying non-linear structure and embeds it into a lower dimensional space and b) affinity propagation (AP), which is a novel clustering technique that combines model- and similarity- based clustering, to automatically obtain exemplar-based clustering. We also define a novel probabilistic clustering technique. The number of clusters associated with a tissue type is indicative of its heterogeneity. By computing the overlap of these clusters in each of the MR protocols, we obtain a measure of the degree of abnormality in a tissue type and the protocol most sensitive in providing that classification. This general framework is applied towards the characterization of NABT in patients with multiple sclerosis. Results demonstrate a greater heterogeneity in NABT surrounding the lesions along with a greater overlap between the NABT and lesion tissue.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409171",
        "reference_list": [
            {
                "year": "2005",
                "id": 83
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Pathology",
                "Protocols",
                "Brain",
                "Lesions",
                "Clustering methods",
                "Biomedical imaging",
                "Image analysis",
                "Magnetic analysis",
                "Radiology",
                "Magnetic resonance"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain",
                "image classification",
                "image segmentation",
                "medical image processing",
                "pattern clustering",
                "tissue engineering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subtle pathology detection",
                "tissue clustering",
                "multiparametric data",
                "affinity propagation",
                "normal appearing brain tissue characterization",
                "magnetic resonance protocols",
                "tissue segmentation",
                "classification based techniques",
                "manifold learning",
                "nonlinear structure",
                "similarity-based clustering",
                "multiple sclerosis"
            ]
        },
        "id": 363,
        "cited_by": []
    },
    {
        "title": "Detection of Complex Vascular Structures using Polar Neighborhood Intensity Profile",
        "authors": [
            "Xiaoning Qian",
            "Matthew P. Brennan",
            "Donald P. Dione",
            "Wawrzyniec L. Dobrucki",
            "Marcel P. Jackowski",
            "Christopher K. Breuer",
            "Albert J. Sinusas",
            "Xenophon Papademetris"
        ],
        "abstract": "Modern medical imaging techniques enable the acquisition of in-vivo high resolution images of the vascular system. Most common methods for the detection of vessels in these images, such as multiscale Hessian-based operators and matched filters, rely on the assumption that, at each voxel there is a single cylinder. Such an assumption is clearly violated at the multitude of branching points that are easily observed in all but the most focused vascular image studies. In this paper, we propose a novel method for detecting vessels in medical images that relaxes this single cylinder constraint. Instead, we extract characteristics of the local intensity profile (in a spherical polar coordinate system) which we term as the polar neighborhood intensity profile enabling us to detect vessels even near branching points. Our method demonstrates improved performance over standard methods on both 2D synthetic images and MRA 3D animal vascular images, particularly close to vessel branching regions. This methodology is also applicable to the detection of other structures such as sheets with the appropriate choice of operators.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409172",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical imaging",
                "Medical diagnostic imaging",
                "Matched filters",
                "Engine cylinders",
                "Coronary arteriosclerosis",
                "Atherosclerosis",
                "X-ray imaging",
                "Optical imaging",
                "Magnetic resonance imaging",
                "In vivo"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "filtering theory",
                "image resolution",
                "matched filters",
                "medical image processing",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "complex vascular structures detection",
                "polar neighborhood intensity profile",
                "medical imaging techniques",
                "in-vivo high resolution images",
                "multiscale Hessian-based operators",
                "matched filters",
                "2D synthetic images",
                "MRA 3D animal vascular images"
            ]
        },
        "id": 364,
        "cited_by": []
    },
    {
        "title": "Optimization Algorithms for Labeling Brain Sulci Based on Graph Matching",
        "authors": [
            "Faguo Yang",
            "Frithjof Kruggel"
        ],
        "abstract": "Graph matching techniques are widely used in pattern recognition problems such as scene description, finger print identification, or face recognition. In this paper, we put forward two optimization methods for graph matching and compare them in the context of brain sulcus identification. The first approach is based on a constraint search in a neighborhood; the second uses a genetic algorithm for optimization. Experiments demonstrate that both methods yield satisfactory identification rates, however, the second method is more general and easier to adapt to similar problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409173",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Optimization methods",
                "Image segmentation",
                "Pattern matching",
                "Humans",
                "Brain",
                "Surface morphology",
                "Pattern recognition",
                "Genetic algorithms",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "brain",
                "face recognition",
                "fingerprint identification",
                "genetic algorithms",
                "graphs",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optimization",
                "brain sulci labeling",
                "graph matching",
                "pattern recognition",
                "scene description",
                "finger print identification",
                "face recognition",
                "genetic algorithm"
            ]
        },
        "id": 365,
        "cited_by": []
    },
    {
        "title": "A Tool for Topographic Analysis of Electrode Contacts in Human Cortical Stimulation",
        "authors": [
            "Jean-Marie Favreau",
            "Simone Hemm",
            "Christophe Nuti",
            "Jerome Coste",
            "Vincent Barra",
            "Jean-Jacques Lemaire"
        ],
        "abstract": "Electric chronic stimulation of the human motor cortex (ECSM) has been reported to alleviate chronic severe pain. However the mechanism of action of ECSM is still hypothetical. This is due mainly to the poor knowledge of 1) the electric diffusion through the multiple structures beneath the epidural contacts (i.e. dura matter, cerebrospinal fluid space, arachnoid membrane, grey and white matter layers, pie mere and vascular tree), 2) the absence of consensus concerning the stimulation parameters (mono versus bipolar stimulation, cathodic or anodic current) and 3) the detailed cortical topography of the contacts. In this study we focused on the precise identification of the cortical areas covered by the electric contacts in a series of twelve patients operated on for ECSM. We propose a new automatic tool for topographic analysis able to compute 2D maps from the 3D anatomic MRI with bijective transformation (point- to-point correspondance). Anatomical regions of interest (AROIs) were visually identified, manually outlined and extracted (Iplan, BrainLab, Germany) for further analysis: 1) for the anatomic structures, on pre operative Tl-weigthed magnetic resonance imaging (MRI), the frontal (superior or Fl, intermediate or F2 and inferior or F3), the pre central and the post central gyrus; 2) for the electrode contacts (Resume, Medtronic, USA), on post operative computerized tomography (CT). After getting white and gray matter membership maps by automatic segmentation, we produced a cortical mask to build a triangular mesh. We defined a homeomorphism between the 3D mesh and a subset of R2 and could apply in consequence the circle packing algorithm. We built depth maps (distance to the skull), distance- to-contact maps (distance to a given electrode contact) and anatomic structure maps. Results showed that it was easier to accurately define the location of the contact projection on the cortex allowing physicians to correlate the benefit with the topography. In particular,...",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409174",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Electrodes",
                "Humans",
                "Magnetic resonance imaging",
                "Pain",
                "Surfaces",
                "Magnetic analysis",
                "Computed tomography",
                "Biomembranes",
                "MONOS devices",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "bioelectric phenomena",
                "biomedical electrodes",
                "biomedical MRI",
                "brain",
                "computerised tomography",
                "image segmentation",
                "medical image processing",
                "neurophysiology",
                "patient treatment"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "topographic analysis",
                "electrode contacts",
                "human cortical stimulation",
                "electric chronic stimulation",
                "human motor cortex",
                "computerized tomography",
                "automatic segmentation"
            ]
        },
        "id": 366,
        "cited_by": []
    },
    {
        "title": "Can Born Approximate the Unborn? A New Validity Criterion for the Born Approximation in Microscopic Imaging",
        "authors": [
            "Sigal Trattner",
            "Micha Feigin",
            "Hayit Greenspan",
            "Nir Sochen"
        ],
        "abstract": "The Nomarski differential interference contrast (DIC) microscopy is of widespread use for observing live biological specimens. In fertility clinics the DIC microscope is used for evaluating human embryo cells. An image formation model for DIC imaging is needed for reconstruction and quantification of the visualized specimens. This calls for a complicated analysis of the interaction of light waves with biological matter. Most works express the solution via the first Born approximation, yet a theoretical bound is known that limits the validity of such approximation to very small objects. We show in this work that the theoretical bound is not directly relevant to microscopic imaging and is far too limiting. We derive a more realistic bound and show that it may justify in many cases the use of the Born approximation in biological cell microscopic imaging. It also provides limits on the validity of the Born expansion that several works violate.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409175",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Scattering",
                "Approximation methods",
                "Microscopy",
                "Interference",
                "Humans",
                "Embryo",
                "Biological system modeling",
                "Image reconstruction",
                "Visualization",
                "Biological cells"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "biological specimen preparation",
                "biology computing",
                "cellular biophysics",
                "image reconstruction",
                "microscopy"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Born approximation",
                "microscopic imaging",
                "validity criterion",
                "Nomarski differential interference contrast",
                "DIC microscopy",
                "live biological specimens",
                "human embryo cells",
                "visualized specimens reconstruction",
                "visualized specimens quantification"
            ]
        },
        "id": 367,
        "cited_by": []
    },
    {
        "title": "Efficient Computation of the Inverse Gradient on Irregular Domains",
        "authors": [
            "Gunnar F\u00e4rneback",
            "Joakim Rydell",
            "Tino Ebbers",
            "Mats Andersson",
            "Hans Knutsson"
        ],
        "abstract": "The inverse gradient problem, finding a scalar field f with a gradient near a given vector field g on some bounded and connected domain \u03a9 \u03f5 R \ud835\udcc3 , can be solved by means of a Poisson equation with inhomogeneous Neumann boundary conditions. We present an elementary derivation of this partial differential equation and an efficient multigrid-based method to numerically compute the inverse gradient on non-rectangular domains. The utility of the method is demonstrated by a range of important medical applications such as phase unwrapping, pressure computation, inverse deformation fields, and fiber bundle tracking.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409176",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 4,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Poisson equations",
                "Biomedical imaging",
                "Boundary conditions",
                "Calculus",
                "Biomedical computing",
                "Biomedical informatics",
                "Biomedical engineering",
                "Physiology",
                "Visualization",
                "Partial differential equations"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image registration",
                "inverse problems",
                "Jacobian matrices",
                "medical image processing",
                "Poisson equation",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "inverse gradient problem",
                "irregular domains",
                "scalar field",
                "vector field",
                "Poisson equation",
                "inhomogeneous Neumann boundary conditions",
                "partial differential equation",
                "multigrid-based method",
                "medical imaging applications",
                "phase unwrapping",
                "pressure computation",
                "inverse deformation fields",
                "fiber bundle tracking",
                "Jacobian matrices",
                "image registration",
                "iterative methods"
            ]
        },
        "id": 368,
        "cited_by": []
    },
    {
        "title": "Object Matching in the Presence of Non-Rigid Deformations Close to Similarities",
        "authors": [
            "E. Balmashnova",
            "B. Platel",
            "L.M.J. Florack",
            "B.M. ter Haar Romeny"
        ],
        "abstract": "In this paper we address the problem of object retrieval based on scale-space interest points, namely top-points. The original retrieval algorithm can only cope with scale-Euclidean transformations. We extend the algorithm to the case of non-rigid transformations like affine and perspective transformations and investigate its robustness. The proposed algorithm is proven to be highly robust under various degrading factors, such as noise, occlusion, rendering artifacts, etc. and can deal with multiple occurrences of the object.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409177",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise robustness",
                "Laplace equations",
                "Biomedical imaging",
                "Image analysis",
                "Biomedical engineering",
                "Image retrieval",
                "Degradation",
                "Biomedical measurements",
                "Noise measurement",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object matching",
                "nonrigid deformations",
                "object retrieval",
                "scale-space interest points",
                "Euclidean transformations",
                "nonrigid transformations"
            ]
        },
        "id": 369,
        "cited_by": []
    },
    {
        "title": "Multi-Object Tracking Through Clutter Using Graph Cuts",
        "authors": [
            "James Malcolm",
            "Yogesh Rathi",
            "Allen Tannenbaum"
        ],
        "abstract": "The standard graph cut technique is a robust method for globally optimal image segmentations. However, because of its global nature, it is prone to capture outlying areas similar to the object of interest. This paper proposes a novel method to constrain the standard graph cut technique for tracking anywhere from one to several objects in regions of interest. For each object, we introduce a pixel penalty based upon distance from a region of interest and so segmentation is biased to remain in this area. Also, we employ a filter predicting the location of the object. The distance penalty is then centered at this location and adoptively scaled based on prediction confidence. This method is capable of tracking multiple interacting objects of different intensity profiles in both gray-scale and color imagery.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409178",
        "reference_list": [
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2005",
                "id": 120
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Narrowband",
                "Optical filters",
                "Image motion analysis",
                "Robustness",
                "Shape",
                "Target tracking",
                "Gray-scale",
                "Color",
                "Focusing"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image colour analysis",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multi-object tracking",
                "graph cut technique",
                "optimal image segmentations",
                "pixel penalty",
                "distance penalty",
                "prediction confidence",
                "color imagery",
                "gray-scale imagery"
            ]
        },
        "id": 370,
        "cited_by": [
            {
                "year": "2009",
                "id": 95
            }
        ]
    },
    {
        "title": "Robust Modelling and Tracking of NonRigid Objects Using Active-GNG",
        "authors": [
            "Anastassia Angelopoulou",
            "Alexandra Psarrou",
            "Gaurav Gupta",
            "Jose Garcia Rodriguez"
        ],
        "abstract": "This paper presents a robust approach to nonrigid modelling and tracking. The contour of the object is described by an active growing neural gas (A-GNG) network which allows the model to re-deform locally. The approach is novel in that the nodes of the network are described by their geometrical position, the underlying local feature structure of the image, and the distance vector between the modal image and any successive images. A second contribution is the correspondence of the nodes which is measured through the calculation of the topographic product, a topology preserving objective function which quantifies the neighbourhood preservation before and after the mapping. As a result, we can achieve the automatic modelling and tracking of objects without using any annotated training sets. Experimental results have shown the superiority of our proposed method over the original growing neural gas (GNG) network.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409179",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Shape",
                "Network topology",
                "Probability distribution",
                "Computer science",
                "Humans",
                "Magnetic resonance imaging",
                "Brain mapping",
                "Tracking",
                "Animation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "neural nets",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active growing neural gas network",
                "image local feature structure",
                "distance vector",
                "objective function"
            ]
        },
        "id": 371,
        "cited_by": []
    },
    {
        "title": "Articulated Shape Matching Using Locally Linear Embedding and Orthogonal Alignment",
        "authors": [
            "Diana Mateus",
            "Fabio Cuzzolin",
            "Radu Horaud",
            "Edmond Boyer"
        ],
        "abstract": "In this paper we propose a method for matching articulated shapes represented as large sets of 3D points by aligning the corresponding embedded clouds generated by locally linear embedding. In particular we show that the problem is equivalent to aligning two sets of points under an orthogonal transformation acting onto the d-dimensional embeddings. The method may well be viewed as belonging to the model-based clustering framework and is implemented as an EM algorithm that alternates between the estimation of correspondences between data-points and the estimation of an optimal alignment transformation. Correspondences are initialized by embedding one set of data- points onto the other one through out-of-sample extension. Results for pairs of voxelsets representing moving persons are presented. Empirical evidence on the influence of the dimension of the embedding space is provided, suggesting that working with higher-dimensional spaces helps matching in challenging real-world scenarios, without collateral effects on the convergence.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409180",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 6,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Clouds",
                "Noise shaping",
                "Embedded computing",
                "Matrix decomposition",
                "Nearest neighbor searches",
                "Clustering algorithms",
                "Convergence",
                "Computer vision",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "expectation-maximisation algorithm",
                "image matching",
                "matrix algebra",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "articulated shape matching method",
                "locally linear embedding",
                "orthogonal alignment",
                "3D points",
                "orthogonal transformation",
                "model-based clustering framework",
                "EM algorithm",
                "computer vision",
                "affinity matrix"
            ]
        },
        "id": 372,
        "cited_by": [
            {
                "year": "2015",
                "id": 186
            }
        ]
    },
    {
        "title": "Symmetries of non-rigid shapes",
        "authors": [
            "Dan Raviv",
            "Alexander M. Bronstein",
            "Michael M. Bronstein",
            "Ron Kimmel"
        ],
        "abstract": "Symmetry and self-similarity is the cornerstone of Nature, exhibiting itself through the shapes of natural creations and ubiquitous laws of physics. Since many natural objects are symmetric, the absence of symmetry can often be an indication of some anomaly or abnormal behavior. Therefore, detection of asymmetries is important in numerous practical applications, including crystallography, medical imaging, and face recognition, to mention a few. Conversely, the assumption of underlying shape symmetry can facilitate solutions to many problems in shape reconstruction and analysis. Traditionally, symmetries are described as extrinsic geometric properties of the shape. While being adequate for rigid shapes, such a description is inappropriate for non-rigid ones. Extrinsic symmetry can be broken as a result of shape deformations, while its intrinsic symmetry is preserved. In this paper, we pose the problem of finding intrinsic symmetries of non-rigid shapes and propose an efficient method for their computation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409181",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 25,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Physics",
                "Biomedical imaging",
                "Face detection",
                "Crystallography",
                "Face recognition",
                "Image reconstruction",
                "Humans",
                "Neoplasms",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical imaging",
                "crystallography",
                "face recognition",
                "fractals",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonrigid shapes symmetry",
                "self-similarity",
                "crystallography",
                "medical imaging",
                "face recognition",
                "shape reconstruction",
                "shape deformations"
            ]
        },
        "id": 373,
        "cited_by": [
            {
                "year": "2009",
                "id": 123
            }
        ]
    },
    {
        "title": "Diffeomorphic Statistical Deformation Models",
        "authors": [
            "Michael S. Hansen",
            "Mads F. Hansen",
            "Rasmus Larsen"
        ],
        "abstract": "In this paper we present a new method for constructing diffeomorphic statistical deformation models in arbitrary dimensional images with a nonlinear generative model and a linear parameter space. Our deformation model is a modified version of the diffeomorphic model introduced by Cootes et al. The modifications ensure that no boundary restriction has to be enforced on the parameter space to prevent folds or tears in the deformation field. For straightforward statistical analysis, principal component analysis and sparse methods, we assume that the parameters for a class of deformations lie on a linear manifold and that the distance between two deformations are given by the metric introduced by the L2-norm in the parameter space. The chosen L2-norm is shown to have a clear and intuitive interpretation on the usual nonlinear manifold. Our model is validated on a set of MR images of corpus callosum with ground truth in form of manual expert annotations, and compared to Cootes's model. We anticipate applications in unconstrained diffeomorphic synthesis of images, e.g. for tracking, segmentation, registration or classification purposes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409182",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Statistical analysis",
                "Principal component analysis",
                "Independent component analysis",
                "Mathematical model",
                "Informatics",
                "Image segmentation",
                "Shape",
                "Kernel",
                "Acceleration"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "diffeomorphic statistical deformation models",
                "arbitrary dimensional images",
                "nonlinear generative model",
                "linear parameter space",
                "deformation model",
                "sparse methods",
                "principal component analysis",
                "MR images",
                "corpus callosum",
                "image registration"
            ]
        },
        "id": 374,
        "cited_by": []
    },
    {
        "title": "Adaptive Parameter Optimization for Real-time Tracking",
        "authors": [
            "Karel Zimmermann",
            "Tomas Svoboda",
            "Jiri Matas"
        ],
        "abstract": "Adaptation of a tracking procedure combined in a common way with a Kalman filter is formulated as an constrained optimization problem, where a trade-off between precision and loss-of-lock probability is explicitly taken into account. While the tracker is learned in order to minimize computational complexity during a learning stage, in a tracking stage the precision is maximized online under a constraint imposed by the loss-of-lock probability resulting in an optimal setting of the tracking procedure. We experimentally show that the proposed method converges to a steady solution in all variables. In contrast to a common Kalman filter based tracking, we achieve a significantly lower state covariance matrix. We also show, that if the covariance matrix is continuously updated, the method is able to adapt to a different situations. If a dynamic model is precise enough the tracker is allowed to spend a longer time with a fine motion estimation, however, if the motion gets saccadic, i.e. unpredictable by the dynamic model, the method automatically gives up the precision in order to avoid loss-of-lock.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409183",
        "reference_list": [
            {
                "year": "2003",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Constraint optimization",
                "Error correction",
                "Computational complexity",
                "Motion measurement",
                "Covariance matrix",
                "Tracking",
                "Time measurement",
                "Design optimization",
                "Cybernetics"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "covariance matrices",
                "Kalman filters",
                "motion estimation",
                "optical tracking",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive parameter optimization",
                "real-time tracking",
                "Kalman filter",
                "constrained optimization problem",
                "computational complexity",
                "loss-of-lock probability",
                "covariance matrix",
                "motion estimation"
            ]
        },
        "id": 375,
        "cited_by": []
    },
    {
        "title": "Finite-Element Level-Set Curve Particles",
        "authors": [
            "Tingting Jiang",
            "Carlo Tomasi"
        ],
        "abstract": "Particle filters encode a time-evolving probability density by maintaining a random sample from it. Level sets represent closed curves as zero crossings of functions of two variables. The combination of level sets and particle filters presents many conceptual advantages when tracking uncertain, evolving boundaries over time, but the cost of combining these two ideas seems prima facie prohibitive. A previous publication showed that a large number of virtual level set particles can be tracked with a logarithmic amount of work for propagation and update. We now make level- set curve particles more efficient by borrowing ideas from the Finite Element Method (FEM). This improves level-set curve particles in both running time (by a constant factor) and accuracy of the results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409184",
        "reference_list": [
            {
                "year": "2003",
                "id": 139
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Finite element methods",
                "Level set",
                "Particle tracking",
                "Particle filters",
                "Animals",
                "Clouds",
                "Encoding",
                "Shape measurement",
                "Particle measurements",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "finite element analysis",
                "particle filtering (numerical methods)",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "finite-element level-set curve particles",
                "particle filters",
                "time-evolving probability density",
                "random sample",
                "zero crossings",
                "finite element method"
            ]
        },
        "id": 376,
        "cited_by": []
    },
    {
        "title": "Bilinear Active Appearance Models",
        "authors": [
            "Jose Gonzalez-Mora",
            "Fernando De la Torre",
            "Rajesh Murthi",
            "Nicolas Guil",
            "Emilio L. Zapata"
        ],
        "abstract": "Appearance models have been applied to model the space of human faces over the last two decades. In particular, active appearance models (AAMs) have been successfully used for face tracking, synthesis and recognition, and they are one of the state-of-the-art approaches due to its efficiency and representational power. Although widely employed, AAMs suffer from a few drawbacks, such as the inability to isolate pose, identity and expression changes. This paper proposes Bilinear Active Appearance Models (BAAMs), an extension of AAMs, that effectively decouple changes due to pose and expression/identity. We derive a gradient-descent algorithm to efficiently fit BAAMs to new images. Experimental results show how BAAMs improve generalization and convergence with respect to the linear model. In addition, we illustrate decoupling benefits of BAAMs in face recognition across pose. We show how the pose normalization provided by BAAMs increase the recognition performance of commercial systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409185",
        "reference_list": [
            {
                "year": "2003",
                "id": 193
            },
            {
                "year": "2003",
                "id": 126
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 10,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Principal component analysis",
                "Face detection",
                "Face recognition",
                "Humans",
                "Computer vision",
                "Active shape model",
                "Computer architecture",
                "Orbital robotics",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "gradient methods",
                "pose estimation",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bilinear active appearance models",
                "human faces",
                "face tracking",
                "face synthesis",
                "face recognition",
                "pose estimation",
                "gradient-descent algorithm",
                "pose normalization"
            ]
        },
        "id": 377,
        "cited_by": []
    },
    {
        "title": "Object Localisation Using Generative Probability Model for Spatial Constellation and Local Image Features",
        "authors": [
            "J.-K. Kamarainen",
            "M. Hamouz",
            "J. Kittler",
            "P. Paalanen",
            "J. Ilonen",
            "A. Drobchenko"
        ],
        "abstract": "In this paper we apply state-of-the-art approach to object detection and localisation by incorporating local descriptors and their spatial configuration into a generative probability model. In contrast to the recent semi- supervised methods we do not utilise interest point detectors, but apply a supervised approach where local image features (landmarks) are annotated in a training set and therefore their appearance and spatial variation can be learnt. Our method enables working in purely probabilistic search spaces providing a MAP estimate of object location, and in contrast to the recent methods, no background class needs to be formed. Using the training set we can estimate pdfs for both spatial constellation and local feature appearance. By applying an inference bias that the largest pdf mode has probability one, we are able to combine prior information (spatial configuration of the features) and observations (image feature appearance) into posterior distribution which can be generatively sampled, e.g. using MCMC techniques. The MCMC methods are sensitive to initialisation, but as a solution, we also propose a very efficient and accurate RANSAC-based method for finding good initial hypotheses of object poses. The complete method can robustly and accurately detect and localise objects under any homography.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409186",
        "reference_list": [
            {
                "year": "2005",
                "id": 225
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Robustness",
                "Solid modeling",
                "Speech processing",
                "Signal processing",
                "Machine vision",
                "Pattern recognition",
                "Signal generators",
                "Detectors",
                "Phase detection"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "learning (artificial intelligence)",
                "maximum likelihood estimation",
                "object detection",
                "search problems",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object localisation",
                "generative probability model",
                "spatial constellation",
                "local image features",
                "object detection",
                "supervised approach",
                "training set",
                "probabilistic search spaces",
                "MAP estimate",
                "inference bias",
                "posterior distribution",
                "MCMC techniques",
                "RANSAC-based method"
            ]
        },
        "id": 378,
        "cited_by": []
    },
    {
        "title": "Linear Predictors for Fast Simultaneous Modeling and Tracking",
        "authors": [
            "Liam Ellis",
            "Nicholas Dowson",
            "Jiri Matas",
            "Richard Bowden"
        ],
        "abstract": "An approach for fast tracking of arbitrary image features with no prior model and no offline learning stage is presented. Fast tracking is achieved using banks of linear displacement predictors learnt online. A multi-modal appearance model is also learnt on-the-fly that facilitates the selection of subsets of predictors suitable for prediction in the next frame. The approach is demonstrated in real-time on a number of challenging video sequences and experimentally compared to other simultaneous modeling and tracking approaches with favourable results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409187",
        "reference_list": [
            {
                "year": "2003",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Target tracking",
                "Active appearance model",
                "Video sequences",
                "Robustness",
                "Performance evaluation",
                "Vectors",
                "Speech processing",
                "Signal processing",
                "Machine learning"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear predictors",
                "fast simultaneous modeling",
                "fast simultaneous tracking",
                "arbitrary image features",
                "offline learning",
                "linear displacement predictors",
                "multimodal appearance model",
                "video sequences"
            ]
        },
        "id": 379,
        "cited_by": []
    },
    {
        "title": "Non-Rigid Object Alignment with a Mismatch Template Based on Exhaustive Local Search",
        "authors": [
            "Yang Wang",
            "Simon Lucey",
            "Jeffrey Cohn"
        ],
        "abstract": "Non-rigid object alignment is especially challenging when only a single appearance template is available and target and template images fail to match. Two sources of discrepancy between target and template are changes in illumination and non-rigid motion. Because most existing methods rely on a holistic representation for the alignment process, they require multiple training images to capture appearance variance. We developed a patch-based method that requires only a single appearance template of the object. Specifically, we fit the patch-based face model to an unseen image using an exhaustive local search and constrain the local warp updates within a global warping space. Our approach is not limited to intensity values or gradients, and therefore offers a natural framework to integrate multiple local features, such as filter responses, to increase robustness to large initialization error, illumination changes and non-rigid deformations. This approach was evaluated experimentally on more than 100 subjects for multiple illumination conditions and facial expressions. In all the experiments, our patch-based method outperforms the holistic gradient descent method in terms of accuracy and robustness of feature alignment and image registration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409188",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 2,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Robustness",
                "Filters",
                "Image registration",
                "Shape",
                "Computer vision",
                "Active appearance model",
                "Robots",
                "Testing",
                "Computer errors"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "gradient methods",
                "image matching",
                "image registration",
                "image representation",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonrigid object alignment",
                "exhaustive local search",
                "patch-based face model",
                "global warping space",
                "mismatch template",
                "feature alignment",
                "image registration",
                "image matching",
                "gradient descent method"
            ]
        },
        "id": 380,
        "cited_by": []
    },
    {
        "title": "Improving Stereo Sub-Pixel Accuracy for Long Range Stereo",
        "authors": [
            "Stefan K. Gehrig",
            "Uwe Franke"
        ],
        "abstract": "Dense stereo algorithms are able to estimate disparities at all pixels including untextured regions. Typically these disparities are evaluated at integer disparity steps. A subsequent sub-pixel interpolation often fails to propagate smoothness constraints on a sub-pixel level. The determination of sub-pixel accurate disparities is an active field of research, however, most sub-pixel estimation algorithms focus on textured image areas in order to show their precision. We propose to increase the sub-pixel accuracy in low- textured regions in three possible ways: First, we present an analysis that shows the benefit of evaluating the disparity space at fractional disparities. Second, we introduce a new disparity smoothing algorithm that preserves depth discontinuities and enforces smoothness on a sub-pixel level. Third, we present a novel stereo constraint (gravitational constraint) that assumes sorted disparity values in vertical direction and guides global algorithms to reduce false matches, especially in low-textured regions. Our goal in this work is to obtain an accurate 3D reconstruction. Large- scale 3D reconstruction will benefit heavily from these sub- pixel refinements, especially with a multi-baseline extension. Results based on semi-global matching , obtained with the above mentioned algorithmic extensions are shown for the Middlebury stereo ground truth data sets. The presented improvements, called ImproveSubPix, turn out to be one of the top-performing algorithms when evaluating the set on a sub-pixel level while being computationally efficient. Additional results are presented for urban scenes. The three improvements are independent of the underlying type of stereo algorithm and can also be applied to sparse stereo algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409212",
        "reference_list": [
            {
                "year": "2003",
                "id": 136
            },
            {
                "year": "2005",
                "id": 71
            },
            {
                "year": "2005",
                "id": 118
            },
            {
                "year": "2001",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 18,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Interpolation",
                "Image reconstruction",
                "Smoothing methods",
                "Layout",
                "Stereo vision",
                "Yield estimation",
                "Belief propagation",
                "Surface fitting",
                "Image motion analysis",
                "Memory"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image reconstruction",
                "image texture",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo sub-pixel accuracy",
                "disparity smoothing algorithm",
                "3D reconstruction",
                "semi-global matching",
                "Middlebury stereo ground truth data sets",
                "improvesubpix",
                "image texture"
            ]
        },
        "id": 381,
        "cited_by": []
    },
    {
        "title": "Semiautomatic registration between ground-level panoramas and an orthorectified aerial image for building modeling",
        "authors": [
            "Lu Wang",
            "Suya You",
            "Ulrich Neumann"
        ],
        "abstract": "Aerial imagery and ground-level imagery are two complementary data sources for architectural modeling. How to integrate them is a critical issue in creating complete, photo-realistic and large-scale urban models. We describe a semiautomatic approach of detecting feature correspondences between ground-level images and the building footprint in an orthorectified aerial image. The ground-level images are stitched into panoramas in order to obtain a wide camera field of view. Line segments are extracted from ground-level images. Their corresponding segments on the building footprints are automatically detected through a voting process. Meanwhile the camera pose of the ground-level images is also obtained. Wrong correspondences are corrected through user interaction. Later, the height values of the building roof corners are computed and a piece-wise planar 3D model with photo-realistic facade and roof texture is then created.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409213",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 0,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image segmentation",
                "Buildings",
                "Voting",
                "Laser radar",
                "Laser modes",
                "Large scale integration",
                "Urban planning",
                "Remote sensing",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "architectural CAD",
                "feature extraction",
                "image registration",
                "image texture",
                "realistic images",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semiautomatic registration",
                "building modeling",
                "orthorectified aerial imagery",
                "ground-level imagery",
                "architectural modeling",
                "photorealistic model",
                "large-scale urban model",
                "feature extraction",
                "user interaction",
                "piece-wise planar 3D model",
                "building roof texture"
            ]
        },
        "id": 382,
        "cited_by": []
    },
    {
        "title": "Automatic Camera Network Localization using Object Image Tracks",
        "authors": [
            "Marci Meingast",
            "Songhwai Oh",
            "Shankar Sastry"
        ],
        "abstract": "Camera networks are being used in more applications as different types of sensor networks are used to instrument large spaces. Here we show a method for localizing the cameras in a camera network to recover the orientation and position up to scale of each camera, even when cameras are wide-baseline or have different photometric properties. Using moving objects in the scene, we use an intra-camera step and an inter-camera step in order to localize. The intra-camera step compares frames from a single camera to build the tracks of the objects in the image plane of the camera. The inter-camera step uses these object image tracks from each camera as features for correspondence between cameras. We demonstrate this idea on both simulated and real data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409214",
        "reference_list": [
            {
                "year": "2001",
                "id": 108
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 2,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Light sources",
                "Calibration",
                "Computer science",
                "Smart cameras",
                "Head",
                "Humans",
                "Application software",
                "Image sensors",
                "Instruments"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "feature extraction",
                "image sensors",
                "object recognition",
                "optical tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic camera network localization",
                "sensor network",
                "object image tracking",
                "image plane",
                "feature extraction"
            ]
        },
        "id": 383,
        "cited_by": [
            {
                "year": "2013",
                "id": 139
            }
        ]
    },
    {
        "title": "A systematic approach for 2D-image to 3D-range registration in urban environments",
        "authors": [
            "Lingyun Liu",
            "Ioannis Stamos"
        ],
        "abstract": "The photorealistic modeling of large-scale objects, such as urban scenes, requires the combination of range sensing technology and digital photography. In this paper, we attack the key problem of camera pose estimation, in an automatic and efficient way. First, the camera orientation is recovered by matching vanishing points (extracted from 2D images) with 3D directions (derived from a 3D range model). Then, a hypothesis-and-test algorithm computes the camera positions with respect to the 3D range model by matching corresponding 2D and 3D linear features. The camera positions are further optimized by minimizing a line-to-line distance. The advantage of our method over earlier work has to do with the fact we do not need to rely on extracted planar facades, or other higher-order features; we are utilizing low- level linear features. That makes this method more general, robust, and efficient. Our method can also be enhanced by the incorporation of traditional structure-from-motion algorithms. We have also developed a user-interface for allowing users to accurately texture-map 2D images onto 3D range models at interactive rates. We have tested our system in a large variety of urban scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409215",
        "reference_list": [
            {
                "year": "2001",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Large-scale systems",
                "Digital photography",
                "Earth",
                "Geometry",
                "Educational institutions",
                "Robustness",
                "System testing",
                "Urban planning"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "feature extraction",
                "image matching",
                "image registration",
                "image texture",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "2D-image registration",
                "3D-range registration",
                "photorealistic modeling",
                "large-scale objects",
                "urban scenes",
                "range sensing technology",
                "digital photography",
                "camera pose estimation",
                "linear features",
                "line-to-line distance",
                "higher-order features",
                "structure-from-motion algorithms",
                "user-interface"
            ]
        },
        "id": 384,
        "cited_by": []
    },
    {
        "title": "Towards Wiki-based Dense City Modeling",
        "authors": [
            "Arnold Irschara",
            "Christopher Zach",
            "Horst Bischof"
        ],
        "abstract": "This work reports on the advances and on the current status of a terrestrial city modeling approach, which uses images contributed by end-users as input. Hence, the Wiki principle well known from textual knowledge databases is transferred to the goal of incrementally building a virtual representation of the occupied habitat. In order to achieve this objective, many state-of-the-art computer vision methods must be applied and modified according to this task. We describe the utilized 3D vision methods and show initial results obtained from the current image database acquired by in-house participants.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409216",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2007",
                "id": 142
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 10,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Cities and towns",
                "Cameras",
                "Image databases",
                "Calibration",
                "Application software",
                "Computer graphics",
                "Earth",
                "Lenses",
                "Computer vision",
                "Computational geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "solid modelling",
                "virtual reality",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "wild-based dense city modeling",
                "terrestrial city modeling",
                "Wiki principle",
                "textual knowledge databases",
                "virtual representation",
                "computer vision",
                "3D vision",
                "image database"
            ]
        },
        "id": 385,
        "cited_by": []
    },
    {
        "title": "Simplifying the Reconstruction of 3D Models using Parameter Elimination",
        "authors": [
            "Daniel G. Aliaga",
            "Ji Zhang",
            "Mireille Boutin"
        ],
        "abstract": "Reconstructing large models from images is a significant challenge for computer vision, computer graphics, and related fields. In this paper, we present an approach for simplifying the reconstruction process by mathematically eliminating external camera parameters. This results in less parameters to estimate and in an overall significantly more robust and accurate reconstruction. We reformulate the problem in such a manner as to be able to identify invariants, eliminate superfluous parameters, and measure the performance of our formulation under various conditions. We compare a two-step camera orientation-free method, where the majority of the points are reconstructed using a linear equation set, and a camera position-and- orientation free method, using a degree-two equation set. Both approaches use a full perspective camera and are applied to synthetic and real-world datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409217",
        "reference_list": [
            {
                "year": "2003",
                "id": 27
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Computer vision",
                "Layout",
                "Parameter estimation",
                "Mathematical model",
                "Computer graphics",
                "Robustness",
                "Equations",
                "Global Positioning System"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "equations",
                "image reconstruction",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D model",
                "image reconstruction",
                "computer vision",
                "computer graphics",
                "camera parameter elimination",
                "linear equation set",
                "camera position-orientation free method"
            ]
        },
        "id": 386,
        "cited_by": []
    },
    {
        "title": "Evaluation of Large Scale Scene Reconstruction",
        "authors": [
            "Paul Merrell",
            "Philippos Mordohai",
            "Jan-Michael Frahm",
            "Marc Pollefeys"
        ],
        "abstract": "We present an evaluation methodology and data for large scale video-based 3D reconstruction. We evaluate the effects of several parameters and draw conclusions that can be useful for practical systems operating in uncontrolled environments. Unlike the benchmark datasets used for the binocular stereo and multi-view reconstruction evaluations, which were collected under well-controlled conditions, our datasets are captured outdoors using video cameras mounted on a moving vehicle. As a result, the videos are much more realistic and include phenomena such as exposure changes from viewing both bright and dim surfaces, objects at varying distances from the camera, and objects of varying size and degrees of texture. The dataset includes ground truth models and precise camera pose information. We also present an evaluation methodology applicable to reconstructions of large scale environments. We evaluate the accuracy and completeness of reconstructions obtained by two fast, visibility-based depth map fusion algorithms as parameters vary.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409218",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 7,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Large-scale systems",
                "Layout",
                "Cameras",
                "Vehicles",
                "Surface reconstruction",
                "Voting",
                "Stereo vision",
                "Cities and towns",
                "Fusion power generation",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image texture",
                "video cameras",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene reconstruction",
                "large scale video-based 3D reconstruction",
                "video camera",
                "image texture",
                "ground truth model",
                "camera pose information"
            ]
        },
        "id": 387,
        "cited_by": []
    },
    {
        "title": "3-D Reconstruction from Sparse Views using Monocular Vision",
        "authors": [
            "Ashutosh Saxena",
            "Min Sun",
            "Andrew Y. Ng"
        ],
        "abstract": "We consider the task of creating a 3-d model of a large novel environment, given only a small number of images of the scene. This is a difficult problem, because if the images are taken from very different viewpoints or if they contain similar-looking structures, then most geometric reconstruction methods will have great difficulty finding good correspondences. Further, the reconstructions given by most algorithms include only points in 3-d that were observed in two or more images; a point observed only in a single image would not be reconstructed. In this paper, we show how monocular image cues can be combined with triangulation cues to build a photo-realistic model of a scene given only a few images\u2014even ones taken from very different viewpoints or with little overlap. Our approach begins by over-segmenting each image into small patches (superpixels). It then simultaneously tries to infer the 3-d position and orientation of every superpixel in every image. This is done using a Markov Random Field (MRF) which simultaneously reasons about monocular cues and about the relations between multiple image patches, both within the same image and across different images (triangulation cues). MAP inference in our model is efficiently approximated using a series of linear programs, and our algorithm scales well to a large number of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/4409219",
        "reference_list": [
            {
                "year": "2007",
                "id": 291
            },
            {
                "year": "2005",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Layout",
                "Image reconstruction",
                "Clouds",
                "Sun",
                "Computer science",
                "Reconstruction algorithms",
                "Markov random fields",
                "Inference algorithms",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "computational geometry",
                "computer vision",
                "image reconstruction",
                "image segmentation",
                "linear programming",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "monocular vision",
                "3D geometric reconstruction",
                "monocular image cues",
                "photo-realistic model",
                "image segmentation",
                "Markov random field",
                "linear program",
                "approximation theory"
            ]
        },
        "id": 388,
        "cited_by": [
            {
                "year": "2013",
                "id": 4
            }
        ]
    }
]