[
    {
        "title": "HOGgles: Visualizing Object Detection Features",
        "authors": [
            "Carl Vondrick",
            "Aditya Khosla",
            "Tomasz Malisiewicz",
            "Antonio Torralba"
        ],
        "abstract": "We introduce algorithms to visualize feature spaces used by object detectors. The tools in this paper allow a human to put on 'HOG goggles' and perceive the visual world as a HOG based object detector sees it. We found that these visualizations allow us to analyze object detection systems in new ways and gain new insight into the detector's failures. For example, when we visualize the features for high scoring false alarms, we discovered that, although they are clearly wrong in image space, they do look deceptively similar to true positives in feature space. This result suggests that many of these false alarms are caused by our choice of feature space, and indicates that creating a better learning algorithm or building bigger datasets is unlikely to correct these errors. By visualizing feature spaces, we can gain a more intuitive understanding of our detection systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751109",
        "reference_list": [
            {
                "year": "2011",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 72,
            "other": 46,
            "total": 118
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Dictionaries",
                "Detectors",
                "Feature extraction",
                "Image reconstruction",
                "Object detection",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "data visualisation",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object detection feature visualization",
                "HOGgles",
                "HOG goggles",
                "detector failures",
                "high scoring false alarms",
                "image space",
                "false alarms",
                "learning algorithm",
                "error correction"
            ],
            "Author Keywords": [
                "object detection",
                "hog",
                "visualization",
                "hoggles"
            ]
        },
        "id": 0,
        "cited_by": [
            {
                "year": "2017",
                "id": 64
            },
            {
                "year": "2015",
                "id": 135
            },
            {
                "year": "2015",
                "id": 320
            }
        ]
    },
    {
        "title": "How Do You Tell a Blackbird from a Crow?",
        "authors": [
            "Thomas Berg",
            "Peter N. Belhumeur"
        ],
        "abstract": "How do you tell a blackbird from a crow? There has been great progress toward automatic methods for visual recognition, including fine-grained visual categorization in which the classes to be distinguished are very similar. In a task such as bird species recognition, automatic recognition systems can now exceed the performance of non-experts - most people are challenged to name a couple dozen bird species, let alone identify them. This leads us to the question, \"Can a recognition system show humans what to look for when identifying classes (in this case birds)?\" In the context of fine-grained visual categorization, we show that we can automatically determine which classes are most visually similar, discover what visual features distinguish very similar classes, and illustrate the key features in a way meaningful to humans. Running these methods on a dataset of bird images, we can generate a visual field guide to birds which includes a tree of similarity that displays the similarity relations between all species, pages for each species showing the most similar other species, and pages for each pair of similar species illustrating their differences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751110",
        "reference_list": [
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 321
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Birds",
                "Visualization",
                "Vegetation",
                "Feature extraction",
                "Vectors",
                "Beak",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "blackbird",
                "crow",
                "visual recognition",
                "fine-grained visual categorization",
                "automatic recognition systems",
                "visual features",
                "bird images"
            ],
            "Author Keywords": [
                "fine-grained recognition",
                "visual similarity",
                "field guide"
            ]
        },
        "id": 1,
        "cited_by": []
    },
    {
        "title": "Regionlets for Generic Object Detection",
        "authors": [
            "Xiaoyu Wang",
            "Ming Yang",
            "Shenghuo Zhu",
            "Yuanqing Lin"
        ],
        "abstract": "Generic object detection is confronted by dealing with different degrees of variations in distinct object classes with tractable computations, which demands for descriptive and flexible object representations that are also efficient to evaluate for many locations. In view of this, we propose to model an object class by a cascaded boosting classifier which integrates various types of features from competing local regions, named as region lets. A region let is a base feature extraction region defined proportionally to a detection window at an arbitrary resolution (i.e. size and aspect ratio). These region lets are organized in small groups with stable relative positions to delineate fine grained spatial layouts inside objects. Their features are aggregated to a one-dimensional feature within one group so as to tolerate deformations. Then we evaluate the object bounding box proposal in selective search from segmentation cues, limiting the evaluation locations to thousands. Our approach significantly outperforms the state-of-the-art on popular multi-class detection benchmark datasets with a single method, without any contexts. It achieves the detection mean average precision of 41.7% on the PASCAL VOC 2007 dataset and 39.7% on the VOC 2010 for 20 object categories. It achieves 14.7% mean average precision on the Image Net dataset for 200 object categories, outperforming the latest deformable part-based model (DPM) by 4.7%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751111",
        "reference_list": [
            {
                "year": "2009",
                "id": 29
            },
            {
                "year": "2009",
                "id": 30
            },
            {
                "year": "2011",
                "id": 64
            },
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2009",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 118,
            "other": 42,
            "total": 160
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Boosting",
                "Object detection",
                "Deformable models",
                "Prototypes",
                "Search problems",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image representation",
                "object detection",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "regionlets",
                "generic object detection",
                "object classes",
                "tractable computations",
                "flexible object representations",
                "descriptive object representations",
                "cascaded boosting classifier",
                "local regions",
                "feature extraction region",
                "detection window",
                "arbitrary resolution",
                "fine-grained spatial layouts",
                "one-dimensional feature",
                "object bounding box proposal",
                "segmentation cues",
                "multiclass detection benchmark datasets",
                "detection mean average precision",
                "PASCAL VOC 2007 dataset",
                "VOC 2010",
                "object categories",
                "ImageNet dataset",
                "deformable part-based model",
                "DPM"
            ],
            "Author Keywords": [
                "Regionlet",
                "Object Detection",
                "DPM",
                "Deformation",
                "Subcategory",
                "PASCAL",
                "ImageNet",
                "Detection"
            ]
        },
        "id": 2,
        "cited_by": [
            {
                "year": "2017",
                "id": 117
            },
            {
                "year": "2015",
                "id": 5
            },
            {
                "year": "2015",
                "id": 31
            },
            {
                "year": "2015",
                "id": 111
            },
            {
                "year": "2015",
                "id": 158
            },
            {
                "year": "2015",
                "id": 211
            },
            {
                "year": "2015",
                "id": 212
            },
            {
                "year": "2015",
                "id": 225
            },
            {
                "year": "2015",
                "id": 278
            },
            {
                "year": "2015",
                "id": 287
            },
            {
                "year": "2015",
                "id": 338
            },
            {
                "year": "2015",
                "id": 375
            },
            {
                "year": "2015",
                "id": 487
            }
        ]
    },
    {
        "title": "Learning Graphs to Match",
        "authors": [
            "Minsu Cho",
            "Karteek Alahari",
            "Jean Ponce"
        ],
        "abstract": "Many tasks in computer vision are formulated as graph matching problems. Despite the NP-hard nature of the problem, fast and accurate approximations have led to significant progress in a wide range of applications. Learning graph models from observed data, however, still remains a challenging issue. This paper presents an effective scheme to parameterize a graph model, and learn its structural attributes for visual object matching. For this, we propose a graph representation with histogram-based attributes, and optimize them to increase the matching accuracy. Experimental evaluations on synthetic and real image datasets demonstrate the effectiveness of our approach, and show significant improvement in matching accuracy over graphs with pre-defined structures.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751112",
        "reference_list": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 28,
            "total": 62
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Vectors",
                "Optimization",
                "Context",
                "Computer vision",
                "Computational modeling",
                "Learning systems"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "graph matching problems",
                "NP-hard problem",
                "graph model",
                "visual object matching",
                "histogram-based attributes",
                "graph representation"
            ],
            "Author Keywords": [
                "graph matching",
                "graph learning",
                "feature correspondence",
                "object recognition"
            ]
        },
        "id": 3,
        "cited_by": [
            {
                "year": "2015",
                "id": 6
            },
            {
                "year": "2015",
                "id": 12
            },
            {
                "year": "2015",
                "id": 22
            }
        ]
    },
    {
        "title": "Shape Anchors for Data-Driven Multi-view Reconstruction",
        "authors": [
            "Andrew Owens",
            "Jianxiong Xiao",
            "Antonio Torralba",
            "William Freeman"
        ],
        "abstract": "We present a data-driven method for building dense 3D reconstructions using a combination of recognition and multi-view cues. Our approach is based on the idea that there are image patches that are so distinctive that we can accurately estimate their latent 3D shapes solely using recognition. We call these patches shape anchors, and we use them as the basis of a multi-view reconstruction system that transfers dense, complex geometry between scenes. We \"anchor\" our 3D interpretation from these patches, using them to predict geometry for parts of the scene that are relatively ambiguous. The resulting algorithm produces dense reconstructions from stereo point clouds that are sparse and noisy, and we demonstrate it on a challenging dataset of real-world, indoor scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751113",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 283
            },
            {
                "year": "2009",
                "id": 10
            },
            {
                "year": "2007",
                "id": 388
            },
            {
                "year": "2013",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image reconstruction",
                "Three-dimensional displays",
                "Geometry",
                "Databases",
                "Cameras",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image reconstruction",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape anchor",
                "data-driven multiview reconstruction",
                "dense 3D reconstruction",
                "image recognition",
                "image patches",
                "stereo point clouds"
            ]
        },
        "id": 4,
        "cited_by": [
            {
                "year": "2015",
                "id": 245
            },
            {
                "year": "2013",
                "id": 202
            }
        ]
    },
    {
        "title": "Deterministic Fitting of Multiple Structures Using Iterative MaxFS with Inlier Scale Estimation",
        "authors": [
            "Kwang Hee Lee",
            "Sang Wook Lee"
        ],
        "abstract": "We present an efficient deterministic hypothesis generation algorithm for robust fitting of multiple structures based on the maximum feasible subsystem (MaxFS) framework. Despite its advantage, a global optimization method such as MaxFS has two main limitations for geometric model fitting. First, its performance is much influenced by the user-specified inlier scale. Second, it is computationally inefficient for large data. The presented algorithm, called iterative MaxFS with inlier scale (IMaxFS-ISE), iteratively estimates model parameters and inlier scale and also overcomes the second limitation by reducing data for the MaxFS problem. The IMaxFS-ISE algorithm generates hypotheses only with top-n ranked subsets based on matching scores and data fitting residuals. This reduction of data for the MaxFS problem makes the algorithm computationally realistic. A sequential \"fitting-and-removing\" procedure is repeated until overall energy function does not decrease. Experimental results demonstrate that our method can generate more reliable and consistent hypotheses than random sampling-based methods for estimating multiple structures from data with many outliers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751114",
        "reference_list": [
            {
                "year": "2009",
                "id": 137
            },
            {
                "year": "2009",
                "id": 282
            },
            {
                "year": "2009",
                "id": 269
            },
            {
                "year": "2009",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Conferences",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "iterative methods",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple structures fitting",
                "iterative MaxFS",
                "inlier scale estimation",
                "deterministic hypothesis generation algorithm",
                "maximum feasible subsystem",
                "global optimization method",
                "user-specified inlier scale",
                "IMaxFS-ISE algorithm",
                "matching scores",
                "data fitting residuals",
                "sequential fitting-and-removing procedure",
                "energy function",
                "computer vision"
            ],
            "Author Keywords": [
                "MaxFS",
                "fitting of multiple strucutres",
                "inlier scale"
            ]
        },
        "id": 5,
        "cited_by": []
    },
    {
        "title": "Constant Time Weighted Median Filtering for Stereo Matching and Beyond",
        "authors": [
            "Ziyang Ma",
            "Kaiming He",
            "Yichen Wei",
            "Jian Sun",
            "Enhua Wu"
        ],
        "abstract": "Despite the continuous advances in local stereo matching for years, most efforts are on developing robust cost computation and aggregation methods. Little attention has been seriously paid to the disparity refinement. In this work, we study weighted median filtering for disparity refinement. We discover that with this refinement, even the simple box filter aggregation achieves comparable accuracy with various sophisticated aggregation methods (with the same refinement). This is due to the nice weighted median filtering properties of removing outlier error while respecting edges/structures. This reveals that the previously overlooked refinement can be at least as crucial as aggregation. We also develop the first constant time algorithm for the previously time-consuming weighted median filter. This makes the simple combination ``box aggregation + weighted median'' an attractive solution in practice for both speed and accuracy. As a byproduct, the fast weighted median filtering unleashes its potential in other applications that were hampered by high complexities. We show its superiority in various applications such as depth up sampling, clip-art JPEG artifact removal, and image stylization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751115",
        "reference_list": [
            {
                "year": "2011",
                "id": 198
            }
        ],
        "citation": {
            "ieee": 99,
            "other": 40,
            "total": 139
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Filtering",
                "Accuracy",
                "Error analysis",
                "Complexity theory",
                "Image edge detection",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "median filters",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth up sampling",
                "image stylization",
                "JPEG artifact removal",
                "outlier error removal",
                "simple box filter aggregation",
                "disparity refinement",
                "aggregation methods",
                "robust cost computation",
                "local stereo matching",
                "constant time weighted median filtering properties"
            ]
        },
        "id": 6,
        "cited_by": [
            {
                "year": "2017",
                "id": 263
            },
            {
                "year": "2015",
                "id": 380
            }
        ]
    },
    {
        "title": "Refractive Structure-from-Motion on Underwater Images",
        "authors": [
            "Anne Jordt-Sedlazeck",
            "Reinhard Koch"
        ],
        "abstract": "In underwater environments, cameras need to be confined in an underwater housing, viewing the scene through a piece of glass. In case of flat port underwater housings, light rays entering the camera housing are refracted twice, due to different medium densities of water, glass, and air. This causes the usually linear rays of light to bend and the commonly used pinhole camera model to be invalid. When using the pinhole camera model without explicitly modeling refraction in Structure-from-Motion (SfM) methods, a systematic model error occurs. Therefore, in this paper, we propose a system for computing camera path and 3D points with explicit incorporation of refraction using new methods for pose estimation. Additionally, a new error function is introduced for non-linear optimization, especially bundle adjustment. The proposed method allows to increase reconstruction accuracy and is evaluated in a set of experiments, where the proposed method's performance is compared to SfM with the perspective camera model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751116",
        "reference_list": [
            {
                "year": "2011",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 9,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Glass",
                "Optimization",
                "Noise",
                "Computational modeling",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "image sensors",
                "motion estimation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "refractive structure-from-motion",
                "underwater images",
                "underwater environments",
                "underwater housing",
                "light rays",
                "pinhole camera model",
                "SfM",
                "systematic model error",
                "camera path computing",
                "pose estimation",
                "nonlinear optimization"
            ],
            "Author Keywords": [
                "Structure from Motion",
                "underwater",
                "refraction",
                "3D model"
            ]
        },
        "id": 7,
        "cited_by": []
    },
    {
        "title": "Live Metric 3D Reconstruction on Mobile Phones",
        "authors": [
            "Petri Tanskanen",
            "Kalin Kolev",
            "Lorenz Meier",
            "Federico Camposeco",
            "Olivier Saurer",
            "Marc Pollefeys"
        ],
        "abstract": "In this paper, we propose a complete on-device 3D reconstruction pipeline for mobile monocular hand-held devices, which generates dense 3D models with absolute scale on-site while simultaneously supplying the user with real-time interactive feedback. The method fills a gap in current cloud-based mobile reconstruction services as it ensures at capture time that the acquired image set fulfills desired quality and completeness criteria. In contrast to existing systems, the developed framework offers multiple innovative solutions. In particular, we investigate the usability of the available on-device inertial sensors to make the tracking and mapping process more resilient to rapid motions and to estimate the metric scale of the captured scene. Moreover, we propose an efficient and accurate scheme for dense stereo matching which allows to reduce the processing time to interactive speed. We demonstrate the performance of the reconstruction pipeline on multiple challenging indoor and outdoor scenes of different size and depth variability.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751117",
        "reference_list": [
            {
                "year": "2011",
                "id": 295
            },
            {
                "year": "2011",
                "id": 326
            }
        ],
        "citation": {
            "ieee": 70,
            "other": 45,
            "total": 115
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Visualization",
                "Cameras",
                "Estimation",
                "Tracking",
                "Accelerometers",
                "Sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "cloud computing",
                "image matching",
                "image reconstruction",
                "mobile handsets",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "live metric 3D reconstruction",
                "mobile phones",
                "on-device 3D reconstruction pipeline",
                "mobile monocular handheld devices",
                "dense 3D model generation",
                "absolute scale on-site",
                "real-time interactive feedback",
                "cloud-based mobile reconstruction services",
                "capture time",
                "quality criteria",
                "completeness criteria",
                "on-device inertial sensor usability",
                "tracking process",
                "mapping process",
                "dense stereo matching",
                "processing time reduction",
                "interactive speed",
                "indoor scene",
                "outdoor scene",
                "depth variability"
            ]
        },
        "id": 8,
        "cited_by": [
            {
                "year": "2017",
                "id": 410
            },
            {
                "year": "2015",
                "id": 102
            }
        ]
    },
    {
        "title": "Joint Subspace Stabilization for Stereoscopic Video",
        "authors": [
            "Feng Liu",
            "Yuzhen Niu",
            "Hailin Jin"
        ],
        "abstract": "Shaky stereoscopic video is not only unpleasant to watch but may also cause 3D fatigue. Stabilizing the left and right view of a stereoscopic video separately using a monocular stabilization method tends to both introduce undesirable vertical disparities and damage horizontal disparities, which may destroy the stereoscopic viewing experience. In this paper, we present a joint subspace stabilization method for stereoscopic video. We prove that the low-rank subspace constraint for monocular video [10] also holds for stereoscopic video. Particularly, the feature trajectories from the left and right video share the same subspace. Based on this proof, we develop a stereo subspace stabilization method that jointly computes a common subspace from the left and right video and uses it to stabilize the two videos simultaneously. Our method meets the stereoscopic constraints without 3D reconstruction or explicit left-right correspondence. We test our method on a variety of stereoscopic videos with different scene content and camera motion. The experiments show that our method achieves high-quality stabilization for stereoscopic video in a robust and efficient way.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751118",
        "reference_list": [
            {
                "year": "2009",
                "id": 179
            },
            {
                "year": "2009",
                "id": 43
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Trajectory",
                "Stereo image processing",
                "Three-dimensional displays",
                "Joints",
                "Equations",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "feature extraction",
                "stereo image processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subspace stabilization",
                "shaky stereoscopic video",
                "3D fatigue",
                "monocular stabilization method",
                "vertical disparities",
                "damage horizontal disparities",
                "subspace stabilization method",
                "low-rank subspace constraint",
                "monocular video",
                "feature trajectories",
                "stereo subspace stabilization method",
                "stereoscopic constraints",
                "3D reconstruction",
                "camera motion"
            ]
        },
        "id": 9,
        "cited_by": []
    },
    {
        "title": "Video Synopsis by Heterogeneous Multi-source Correlation",
        "authors": [
            "Xiatian Zhu",
            "Chen Change Loy",
            "Shaogang Gong"
        ],
        "abstract": "Generating coherent synopsis for surveillance video stream remains a formidable challenge due to the ambiguity and uncertainty inherent to visual observations. In contrast to existing video synopsis approaches that rely on visual cues alone, we propose a novel multi-source synopsis framework capable of correlating visual data and independent non-visual auxiliary information to better describe and summarise subtle physical events in complex scenes. Specifically, our unsupervised framework is capable of seamlessly uncovering latent correlations among heterogeneous types of data sources, despite the non-trivial heteroscedasticity and dimensionality discrepancy problems. Additionally, the proposed model is robust to partial or missing non-visual information. We demonstrate the effectiveness of our framework on two crowded public surveillance datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751119",
        "reference_list": [
            {
                "year": "2013",
                "id": 241
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 5,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Training",
                "Feature extraction",
                "Data models",
                "Correlation",
                "Surveillance",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "unsupervised learning",
                "video streaming",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "heterogeneous multisource correlation",
                "video stream surveillance",
                "video synopsis approach",
                "novel multisource synopsis framework",
                "unsupervised framework",
                "nontrivial heteroscedasticity",
                "dimensionality discrepancy problems",
                "crowded public surveillance datasets"
            ],
            "Author Keywords": [
                "video synopsis",
                "learning heterogeneous data sources",
                "multi-source correlation",
                "noisy data",
                "partial/missing data"
            ]
        },
        "id": 10,
        "cited_by": [
            {
                "year": "2013",
                "id": 55
            },
            {
                "year": "2013",
                "id": 241
            }
        ]
    },
    {
        "title": "DCSH - Matching Patches in RGBD Images",
        "authors": [
            "Yaron Eshet",
            "Simon Korman",
            "Eyal Ofek",
            "Shai Avidan"
        ],
        "abstract": "We extend patch based methods to work on patches in 3D space. We start with Coherency Sensitive Hashing (CSH), which is an algorithm for matching patches between two RGB images, and extend it to work with RGBD images. This is done by warping all 3D patches to a common virtual plane in which CSH is performed. To avoid noise due to warping of patches of various normals and depths, we estimate a group of dominant planes and compute CSH on each plane separately, before merging the matching patches. The result is DCSH - an algorithm that matches world (3D) patches in order to guide the search for image plane matches. An independent contribution is an extension of CSH, which we term Social-CSH. It allows a major speedup of the k nearest neighbor (kNN) version of CSH - its runtime growing linearly, rather than quadratic ally, in k. Social-CSH is used as a subcomponent of DCSH when many NNs are required, as in the case of image denoising. We show the benefits of using depth information to image reconstruction and image denoising, demonstrated on several RGBD images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751120",
        "reference_list": [
            {
                "year": "2011",
                "id": 203
            },
            {
                "year": "2011",
                "id": 205
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Standards",
                "Image reconstruction",
                "Accuracy",
                "Data structures",
                "Boolean functions",
                "Runtime"
            ],
            "INSPEC: Controlled Indexing": [
                "image denoising",
                "image matching",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "DCSH",
                "patch matching",
                "RGBD images",
                "patch-based methods",
                "3D space",
                "coherency sensitive hashing",
                "3D patch warping",
                "common virtual plane",
                "dominant plane group estimation",
                "image plane match",
                "Social-CSH",
                "k nearest neighbor version",
                "kNN version",
                "image denoising",
                "depth information",
                "image reconstruction"
            ]
        },
        "id": 11,
        "cited_by": []
    },
    {
        "title": "Scene Text Localization and Recognition with Oriented Stroke Detection",
        "authors": [
            "Luk\u00e1 Neumann",
            "Jiri Matas"
        ],
        "abstract": "An unconstrained end-to-end text localization and recognition method is presented. The method introduces a novel approach for character detection and recognition which combines the advantages of sliding-window and connected component methods. Characters are detected and recognized as image regions which contain strokes of specific orientations in a specific relative position, where the strokes are efficiently detected by convolving the image gradient field with a set of oriented bar filters. Additionally, a novel character representation efficiently calculated from the values obtained in the stroke detection phase is introduced. The representation is robust to shift at the stroke level, which makes it less sensitive to intra-class variations and the noise induced by normalizing character size and positioning. The effectiveness of the representation is demonstrated by the results achieved in the classification of real-world characters using an euclidian nearest-neighbor classifier trained on synthetic data in a plain form. The method was evaluated on a standard dataset, where it achieves state-of-the-art results in both text localization and recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751121",
        "reference_list": [
            {
                "year": "2011",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 64,
            "other": 36,
            "total": 100
        },
        "keywords": {
            "IEEE Keywords": [
                "Text recognition",
                "Character recognition",
                "Training",
                "Robustness",
                "Noise",
                "Optical character recognition software",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "character recognition",
                "image representation",
                "text detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene text localization",
                "scene text recognition",
                "oriented stroke detection",
                "unconstrained end-to-end text localization method",
                "unconstrained end-to-end text recognition method",
                "character detection",
                "character recognition",
                "sliding-window method",
                "connected component method",
                "image regions",
                "specific relative position",
                "stroke detection efficiency",
                "image gradient field",
                "oriented bar filter set",
                "character representation",
                "stroke detection phase",
                "stroke level",
                "intraclass variations",
                "character size normalization",
                "positioning normalization",
                "Euclidian nearest-neighbor classifier",
                "standard dataset"
            ],
            "Author Keywords": [
                "text-in-the-wild",
                "text localization",
                "text recognition",
                "nature scene text"
            ]
        },
        "id": 12,
        "cited_by": [
            {
                "year": "2017",
                "id": 550
            },
            {
                "year": "2015",
                "id": 138
            }
        ]
    },
    {
        "title": "Adapting Classification Cascades to New Domains",
        "authors": [
            "Vidit Jain",
            "Sachin Sudhakar Farfade"
        ],
        "abstract": "Classification cascades have been very effective for object detection. Such a cascade fails to perform well in data domains with variations in appearances that may not be captured in the training examples. This limited generalization severely restricts the domains for which they can be used effectively. A common approach to address this limitation is to train a new cascade of classifiers from scratch for each of the new domains. Building separate detectors for each of the different domains requires huge annotation and computational effort, making it not scalable to a large number of data domains. Here we present an algorithm for quickly adapting a pre-trained cascade of classifiers - using a small number of labeled positive instances from a different yet similar data domain. In our experiments with images of human babies and human-like characters from movies, we demonstrate that the adapted cascade significantly outperforms both of the original cascade and the one trained from scratch using the given training examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751122",
        "reference_list": [
            {
                "year": "2007",
                "id": 9
            },
            {
                "year": "2007",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Detectors",
                "Motion pictures",
                "Adaptation models",
                "Pediatrics",
                "Face detection",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "data domains",
                "pretrained classifier cascade",
                "labeled positive instances",
                "human-like characters",
                "movies",
                "classification cascade",
                "object detection"
            ],
            "Author Keywords": [
                "Face detection",
                "domain adaptation",
                "classification cascades"
            ]
        },
        "id": 13,
        "cited_by": []
    },
    {
        "title": "Deep Learning Identity-Preserving Face Space",
        "authors": [
            "Zhenyao Zhu",
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Face recognition with large pose and illumination variations is a challenging problem in computer vision. This paper addresses this challenge by proposing a new learning based face representation: the face identity-preserving (FIP) features. Unlike conventional face descriptors, the FIP features can significantly reduce intra-identity variances, while maintaining discriminative ness between identities. Moreover, the FIP features extracted from an image under any pose and illumination can be used to reconstruct its face image in the canonical view. This property makes it possible to improve the performance of traditional descriptors, such as LBP [2] and Gabor [31], which can be extracted from our reconstructed images in the canonical view to eliminate variations. In order to learn the FIP features, we carefully design a deep network that combines the feature extraction layers and the reconstruction layer. The former encodes a face image into the FIP features, while the latter transforms them to an image in the canonical view. Extensive experiments on the large MultiPIE face database [7] demonstrate that it significantly outperforms the state-of-the-art face recognition methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751123",
        "reference_list": [
            {
                "year": "2011",
                "id": 118
            },
            {
                "year": "2011",
                "id": 317
            },
            {
                "year": "2013",
                "id": 185
            },
            {
                "year": "2011",
                "id": 256
            },
            {
                "year": "2005",
                "id": 101
            }
        ],
        "citation": {
            "ieee": 101,
            "other": 44,
            "total": 145
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Lighting",
                "Feature extraction",
                "Image reconstruction",
                "Training",
                "Face recognition",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image reconstruction",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep learning identity-preserving face space",
                "illumination variations",
                "large pose variations",
                "computer vision",
                "learning based face representation",
                "face identity-preserving feature",
                "FIP feature extraction",
                "face descriptors",
                "intra-identity variance reduction",
                "canonical view",
                "image reconstruction layer",
                "face image encoding",
                "MultiPIE face database",
                "face recognition methods"
            ],
            "Author Keywords": [
                "deep learning",
                "face recognition"
            ]
        },
        "id": 14,
        "cited_by": [
            {
                "year": "2017",
                "id": 170
            },
            {
                "year": "2017",
                "id": 257
            },
            {
                "year": "2017",
                "id": 394
            },
            {
                "year": "2017",
                "id": 420
            },
            {
                "year": "2015",
                "id": 211
            },
            {
                "year": "2015",
                "id": 405
            },
            {
                "year": "2015",
                "id": 409
            },
            {
                "year": "2015",
                "id": 431
            },
            {
                "year": "2015",
                "id": 432
            },
            {
                "year": "2013",
                "id": 15
            },
            {
                "year": "2013",
                "id": 185
            },
            {
                "year": "2013",
                "id": 330
            },
            {
                "year": "2013",
                "id": 410
            }
        ]
    },
    {
        "title": "Multi-stage Contextual Deep Learning for Pedestrian Detection",
        "authors": [
            "Xingyu Zeng",
            "Wanli Ouyang",
            "Xiaogang Wang"
        ],
        "abstract": "Cascaded classifiers have been widely used in pedestrian detection and achieved great success. These classifiers are trained sequentially without joint optimization. In this paper, we propose a new deep model that can jointly train multi-stage classifiers through several stages of back propagation. It keeps the score map output by a classifier within a local region and uses it as contextual information to support the decision at the next stage. Through a specific design of the training strategy, this deep architecture is able to simulate the cascaded classifiers by mining hard samples to train the network stage-by-stage. Each classifier handles samples at a different difficulty level. Unsupervised pre-training and specifically designed stage-wise supervised training are used to regularize the optimization problem. Both theoretical analysis and experimental results show that the training strategy helps to avoid over fitting. Experimental results on three datasets (Caltech, ETH and TUD-Brussels) show that our approach outperforms the state-of-the-art approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751124",
        "reference_list": [
            {
                "year": "2007",
                "id": 252
            },
            {
                "year": "2013",
                "id": 330
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2009",
                "id": 3
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2009",
                "id": 4
            },
            {
                "year": "2005",
                "id": 11
            },
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 49,
            "other": 19,
            "total": 68
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Cascading style sheets",
                "Support vector machines",
                "Computer architecture",
                "Computational modeling",
                "Optimization",
                "Context modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "backpropagation",
                "optimisation",
                "pattern classification",
                "pedestrians"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multistage contextual deep learning",
                "pedestrian detection",
                "cascaded classifiers",
                "multistage classifiers",
                "backpropagation",
                "unsupervised pretraining",
                "stage-wise supervised training",
                "optimization"
            ]
        },
        "id": 15,
        "cited_by": [
            {
                "year": "2015",
                "id": 46
            },
            {
                "year": "2015",
                "id": 211
            },
            {
                "year": "2015",
                "id": 212
            },
            {
                "year": "2013",
                "id": 256
            }
        ]
    },
    {
        "title": "Unsupervised Random Forest Manifold Alignment for Lipreading",
        "authors": [
            "Yuru Pei",
            "Tae-Kyun Kim",
            "Hongbin Zha"
        ],
        "abstract": "Lip reading from visual channels remains a challenging topic considering the various speaking characteristics. In this paper, we address an efficient lip reading approach by investigating the unsupervised random forest manifold alignment (RFMA). The density random forest is employed to estimate affinity of patch trajectories in speaking facial videos. We propose novel criteria for node splitting to avoid the rank-deficiency in learning density forests. By virtue of the hierarchical structure of random forests, the trajectory affinities are measured efficiently, which are used to find embeddings of the speaking video clips by a graph-based algorithm. Lip reading is formulated as matching between manifolds of query and reference video clips. We employ the manifold alignment technique for matching, where the L \u221e -norm-based manifold-to-manifold distance is proposed to find the matching pairs. We apply this random forest manifold alignment technique to various video data sets captured by consumer cameras. The experiments demonstrate that lip reading can be performed effectively, and outperform state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751125",
        "reference_list": [
            {
                "year": "2005",
                "id": 186
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 18,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Videos",
                "Trajectory",
                "Vegetation",
                "Image color analysis",
                "Covariance matrices",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised random forest manifold alignment",
                "visual channels",
                "speaking characteristics",
                "lipreading approach",
                "RFMA",
                "density random forest",
                "patch trajectories",
                "speaking facial videos",
                "rank-deficiency",
                "learning density forests",
                "hierarchical structure",
                "speaking video clips",
                "graph-based algorithm",
                "query manifolds",
                "reference video clips",
                "L\u221e-norm-based manifold-to-manifold distance",
                "matching pairs",
                "random forest manifold alignment technique",
                "consumer cameras"
            ],
            "Author Keywords": [
                "Unsupervised Random Forest",
                "Manifold Alignment",
                "Lipreading",
                "RFMA"
            ]
        },
        "id": 16,
        "cited_by": [
            {
                "year": "2015",
                "id": 17
            }
        ]
    },
    {
        "title": "Calibration-Free Gaze Estimation Using Human Gaze Patterns",
        "authors": [
            "Fares Alnajar",
            "Theo Gevers",
            "Roberto Valenti",
            "Sennay Ghebreab"
        ],
        "abstract": "We present a novel method to auto-calibrate gaze estimators based on gaze patterns obtained from other viewers. Our method is based on the observation that the gaze patterns of humans are indicative of where a new viewer will look at. When a new viewer is looking at a stimulus, we first estimate a topology of gaze points (initial gaze points). Next, these points are transformed so that they match the gaze patterns of other humans to find the correct gaze points. In a flexible uncalibrated setup with a web camera and no chin rest, the proposed method was tested on ten subjects and ten images. The method estimates the gaze points after looking at a stimulus for a few seconds with an average accuracy of 4:3\u00b0. Although the reported performance is lower than what could be achieved with dedicated hardware or calibrated setup, the proposed method still provides a sufficient accuracy to trace the viewer attention. This is promising considering the fact that auto-calibration is done in a flexible setup, without the use of a chin rest, and based only on a few seconds of gaze initialization data. To the best of our knowledge, this is the first work to use human gaze patterns in order to auto-calibrate gaze estimators.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751126",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 8,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Pattern matching",
                "Cameras",
                "Calibration",
                "Manifolds",
                "Accuracy",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "gaze tracking",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "calibration-free gaze estimation",
                "human gaze patterns",
                "flexible uncalibrated setup",
                "Web camera"
            ]
        },
        "id": 17,
        "cited_by": []
    },
    {
        "title": "Partial Sum Minimization of Singular Values in RPCA for Low-Level Vision",
        "authors": [
            "Tae-Hyun Oh",
            "Hyeongwoo Kim",
            "Yu-Wing Tai",
            "Jean-Charles Bazin",
            "In So Kweon"
        ],
        "abstract": "Robust Principal Component Analysis (RPCA) via rank minimization is a powerful tool for recovering underlying low-rank structure of clean data corrupted with sparse noise/outliers. In many low-level vision problems, not only it is known that the underlying structure of clean data is low-rank, but the exact rank of clean data is also known. Yet, when applying conventional rank minimization for those problems, the objective function is formulated in a way that does not fully utilize a priori target rank information about the problems. This observation motivates us to investigate whether there is a better alternative solution when using rank minimization. In this paper, instead of minimizing the nuclear norm, we propose to minimize the partial sum of singular values. The proposed objective function implicitly encourages the target rank constraint in rank minimization. Our experimental analyses show that our approach performs better than conventional rank minimization when the number of samples is deficient, while the solutions obtained by the two approaches are almost identical when the number of samples is more than sufficient. We apply our approach to various low-level vision problems, e.g. high dynamic range imaging, photometric stereo and image alignment, and show that our results outperform those obtained by the conventional nuclear norm rank minimization method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751127",
        "reference_list": [
            {
                "year": "2001",
                "id": 49
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 8,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Minimization",
                "Linear programming",
                "Robustness",
                "Sparse matrices",
                "Stereo vision",
                "Vectors",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "minimisation",
                "principal component analysis",
                "singular value decomposition",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "partial sum minimization",
                "singular values",
                "RPCA",
                "robust principal component analysis",
                "low rank structure",
                "sparse noise",
                "outliers",
                "low level vision problems",
                "clean data structure",
                "exact rank",
                "rank information",
                "target rank constraint",
                "conventional rank minimization",
                "high dynamic range imaging",
                "photometric stereo",
                "image alignment",
                "nuclear norm rank minimization method"
            ],
            "Author Keywords": [
                "Rank minimization",
                "Robust PCA",
                "Nuclear Norm",
                "Partial sum of singular values"
            ]
        },
        "id": 18,
        "cited_by": [
            {
                "year": "2017",
                "id": 185
            },
            {
                "year": "2015",
                "id": 475
            }
        ]
    },
    {
        "title": "Saliency Detection: A Boolean Map Approach",
        "authors": [
            "Jianming Zhang",
            "Stan Sclaroff"
        ],
        "abstract": "A novel Boolean Map based Saliency (BMS) model is proposed. An image is characterized by a set of binary images, which are generated by randomly thresholding the image's color channels. Based on a Gestalt principle of figure-ground segregation, BMS computes saliency maps by analyzing the topological structure of Boolean maps. BMS is simple to implement and efficient to run. Despite its simplicity, BMS consistently achieves state-of-the-art performance compared with ten leading methods on five eye tracking datasets. Furthermore, BMS is also shown to be advantageous in salient object detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751128",
        "reference_list": [],
        "citation": {
            "ieee": 127,
            "other": 76,
            "total": 203
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Visualization",
                "Kernel",
                "Measurement",
                "Standards",
                "Object detection",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "Boolean functions",
                "image colour analysis",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "saliency detection",
                "Boolean map approach",
                "Boolean map based saliency model",
                "BMS model",
                "binary images",
                "image color channels",
                "Gestalt principle",
                "figure-ground segregation",
                "eye tracking datasets",
                "salient object detection"
            ],
            "Author Keywords": [
                "visual saliency",
                "eye fixation",
                "salient object detection"
            ]
        },
        "id": 19,
        "cited_by": [
            {
                "year": "2017",
                "id": 178
            },
            {
                "year": "2017",
                "id": 503
            },
            {
                "year": "2015",
                "id": 21
            },
            {
                "year": "2015",
                "id": 24
            },
            {
                "year": "2015",
                "id": 29
            },
            {
                "year": "2015",
                "id": 156
            },
            {
                "year": "2015",
                "id": 436
            }
        ]
    },
    {
        "title": "Topology-Constrained Layered Tracking with Latent Flow",
        "authors": [
            "Jason Chang",
            "John W. Fisher"
        ],
        "abstract": "We present an integrated probabilistic model for layered object tracking that combines dynamics on implicit shape representations, topological shape constraints, adaptive appearance models, and layered flow. The generative model combines the evolution of appearances and layer shapes with a Gaussian process flow and explicit layer ordering. Efficient MCMC sampling algorithms are developed to enable a particle filtering approach while reasoning about the distribution of object boundaries in video. We demonstrate the utility of the proposed tracking algorithm on a wide variety of video sources while achieving state-of-the-art results on a boundary-accurate tracking dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751129",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            },
            {
                "year": "2001",
                "id": 148
            },
            {
                "year": "2011",
                "id": 147
            },
            {
                "year": "2011",
                "id": 253
            },
            {
                "year": "2003",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Kernel",
                "Topology",
                "Proposals",
                "Image color analysis",
                "Computational modeling",
                "Gaussian processes"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image representation",
                "Markov processes",
                "Monte Carlo methods",
                "object tracking",
                "particle filtering (numerical methods)",
                "shape recognition",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "topology-constrained layered tracking",
                "latent flow",
                "integrated probabilistic model",
                "layered object tracking",
                "implicit shape representations",
                "topological shape constraints",
                "adaptive appearance model",
                "layered flow",
                "generative model",
                "layer shapes",
                "appearance evolution",
                "Gaussian process flow",
                "explicit layer ordering",
                "MCMC sampling algorithm",
                "particle filtering approach",
                "object boundary distribution",
                "video sources",
                "boundary-accurate tracking dataset"
            ],
            "Author Keywords": [
                "tracking",
                "layers",
                "topology",
                "flow"
            ]
        },
        "id": 20,
        "cited_by": [
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2015",
                "id": 492
            }
        ]
    },
    {
        "title": "Stacked Predictive Sparse Coding for Classification of Distinct Regions in Tumor Histopathology",
        "authors": [
            "Hang Chang",
            "Yin Zhou",
            "Paul Spellman",
            "Bahram Parvin"
        ],
        "abstract": "Image-based classification of histology sections, in terms of distinct components (e.g., tumor, stroma, normal), provides a series of indices for tumor composition. Furthermore, aggregation of these indices, from each whole slide image (WSI) in a large cohort, can provide predictive models of the clinical outcome. However, performance of the existing techniques is hindered as a result of large technical variations and biological heterogeneities that are always present in a large cohort. We propose a system that automatically learns a series of basis functions for representing the underlying spatial distribution using stacked predictive sparse decomposition (PSD). The learned representation is then fed into the spatial pyramid matching framework (SPM) with a linear SVM classifier. The system has been evaluated for classification of (a) distinct histological components for two cohorts of tumor types, and (b) colony organization of normal and malignant cell lines in 3D cell culture models. Throughput has been increased through the utility of graphical processing unit (GPU), and evaluation indicates a superior performance results, compared with previous research.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751130",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 5,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Tumors",
                "Feature extraction",
                "Kernel",
                "Support vector machines",
                "Histograms",
                "Image color analysis",
                "Biology"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image matching",
                "medical image processing",
                "support vector machines",
                "tumours"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stacked predictive sparse coding",
                "tumor histopathology",
                "image-based classification",
                "whole slide image",
                "WSI",
                "biological heterogeneities",
                "spatial distribution",
                "stacked predictive sparse decomposition",
                "PSD",
                "spatial pyramid matching framework",
                "SPM",
                "linear SVM classifier",
                "colony organization",
                "3D cell culture models",
                "graphical processing unit",
                "malignant cell lines",
                "normal cell lines"
            ]
        },
        "id": 21,
        "cited_by": []
    },
    {
        "title": "Higher Order Matching for Consistent Multiple Target Tracking",
        "authors": [
            "Chetan Arora",
            "Amir Globerson"
        ],
        "abstract": "This paper addresses the data assignment problem in multi frame multi object tracking in video sequences. Traditional methods employing maximum weight bipartite matching offer limited temporal modeling. It has recently been shown [6, 8, 24] that incorporating higher order temporal constraints improves the assignment solution. Finding maximum weight matching with higher order constraints is however NP-hard and the solutions proposed until now have either been greedy [8] or rely on greedy rounding of the solution obtained from spectral techniques [15]. We propose a novel algorithm to find the approximate solution to data assignment problem with higher order temporal constraints using the method of dual decomposition and the MPLP message passing algorithm [21]. We compare the proposed algorithm with an implementation of [8] and [15] and show that proposed technique provides better solution with a bound on approximation factor for each inferred solution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751131",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 0,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Approximation algorithms",
                "Approximation methods",
                "Trajectory",
                "Upper bound",
                "Smoothing methods",
                "Computer vision",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image matching",
                "image sequences",
                "message passing",
                "object tracking",
                "target tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "data assignment problem",
                "multiframe multiobject tracking",
                "video sequences",
                "maximum weight matching",
                "higher order temporal constraints",
                "dual decomposition",
                "MPLP message passing algorithm",
                "approximation factor",
                "higher order matching",
                "consistent multiple target tracking"
            ]
        },
        "id": 22,
        "cited_by": [
            {
                "year": "2015",
                "id": 487
            }
        ]
    },
    {
        "title": "A General Dense Image Matching Framework Combining Direct and Feature-Based Costs",
        "authors": [
            "Jim Braux-Zin",
            "Romain Dupont",
            "Adrien Bartoli"
        ],
        "abstract": "Dense motion field estimation (typically optical flow, stereo disparity and surface registration) is a key computer vision problem. Many solutions have been proposed to compute small or large displacements, narrow or wide baseline stereo disparity, but a unified methodology is still lacking. We here introduce a general framework that robustly combines direct and feature-based matching. The feature-based cost is built around a novel robust distance function that handles key points and ``weak'' features such as segments. It allows us to use putative feature matches which may contain mismatches to guide dense motion estimation out of local minima. Our framework uses a robust direct data term (AD-Census). It is implemented with a powerful second order Total Generalized Variation regularization with external and self-occlusion reasoning. Our framework achieves state of the art performance in several cases (standard optical flow benchmarks, wide-baseline stereo and non-rigid surface registration). Our framework has a modular design that customizes to specific application needs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751132",
        "reference_list": [
            {
                "year": "2009",
                "id": 168
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 13,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Optical imaging",
                "Image segmentation",
                "Estimation",
                "Benchmark testing",
                "Cost function",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image matching",
                "image sequences",
                "inference mechanisms",
                "motion estimation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense image matching framework",
                "feature-based costs",
                "direct costs",
                "dense motion field estimation",
                "stereo disparity",
                "surface registration",
                "key computer vision problem",
                "wide baseline stereo disparity",
                "narrow baseline stereo disparity",
                "feature-based matching",
                "direct-based matching",
                "robust distance function",
                "putative feature matches",
                "local minima",
                "robust direct data term",
                "AD-Census",
                "second order total generalized variation regularization",
                "nonrigid surface registration",
                "standard optical flow benchmarks",
                "self-occlusion reasoning"
            ],
            "Author Keywords": [
                "optical flow",
                "stereo",
                "wide-baseline",
                "non-rigid surface registration",
                "large displacements",
                "features",
                "SIFT",
                "segments"
            ]
        },
        "id": 23,
        "cited_by": [
            {
                "year": "2015",
                "id": 448
            }
        ]
    },
    {
        "title": "Revisiting Example Dependent Cost-Sensitive Learning with Decision Trees",
        "authors": [
            "Oisin Mac Aodha",
            "Gabriel J. Brostow"
        ],
        "abstract": "Typical approaches to classification treat class labels as disjoint. For each training example, it is assumed that there is only one class label that correctly describes it, and that all other labels are equally bad. We know however, that good and bad labels are too simplistic in many scenarios, hurting accuracy. In the realm of example dependent cost-sensitive learning, each label is instead a vector representing a data point's affinity for each of the classes. At test time, our goal is not to minimize the misclassification rate, but to maximize that affinity. We propose a novel example dependent cost-sensitive impurity measure for decision trees. Our experiments show that this new impurity measure improves test performance while still retaining the fast test times of standard classification trees. We compare our approach to classification trees and other cost-sensitive methods on three computer vision problems, tracking, descriptor matching, and optical flow, and show improvements in all three domains.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751133",
        "reference_list": [
            {
                "year": "2011",
                "id": 278
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 6,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Impurities",
                "Vectors",
                "Training",
                "Vegetation",
                "Decision trees",
                "Tracking",
                "Standards"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "decision trees",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "example dependent cost-sensitive learning",
                "decision trees",
                "data point affinity",
                "example dependent cost-sensitive impurity measure",
                "standard classification trees",
                "computer vision problems",
                "tracking",
                "descriptor matching",
                "optical flow"
            ]
        },
        "id": 24,
        "cited_by": []
    },
    {
        "title": "Modeling Self-Occlusions in Dynamic Shape and Appearance Tracking",
        "authors": [
            "Yanchao Yang",
            "Ganesh Sundaramoorthi"
        ],
        "abstract": "We present a method to track the precise shape of a dynamic object in video. Joint dynamic shape and appearance models, in which a template of the object is propagated to match the object shape and radiance in the next frame, are advantageous over methods employing global image statistics in cases of complex object radiance and cluttered background. In cases of complex 3D object motion and relative viewpoint change, self-occlusions and dis-occlusions of the object are prominent, and current methods employing joint shape and appearance models are unable to accurately adapt to new shape and appearance information, leading to inaccurate shape detection. In this work, we model self-occlusions and dis-occlusions in a joint shape and appearance tracking framework. Experiments on video exhibiting occlusion/dis-occlusion, complex radiance and background show that occlusion/dis-occlusion modeling leads to superior shape accuracy compared to recent methods employing joint shape/appearance models or employing global statistics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751134",
        "reference_list": [
            {
                "year": "2007",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 1,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Joints",
                "Optimization",
                "Three-dimensional displays",
                "Noise",
                "Computational modeling",
                "Optical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "object tracking",
                "shape recognition",
                "statistical analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "self-occlusions modeling",
                "dynamic shape tracking",
                "appearance tracking",
                "dynamic object",
                "video",
                "global image statistics",
                "complex object radiance",
                "cluttered background",
                "complex 3D object motion",
                "relative viewpoint change",
                "joint shape-appearance models",
                "inaccurate shape detection",
                "disocclusions"
            ],
            "Author Keywords": [
                "object tracking",
                "shape tracking",
                "occlusions",
                "dis-occlusions",
                "level set methods",
                "optical flow"
            ]
        },
        "id": 25,
        "cited_by": [
            {
                "year": "2015",
                "id": 220
            }
        ]
    },
    {
        "title": "A Convex Optimization Framework for Active Learning",
        "authors": [
            "Ehsan Elhamifar",
            "Guillermo Sapiro",
            "Allen Yang",
            "S. Shankar Sasrty"
        ],
        "abstract": "In many image/video/web classification problems, we have access to a large number of unlabeled samples. However, it is typically expensive and time consuming to obtain labels for the samples. Active learning is the problem of progressively selecting and annotating the most informative unlabeled samples, in order to obtain a high classification performance. Most existing active learning algorithms select only one sample at a time prior to retraining the classifier. Hence, they are computationally expensive and cannot take advantage of parallel labeling systems such as Mechanical Turk. On the other hand, algorithms that allow the selection of multiple samples prior to retraining the classifier, may select samples that have significant information overlap or they involve solving a non-convex optimization. More importantly, the majority of active learning algorithms are developed for a certain classifier type such as SVM. In this paper, we develop an efficient active learning framework based on convex programming, which can select multiple samples at a time for annotation. Unlike the state of the art, our algorithm can be used in conjunction with any type of classifiers, including those of the family of the recently proposed Sparse Representation-based Classification (SRC). We use the two principles of classifier uncertainty and sample diversity in order to guide the optimization program towards selecting the most informative unlabeled samples, which have the least information overlap. Our method can incorporate the data distribution in the selection process by using the appropriate dissimilarity between pairs of samples. We show the effectiveness of our framework in person detection, scene categorization and face recognition on real-world datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751135",
        "reference_list": [
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2011",
                "id": 177
            },
            {
                "year": "2003",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 10,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Labeling",
                "Support vector machines",
                "Optimization",
                "Uncertainty",
                "Encoding",
                "Convex functions"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "face recognition",
                "image classification",
                "image representation",
                "learning (artificial intelligence)",
                "object detection",
                "uncertainty handling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convex optimization",
                "image classification problem",
                "video classification problem",
                "Web classification problem",
                "classification performance",
                "active learning algorithm",
                "convex programming",
                "multiple sample selection",
                "sparse representation-based classification",
                "SRC",
                "classifier uncertainty",
                "sample diversity",
                "data distribution",
                "information overlap",
                "person detection",
                "scene categorization",
                "face recognition"
            ]
        },
        "id": 26,
        "cited_by": [
            {
                "year": "2017",
                "id": 114
            },
            {
                "year": "2015",
                "id": 331
            }
        ]
    },
    {
        "title": "A Generalized Iterated Shrinkage Algorithm for Non-convex Sparse Coding",
        "authors": [
            "Wangmeng Zuo",
            "Deyu Meng",
            "Lei Zhang",
            "Xiangchu Feng",
            "David Zhang"
        ],
        "abstract": "In many sparse coding based image restoration and image classification problems, using non-convex I p -norm minimization (0 \u2264 p <; 1) can often obtain better results than the convex l 1 -norm minimization. A number of algorithms, e.g., iteratively reweighted least squares (IRLS), iteratively thresholding method (ITM-I p ), and look-up table (LUT), have been proposed for non-convex I p -norm sparse coding, while some analytic solutions have been suggested for some specific values of p. In this paper, by extending the popular soft-thresholding operator, we propose a generalized iterated shrinkage algorithm (GISA) for I p -norm non-convex sparse coding. Unlike the analytic solutions, the proposed GISA algorithm is easy to implement, and can be adopted for solving non-convex sparse coding problems with arbitrary p values. Compared with LUT, GISA is more general and does not need to compute and store the look-up tables. Compared with IRLS and ITM-I p , GISA is theoretically more solid and can achieve more accurate solutions. Experiments on image restoration and sparse coding based face recognition are conducted to validate the performance of GISA.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751136",
        "reference_list": [],
        "citation": {
            "ieee": 55,
            "other": 41,
            "total": 96
        },
        "keywords": {
            "IEEE Keywords": [
                "Table lookup",
                "Encoding",
                "Minimization",
                "Deconvolution",
                "Image coding",
                "Image restoration",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "concave programming",
                "face recognition",
                "image classification",
                "image coding",
                "image restoration",
                "iterative methods",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generalized iterated shrinkage algorithm",
                "sparse coding-based image restoration problem",
                "sparse coding-based image classification problem",
                "nonconvex Ip-norm minimization",
                "convex l1-norm minimization",
                "iteratively-reweighted least squares algorithm",
                "IRLS algorithm",
                "iteratively thresholding method algorithm",
                "ITM-Ip algorithm",
                "look-up table",
                "LUT algorithm",
                "nonconvex Ip-norm sparse coding",
                "soft-thresholding operator",
                "GISA algorithm",
                "sparse coding-based face recognition"
            ]
        },
        "id": 27,
        "cited_by": []
    },
    {
        "title": "Latent Space Sparse Subspace Clustering",
        "authors": [
            "Vishal M. Patel",
            "Hien Van Nguyen",
            "Ren\u00e9 Vidal"
        ],
        "abstract": "We propose a novel algorithm called Latent Space Sparse Subspace Clustering for simultaneous dimensionality reduction and clustering of data lying in a union of subspaces. Specifically, we describe a method that learns the projection of data and finds the sparse coefficients in the low-dimensional latent space. Cluster labels are then assigned by applying spectral clustering to a similarity matrix built from these sparse coefficients. An efficient optimization method is proposed and its non-linear extensions based on the kernel methods are presented. One of the main advantages of our method is that it is computationally efficient as the sparse coefficients are found in the low-dimensional latent space. Various experiments show that the proposed method performs better than the competitive state-of-the-art subspace clustering methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751137",
        "reference_list": [
            {
                "year": "2011",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 44,
            "other": 20,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Clustering algorithms",
                "Kernel",
                "Sparse matrices",
                "Trajectory",
                "Cost function",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "matrix algebra",
                "optimisation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "latent space sparse subspace clustering",
                "simultaneous dimensionality reduction",
                "data clustering",
                "data projection",
                "cluster labels",
                "spectral clustering",
                "similarity matrix",
                "optimization method",
                "nonlinear extensions",
                "kernel methods",
                "low-dimensional latent space"
            ],
            "Author Keywords": [
                "Subspace clustering",
                "dimension reduction",
                "sparse optimization"
            ]
        },
        "id": 28,
        "cited_by": [
            {
                "year": "2015",
                "id": 397
            }
        ]
    },
    {
        "title": "Tracking Revisited Using RGBD Camera: Unified Benchmark and Baselines",
        "authors": [
            "Shuran Song",
            "Jianxiong Xiao"
        ],
        "abstract": "Despite significant progress, tracking is still considered to be a very challenging task. Recently, the increasing popularity of depth sensors has made it possible to obtain reliable depth easily. This may be a game changer for tracking, since depth can be used to prevent model drift and handle occlusion. We also observe that current tracking algorithms are mostly evaluated on a very small number of videos collected and annotated by different groups. The lack of a reasonable size and consistently constructed benchmark has prevented a persuasive comparison among different algorithms. In this paper, we construct a unified benchmark dataset of 100 RGBD videos with high diversity, propose different kinds of RGBD tracking algorithms using 2D or 3D model, and present a quantitative comparison of various algorithms with RGB or RGBD input. We aim to lay the foundation for further research in both RGB and RGBD tracking, and our benchmark is available at http://tracking.cs.princeton.edu.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751138",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 74,
            "other": 38,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Three-dimensional displays",
                "Benchmark testing",
                "Videos",
                "Image color analysis",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "object tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth sensors",
                "occlusion handling",
                "video collection",
                "RGBD video",
                "RGBD tracking algorithm",
                "2D model",
                "3D model",
                "RGB tracking",
                "RGBD camera"
            ],
            "Author Keywords": [
                "tracking",
                "RGBD",
                "benchmark"
            ]
        },
        "id": 29,
        "cited_by": [
            {
                "year": "2017",
                "id": 33
            },
            {
                "year": "2015",
                "id": 345
            },
            {
                "year": "2013",
                "id": 202
            }
        ]
    },
    {
        "title": "A Simple Model for Intrinsic Image Decomposition with Depth Cues",
        "authors": [
            "Qifeng Chen",
            "Vladlen Koltun"
        ],
        "abstract": "We present a model for intrinsic decomposition of RGB-D images. Our approach analyzes a single RGB-D image and estimates albedo and shading fields that explain the input. To disambiguate the problem, our model estimates a number of components that jointly account for the reconstructed shading. By decomposing the shading field, we can build in assumptions about image formation that help distinguish reflectance variation from shading. These assumptions are expressed as simple nonlocal regularizers. We evaluate the model on real-world images and on a challenging synthetic dataset. The experimental results demonstrate that the presented approach outperforms prior models for intrinsic decomposition of RGB-D images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751139",
        "reference_list": [
            {
                "year": "2001",
                "id": 90
            },
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 30,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Image color analysis",
                "Image decomposition",
                "Sensors",
                "Material properties",
                "Computational modeling",
                "Image sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "albedo",
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "intrinsic image decomposition",
                "depth cues",
                "albedo estimation",
                "nonlocal regularizers",
                "RGB-D images",
                "shading fields",
                "image formation"
            ]
        },
        "id": 30,
        "cited_by": [
            {
                "year": "2017",
                "id": 330
            },
            {
                "year": "2017",
                "id": 543
            },
            {
                "year": "2015",
                "id": 90
            },
            {
                "year": "2015",
                "id": 333
            },
            {
                "year": "2015",
                "id": 392
            }
        ]
    },
    {
        "title": "Quadruplet-Wise Image Similarity Learning",
        "authors": [
            "Marc T. Law",
            "Nicolas Thome",
            "Matthieu Cord"
        ],
        "abstract": "This paper introduces a novel similarity learning framework. Working with inequality constraints involving quadruplets of images, our approach aims at efficiently modeling similarity from rich or complex semantic label relationships. From these quadruplet-wise constraints, we propose a similarity learning framework relying on a convex optimization scheme. We then study how our metric learning scheme can exploit specific class relationships, such as class ranking (relative attributes), and class taxonomy. We show that classification using the learned metrics gets improved performance over state-of-the-art methods on several datasets. We also evaluate our approach in a new application to learn similarities between web page screenshots in a fully unsupervised way.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751140",
        "reference_list": [
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2007",
                "id": 201
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 11,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Optimization",
                "Training",
                "Face",
                "Vectors",
                "Context",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image processing",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "quadruplet-wise image similarity learning",
                "learning framework",
                "inequality constraints",
                "semantic label",
                "quadruplet-wise constraints",
                "convex optimization scheme",
                "class ranking",
                "class taxonomy",
                "webpage screenshots"
            ],
            "Author Keywords": [
                "metric learning",
                "machine learning"
            ]
        },
        "id": 31,
        "cited_by": [
            {
                "year": "2017",
                "id": 84
            },
            {
                "year": "2017",
                "id": 545
            },
            {
                "year": "2015",
                "id": 471
            }
        ]
    },
    {
        "title": "Complementary Projection Hashing",
        "authors": [
            "Zhongming Jin",
            "Yao Hu",
            "Yue Lin",
            "Debing Zhang",
            "Shiding Lin",
            "Deng Cai",
            "Xuelong Li"
        ],
        "abstract": "Recently, hashing techniques have been widely applied to solve the approximate nearest neighbors search problem in many vision applications. Generally, these hashing approaches generate 2^c buckets, where c is the length of the hash code. A good hashing method should satisfy the following two requirements: 1) mapping the nearby data points into the same bucket or nearby (measured by the Hamming distance) buckets. 2) all the data points are evenly distributed among all the buckets. In this paper, we propose a novel algorithm named Complementary Projection Hashing (CPH) to find the optimal hashing functions which explicitly considers the above two requirements. Specifically, CPH aims at sequentially finding a series of hyper planes (hashing functions) which cross the sparse region of the data. At the same time, the data points are evenly distributed in the hyper cubes generated by these hyper planes. The experiments comparing with the state-of-the-art hashing methods demonstrate the effectiveness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751141",
        "reference_list": [
            {
                "year": "2011",
                "id": 206
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 7,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Distributed databases",
                "Vectors",
                "Binary codes",
                "Hypercubes",
                "Linear programming",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "file organisation",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "complementary projection hashing algorithm",
                "approximate nearest neighbors search problem",
                "vision applications",
                "2c bucket generation",
                "hash code",
                "data points",
                "CPH algorithm",
                "optimal hashing functions",
                "sparse data region",
                "hypercubes",
                "hyperplanes"
            ],
            "Author Keywords": [
                "Approximate Nearest Neighbor Search",
                "Hashing"
            ]
        },
        "id": 32,
        "cited_by": [
            {
                "year": "2015",
                "id": 122
            }
        ]
    },
    {
        "title": "Find the Best Path: An Efficient and Accurate Classifier for Image Hierarchies",
        "authors": [
            "Min Sun",
            "Wan Huang",
            "Silvio Savarese"
        ],
        "abstract": "Many methods have been proposed to solve the image classification problem for a large number of categories. Among them, methods based on tree-based representations achieve good trade-off between accuracy and test time efficiency. While focusing on learning a tree-shaped hierarchy and the corresponding set of classifiers, most of them [11, 2, 14] use a greedy prediction algorithm for test time efficiency. We argue that the dramatic decrease in accuracy at high efficiency is caused by the specific design choice of the learning and greedy prediction algorithms. In this work, we propose a classifier which achieves a better trade-off between efficiency and accuracy with a given tree-shaped hierarchy. First, we convert the classification problem as finding the best path in the hierarchy, and a novel branch-and-bound-like algorithm is introduced to efficiently search for the best path. Second, we jointly train the classifiers using a novel Structured SVM (SSVM) formulation with additional bound constraints. As a result, our method achieves a significant 4.65%, 5.43%, and 4.07% (relative 24.82%, 41.64%, and 109.79%) improvement in accuracy at high efficiency compared to state-of-the-art greedy \"tree-based\" methods [14] on Caltech-256 [15], SUN [32] and Image Net 1K [9] dataset, respectively. Finally, we show that our branch-and-bound-like algorithm naturally ranks the paths in the hierarchy (Fig. 8) so that users can further process them.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751142",
        "reference_list": [
            {
                "year": "2011",
                "id": 263
            },
            {
                "year": "2007",
                "id": 224
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 3,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Prediction algorithms",
                "Upper bound",
                "Complexity theory",
                "Support vector machines",
                "Training",
                "Greedy algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "greedy algorithms",
                "image classification",
                "support vector machines",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image hierarchies",
                "image classification problem",
                "tree-based representations",
                "tree-shaped hierarchy",
                "greedy prediction algorithm",
                "branch-and-bound-like algorithm",
                "structured SVM formulation",
                "SSVM formulation",
                "Caltech-256",
                "SUN",
                "Image Net 1K"
            ],
            "Author Keywords": [
                "large scale image classification",
                "hierarchical classifier",
                "branch-and-bound"
            ]
        },
        "id": 33,
        "cited_by": []
    },
    {
        "title": "Detecting Dynamic Objects with Multi-view Background Subtraction",
        "authors": [
            "Ra\u00fal D\u00edaz",
            "Sam Hallman",
            "Charless C. Fowlkes"
        ],
        "abstract": "The confluence of robust algorithms for structure from motion along with high-coverage mapping and imaging of the world around us suggests that it will soon be feasible to accurately estimate camera pose for a large class photographs taken in outdoor, urban environments. In this paper, we investigate how such information can be used to improve the detection of dynamic objects such as pedestrians and cars. First, we show that when rough camera location is known, we can utilize detectors that have been trained with a scene-specific background model in order to improve detection accuracy. Second, when precise camera pose is available, dense matching to a database of existing images using multi-view stereo provides a way to eliminate static backgrounds such as building facades, akin to background-subtraction often used in video analysis. We evaluate these ideas using a dataset of tourist photos with estimated camera pose. For template-based pedestrian detection, we achieve a 50 percent boost in average precision over baseline.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751143",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2011",
                "id": 84
            },
            {
                "year": "2009",
                "id": 156
            },
            {
                "year": "2001",
                "id": 103
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Cameras",
                "Training",
                "Image reconstruction",
                "Image color analysis",
                "Robustness",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object detection",
                "pedestrians",
                "pose estimation",
                "stereo image processing",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview background subtraction",
                "robust algorithms",
                "high-coverage mapping",
                "high-coverage imaging",
                "camera pose estimation",
                "dynamic object detection",
                "rough camera location",
                "scene-specific background model",
                "detection accuracy improvement",
                "dense matching",
                "image database",
                "multiview stereo",
                "static background elimination",
                "tourist photos dataset",
                "template-based pedestrian detection"
            ]
        },
        "id": 34,
        "cited_by": []
    },
    {
        "title": "Low-Rank Sparse Coding for Image Classification",
        "authors": [
            "Tianzhu Zhang",
            "Bernard Ghanem",
            "Si Liu",
            "Changsheng Xu",
            "Narendra Ahuja"
        ],
        "abstract": "In this paper, we propose a low-rank sparse coding (LRSC) method that exploits local structure information among features in an image for the purpose of image-level classification. LRSC represents densely sampled SIFT descriptors, in a spatial neighborhood, collectively as low-rank, sparse linear combinations of code words. As such, it casts the feature coding problem as a low-rank matrix learning problem, which is different from previous methods that encode features independently. This LRSC has a number of attractive properties. (1) It encourages sparsity in feature codes, locality in codebook construction, and low-rankness for spatial consistency. (2) LRSC encodes local features jointly by considering their low-rank structure information, and is computationally attractive. We evaluate the LRSC by comparing its performance on a set of challenging benchmarks with that of 7 popular coding and other state-of-the-art methods. Our experiments show that by representing local features jointly, LRSC not only outperforms the state-of-the-art in classification accuracy but also improves the time complexity of methods that use a similar sparse linear representation model for feature coding.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751144",
        "reference_list": [
            {
                "year": "2011",
                "id": 337
            },
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 80
            },
            {
                "year": "2007",
                "id": 33
            },
            {
                "year": "2011",
                "id": 316
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2009",
                "id": 55
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 29,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Visualization",
                "Sparse matrices",
                "Image coding",
                "Layout",
                "Feature extraction",
                "Laplace equations"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image coding",
                "learning (artificial intelligence)",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "low-rank sparse coding method",
                "LRSC method",
                "image-level classification",
                "densely sampled SIFT descriptors",
                "spatial neighborhood",
                "feature coding problem",
                "low-rank matrix learning problem",
                "codebook construction",
                "spatial consistency",
                "low-rank structure information",
                "sparse linear representation model"
            ],
            "Author Keywords": [
                "image classification",
                "low-rank",
                "bow"
            ]
        },
        "id": 35,
        "cited_by": [
            {
                "year": "2015",
                "id": 176
            },
            {
                "year": "2015",
                "id": 464
            }
        ]
    },
    {
        "title": "Allocentric Pose Estimation",
        "authors": [
            "M. Jos\u00e9 Oramas M.",
            "Luc De Raedt",
            "Tinne Tuytelaars"
        ],
        "abstract": "The task of object pose estimation has been a challenge since the early days of computer vision. To estimate the pose (or viewpoint) of an object, people have mostly looked at object intrinsic features, such as shape or appearance. Surprisingly, informative features provided by other, external elements in the scene, have so far mostly been ignored. At the same time, contextual cues have been shown to be of great benefit for related tasks such as object detection or action recognition. In this paper, we explore how information from other objects in the scene can be exploited for pose estimation. In particular, we look at object configurations. We show that, starting from noisy object detections and pose estimates, exploiting the estimated pose and location of other objects in the scene can help to estimate the objects' poses more accurately. We explore both a camera-centered as well as an object-centered representation for relations. Experiments on the challenging KITTI dataset show that object configurations can indeed be used as a complementary cue to appearance-based pose estimation. In addition, object-centered relational representations can also assist object detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751145",
        "reference_list": [
            {
                "year": "2007",
                "id": 146
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 3,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Detectors",
                "Three-dimensional displays",
                "Object detection",
                "Shape",
                "Kernel",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "gesture recognition",
                "object recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "allocentric pose estimation",
                "object pose estimation",
                "computer vision",
                "object intrinsic features",
                "informative features",
                "action recognition",
                "object configurations",
                "noisy object detections",
                "pose estimates",
                "object-centered representation",
                "KITTI dataset",
                "appearance-based pose estimation",
                "object-centered relational representations"
            ],
            "Author Keywords": [
                "context",
                "pose",
                "viewpoint",
                "allocentric",
                "collective",
                "configuration"
            ]
        },
        "id": 36,
        "cited_by": []
    },
    {
        "title": "Attribute Pivots for Guiding Relevance Feedback in Image Search",
        "authors": [
            "Adriana Kovashka",
            "Kristen Grauman"
        ],
        "abstract": "In interactive image search, a user iteratively refines his results by giving feedback on exemplar images. Active selection methods aim to elicit useful feedback, but traditional approaches suffer from expensive selection criteria and cannot predict in formativeness reliably due to the imprecision of relevance feedback. To address these drawbacks, we propose to actively select \"pivot\" exemplars for which feedback in the form of a visual comparison will most reduce the system's uncertainty. For example, the system might ask, \"Is your target image more or less crowded than this image?\" Our approach relies on a series of binary search trees in relative attribute space, together with a selection function that predicts the information gain were the user to compare his envisioned target to the next node deeper in a given attribute's tree. It makes interactive search more efficient than existing strategies-both in terms of the system's selection time as well as the user's feedback effort.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751146",
        "reference_list": [
            {
                "year": "2007",
                "id": 232
            },
            {
                "year": "2013",
                "id": 428
            },
            {
                "year": "2011",
                "id": 177
            },
            {
                "year": "2011",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 10,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Visualization",
                "Binary search trees",
                "Entropy",
                "Uncertainty",
                "Training",
                "History"
            ],
            "INSPEC: Controlled Indexing": [
                "tree data structures",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relevance feedback guiding",
                "visual comparison",
                "binary search trees",
                "image search",
                "image selection criteria"
            ],
            "Author Keywords": [
                "relative attributes",
                "image retrieval",
                "relevance feedback",
                "active selection"
            ]
        },
        "id": 37,
        "cited_by": [
            {
                "year": "2017",
                "id": 280
            },
            {
                "year": "2015",
                "id": 119
            },
            {
                "year": "2015",
                "id": 269
            },
            {
                "year": "2013",
                "id": 428
            }
        ]
    },
    {
        "title": "Decomposing Bag of Words Histograms",
        "authors": [
            "Ankit Gandhi",
            "Karteek Alahari",
            "C.V. Jawahar"
        ],
        "abstract": "We aim to decompose a global histogram representation of an image into histograms of its associated objects and regions. This task is formulated as an optimization problem, given a set of linear classifiers, which can effectively discriminate the object categories present in the image. Our decomposition bypasses harder problems associated with accurately localizing and segmenting objects. We evaluate our method on a wide variety of composite histograms, and also compare it with MRF-based solutions. In addition to merely measuring the accuracy of decomposition, we also show the utility of the estimated object and background histograms for the task of image classification on the PASCAL VOC 2007 dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751147",
        "reference_list": [
            {
                "year": "2011",
                "id": 328
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2009",
                "id": 298
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Support vector machines",
                "Image segmentation",
                "Visualization",
                "Accuracy",
                "Clutter",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "global histogram representation",
                "image representation",
                "histogram decomposition",
                "optimization problem",
                "linear classifiers",
                "image segmentation",
                "composite histogram",
                "MRF-based solution",
                "image classification",
                "PASCAL VOC 2007 dataset",
                "image estimation",
                "object categories discrimination"
            ]
        },
        "id": 38,
        "cited_by": [
            {
                "year": "2015",
                "id": 165
            }
        ]
    },
    {
        "title": "SYM-FISH: A Symmetry-Aware Flip Invariant Sketch Histogram Shape Descriptor",
        "authors": [
            "Xiaochun Cao",
            "Hua Zhang",
            "Si Liu",
            "Xiaojie Guo",
            "Liang Lin"
        ],
        "abstract": "Recently, studies on sketch, such as sketch retrieval and sketch classification, have received more attention in the computer vision community. One of its most fundamental and essential problems is how to more effectively describe a sketch image. Many existing descriptors, such as shape context, have achieved great success. In this paper, we propose a new descriptor, namely Symmetric-aware Flip Invariant Sketch Histogram (SYM-FISH) to refine the shape context feature. Its extraction process includes three steps. First the Flip Invariant Sketch Histogram (FISH) descriptor is extracted on the input image, which is a flip-invariant version of the shape context feature. Then we explore the symmetry character of the image by calculating the kurtosis coefficient. Finally, the SYM-FISH is generated by constructing a symmetry table. The new SYM-FISH descriptor supplements the original shape context by encoding the symmetric information, which is a pervasive characteristic of natural scene and objects. We evaluate the efficacy of the novel descriptor in two applications, i.e., sketch retrieval and sketch classification. Extensive experiments on three datasets well demonstrate the effectiveness and robustness of the proposed SYM-FISH descriptor.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751148",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 14,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Marine animals",
                "Context",
                "Visualization",
                "Feature extraction",
                "Histograms",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image retrieval",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SYM-FISH",
                "symmetry aware flip invariant sketch histogram shape descriptor",
                "sketch retrieval",
                "sketch classification",
                "computer vision community",
                "sketch image",
                "shape context feature",
                "feature extraction process",
                "FISH descriptor",
                "kurtosis coefficient",
                "pervasive characteristic"
            ]
        },
        "id": 39,
        "cited_by": []
    },
    {
        "title": "Symbiotic Segmentation and Part Localization for Fine-Grained Categorization",
        "authors": [
            "Yuning Chai",
            "Victor Lempitsky",
            "Andrew Zisserman"
        ],
        "abstract": "We propose a new method for the task of fine-grained visual categorization. The method builds a model of the base-level category that can be fitted to images, producing high-quality foreground segmentation and mid-level part localizations. The model can be learnt from the typical datasets available for fine-grained categorization, where the only annotation provided is a loose bounding box around the instance (e.g. bird) in each image. Both segmentation and part localizations are then used to encode the image content into a highly-discriminative visual signature. The model is symbiotic in that part discovery/localization is helped by segmentation and, conversely, the segmentation is helped by the detection (e.g. part layout). Our model builds on top of the part-based object category detector of Felzenszwalb et al., and also on the powerful Grab Cut segmentation algorithm of Rother et al., and adds a simple spatial saliency coupling between them. In our evaluation, the model improves the categorization accuracy over the state-of-the-art. It also improves over what can be achieved with an analogous system that runs segmentation and part-localization independently.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751149",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 272
            },
            {
                "year": "2009",
                "id": 247
            },
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 78,
            "other": 36,
            "total": 114
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image color analysis",
                "Symbiosis",
                "Birds",
                "Accuracy",
                "Training",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "symbiotic segmentation",
                "fine grained visual categorization",
                "base level category",
                "high quality foreground segmentation",
                "mid level part localizations",
                "part based object category detector",
                "grab cut segmentation algorithm",
                "simple spatial saliency coupling"
            ],
            "Author Keywords": [
                "Computer Vision",
                "Object Recognition",
                "Fine-Grained",
                "Segmentation",
                "Detection"
            ]
        },
        "id": 40,
        "cited_by": [
            {
                "year": "2017",
                "id": 53
            },
            {
                "year": "2017",
                "id": 141
            },
            {
                "year": "2017",
                "id": 319
            },
            {
                "year": "2017",
                "id": 521
            },
            {
                "year": "2017",
                "id": 590
            },
            {
                "year": "2015",
                "id": 11
            },
            {
                "year": "2015",
                "id": 127
            },
            {
                "year": "2015",
                "id": 161
            },
            {
                "year": "2015",
                "id": 267
            },
            {
                "year": "2015",
                "id": 281
            }
        ]
    },
    {
        "title": "Image Set Classification Using Holistic Multiple Order Statistics Features and Localized Multi-kernel Metric Learning",
        "authors": [
            "Jiwen Lu",
            "Gang Wang",
            "Pierre Moulin"
        ],
        "abstract": "This paper presents a new approach for image set classification, where each training and testing example contains a set of image instances of an object captured from varying viewpoints or under varying illuminations. While a number of image set classification methods have been proposed in recent years, most of them model each image set as a single linear subspace or mixture of linear subspaces, which may lose some discriminative information for classification. To address this, we propose exploring multiple order statistics as features of image sets, and develop a localized multi-kernel metric learning (LMKML) algorithm to effectively combine different order statistics information for classification. Our method achieves the state-of-the-art performance on four widely used databases including the Honda/UCSD, CMU Mobo, and Youtube face datasets, and the ETH-80 object dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751150",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2009",
                "id": 274
            }
        ],
        "citation": {
            "ieee": 52,
            "other": 28,
            "total": 80
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Measurement",
                "Training",
                "Feature extraction",
                "Vectors",
                "YouTube",
                "Face"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Honda UCSD face datasets",
                "ETH-80 object dataset",
                "CMU Mobo face datasets",
                "Youtube face datasets",
                "LMKML algorithm",
                "single linear subspace",
                "localized multikernel metric learning algorithm",
                "holistic multiple order statistics features",
                "image set classification method"
            ],
            "Author Keywords": [
                "Image set classification",
                "face recognition",
                "object recognition",
                "metric learning"
            ]
        },
        "id": 41,
        "cited_by": [
            {
                "year": "2017",
                "id": 398
            },
            {
                "year": "2017",
                "id": 414
            },
            {
                "year": "2015",
                "id": 459
            }
        ]
    },
    {
        "title": "Learning a Dictionary of Shape Epitomes with Applications to Image Labeling",
        "authors": [
            "Liang-Chieh Chen",
            "George Papandreou",
            "Alan L. Yuille"
        ],
        "abstract": "The first main contribution of this paper is a novel method for representing images based on a dictionary of shape epitomes. These shape epitomes represent the local edge structure of the image and include hidden variables to encode shift and rotations. They are learnt in an unsupervised manner from ground truth edges. This dictionary is compact but is also able to capture the typical shapes of edges in natural images. In this paper, we illustrate the shape epitomes by applying them to the image labeling task. In other work, described in the supplementary material, we apply them to edge detection and image modeling. We apply shape epitomes to image labeling by using Conditional Random Field (CRF) Models. They are alternatives to the super pixel or pixel representations used in most CRFs. In our approach, the shape of an image patch is encoded by a shape epitome from the dictionary. Unlike the super pixel representation, our method avoids making early decisions which cannot be reversed. Our resulting hierarchical CRFs efficiently capture both local and global class co-occurrence properties. We demonstrate its quantitative and qualitative properties of our approach with image labeling experiments on two standard datasets: MSRC-21 and Stanford Background.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751151",
        "reference_list": [
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2003",
                "id": 4
            },
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2011",
                "id": 1
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Dictionaries",
                "Labeling",
                "Image segmentation",
                "Image edge detection",
                "Adaptation models",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "image representation",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape epitomes dictionary",
                "image labeling",
                "edge detection",
                "image modeling",
                "conditional random field",
                "CRF models",
                "image patch shape"
            ]
        },
        "id": 42,
        "cited_by": [
            {
                "year": "2015",
                "id": 151
            }
        ]
    },
    {
        "title": "Pyramid Coding for Functional Scene Element Recognition in Video Scenes",
        "authors": [
            "Eran Swears",
            "Anthony Hoogs",
            "Kim Boyer"
        ],
        "abstract": "Recognizing functional scene elements in video scenes based on the behaviors of moving objects that interact with them is an emerging problem of interest. Existing approaches have a limited ability to characterize elements such as cross-walks, intersections, and buildings that have low activity, are multi-modal, or have indirect evidence. Our approach recognizes the low activity and multi-model elements (crosswalks/intersections) by introducing a hierarchy of descriptive clusters to form a pyramid of codebooks that is sparse in the number of clusters and dense in content. The incorporation of local behavioral context such as person-enter-building and vehicle-parking nearby enables the detection of elements that do not have direct motion-based evidence, e.g. buildings. These two contributions significantly improve scene element recognition when compared against three state-of-the-art approaches. Results are shown on typical ground level surveillance video and for the first time on the more complex Wide Area Motion Imagery.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751152",
        "reference_list": [
            {
                "year": "2005",
                "id": 190
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Vehicles",
                "Training",
                "Context",
                "Detectors",
                "Clustering algorithms",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "object recognition",
                "video coding",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pyramid coding",
                "functional scene element recognition",
                "video scenes",
                "moving object behaviour",
                "low-activity element characterization",
                "cross-walks",
                "intersections",
                "buildings",
                "multimodel element characterization",
                "descriptive cluster hierarchy",
                "codebook pyramid",
                "local behavioral context incorporation",
                "person-enter-building",
                "vehicle-parking",
                "element detection",
                "direct motion-based evidence",
                "scene element recognition",
                "typical ground level surveillance video",
                "wide area motion imagery"
            ],
            "Author Keywords": [
                "functional recognition",
                "functional learning",
                "functional scene element",
                "pyramid coding",
                "scene understanding",
                "scene learning"
            ]
        },
        "id": 43,
        "cited_by": []
    },
    {
        "title": "Box in the Box: Joint 3D Layout and Object Reasoning from Single Images",
        "authors": [
            "Alexander G. Schwing",
            "Sanja Fidler",
            "Marc Pollefeys",
            "Raquel Urtasun"
        ],
        "abstract": "In this paper we propose an approach to jointly infer the room layout as well as the objects present in the scene. Towards this goal, we propose a branch and bound algorithm which is guaranteed to retrieve the global optimum of the joint problem. The main difficulty resides in taking into account occlusion in order to not over-count the evidence. We introduce a new decomposition method, which generalizes integral geometry to triangular shapes, and allows us to bound the different terms in constant time. We exploit both geometric cues and object detectors as image features and show large improvements in 2D and 3D object detection over state-of-the-art deformable part-based models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751153",
        "reference_list": [
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2009",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 42,
            "other": 13,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Face",
                "Three-dimensional displays",
                "Joints",
                "Cognition",
                "Detectors",
                "Complexity theory"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "geometry",
                "object detection",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Box in the Box",
                "joint 3D layout",
                "object reasoning",
                "single images",
                "room layout",
                "branch and bound algorithm",
                "decomposition method",
                "integral geometry",
                "triangular shapes",
                "object detection"
            ]
        },
        "id": 44,
        "cited_by": [
            {
                "year": "2015",
                "id": 187
            },
            {
                "year": "2015",
                "id": 188
            },
            {
                "year": "2015",
                "id": 291
            },
            {
                "year": "2015",
                "id": 300
            },
            {
                "year": "2013",
                "id": 158
            }
        ]
    },
    {
        "title": "Semantic Transform: Weakly Supervised Semantic Inference for Relating Visual Attributes",
        "authors": [
            "Sukrit Shankar",
            "Joan Lasenby",
            "Roberto Cipolla"
        ],
        "abstract": "Relative (comparative) attributes are promising for thematic ranking of visual entities, which also aids in recognition tasks. However, attribute rank learning often requires a substantial amount of relational supervision, which is highly tedious, and apparently impractical for real-world applications. In this paper, we introduce the Semantic Transform, which under minimal supervision, adaptively finds a semantic feature space along with a class ordering that is related in the best possible way. Such a semantic space is found for every attribute category. To relate the classes under weak supervision, the class ordering needs to be refined according to a cost function in an iterative procedure. This problem is ideally NP-hard, and we thus propose a constrained search tree formulation for the same. Driven by the adaptive semantic feature space representation, our model achieves the best results to date for all of the tasks of relative, absolute and zero-shot classification on two popular datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751154",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2009",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Visualization",
                "Training",
                "Adaptation models",
                "Transforms",
                "Mathematical model",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "iterative methods",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic transform",
                "weakly supervised semantic inference",
                "visual attributes",
                "semantic feature space",
                "NP-hard problem",
                "iterative procedure",
                "constrained search tree formulation",
                "adaptive semantic feature space representation"
            ],
            "Author Keywords": [
                "Ranking",
                "Semantic Descriptions",
                "Optimization"
            ]
        },
        "id": 45,
        "cited_by": []
    },
    {
        "title": "From Subcategories to Visual Composites: A Multi-level Framework for Object Detection",
        "authors": [
            "Tian Lan",
            "Michalis Raptis",
            "Leonid Sigal",
            "Greg Mori"
        ],
        "abstract": "The appearance of an object changes profoundly with pose, camera view and interactions of the object with other objects in the scene. This makes it challenging to learn detectors based on an object-level label (e.g., \"car\"). We postulate that having a richer set of labelings (at different levels of granularity) for an object, including finer-grained subcategories, consistent in appearance and view, and higher order composites - contextual groupings of objects consistent in their spatial layout and appearance, can significantly alleviate these problems. However, obtaining such a rich set of annotations, including annotation of an exponentially growing set of object groupings, is simply not feasible. We propose a weakly-supervised framework for object detection where we discover subcategories and the composites automatically with only traditional object-level category labels as input. To this end, we first propose an exemplar-SVM-based clustering approach, with latent SVM refinement, that discovers a variable length set of discriminative subcategories for each object class. We then develop a structured model for object detection that captures interactions among object subcategories and automatically discovers semantically meaningful and discriminatively relevant visual composites. We show that this model produces state-of-the-art performance on UIUC phrase object detection benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751155",
        "reference_list": [
            {
                "year": "2009",
                "id": 29
            },
            {
                "year": "2011",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 3,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Labeling",
                "Support vector machines",
                "Object detection",
                "Training",
                "Context modeling",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "pattern clustering",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "UIUC phrase object detection benchmark",
                "object subcategories",
                "structured model",
                "object class",
                "discriminative subcategories",
                "variable length set",
                "latent SVM refinement",
                "exemplar-SVM-based clustering approach",
                "object-level category label",
                "weakly-supervised framework",
                "spatial layout",
                "object contextual groupings",
                "higher-order composites",
                "finer-grained subcategories",
                "granularity level",
                "camera view",
                "pose",
                "object appearance",
                "multilevel framework",
                "visual composites"
            ]
        },
        "id": 46,
        "cited_by": []
    },
    {
        "title": "Online Video SEEDS for Temporal Window Objectness",
        "authors": [
            "Michael Van Den Bergh",
            "Gemma Roig",
            "Xavier Boix",
            "Santiago Manen",
            "Luc Van Gool"
        ],
        "abstract": "Super pixel and objectness algorithms are broadly used as a pre-processing step to generate support regions and to speed-up further computations. Recently, many algorithms have been extended to video in order to exploit the temporal consistency between frames. However, most methods are computationally too expensive for real-time applications. We introduce an online, real-time video super pixel algorithm based on the recently proposed SEEDS super pixels. A new capability is incorporated which delivers multiple diverse samples (hypotheses) of super pixels in the same image or video sequence. The multiple samples are shown to provide a strong cue to efficiently measure the objectness of image windows, and we introduce the novel concept of objectness in temporal windows. Experiments show that the video super pixels achieve comparable performance to state-of-the-art offline methods while running at 30 fps on a single 2.8 GHz i7 CPU. State-of-the-art performance on objectness is also demonstrated, yet orders of magnitude faster and extended to temporal windows in video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751156",
        "reference_list": [
            {
                "year": "2011",
                "id": 130
            },
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2011",
                "id": 238
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 10,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Streaming media",
                "Histograms",
                "Optimization",
                "Electron tubes",
                "Partitioning algorithms",
                "Color",
                "Noise"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "object recognition",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online video SEEDS",
                "temporal window objectness algorithms",
                "temporal consistency",
                "real-time video super pixel algorithm",
                "SEEDS super pixels",
                "image sequence",
                "video sequence",
                "image window objectness"
            ]
        },
        "id": 47,
        "cited_by": [
            {
                "year": "2017",
                "id": 29
            },
            {
                "year": "2017",
                "id": 380
            }
        ]
    },
    {
        "title": "Temporally Consistent Superpixels",
        "authors": [
            "Matthias Reso",
            "J\u00f6rn Jachalsky",
            "Bodo Rosenhahn",
            "J\u00f6rn Ostermann"
        ],
        "abstract": "Super pixel algorithms represent a very useful and increasingly popular preprocessing step for a wide range of computer vision applications, as they offer the potential to boost efficiency and effectiveness. In this regards, this paper presents a highly competitive approach for temporally consistent super pixels for video content. The approach is based on energy-minimizing clustering utilizing a novel hybrid clustering strategy for a multi-dimensional feature space working in a global color subspace and local spatial subspaces. Moreover, a new contour evolution based strategy is introduced to ensure spatial coherency of the generated super pixels. For a thorough evaluation the proposed approach is compared to state of the art super voxel algorithms using established benchmarks and shows a superior performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751157",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2011",
                "id": 167
            },
            {
                "year": "2011",
                "id": 56
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 11,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Streaming media",
                "Spatial coherence",
                "Image segmentation",
                "Clustering algorithms",
                "Optical imaging",
                "Benchmark testing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image colour analysis",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporally consistent superpixels",
                "super pixel algorithms",
                "computer vision applications",
                "video content",
                "hybrid clustering strategy",
                "multidimensional feature space",
                "global color subspace",
                "art super voxel algorithms"
            ],
            "Author Keywords": [
                "over-segmentation",
                "superpixel",
                "supervoxel",
                "video segmentation",
                "tracking"
            ]
        },
        "id": 48,
        "cited_by": [
            {
                "year": "2017",
                "id": 380
            }
        ]
    },
    {
        "title": "Semi-supervised Learning for Large Scale Image Cosegmentation",
        "authors": [
            "Zhengxiang Wang",
            "Rujie Liu"
        ],
        "abstract": "This paper introduces to use semi-supervised learning for large scale image co segmentation. Different from traditional unsupervised cosegmentation that does not use any segmentation ground truth, semi-supervised cosegmentation exploits the similarity from both the very limited training image foregrounds, as well as the common object shared between the large number of unsegmented images. This would be a much practical way to effectively co segment a large number of related images simultaneously, where previous unsupervised co segmentation work poorly due to the large variances in appearance between different images and the lack of segmentation ground truth for guidance in co segmentation. For semi-supervised co segmentation in large scale, we propose an effective method by minimizing an energy function, which consists of the inter-image distance, the intra-image distance and the balance term. We also propose an iterative updating algorithm to efficiently solve this energy function, which decomposes the original energy minimization problem into sub-problems, and updates each image alternatively to reduce the number of variables in each sub-problem for computation efficiency. Experiment results on iCoseg and Pascal VOC datasets show that the proposed co segmentation method can effectively co segment hundreds of images in less than one minute. And our semi-supervised co segmentation is able to outperform both unsupervised co segmentation as well as fully supervised single image segmentation, especially when the training data is limited.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751158",
        "reference_list": [
            {
                "year": "2011",
                "id": 328
            },
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2011",
                "id": 21
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "iterative methods",
                "minimisation",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semisupervised learning",
                "large scale image cosegmentation",
                "unsupervised cosegmentation",
                "semisupervised cosegmentation",
                "energy function",
                "inter-image distance",
                "intraimage distance",
                "balance term",
                "iterative updating algorithm",
                "energy minimization problem",
                "Pascal VOC datasets",
                "iCoseg datasets",
                "fully supervised single image segmentation",
                "training data",
                "training image foregrounds"
            ],
            "Author Keywords": [
                "Image cosegmentation",
                "Semi-supervised learning",
                "Energy minimization function",
                "Binary quadratic programming problem"
            ]
        },
        "id": 49,
        "cited_by": [
            {
                "year": "2015",
                "id": 66
            }
        ]
    },
    {
        "title": "Automatic Kronecker Product Model Based Detection of Repeated Patterns in 2D Urban Images",
        "authors": [
            "Juan Liu",
            "Emmanouil Psarakis",
            "Ioannis Stamos"
        ],
        "abstract": "Repeated patterns (such as windows, tiles, balconies and doors) are prominent and significant features in urban scenes. Therefore, detection of these repeated patterns becomes very important for city scene analysis. This paper attacks the problem of repeated patterns detection in a precise, efficient and automatic way, by combining traditional feature extraction followed by a Kronecker product low-rank modeling approach. Our method is tailored for 2D images of building facades. We have developed algorithms for automatic selection of a representative texture within facade images using vanishing points and Harris corners. After rectifying the input images, we describe novel algorithms that extract repeated patterns by using Kronecker product based modeling that is based on a solid theoretical foundation. Our approach is unique and has not ever been used for facade analysis. We have tested our algorithms in a large set of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751159",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Buildings",
                "Vectors",
                "Cost function",
                "Estimation",
                "Periodic structures",
                "Three-dimensional displays",
                "Clustering algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image texture",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic Kronecker product model based detection",
                "2D urban images",
                "repeated patterns detection",
                "feature extraction",
                "building facades",
                "texture representation",
                "vanishing points",
                "Harris corners"
            ],
            "Author Keywords": [
                "Grouping",
                "Segmentation",
                "Repetition Detection",
                "City Analysis"
            ]
        },
        "id": 50,
        "cited_by": []
    },
    {
        "title": "Group Norm for Learning Structured SVMs with Unstructured Latent Variables",
        "authors": [
            "Daozheng Chen",
            "Dhruv Batra",
            "William T. Freeman"
        ],
        "abstract": "Latent variables models have been applied to a number of computer vision problems. However, the complexity of the latent space is typically left as a free design choice. A larger latent space results in a more expressive model, but such models are prone to over fitting and are slower to perform inference with. The goal of this paper is to regularize the complexity of the latent space and learn which hidden states are really relevant for prediction. Specifically, we propose using group-sparsity-inducing regularizers such as \u2113 1 -\u2113 2 to estimate the parameters of Structured SVMs with unstructured latent variables. Our experiments on digit recognition and object detection show that our approach is indeed able to control the complexity of latent space without any significant loss in accuracy of the learnt model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751160",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Vectors",
                "Computational modeling",
                "Complexity theory",
                "Training",
                "Adaptation models",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "handwritten character recognition",
                "learning (artificial intelligence)",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "group norm",
                "structured SVM learning",
                "unstructured latent variables",
                "latent variables models",
                "computer vision problems",
                "latent space complexity",
                "group-sparsity-inducing regularizers",
                "object detection",
                "handwritten digit recognition"
            ],
            "Author Keywords": [
                "Latent variable models",
                "Latent Structured SVMs",
                "Latent SVMs",
                "Group Norm",
                "State Learning",
                "Coordinate Descent",
                "Concave-Convex Procedure",
                "Object Detection",
                "Deformable Part Models"
            ]
        },
        "id": 51,
        "cited_by": []
    },
    {
        "title": "Alternating Regression Forests for Object Detection and Pose Estimation",
        "authors": [
            "Samuel Schulter",
            "Christian Leistner",
            "Paul Wohlhart",
            "Peter M. Roth",
            "Horst Bischof"
        ],
        "abstract": "We present Alternating Regression Forests (ARFs), a novel regression algorithm that learns a Random Forest by optimizing a global loss function over all trees. This interrelates the information of single trees during the training phase and results in more accurate predictions. ARFs can minimize any differentiable regression loss without sacrificing the appealing properties of Random Forests, like low computational complexity during both, training and testing. Inspired by recent developments for classification [19], we derive a new algorithm capable of dealing with different regression loss functions, discuss its properties and investigate the relations to other methods like Boosted Trees. We evaluate ARFs on standard machine learning benchmarks, where we observe better generalization power compared to both standard Random Forests and Boosted Trees. Moreover, we apply the proposed regressor to two computer vision applications: object detection and head pose estimation from depth images. ARFs outperform the Random Forest baselines in both tasks, illustrating the importance of optimizing a common loss function for all trees.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751161",
        "reference_list": [
            {
                "year": "2011",
                "id": 52
            },
            {
                "year": "2011",
                "id": 10
            },
            {
                "year": "2009",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 9,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Vegetation",
                "Training",
                "Regression tree analysis",
                "Boosting",
                "Standards",
                "Estimation",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "image classification",
                "learning (artificial intelligence)",
                "object detection",
                "pose estimation",
                "regression analysis",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object detection",
                "pose estimation",
                "alternating regression forests",
                "regression algorithm",
                "global loss function",
                "differentiable regression",
                "random forests",
                "computational complexity",
                "regression loss functions",
                "boosted trees",
                "standard machine learning",
                "generalization power",
                "depth images",
                "computer vision"
            ],
            "Author Keywords": [
                "Random Forest",
                "Regression",
                "Object Detection",
                "Head Pose Estimation"
            ]
        },
        "id": 52,
        "cited_by": []
    },
    {
        "title": "Dynamic Label Propagation for Semi-supervised Multi-class Multi-label Classification",
        "authors": [
            "Bo Wang",
            "Zhuowen Tu",
            "John K. Tsotsos"
        ],
        "abstract": "In graph-based semi-supervised learning approaches, the classification rate is highly dependent on the size of the availabel labeled data, as well as the accuracy of the similarity measures. Here, we propose a semi-supervised multi-class/multi-label classification scheme, dynamic label propagation (DLP), which performs transductive learning through propagation in a dynamic process. Existing semi-supervised classification methods often have difficulty in dealing with multi-class/multi-label problems due to the lack in consideration of label correlation, our algorithm instead emphasizes dynamic metric fusion with label information. Significant improvement over the state-of-the-art methods is observed on benchmark datasets for both multi-class and multi-label tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751162",
        "reference_list": [
            {
                "year": "2011",
                "id": 100
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 5,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Heuristic algorithms",
                "Diffusion processes",
                "Measurement",
                "Manifolds"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic label propagation",
                "graph-based semisupervised learning approach",
                "classification rate",
                "similarity measure accuracy",
                "semisupervised multiclass multilabel classification scheme",
                "DLP",
                "transductive learning",
                "dynamic process",
                "multiclass-multilabel problem",
                "label correlation consideration",
                "dynamic metric fusion",
                "label information"
            ],
            "Author Keywords": [
                "Dynamic Label Propagation",
                "Multi-class",
                "Multi-label"
            ]
        },
        "id": 53,
        "cited_by": []
    },
    {
        "title": "Translating Video Content to Natural Language Descriptions",
        "authors": [
            "Marcus Rohrbach",
            "Wei Qiu",
            "Ivan Titov",
            "Stefan Thater",
            "Manfred Pinkal",
            "Bernt Schiele"
        ],
        "abstract": "Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset, which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751163",
        "reference_list": [
            {
                "year": "2013",
                "id": 338
            },
            {
                "year": "2013",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 64,
            "other": 48,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Semantics",
                "Training",
                "Natural languages",
                "Predictive models",
                "Training data",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "language translation",
                "natural language processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video content translation",
                "natural language descriptions",
                "visual perceptions",
                "semantic representation",
                "visual content",
                "activity labels",
                "object labels",
                "textual descriptions",
                "statistical machine translation",
                "TACoS dataset",
                "video descriptions",
                "image description task",
                "computer vision"
            ]
        },
        "id": 54,
        "cited_by": [
            {
                "year": "2017",
                "id": 354
            },
            {
                "year": "2017",
                "id": 435
            },
            {
                "year": "2017",
                "id": 608
            },
            {
                "year": "2015",
                "id": 270
            },
            {
                "year": "2015",
                "id": 289
            },
            {
                "year": "2015",
                "id": 498
            },
            {
                "year": "2015",
                "id": 503
            },
            {
                "year": "2015",
                "id": 506
            },
            {
                "year": "2013",
                "id": 112
            }
        ]
    },
    {
        "title": "POP: Person Re-identification Post-rank Optimisation",
        "authors": [
            "Chunxiao Liu",
            "Chen Change Loy",
            "Shaogang Gong",
            "Guijin Wang"
        ],
        "abstract": "Owing to visual ambiguities and disparities, person re-identification methods inevitably produce sub optimal rank-list, which still requires exhaustive human eyeballing to identify the correct target from hundreds of different likely-candidates. Existing re-identification studies focus on improving the ranking performance, but rarely look into the critical problem of optimising the time-consuming and error-prone post-rank visual search at the user end. In this study, we present a novel one-shot Post-rank Optimization (POP) method, which allows a user to quickly refine their search by either \"one-shot\" or a couple of sparse negative selections during a re-identification process. We conduct systematic behavioural studies to understand user's searching behaviour and show that the proposed method allows correct re-identification to converge 2.6 times faster than the conventional exhaustive search. Importantly, through extensive evaluations we demonstrate that the method is capable of achieving significant improvement over the state-of-the-art distance metric learning based ranking models, even with just \"one shot\" feedback optimisation, by as much as over 30% performance improvement for rank 1 re-identification on the VIPeR and i-LIDS datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751164",
        "reference_list": [
            {
                "year": "2013",
                "id": 315
            },
            {
                "year": "2013",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 25,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Probes",
                "Visualization",
                "Cameras",
                "Optimization",
                "Vegetation",
                "Context",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "person reidentification post-rank optimisation",
                "visual ambiguity",
                "visual disparity",
                "exhaustive human eyeballing",
                "ranking performance",
                "time-consuming post rank visual search",
                "error-prone post-rank visual search",
                "one-shot post-rank optimization method",
                "POP method",
                "sparse negative selections",
                "systematic behavioural study",
                "searching behaviour",
                "state-of-the-art distance metric learning",
                "one shot feedback optimisation",
                "performance improvement",
                "VIPeR dataset",
                "i-LIDS dataset"
            ],
            "Author Keywords": [
                "visual surveillance",
                "person re-identification",
                "ranking",
                "manifold",
                "information retrieval",
                "human computer interaction"
            ]
        },
        "id": 55,
        "cited_by": [
            {
                "year": "2017",
                "id": 256
            },
            {
                "year": "2017",
                "id": 417
            },
            {
                "year": "2015",
                "id": 124
            },
            {
                "year": "2015",
                "id": 145
            },
            {
                "year": "2015",
                "id": 522
            },
            {
                "year": "2013",
                "id": 315
            }
        ]
    },
    {
        "title": "Separating Reflective and Fluorescent Components Using High Frequency Illumination in the Spectral Domain",
        "authors": [
            "Ying Fu",
            "Antony Lam",
            "Imari Sato",
            "Takahiro Okabe",
            "Yoichi Sato"
        ],
        "abstract": "Hyper spectral imaging is beneficial to many applications but current methods do not consider fluorescent effects which are present in everyday items ranging from paper, to clothing, to even our food. Furthermore, everyday fluorescent items exhibit a mix of reflectance and fluorescence. So proper separation of these components is necessary for analyzing them. In this paper, we demonstrate efficient separation and recovery of reflective and fluorescent emission spectra through the use of high frequency illumination in the spectral domain. With the obtained fluorescent emission spectra from our high frequency illuminants, we then present to our knowledge, the first method for estimating the fluorescent absorption spectrum of a material given its emission spectrum. Conventional bispectral measurement of absorption and emission spectra needs to examine all combinations of incident and observed light wavelengths. In contrast, our method requires only two hyper spectral images. The effectiveness of our proposed methods are then evaluated through a combination of simulation and real experiments. We also demonstrate an application of our method to synthetic relighting of real scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751166",
        "reference_list": [
            {
                "year": "2007",
                "id": 250
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Absorption",
                "Lighting",
                "Materials",
                "Spectral analysis",
                "Surface waves",
                "Frequency-domain analysis",
                "Fluorescence"
            ],
            "INSPEC: Controlled Indexing": [
                "electromagnetic wave absorption",
                "hyperspectral imaging",
                "image processing",
                "lighting",
                "reflectivity"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "separating reflective",
                "fluorescent components",
                "high frequency illumination",
                "spectral domain",
                "fluorescent effects",
                "fluorescent items",
                "reflectance",
                "reflective emission spectra",
                "fluorescent emission spectra",
                "fluorescent absorption spectrum",
                "emission spectrum",
                "bispectral measurement",
                "emission spectra",
                "absorption spectra",
                "light wavelengths",
                "hyperspectral images"
            ]
        },
        "id": 56,
        "cited_by": [
            {
                "year": "2015",
                "id": 393
            }
        ]
    },
    {
        "title": "Rolling Shutter Stereo",
        "authors": [
            "Olivier Saurer",
            "Kevin K\u00f6ser",
            "Jean-Yves Bouguet",
            "Marc Pollefeys"
        ],
        "abstract": "A huge fraction of cameras used nowadays is based on CMOS sensors with a rolling shutter that exposes the image line by line. For dynamic scenes/cameras this introduces undesired effects like stretch, shear and wobble. It has been shown earlier that rotational shake induced rolling shutter effects in hand-held cell phone capture can be compensated based on an estimate of the camera rotation. In contrast, we analyse the case of significant camera motion, e.g.\\ where a bypassing street level capture vehicle uses a rolling shutter camera in a 3D reconstruction framework. The introduced error is depth dependent and cannot be compensated based on camera motion/rotation alone, invalidating also rectification for stereo camera systems. On top, significant lens distortion as often present in wide angle cameras intertwines with rolling shutter effects as it changes the time at which a certain 3D point is seen. We show that naive 3D reconstructions (assuming global shutter) will deliver biased geometry already for very mild assumptions on vehicle speed and resolution. We then develop rolling shutter dense multiview stereo algorithms that solve for time of exposure and depth at the same time, even in the presence of lens distortion and perform an evaluation on ground truth laser scan models as well as on real street-level data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751167",
        "reference_list": [
            {
                "year": "2003",
                "id": 130
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 5,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Lenses",
                "Cellular phones",
                "Sensors",
                "Vehicles",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "CMOS image sensors",
                "image reconstruction",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera fraction",
                "CMOS sensors",
                "dynamic scene-camera",
                "stretch effect",
                "shear effect",
                "wobble effect",
                "rotational shake",
                "rolling shutter effect",
                "hand-held cell phone capture",
                "camera rotation estimation",
                "camera motion",
                "street level capture vehicle",
                "3D reconstruction framework",
                "rolling shutter camera",
                "stereo camera systems",
                "lens distortion",
                "wide-angle cameras",
                "global shutter",
                "biased geometry",
                "vehicle speed",
                "rolling shutter dense multiview stereo algorithm",
                "exposure time",
                "ground truth laser scan model"
            ],
            "Author Keywords": [
                "Stereo",
                "Rolling Shutter"
            ]
        },
        "id": 57,
        "cited_by": [
            {
                "year": "2017",
                "id": 91
            },
            {
                "year": "2017",
                "id": 98
            }
        ]
    },
    {
        "title": "Elastic Fragments for Dense Scene Reconstruction",
        "authors": [
            "Qian-Yi Zhou",
            "Stephen Miller",
            "Vladlen Koltun"
        ],
        "abstract": "We present an approach to reconstruction of detailed scene geometry from range video. Range data produced by commodity handheld cameras suffers from high-frequency errors and low-frequency distortion. Our approach deals with both sources of error by reconstructing locally smooth scene fragments and letting these fragments deform in order to align to each other. We develop a volumetric registration formulation that leverages the smoothness of the deformation to make optimization practical for large scenes. Experimental results demonstrate that our approach substantially increases the fidelity of complex scene geometry reconstructed with commodity handheld cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751168",
        "reference_list": [
            {
                "year": "2003",
                "id": 183
            },
            {
                "year": "2011",
                "id": 295
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 16,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Cameras",
                "Geometry",
                "Optimization",
                "Surface reconstruction",
                "Sensors",
                "Trajectory"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image reconstruction",
                "image registration",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene geometry",
                "dense scene reconstruction",
                "high-frequency errors",
                "low-frequency distortion",
                "volumetric registration formulation",
                "optimization",
                "complex scene geometry",
                "elastic fragments"
            ]
        },
        "id": 58,
        "cited_by": [
            {
                "year": "2015",
                "id": 245
            },
            {
                "year": "2015",
                "id": 344
            }
        ]
    },
    {
        "title": "A Global Linear Method for Camera Pose Registration",
        "authors": [
            "Nianjuan Jiang",
            "Zhaopeng Cui",
            "Ping Tan"
        ],
        "abstract": "We present a linear method for global camera pose registration from pair wise relative poses encoded in essential matrices. Our method minimizes an approximate geometric error to enforce the triangular relationship in camera triplets. This formulation does not suffer from the typical `unbalanced scale' problem in linear methods relying on pair wise translation direction constraints, i.e. an algebraic error, nor the system degeneracy from collinear motion. In the case of three cameras, our method provides a good linear approximation of the trifocal tensor. It can be directly scaled up to register multiple cameras. The results obtained are accurate for point triangulation and can serve as a good initialization for final bundle adjustment. We evaluate the algorithm performance with different types of data and demonstrate its effectiveness. Our system produces good accuracy, robustness, and outperforms some well-known systems on efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751169",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2007",
                "id": 247
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 18,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Equations",
                "Three-dimensional displays",
                "Mathematical model",
                "Robustness",
                "Tensile stress"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image coding",
                "image registration",
                "matrix algebra",
                "pose estimation",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "global linear method",
                "global camera pose registration",
                "pair wise relative pose encoding",
                "essential matrices",
                "approximate geometric error minimization",
                "camera triplets",
                "unbalanced scale problem",
                "pair wise translation direction constraints",
                "trifocal tensor",
                "multiple-camera registration",
                "point triangulation",
                "bundle adjustment"
            ],
            "Author Keywords": [
                "structure-from-motion"
            ]
        },
        "id": 59,
        "cited_by": [
            {
                "year": "2015",
                "id": 89
            },
            {
                "year": "2015",
                "id": 96
            }
        ]
    },
    {
        "title": "A Rotational Stereo Model Based on XSlit Imaging",
        "authors": [
            "Jinwei Ye",
            "Yu Ji",
            "Jingyi Yu"
        ],
        "abstract": "Traditional stereo matching assumes perspective viewing cameras under a translational motion: the second camera is translated away from the first one to create parallax. In this paper, we investigate a different, rotational stereo model on a special multi-perspective camera, the XSlit camera [9, 24]. We show that rotational XSlit (R-XSlit) stereo can be effectively created by fixing the sensor and slit locations but switching the two slits' directions. We first derive the epipolar geometry of R-XSlit in the 4D light field ray space. Our derivation leads to a simple but effective scheme for locating corresponding epipolar \"curves\". To conduct stereo matching, we further derive a new disparity term in our model and develop a patch-based graph-cut solution. To validate our theory, we assemble an XSlit lens by using a pair of cylindrical lenses coupled with slit-shaped apertures. The XSlit lens can be mounted on commodity cameras where the slit directions are adjustable to form desirable R-XSlit pairs. We show through experiments that R-XSlit provides a potentially advantageous imaging system for conducting fixed-location, dynamic baseline stereo.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751170",
        "reference_list": [
            {
                "year": "2003",
                "id": 130
            },
            {
                "year": "2001",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Lenses",
                "Geometry",
                "Stereo vision",
                "Three-dimensional displays",
                "Apertures"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "graph theory",
                "image matching",
                "image motion analysis",
                "lenses",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "XSlit imaging system",
                "stereo matching",
                "perspective viewing cameras",
                "translational motion",
                "parallax",
                "multiperspective camera",
                "XSlit camera",
                "rotational XSlit stereo model",
                "R-XSlit",
                "epipolar geometry",
                "4D light field ray space",
                "epipolar curves",
                "patch-based graph-cut solution",
                "cylindrical lenses",
                "slit-shaped apertures",
                "commodity cameras",
                "fixed-location dynamic baseline stereo",
                "XSlit lens"
            ],
            "Author Keywords": [
                "Multi-perspective Imaging",
                "XSlit Camera",
                "Stereo",
                "3D Imaging"
            ]
        },
        "id": 60,
        "cited_by": [
            {
                "year": "2017",
                "id": 102
            },
            {
                "year": "2015",
                "id": 382
            }
        ]
    },
    {
        "title": "Lifting 3D Manhattan Lines from a Single Image",
        "authors": [
            "Srikumar Ramalingam",
            "Matthew Brand"
        ],
        "abstract": "We propose a novel and an efficient method for reconstructing the 3D arrangement of lines extracted from a single image, using vanishing points, orthogonal structure, and an optimization procedure that considers all plausible connectivity constraints between lines. Line detection identifies a large number of salient lines that intersect or nearly intersect in an image, but relatively a few of these apparent junctions correspond to real intersections in the 3D scene. We use linear programming (LP) to identify a minimal set of least-violated connectivity constraints that are sufficient to unambiguously reconstruct the 3D lines. In contrast to prior solutions that primarily focused on well-behaved synthetic line drawings with severely restricting assumptions, we develop an algorithm that can work on real images. The algorithm produces line reconstruction by identifying 95% correct connectivity constraints in York Urban database, with a total computation time of 1 second per image.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751171",
        "reference_list": [
            {
                "year": "2011",
                "id": 283
            },
            {
                "year": "2009",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 7,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Junctions",
                "Image reconstruction",
                "Cameras",
                "Labeling",
                "Image edge detection",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "linear programming",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D Manhattan lines",
                "3D lines arrangement",
                "orthogonal structure",
                "vanishing points",
                "single image",
                "optimization procedure",
                "line detection",
                "salient lines",
                "3D scene",
                "linear programming",
                "least-violated connectivity constraints",
                "York Urban database",
                "connectivity constraints"
            ],
            "Author Keywords": [
                "line drawing",
                "single view reconstruction",
                "junctions",
                "linear program"
            ]
        },
        "id": 61,
        "cited_by": []
    },
    {
        "title": "Dynamic Probabilistic Volumetric Models",
        "authors": [
            "Ali Osman Ulusoy",
            "Octavian Biris",
            "Joseph L. Mundy"
        ],
        "abstract": "This paper presents a probabilistic volumetric framework for image based modeling of general dynamic 3-d scenes. The framework is targeted towards high quality modeling of complex scenes evolving over thousands of frames. Extensive storage and computational resources are required in processing large scale space-time (4-d) data. Existing methods typically store separate 3-d models at each time step and do not address such limitations. A novel 4-d representation is proposed that adaptively subdivides in space and time to explain the appearance of 3-d dynamic surfaces. This representation is shown to achieve compression of 4-d data and provide efficient spatio-temporal processing. The advances of the proposed framework is demonstrated on standard datasets using free-viewpoint video and 3-d tracking applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751172",
        "reference_list": [
            {
                "year": "2011",
                "id": 297
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Computational modeling",
                "Probabilistic logic",
                "Rendering (computer graphics)",
                "Octrees",
                "Graphics processing units"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "object tracking",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic probabilistic volumetric models",
                "probabilistic volumetric framework",
                "image based modeling",
                "image representation",
                "spatio temporal processing",
                "free viewpoint video"
            ],
            "Author Keywords": [
                "image based modeling",
                "dynamic scene analysis",
                "4-d compression",
                "free viewpoint video",
                "3-d tracking"
            ]
        },
        "id": 62,
        "cited_by": []
    },
    {
        "title": "Network Principles for SfM: Disambiguating Repeated Structures with Local Context",
        "authors": [
            "Kyle Wilson",
            "Noah Snavely"
        ],
        "abstract": "Repeated features are common in urban scenes. Many objects, such as clock towers with nearly identical sides, or domes with strong radial symmetries, pose challenges for structure from motion. When similar but distinct features are mistakenly equated, the resulting 3D reconstructions can have errors ranging from phantom walls and superimposed structures to a complete failure to reconstruct. We present a new approach to solving such problems by considering the local visibility structure of such repeated features. Drawing upon network theory, we present a new way of scoring features using a measure of local clustering. Our model leads to a simple, fast, and highly scalable technique for disambiguating repeated features based on an analysis of an underlying visibility graph, without relying on explicit geometric reasoning. We demonstrate our method on several very large datasets drawn from Internet photo collections, and compare it to a more traditional geometry-based disambiguation technique.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751173",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 8,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Three-dimensional displays",
                "Context",
                "Poles and towers",
                "Internet",
                "Cognition",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "geometry",
                "image reconstruction",
                "inference mechanisms",
                "network theory (graphs)",
                "pattern clustering",
                "visibility"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SfM",
                "network principles",
                "urban scenes",
                "radial symmetries",
                "structure from motion",
                "3D reconstructions",
                "phantom walls",
                "superimposed structures",
                "local visibility structure",
                "network theory",
                "local clustering",
                "repeated feature disambiguation",
                "underlying visibility graph",
                "geometric reasoning",
                "Internet photo collections",
                "geometry-based disambiguation technique"
            ],
            "Author Keywords": [
                "structure from motion",
                "disambiguation",
                "clustering coefficient",
                "internet photo collections"
            ]
        },
        "id": 63,
        "cited_by": [
            {
                "year": "2015",
                "id": 96
            },
            {
                "year": "2015",
                "id": 237
            }
        ]
    },
    {
        "title": "Efficient and Robust Large-Scale Rotation Averaging",
        "authors": [
            "Avishek Chatterjee",
            "Venu Madhav Govindu"
        ],
        "abstract": "In this paper we address the problem of robust and efficient averaging of relative 3D rotations. Apart from having an interesting geometric structure, robust rotation averaging addresses the need for a good initialization for large scale optimization used in structure-from-motion pipelines. Such pipelines often use unstructured image datasets harvested from the internet thereby requiring an initialization method that is robust to outliers. Our approach works on the Lie group structure of 3D rotations and solves the problem of large-scale robust rotation averaging in two ways. Firstly, we use modern \u2113 1 optimizers to carry out robust averaging of relative rotations that is efficient, scalable and robust to outliers. In addition, we also develop a two step method that uses the \u2113 1 solution as an initialisation for an iteratively reweighted least squares (IRLS) approach. These methods achieve excellent results on large-scale, real world datasets and significantly outperform existing methods, i.e. the state-of-the-art discrete-continuous optimization method of [3] as well as the Weiszfeld method of [8]. We demonstrate the efficacy of our method on two large scale real world datasets and also provide the results of the two aforementioned methods for comparison.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751174",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 23,
            "total": 58
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Cameras",
                "Three-dimensional displays",
                "Vectors",
                "Optimization",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image motion analysis",
                "iterative methods",
                "least squares approximations",
                "motion estimation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust large-scale rotation averaging",
                "relative 3D rotations",
                "geometric structure",
                "large scale optimization",
                "unstructured image datasets",
                "Internet",
                "initialization method",
                "Lie group structure",
                "\u21131 optimizers",
                "iteratively reweighted least squares approach",
                "IRLS approach",
                "discrete-continuous optimization method",
                "Weiszfeld method",
                "global camera motion estimation"
            ],
            "Author Keywords": [
                "structure from motion",
                "rotation averaging",
                "lie group",
                "robust estimation"
            ]
        },
        "id": 64,
        "cited_by": [
            {
                "year": "2015",
                "id": 89
            },
            {
                "year": "2015",
                "id": 96
            }
        ]
    },
    {
        "title": "Pose Estimation with Unknown Focal Length Using Points, Directions and Lines",
        "authors": [
            "Yubin Kuang",
            "Kalle \u00c5str\u00f6m"
        ],
        "abstract": "In this paper, we study the geometry problems of estimating camera pose with unknown focal length using combination of geometric primitives. We consider points, lines and also rich features such as quivers, i.e.\\ points with one or more directions. We formulate the problems as polynomial systems where the constraints for different primitives are handled in a unified way. We develop efficient polynomial solvers for each of the derived cases with different combinations of primitives. The availability of these solvers enables robust pose estimation with unknown focal length for wider classes of features. Such rich features allow for fewer feature correspondences and generate larger inlier sets with higher probability. We demonstrate in synthetic experiments that our solvers are fast and numerically stable. For real images, we show that our solvers can be used in RANSAC loops to provide good initial solutions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751175",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 6,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Polynomials",
                "Noise",
                "Mathematical model",
                "Estimation",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "feature extraction",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera pose estimation",
                "unknown focal length",
                "geometry problem",
                "geometric primitive combination",
                "quivers",
                "points",
                "directions",
                "lines",
                "polynomial systems",
                "polynomial solvers",
                "robust pose estimation",
                "feature class",
                "feature correspondence",
                "synthetic experiments",
                "RANSAC loops"
            ]
        },
        "id": 65,
        "cited_by": []
    },
    {
        "title": "Unsupervised Intrinsic Calibration from a Single Frame Using a \"Plumb-Line\" Approach",
        "authors": [
            "R. Melo",
            "M. Antunes",
            "J.P. Barreto",
            "G. Falc\u00e3o",
            "N. Gon\u00e7alves"
        ],
        "abstract": "Estimating the amount and center of distortion from lines in the scene has been addressed in the literature by the so-called ``plumb-line'' approach. In this paper we propose a new geometric method to estimate not only the distortion parameters but the entire camera calibration (up to an ``angular'' scale factor) using a minimum of 3 lines. We propose a new framework for the unsupervised simultaneous detection of natural image of lines and camera parameters estimation, enabling a robust calibration from a single image. Comparative experiments with existing automatic approaches for the distortion estimation and with ground truth data are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751176",
        "reference_list": [
            {
                "year": "2005",
                "id": 80
            },
            {
                "year": "2009",
                "id": 105
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Cameras",
                "Vectors",
                "Equations",
                "Three-dimensional displays",
                "Mathematical model",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "object detection",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic approach",
                "robust calibration",
                "line natural image detection",
                "unsupervised simultaneous detection",
                "angular scale factor",
                "camera calibration estimation",
                "distortion parameter estimation",
                "geometric method",
                "distortion amount estimation",
                "distortion center estimation",
                "plumb-line approach",
                "unsupervised intrinsic calibration"
            ],
            "Author Keywords": [
                "Plumd-line",
                "calibration",
                "HFL",
                "Unsupervised",
                "Distortion"
            ]
        },
        "id": 66,
        "cited_by": []
    },
    {
        "title": "Structured Light in Sunlight",
        "authors": [
            "Mohit Gupta",
            "Qi Yin",
            "Shree K. Nayar"
        ],
        "abstract": "Strong ambient illumination severely degrades the performance of structured light based techniques. This is especially true in outdoor scenarios, where the structured light sources have to compete with sunlight, whose power is often 2-5 orders of magnitude larger than the projected light. In this paper, we propose the concept of light concentration to overcome strong ambient illumination. Our key observation is that given a fixed light (power) budget, it is always better to allocate it sequentially in several portions of the scene, as compared to spreading it over the entire scene at once. For a desired level of accuracy, we show that by distributing light appropriately, the proposed approach requires 1-2 orders lower acquisition time than existing approaches. Our approach is illumination-adaptive as the optimal light distribution is determined based on a measurement of the ambient illumination level. Since current light sources have a fixed light distribution, we have built a prototype light source that supports flexible light distribution by controlling the scanning speed of a laser scanner. We show several high quality 3D scanning results in a wide range of outdoor scenarios. The proposed approach will benefit 3D vision systems that need to operate outdoors under extreme ambient illumination levels on a limited time and power budget.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751177",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 12,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Signal to noise ratio",
                "Light sources",
                "Three-dimensional displays",
                "Accuracy",
                "Encoding"
            ],
            "INSPEC: Controlled Indexing": [
                "brightness",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sunlight",
                "ambient illumination",
                "structured light based techniques",
                "light-concentration",
                "acquisition time",
                "optimal light distribution",
                "light sources",
                "light distribution",
                "laser scanner",
                "high quality 3D scanning",
                "3D vision systems",
                "ambient illumination levels",
                "3D reconstructions"
            ],
            "Author Keywords": [
                "structured light",
                "3D scanning",
                "ambient illumination",
                "outdoors",
                "projector",
                "camera",
                "robotics",
                "autonomous navigation",
                "pico projector",
                "low-power projector",
                "light source"
            ]
        },
        "id": 67,
        "cited_by": [
            {
                "year": "2015",
                "id": 482
            }
        ]
    },
    {
        "title": "Content-Aware Rotation",
        "authors": [
            "Kaiming He",
            "Huiwen Chang",
            "Jian Sun"
        ],
        "abstract": "We present an image editing tool called Content-Aware Rotation. Casually shot photos can appear tilted, and are often corrected by rotation and cropping. This trivial solution may remove desired content and hurt image integrity. Instead of doing rigid rotation, we propose a warping method that creates the perception of rotation and avoids cropping. Human vision studies suggest that the perception of rotation is mainly due to horizontal/vertical lines. We design an optimization-based method that preserves the rotation of horizontal/vertical lines, maintains the completeness of the image content, and reduces the warping distortion. An efficient algorithm is developed to address the challenging optimization. We demonstrate our content-aware rotation method on a variety of practical cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751178",
        "reference_list": [
            {
                "year": "2007",
                "id": 170
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Vectors",
                "Optimization",
                "Three-dimensional displays",
                "Erbium",
                "Software",
                "Interpolation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "content-aware rotation",
                "image editing tool",
                "casually shot photos",
                "cropping",
                "image integrity",
                "rigid rotation",
                "warping method",
                "human vision studies",
                "horizontal-vertical lines",
                "optimization-based method",
                "image content",
                "warping distortion"
            ]
        },
        "id": 68,
        "cited_by": []
    },
    {
        "title": "Fast Direct Super-Resolution by Simple Functions",
        "authors": [
            "Chih-Yuan Yang",
            "Ming-Hsuan Yang"
        ],
        "abstract": "The goal of single-image super-resolution is to generate a high-quality high-resolution image based on a given low-resolution input. It is an ill-posed problem which requires exemplars or priors to better reconstruct the missing high-resolution image details. In this paper, we propose to split the feature space into numerous subspaces and collect exemplars to learn priors for each subspace, thereby creating effective mapping functions. The use of split input space facilitates both feasibility of using simple functions for super-resolution, and efficiency of generating high-resolution results. High-quality high-resolution images are reconstructed based on the effective learned priors. Experimental results demonstrate that the proposed algorithm performs efficiently and effectively over state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751179",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 95,
            "other": 58,
            "total": 153
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Image edge detection",
                "Image resolution",
                "Kernel",
                "Feature extraction",
                "Interpolation",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast direct super-resolution",
                "single-image super-resolution",
                "high-quality high-resolution image",
                "split input space",
                "image reconstruction"
            ],
            "Author Keywords": [
                "single-image super-resolution",
                "fast",
                "cluster",
                "subspace",
                "linear regression"
            ]
        },
        "id": 69,
        "cited_by": [
            {
                "year": "2017",
                "id": 177
            },
            {
                "year": "2015",
                "id": 34
            },
            {
                "year": "2015",
                "id": 36
            },
            {
                "year": "2015",
                "id": 203
            }
        ]
    },
    {
        "title": "Recognizing Text with Perspective Distortion in Natural Scenes",
        "authors": [
            "Trung Quy Phan",
            "Palaiahnakote Shivakumara",
            "Shangxuan Tian",
            "Chew Lim Tan"
        ],
        "abstract": "This paper presents an approach to text recognition in natural scene images. Unlike most existing works which assume that texts are horizontal and frontal parallel to the image plane, our method is able to recognize perspective texts of arbitrary orientations. For individual character recognition, we adopt a bag-of-key points approach, in which Scale Invariant Feature Transform (SIFT) descriptors are extracted densely and quantized using a pre-trained vocabulary. Following [1, 2], the context information is utilized through lexicons. We formulate word recognition as finding the optimal alignment between the set of characters and the list of lexicon words. Furthermore, we introduce a new dataset called StreetViewText-Perspective, which contains texts in street images with a great variety of viewpoints. Experimental results on public datasets and the proposed dataset show that our method significantly outperforms the state-of-the-art on perspective texts of arbitrary orientations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751180",
        "reference_list": [
            {
                "year": "2011",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 16,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Text recognition",
                "Character recognition",
                "Feature extraction",
                "Equations",
                "Image recognition",
                "Context",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "character recognition",
                "distortion",
                "image processing",
                "text analysis",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "text recognition",
                "perspective distortion",
                "natural scene images",
                "image plane",
                "character recognition",
                "scale invariant feature transform",
                "SIFT descriptors",
                "pretrained vocabulary",
                "StreetViewText-Perspective"
            ]
        },
        "id": 70,
        "cited_by": []
    },
    {
        "title": "Rank Minimization across Appearance and Shape for AAM Ensemble Fitting",
        "authors": [
            "Xin Cheng",
            "Sridha Sridharan",
            "Jason Saragih",
            "Simon Lucey"
        ],
        "abstract": "Active Appearance Models (AAMs) employ a paradigm of inverting a synthesis model of how an object can vary in terms of shape and appearance. As a result, the ability of AAMs to register an unseen object image is intrinsically linked to two factors. First, how well the synthesis model can reconstruct the object image. Second, the degrees of freedom in the model. Fewer degrees of freedom yield a higher likelihood of good fitting performance. In this paper we look at how these seemingly contrasting factors can complement one another for the problem of AAM fitting of an ensemble of images stemming from a constrained set (e.g. an ensemble of face images of the same person).",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751181",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Shape",
                "Vectors",
                "Face",
                "Databases",
                "Linear programming",
                "Jacobian matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image reconstruction",
                "image registration",
                "minimisation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "rank minimization",
                "image stemming",
                "degrees of freedom",
                "object image registeration",
                "object appearance",
                "active appearance model",
                "AAM ensemble fitting shape"
            ],
            "Author Keywords": [
                "Face Alignment",
                "Deformable Object Alignment",
                "Face Registration"
            ]
        },
        "id": 71,
        "cited_by": [
            {
                "year": "2015",
                "id": 433
            }
        ]
    },
    {
        "title": "Face Recognition via Archetype Hull Ranking",
        "authors": [
            "Yuanjun Xiong",
            "Wei Liu",
            "Deli Zhao",
            "Xiaoou Tang"
        ],
        "abstract": "The archetype hull model is playing an important role in large-scale data analytics and mining, but rarely applied to vision problems. In this paper, we migrate such a geometric model to address face recognition and verification together through proposing a unified archetype hull ranking framework. Upon a scalable graph characterized by a compact set of archetype exemplars whose convex hull encompasses most of the training images, the proposed framework explicitly captures the relevance between any query and the stored archetypes, yielding a rank vector over the archetype hull. The archetype hull ranking is then executed for every block of face images to generate a block wise similarity measure that is achieved by comparing two different rank vectors with respect to the same archetype hull. After integrating block wise similarity measurements with learned importance weights, we accomplish a sensible face similarity measure which can support robust and effective face recognition and verification. We evaluate the face similarity measure in terms of experiments performed on three benchmark face databases Multi-PIE, Pubfig83, and LFW, demonstrating its performance superior to the state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751182",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Training",
                "Vectors",
                "Face recognition",
                "Weight measurement",
                "Feature extraction",
                "Approximation algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "graph theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "archetype hull model",
                "large-scale data analytics",
                "data mining",
                "geometric model",
                "unified archetype hull ranking framework",
                "scalable graph characterization",
                "archetype exemplars",
                "convex hull",
                "training images",
                "query",
                "stored archetypes",
                "rank vector",
                "face images",
                "block wise similarity measurement",
                "robust face recognition",
                "effective face recognition",
                "face similarity measure",
                "multiPIE benchmark face database",
                "Pubfig83 benchmark face database",
                "LFW benchmark face database"
            ]
        },
        "id": 72,
        "cited_by": []
    },
    {
        "title": "Optimization Problems for Fast AAM Fitting in-the-Wild",
        "authors": [
            "Georgios Tzimiropoulos",
            "Maja Pantic"
        ],
        "abstract": "We describe a very simple framework for deriving the most-well known optimization problems in Active Appearance Models (AAMs), and most importantly for providing efficient solutions. Our formulation results in two optimization problems for fast and exact AAM fitting, and one new algorithm which has the important advantage of being applicable to 3D. We show that the dominant cost for both forward and inverse algorithms is a few times mN which is the cost of projecting an image onto the appearance subspace. This makes both algorithms not only computationally realizable but also very attractive speed-wise for most current systems. Because exact AAM fitting is no longer computationally prohibitive, we trained AAMs in-the-wild with the goal of investigating whether AAMs benefit from such a training process. Our results show that although we did not use sophisticated shape priors, robust features or robust norms for improving performance, AAMs perform notably well and in some cases comparably with current state-of-the-art methods. We provide Matlab source code for training, fitting and reproducing the results presented in this paper at http://ibug.doc.ic.ac.uk/resources.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751183",
        "reference_list": [
            {
                "year": "2007",
                "id": 266
            }
        ],
        "citation": {
            "ieee": 72,
            "other": 31,
            "total": 103
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Shape",
                "Silicon carbide",
                "Robustness",
                "Optimization",
                "Jacobian matrices",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "iterative methods",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast AAM fitting in-the-wild",
                "active appearance models",
                "optimization problems",
                "dominant cost",
                "inverse algorithms",
                "forward algorithms",
                "exact AAM fitting",
                "fast AAM fitting",
                "training process",
                "Matlab source code",
                "computer vision",
                "iterative process"
            ],
            "Author Keywords": [
                "Active Appearance Models",
                "AAMs",
                "Face alignment",
                "In-the-wild"
            ]
        },
        "id": 73,
        "cited_by": [
            {
                "year": "2017",
                "id": 496
            },
            {
                "year": "2015",
                "id": 412
            },
            {
                "year": "2015",
                "id": 424
            },
            {
                "year": "2015",
                "id": 424
            }
        ]
    },
    {
        "title": "Robust Feature Set Matching for Partial Face Recognition",
        "authors": [
            "Renliang Weng",
            "Jiwen Lu",
            "Junlin Hu",
            "Gao Yang",
            "Yap-Peng Tan"
        ],
        "abstract": "Over the past two decades, a number of face recognition methods have been proposed in the literature. Most of them use holistic face images to recognize people. However, human faces are easily occluded by other objects in many real-world scenarios and we have to recognize the person of interest from his/her partial faces. In this paper, we propose a new partial face recognition approach by using feature set matching, which is able to align partial face patches to holistic gallery faces automatically and is robust to occlusions and illumination changes. Given each gallery image and probe face patch, we first detect key points and extract their local features. Then, we propose a Metric Learned Extended Robust Point Matching (MLERPM) method to discriminatively match local feature sets of a pair of gallery and probe samples. Lastly, the similarity of two faces is converted as the distance between two feature sets. Experimental results on three public face databases are presented to show the effectiveness of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751184",
        "reference_list": [
            {
                "year": "2011",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 7,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Probes",
                "Face recognition",
                "Feature extraction",
                "Measurement",
                "Robustness",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust feature set matching",
                "partial face recognition method",
                "holistic face images",
                "holistic gallery faces",
                "local feature extraction",
                "metric learned extended robust point matching method",
                "public face databases"
            ]
        },
        "id": 74,
        "cited_by": [
            {
                "year": "2015",
                "id": 522
            }
        ]
    },
    {
        "title": "Cross-View Action Recognition over Heterogeneous Feature Spaces",
        "authors": [
            "Xinxiao Wu",
            "Han Wang",
            "Cuiwei Liu",
            "Yunde Jia"
        ],
        "abstract": "In cross-view action recognition, \"what you saw\" in one view is different from \"what you recognize\" in another view. The data distribution even the feature space can change from one view to another due to the appearance and motion of actions drastically vary across different views. In this paper, we address the problem of transferring action models learned in one view (source view) to another different view (target view), where action instances from these two views are represented by heterogeneous features. A novel learning method, called Heterogeneous Transfer Discriminantanalysis of Canonical Correlations (HTDCC), is proposed to learn a discriminative common feature space for linking source and target views to transfer knowledge between them. Two projection matrices that respectively map data from source and target views into the common space are optimized via simultaneously minimizing the canonical correlations of inter-class samples and maximizing the intraclass canonical correlations. Our model is neither restricted to corresponding action instances in the two views nor restricted to the same type of feature, and can handle only a few or even no labeled samples available in the target view. To reduce the data distribution mismatch between the source and target views in the common feature space, a nonparametric criterion is included in the objective function. We additionally propose a joint weight learning method to fuse multiple source-view action classifiers for recognition in the target view. Different combination weights are assigned to different source views, with each weight presenting how contributive the corresponding source view is to the target view. The proposed method is evaluated on the IXMAS multi-view dataset and achieves promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751185",
        "reference_list": [
            {
                "year": "2007",
                "id": 10
            },
            {
                "year": "2005",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 9,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Training",
                "Training data",
                "Target recognition",
                "Optical imaging",
                "Joints",
                "Learning systems"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image fusion",
                "learning (artificial intelligence)",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "IXMAS multiview dataset",
                "multiple source-view action classifier fusion",
                "joint weight learning method",
                "objective function",
                "nonparametric criterion",
                "data distribution mismatch",
                "intraclass canonical correlations",
                "projection matrices",
                "discriminative common feature space",
                "HTDCC",
                "heterogeneous transfer discriminant analysis of canonical correlations",
                "heterogeneous feature spaces",
                "cross-view action recognition"
            ]
        },
        "id": 75,
        "cited_by": [
            {
                "year": "2017",
                "id": 223
            }
        ]
    },
    {
        "title": "Efficient Image Dehazing with Boundary Constraint and Contextual Regularization",
        "authors": [
            "Gaofeng Meng",
            "Ying Wang",
            "Jiangyong Duan",
            "Shiming Xiang",
            "Chunhong Pan"
        ],
        "abstract": "Images captured in foggy weather conditions often suffer from bad visibility. In this paper, we propose an efficient regularization method to remove hazes from a single input image. Our method benefits much from an exploration on the inherent boundary constraint on the transmission function. This constraint, combined with a weighted L 1 -norm based contextual regularization, is modeled into an optimization problem to estimate the unknown scene transmission. A quite efficient algorithm based on variable splitting is also presented to solve the problem. The proposed method requires only a few general assumptions and can restore a high-quality haze-free image with faithful colors and fine image details. Experimental results on a variety of haze images demonstrate the effectiveness and efficiency of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751186",
        "reference_list": [
            {
                "year": "2009",
                "id": 218
            },
            {
                "year": "2009",
                "id": 283
            }
        ],
        "citation": {
            "ieee": 155,
            "other": 82,
            "total": 237
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Atmospheric modeling",
                "Image edge detection",
                "Estimation",
                "Optimization",
                "Extrapolation",
                "Meteorology"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image dehazing",
                "boundary constraint",
                "contextual regularization",
                "foggy weather conditions",
                "transmission function",
                "optimization problem",
                "variable splitting",
                "haze images",
                "image colour analysis"
            ],
            "Author Keywords": [
                "image processing",
                "single image dehazing",
                "visibility enhancement"
            ]
        },
        "id": 76,
        "cited_by": [
            {
                "year": "2017",
                "id": 253
            },
            {
                "year": "2017",
                "id": 501
            }
        ]
    },
    {
        "title": "From Where and How to What We See",
        "authors": [
            "S. Karthikeyan",
            "Vignesh Jagadeesh",
            "Renuka Shenoy",
            "Miguel Ecksteinz",
            "B.S. Manjunath"
        ],
        "abstract": "Eye movement studies have confirmed that overt attention is highly biased towards faces and text regions in images. In this paper we explore a novel problem of predicting face and text regions in images using eye tracking data from multiple subjects. The problem is challenging as we aim to predict the semantics (face/text/background) only from eye tracking data without utilizing any image information. The proposed algorithm spatially clusters eye tracking data obtained in an image into different coherent groups and subsequently models the likelihood of the clusters containing faces and text using a fully connected Markov Random Field (MRF). Given the eye tracking data from a test image, it predicts potential face/head (humans, dogs and cats) and text locations reliably. Furthermore, the approach can be used to select regions of interest for further analysis by object detectors for faces and text. The hybrid eye position/object detector approach achieves better detection performance and reduced computation time compared to using only the object detection algorithm. We also present a new eye tracking dataset on 300 images selected from ICDAR, Street-view, Flickr and Oxford-IIIT Pet Dataset from 15 subjects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751187",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2009",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 1,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Clustering algorithms",
                "Image color analysis",
                "Dogs",
                "Cats",
                "Tracking",
                "Reliability"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "gaze tracking",
                "Markov processes",
                "text detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "eye movement",
                "face regions",
                "text regions",
                "semantics prediction",
                "image information",
                "spatially clusters eye tracking data",
                "Markov random field",
                "test image",
                "text locations",
                "object detectors",
                "hybrid eye position-object detector approach",
                "computation time",
                "detection performance",
                "eye tracking dataset",
                "ICDAR dataset",
                "Street-view dataset",
                "Flickr dataset",
                "Oxford-IIIT Pet Dataset"
            ],
            "Author Keywords": [
                "Eye Tracking",
                "Dog and Cat Detection",
                "Text Detection"
            ]
        },
        "id": 77,
        "cited_by": []
    },
    {
        "title": "Restoring an Image Taken through a Window Covered with Dirt or Rain",
        "authors": [
            "David Eigen",
            "Dilip Krishnan",
            "Rob Fergus"
        ],
        "abstract": "Photographs taken through a window are often compromised by dirt or rain present on the window surface. Common cases of this include pictures taken from inside a vehicle, or outdoor security cameras mounted inside a protective enclosure. At capture time, defocus can be used to remove the artifacts, but this relies on achieving a shallow depth-of-field and placement of the camera close to the window. Instead, we present a post-capture image processing solution that can remove localized rain and dirt artifacts from a single image. We collect a dataset of clean/corrupted image pairs which are then used to train a specialized form of convolutional neural network. This learns how to map corrupted image patches to clean ones, implicitly capturing the characteristic appearance of dirt and water droplets in natural images. Our models demonstrate effective removal of dirt and rain in outdoor test conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751188",
        "reference_list": [
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 86,
            "other": 31,
            "total": 117
        },
        "keywords": {
            "IEEE Keywords": [
                "Rain",
                "Training",
                "Kernel",
                "Cameras",
                "Neural networks",
                "Noise",
                "Image restoration"
            ],
            "INSPEC: Controlled Indexing": [
                "image denoising",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image restoration",
                "post-capture image processing solution",
                "localized dirt artifacts removal",
                "localized rain artifacts removal",
                "clean-corrupted image pairs",
                "convolutional neural network",
                "corrupted image patches",
                "water droplets",
                "natural images"
            ]
        },
        "id": 78,
        "cited_by": [
            {
                "year": "2017",
                "id": 263
            },
            {
                "year": "2017",
                "id": 266
            },
            {
                "year": "2017",
                "id": 444
            },
            {
                "year": "2017",
                "id": 478
            },
            {
                "year": "2017",
                "id": 478
            },
            {
                "year": "2015",
                "id": 59
            },
            {
                "year": "2015",
                "id": 379
            }
        ]
    },
    {
        "title": "Uncertainty-Driven Efficiently-Sampled Sparse Graphical Models for Concurrent Tumor Segmentation and Atlas Registration",
        "authors": [
            "Sarah Parisot",
            "William Wells",
            "St\u00e9phane Chemouny",
            "Hugues Duffau",
            "Nikos Paragios"
        ],
        "abstract": "Graph-based methods have become popular in recent years and have successfully addressed tasks like segmentation and deformable registration. Their main strength is optimality of the obtained solution while their main limitation is the lack of precision due to the grid-like representations and the discrete nature of the quantized search space. In this paper we introduce a novel approach for combined segmentation/registration of brain tumors that adapts graph and sampling resolution according to the image content. To this end we estimate the segmentation and registration marginals towards adaptive graph resolution and intelligent definition of the search space. This information is considered in a hierarchical framework where uncertainties are propagated in a natural manner. State of the art results in the joint segmentation/registration of brain images with low-grade gliomas demonstrate the potential of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751189",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Tumors",
                "Uncertainty",
                "Image segmentation",
                "Image resolution",
                "Labeling",
                "Visualization",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "brain",
                "graph theory",
                "image registration",
                "image segmentation",
                "medical image processing",
                "tumours"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "uncertainty-driven sparse graphical model",
                "efficiently-sampled sparse graphical model",
                "concurrent tumor segmentation",
                "atlas registration",
                "graph-based method",
                "deformable registration",
                "grid-like representation",
                "quantized search space",
                "brain tumor",
                "adaptive graph resolution",
                "low-grade gliomas"
            ]
        },
        "id": 79,
        "cited_by": []
    },
    {
        "title": "Tracking via Robust Multi-task Multi-view Joint Sparse Representation",
        "authors": [
            "Zhibin Hong",
            "Xue Mei",
            "Danil Prokhorov",
            "Dacheng Tao"
        ],
        "abstract": "Combining multiple observation views has proven beneficial for tracking. In this paper, we cast tracking as a novel multi-task multi-view sparse learning problem and exploit the cues from multiple views including various types of visual features, such as intensity, color, and edge, where each feature observation can be sparsely represented by a linear combination of atoms from an adaptive feature dictionary. The proposed method is integrated in a particle filter framework where every view in each particle is regarded as an individual task. We jointly consider the underlying relationship between tasks across different views and different particles, and tackle it in a unified robust multi-task formulation. In addition, to capture the frequently emerging outlier tasks, we decompose the representation matrix to two collaborative components which enable a more robust and accurate approximation. We show that the proposed formulation can be efficiently solved using the Accelerated Proximal Gradient method with a small number of closed-form updates. The presented tracker is implemented using four types of features and is tested on numerous benchmark video sequences. Both the qualitative and quantitative results demonstrate the superior performance of the proposed approach compared to several state-of-the-art trackers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751190",
        "reference_list": [
            {
                "year": "2007",
                "id": 116
            },
            {
                "year": "2011",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 48,
            "other": 31,
            "total": 79
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Robustness",
                "Visualization",
                "Dictionaries",
                "Joints",
                "Matrix decomposition",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "gradient methods",
                "image representation",
                "image sequences",
                "matrix decomposition",
                "particle filtering (numerical methods)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "benchmark video sequences",
                "closed-form updates",
                "accelerated proximal gradient method",
                "collaborative components",
                "representation matrix decomposition",
                "outlier tasks",
                "unified robust multitask formulation",
                "particle filter framework",
                "adaptive feature dictionary",
                "linear atom combination",
                "feature observation",
                "edge",
                "color",
                "intensity",
                "visual features",
                "multitask multiview sparse learning problem",
                "observation view",
                "robust multitask multiview joint sparse representation"
            ],
            "Author Keywords": [
                "Tracking",
                "Multi-task",
                "Multi-view",
                "Outliers",
                "Sparse Representation"
            ]
        },
        "id": 80,
        "cited_by": []
    },
    {
        "title": "Online Robust Non-negative Dictionary Learning for Visual Tracking",
        "authors": [
            "Naiyan Wang",
            "Jingdong Wang",
            "Dit-Yan Yeung"
        ],
        "abstract": "This paper studies the visual tracking problem in video sequences and presents a novel robust sparse tracker under the particle filter framework. In particular, we propose an online robust non-negative dictionary learning algorithm for updating the object templates so that each learned template can capture a distinctive aspect of the tracked object. Another appealing property of this approach is that it can automatically detect and reject the occlusion and cluttered background in a principled way. In addition, we propose a new particle representation formulation using the Huber loss function. The advantage is that it can yield robust estimation without using trivial templates adopted by previous sparse trackers, leading to faster computation. We also reveal the equivalence between this new formulation and the previous one which uses trivial templates. The proposed tracker is empirically compared with state-of-the-art trackers on some challenging video sequences. Both quantitative and qualitative comparisons show that our proposed tracker is superior and more stable.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751191",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 67,
            "other": 45,
            "total": 112
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Dictionaries",
                "Visualization",
                "Video sequences",
                "Object tracking",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "learning (artificial intelligence)",
                "object tracking",
                "particle filtering (numerical methods)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online robust nonnegative dictionary learning algorithm",
                "visual tracking problem",
                "video sequences",
                "novel robust sparse tracker",
                "particle filter framework",
                "object tracking",
                "particle representation formulation",
                "Huber loss function"
            ]
        },
        "id": 81,
        "cited_by": [
            {
                "year": "2017",
                "id": 517
            },
            {
                "year": "2015",
                "id": 346
            },
            {
                "year": "2015",
                "id": 349
            },
            {
                "year": "2015",
                "id": 480
            }
        ]
    },
    {
        "title": "Robust Object Tracking with Online Multi-lifespan Dictionary Learning",
        "authors": [
            "Junliang Xing",
            "Jin Gao",
            "Bing Li",
            "Weiming Hu",
            "Shuicheng Yan"
        ],
        "abstract": "Recently, sparse representation has been introduced for robust object tracking. By representing the object sparsely, i.e., using only a few templates via L1-norm minimization, these so-called L1-trackers exhibit promising tracking results. In this work, we address the object template building and updating problem in these L1-tracking approaches, which has not been fully studied. We propose to perform template updating, in a new perspective, as an online incremental dictionary learning problem, which is efficiently solved through an online optimization procedure. To guarantee the robustness and adaptability of the tracking algorithm, we also propose to build a multi-lifespan dictionary model. By building target dictionaries of different life spans, effective object observations can be obtained to deal with the well-known drifting problem in tracking and thus improve the tracking accuracy. We derive effective observation models both generatively and discriminatively based on the online multi-lifespan dictionary learning model and deploy them to the Bayesian sequential estimation framework to perform tracking. The proposed approach has been extensively evaluated on ten challenging video sequences. Experimental results demonstrate the effectiveness of the online learned templates, as well as the state-of-the-art tracking performance of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751192",
        "reference_list": [],
        "citation": {
            "ieee": 24,
            "other": 16,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Target tracking",
                "Adaptation models",
                "Robustness",
                "Object tracking",
                "Superluminescent diodes",
                "Buildings"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "learning (artificial intelligence)",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online learned templates",
                "video sequences",
                "Bayesian sequential estimation",
                "online multilifespan dictionary learning model",
                "tracking accuracy",
                "target dictionaries",
                "multilifespan dictionary model",
                "tracking algorithm",
                "online optimization procedure",
                "online incremental dictionary learning problem",
                "L1-tracking approaches",
                "object updating problem",
                "object template building",
                "L1-trackers",
                "L1-norm minimization",
                "sparse representation",
                "online multilifespan dictionary learning",
                "robust object tracking"
            ],
            "Author Keywords": [
                "Object tracking",
                "template update",
                "sparse representation",
                "dictionary learning"
            ]
        },
        "id": 82,
        "cited_by": [
            {
                "year": "2015",
                "id": 346
            }
        ]
    },
    {
        "title": "Depth from Combining Defocus and Correspondence Using Light-Field Cameras",
        "authors": [
            "Michael W. Tao",
            "Sunil Hadap",
            "Jitendra Malik",
            "Ravi Ramamoorthi"
        ],
        "abstract": "Light-field cameras have recently become available to the consumer market. An array of micro-lenses captures enough information that one can refocus images after acquisition, as well as shift one's viewpoint within the sub-apertures of the main lens, effectively obtaining multiple views. Thus, depth cues from both defocus and correspondence are available simultaneously in a single capture. Previously, defocus could be achieved only through multiple image exposures focused at different depths, while correspondence cues needed multiple exposures at different viewpoints or multiple cameras, moreover, both cues could not easily be obtained together. In this paper, we present a novel simple and principled algorithm that computes dense depth estimation by combining both defocus and correspondence depth cues. We analyze the x-u 2D epipolar image (EPI), where by convention we assume the spatial x coordinate is horizontal and the angular u coordinate is vertical (our final algorithm uses the full 4D EPI). We show that defocus depth cues are obtained by computing the horizontal (spatial) variance after vertical (angular) integration, and correspondence depth cues by computing the vertical (angular) variance. We then show how to combine the two cues into a high quality depth map, suitable for computer vision applications such as matting, full control of depth-of-field, and surface reconstruction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751193",
        "reference_list": [
            {
                "year": "2007",
                "id": 64
            }
        ],
        "citation": {
            "ieee": 144,
            "other": 67,
            "total": 211
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Estimation",
                "Apertures",
                "Lenses",
                "Three-dimensional displays",
                "Noise measurement",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "microlenses"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "light-field cameras",
                "microlens array",
                "image refocus",
                "viewpoint shift",
                "lens subapertures",
                "multiple-image exposure",
                "correspondence depth cue",
                "defocus depth cue",
                "dense depth estimation",
                "x-u 2D epipolar image",
                "horizontal spatial coordinate",
                "vertical angular coordinate",
                "full-4D EPI",
                "horizontal variance",
                "vertical integration",
                "vertical variance",
                "high-quality depth map",
                "computer vision application",
                "matting",
                "depth-of-field control",
                "surface reconstruction"
            ]
        },
        "id": 83,
        "cited_by": [
            {
                "year": "2017",
                "id": 1
            },
            {
                "year": "2017",
                "id": 99
            },
            {
                "year": "2017",
                "id": 237
            },
            {
                "year": "2017",
                "id": 253
            },
            {
                "year": "2017",
                "id": 486
            },
            {
                "year": "2015",
                "id": 385
            },
            {
                "year": "2015",
                "id": 389
            },
            {
                "year": "2015",
                "id": 390
            }
        ]
    },
    {
        "title": "Affine-Constrained Group Sparse Coding and Its Application to Image-Based Classifications",
        "authors": [
            "Yu-Tseh Chi",
            "Mohsen Ali",
            "Muhammad Rushdi",
            "Jeffrey Ho"
        ],
        "abstract": "This paper proposes a novel approach for sparse coding that further improves upon the sparse representation-based classification (SRC) framework. The proposed framework, Affine-Constrained Group Sparse Coding (ACGSC), extends the current SRC framework to classification problems with multiple input samples. Geometrically, the affineconstrained group sparse coding essentially searches for the vector in the convex hull spanned by the input vectors that can best be sparse coded using the given dictionary. The resulting objective function is still convex and can be efficiently optimized using iterative block-coordinate descent scheme that is guaranteed to converge. Furthermore, we provide a form of sparse recovery result that guarantees, at least theoretically, that the classification performance of the constrained group sparse coding should be at least as good as the group sparse coding. We have evaluated the proposed approach using three different recognition experiments that involve illumination variation of faces and textures, and face recognition under occlusions. Preliminary experiments have demonstrated the effectiveness of the proposed approach, and in particular, the results from the recognition/occlusion experiment are surprisingly accurate and robust.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751194",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 1,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Encoding",
                "Dictionaries",
                "Training",
                "Lighting",
                "Face recognition",
                "Sparse matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image coding",
                "image representation",
                "image texture",
                "iterative methods",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "affine-constrained group sparse coding",
                "image-based classifications",
                "sparse representation-based classification framework",
                "SRC",
                "ACGSC",
                "convex hull",
                "objective function",
                "iterative block-coordinate descent scheme",
                "illumination variation",
                "face recognition",
                "textures"
            ],
            "Author Keywords": [
                "Sparse coding",
                "group",
                "affine",
                "classification"
            ]
        },
        "id": 84,
        "cited_by": []
    },
    {
        "title": "Sparse Variation Dictionary Learning for Face Recognition with a Single Training Sample per Person",
        "authors": [
            "Meng Yang",
            "Luc Van",
            "Lei Zhang"
        ],
        "abstract": "Face recognition (FR) with a single training sample per person (STSPP) is a very challenging problem due to the lack of information to predict the variations in the query sample. Sparse representation based classification has shown interesting results in robust FR, however, its performance will deteriorate much for FR with STSPP. To address this issue, in this paper we learn a sparse variation dictionary from a generic training set to improve the query sample representation by STSPP. Instead of learning from the generic training set independently w.r.t. the gallery set, the proposed sparse variation dictionary learning (SVDL) method is adaptive to the gallery set by jointly learning a projection to connect the generic training set with the gallery set. The learnt sparse variation dictionary can be easily integrated into the framework of sparse representation based classification so that various variations in face images, including illumination, expression, occlusion, pose, etc., can be better handled. Experiments on the large-scale CMU Multi-PIE, FRGC and LFW databases demonstrate the promising performance of SVDL on FR with STSPP.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751195",
        "reference_list": [
            {
                "year": "2007",
                "id": 19
            },
            {
                "year": "2009",
                "id": 46
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 34,
            "total": 77
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Training",
                "Face",
                "Lighting",
                "Databases",
                "Bismuth",
                "Encoding"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparse variation dictionary learning",
                "face recognition",
                "single training sample per person",
                "STSPP",
                "query sample representation",
                "generic training set",
                "SVDL method",
                "large-scale CMU multipie",
                "FRGC",
                "LFW databases"
            ]
        },
        "id": 85,
        "cited_by": []
    },
    {
        "title": "On the Mean Curvature Flow on Graphs with Applications in Image and Manifold Processing",
        "authors": [
            "Abdallah El",
            "Abderrahim Elmoataz",
            "Ahcene Sadi"
        ],
        "abstract": "In this paper, we propose an adaptation and transcription of the mean curvature level set equation on a general discrete domain (weighted graphs with arbitrary topology). We introduce the perimeters on graph using difference operators and define the curvature as the first variation of these perimeters. Our proposed approach of mean curvature unifies both local and non local notions of mean curvature on Euclidean domains. Furthermore, it allows the extension to the processing of manifolds and data which can be represented by graphs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751196",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Level set",
                "Mathematical model",
                "Manifolds",
                "Difference equations",
                "Image processing",
                "Heuristic algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mean curvature flow",
                "image processing",
                "manifold processing",
                "mean curvature level set equation",
                "weighted graphs",
                "arbitrary topology",
                "general discrete domain",
                "Euclidean domains",
                "graph representation"
            ],
            "Author Keywords": [
                "Mean curvature",
                "PdE on graphs",
                "data restoration"
            ]
        },
        "id": 86,
        "cited_by": []
    },
    {
        "title": "Perceptual Fidelity Aware Mean Squared Error",
        "authors": [
            "Wufeng Xue",
            "Xuanqin Mou",
            "Lei Zhang",
            "Xiangchu Feng"
        ],
        "abstract": "How to measure the perceptual quality of natural images is an important problem in low level vision. It is known that the Mean Squared Error (MSE) is not an effective index to describe the perceptual fidelity of images. Numerous perceptual fidelity indices have been developed, while the representatives include the Structural SIMilarity (SSIM) index and its variants. However, most of those perceptual measures are nonlinear, and they cannot be easily dopted as an objective function to minimize in various low level vision tasks. Can MSE be perceptual fidelity aware after some minor adaptation? In this paper we propose a simple framework to enhance the perceptual fidelity awareness of MSE by introducing an l2-norm structural error term to it. Such a Structural MSE (SMSE) can lead to very competitive image quality assessment (IQA) results. More surprisingly, we show that by using certain structure extractors, SMSE can be further turned into a Gaussian smoothed MSE (i.e., the Euclidean distance between the original and distorted images after Gaussian smooth filtering), which is much simpler to calculate but achieves rather better IQA performance than SSIM. The so called Perceptual-fidelity Aware MSE (PAMSE) can have great potentials in applications such as perceptual image coding and perceptual image restoration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751197",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Feature extraction",
                "Laplace equations",
                "Image coding",
                "Indexes",
                "Image restoration"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "mean square error methods",
                "smoothing methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "l2-norm structural error",
                "perceptual image restoration",
                "perceptual image coding",
                "perceptual-fidelity aware MSE",
                "Gaussian smooth filtering",
                "distorted images",
                "Euclidean distance",
                "Gaussian smoothed MSE",
                "IQA performance",
                "image quality assessment",
                "SMSE",
                "structural MSE",
                "structural similarity index",
                "natural images",
                "perceptual quality",
                "perceptual fidelity aware",
                "mean squared error"
            ]
        },
        "id": 87,
        "cited_by": []
    },
    {
        "title": "Real-World Normal Map Capture for Nearly Flat Reflective Surfaces",
        "authors": [
            "Bastien Jacquet",
            "Christian H\u00e4ne",
            "Kevin K\u00f6ser",
            "Marc Pollefeys"
        ],
        "abstract": "Although specular objects have gained interest in recent years, virtually no approaches exist for marker less reconstruction of reflective scenes in the wild. In this work, we present a practical approach to capturing normal maps in real-world scenes using video only. We focus on nearly planar surfaces such as windows, facades from glass or metal, or frames, screens and other indoor objects and show how normal maps of these can be obtained without the use of an artificial calibration object. Rather, we track the reflections of real-world straight lines, while moving with a hand-held or vehicle-mounted camera in front of the object. In contrast to error-prone local edge tracking, we obtain the reflections by a robust, global segmentation technique of an ortho-rectified 3D video cube that also naturally allows efficient user interaction. Then, at each point of the reflective surface, the resulting 2D-curve to 3D-line correspondence provides a novel quadratic constraint on the local surface normal. This allows to globally solve for the shape by integrability and smoothness constraints and easily supports the usage of multiple lines. We demonstrate the technique on several objects and facades.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751198",
        "reference_list": [
            {
                "year": "2005",
                "id": 189
            },
            {
                "year": "2011",
                "id": 295
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Surface reconstruction",
                "Windows",
                "Sea surface",
                "Glass",
                "Calibration"
            ],
            "INSPEC: Controlled Indexing": [
                "cartography",
                "image reconstruction",
                "image segmentation",
                "object tracking",
                "user interfaces",
                "video cameras",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-world normal map capture",
                "nearly flat reflective surfaces",
                "specular objects",
                "markerless reconstruction",
                "reflective scenes",
                "artificial calibration object",
                "real-world straight lines",
                "vehicle-mounted camera",
                "hand-held camera",
                "error-prone local edge tracking",
                "global segmentation technique",
                "ortho-rectified 3D video cube",
                "user interaction",
                "reflective surface",
                "2D-curve",
                "3D-line correspondence",
                "quadratic constraint",
                "local surface normal map",
                "multiple lines"
            ],
            "Author Keywords": [
                "specular surface reconstruction",
                "reflection",
                "reflective surface reconstruction",
                "normal map capture"
            ]
        },
        "id": 88,
        "cited_by": []
    },
    {
        "title": "Human Attribute Recognition by Rich Appearance Dictionary",
        "authors": [
            "Jungseock Joo",
            "Shuo Wang",
            "Song-Chun Zhu"
        ],
        "abstract": "We present a part-based approach to the problem of human attribute recognition from a single image of a human body. To recognize the attributes of human from the body parts, it is important to reliably detect the parts. This is a challenging task due to the geometric variation such as articulation and view-point changes as well as the appearance variation of the parts arisen from versatile clothing types. The prior works have primarily focused on handling geometric variation by relying on pre-trained part detectors or pose estimators, which require manual part annotation, but the appearance variation has been relatively neglected in these works. This paper explores the importance of the appearance variation, which is directly related to the main task, attribute recognition. To this end, we propose to learn a rich appearance part dictionary of human with significantly less supervision by decomposing image lattice into overlapping windows at multiscale and iteratively refining local appearance templates. We also present quantitative results in which our proposed method outperforms the existing approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751199",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 4,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Dictionaries",
                "Training",
                "Image recognition",
                "Reliability",
                "Face",
                "Clothing"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human attribute recognition",
                "rich appearance dictionary",
                "part-based approach",
                "versatile clothing types",
                "geometric variation",
                "pre-trained part detectors",
                "pose estimators",
                "image lattice decomposition",
                "local appearance templates"
            ],
            "Author Keywords": [
                "Human Attribute",
                "Fine-grained Recognition",
                "Weakly-Supervised Learning"
            ]
        },
        "id": 89,
        "cited_by": [
            {
                "year": "2015",
                "id": 264
            },
            {
                "year": "2015",
                "id": 414
            }
        ]
    },
    {
        "title": "Deformable Part Descriptors for Fine-Grained Recognition and Attribute Prediction",
        "authors": [
            "Ning Zhang",
            "Ryan Farrell",
            "Forrest Iandola",
            "Trevor Darrell"
        ],
        "abstract": "Recognizing objects in fine-grained domains can be extremely challenging due to the subtle differences between subcategories. Discriminative markings are often highly localized, leading traditional object recognition approaches to struggle with the large pose variation often present in these domains. Pose-normalization seeks to align training exemplars, either piecewise by part or globally for the whole object, effectively factoring out differences in pose and in viewing angle. Prior approaches relied on computationally-expensive filter ensembles for part localization and required extensive supervision. This paper proposes two pose-normalized descriptors based on computationally-efficient deformable part models. The first leverages the semantics inherent in strongly-supervised DPM parts. The second exploits weak semantic annotations to learn cross-component correspondences, computing pose-normalized descriptors from the latent parts of a weakly-supervised DPM. These representations enable pooling across pose and viewpoint, in turn facilitating tasks such as fine-grained recognition and attribute prediction. Experiments conducted on the Caltech-UCSD Birds 200 dataset and Berkeley Human Attribute dataset demonstrate significant improvements of our approach over state-of-art algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751200",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 232
            },
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 83,
            "other": 38,
            "total": 121
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Training",
                "Birds",
                "Feature extraction",
                "Equations",
                "Head",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Berkeley human attribute dataset",
                "Caltech-UCSD Birds 200 dataset",
                "cross-component correspondence learning",
                "weak semantic annotations",
                "DPM parts",
                "pose-normalized descriptors",
                "part localization",
                "training exemplar alignment",
                "pose-normalization",
                "pose variation",
                "discriminative marking",
                "object recognition",
                "attribute prediction",
                "fine-grained recognition",
                "deformable part descriptors"
            ]
        },
        "id": 90,
        "cited_by": [
            {
                "year": "2015",
                "id": 11
            },
            {
                "year": "2015",
                "id": 127
            },
            {
                "year": "2015",
                "id": 264
            },
            {
                "year": "2015",
                "id": 267
            },
            {
                "year": "2015",
                "id": 281
            }
        ]
    },
    {
        "title": "Handling Uncertain Tags in Visual Recognition",
        "authors": [
            "Arash Vahdat",
            "Greg Mori"
        ],
        "abstract": "Gathering accurate training data for recognizing a set of attributes or tags on images or videos is a challenge. Obtaining labels via manual effort or from weakly-supervised data typically results in noisy training labels. We develop the FlipSVM, a novel algorithm for handling these noisy, structured labels. The FlipSVM models label noise by \"flipping\" labels on training examples. We show empirically that the FlipSVM is effective on images-and-attributes and video tagging datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751201",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 261
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 7,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Training",
                "Noise measurement",
                "Support vector machines",
                "Noise",
                "Labeling",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "support vector machines",
                "video retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "uncertain tags",
                "visual recognition",
                "FlipSVM",
                "images-and-attributes",
                "video tagging dataset",
                "label noise"
            ]
        },
        "id": 91,
        "cited_by": []
    },
    {
        "title": "Implied Feedback: Learning Nuances of User Behavior in Image Search",
        "authors": [
            "Devi Parikh",
            "Kristen Grauman"
        ],
        "abstract": "User feedback helps an image search system refine its relevance predictions, tailoring the search towards the user's preferences. Existing methods simply take feedback at face value: clicking on an image means the user wants things like it, commenting that an image lacks a specific attribute means the user wants things that have it. However, we expect there is actually more information behind the user's literal feedback. In particular, a user's (possibly subconscious) search strategy leads him to comment on certain images rather than others, based on how any of the visible candidate images compare to the desired content. For example, he may be more likely to give negative feedback on an irrelevant image that is relatively close to his target, as opposed to bothering with one that is altogether different. We introduce novel features to capitalize on such implied feedback cues, and learn a ranking function that uses them to improve the system's relevance estimates. We validate the approach with real users searching for shoes, faces, or scenes using two different modes of feedback: binary relevance feedback and relative attributes-based feedback. The results show that retrieval improves significantly when the system accounts for the learned behaviors. We show that the nuances learned are domain-invariant, and useful for both generic user-independent search as well as personalized user-specific search.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751202",
        "reference_list": [
            {
                "year": "2007",
                "id": 232
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Feature extraction",
                "Search engines",
                "Face",
                "Footwear",
                "Visualization",
                "Negative feedback"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "user behavior",
                "user feedback",
                "image search system",
                "user search strategy",
                "ranking function",
                "binary relevance feedback",
                "relative attributes-based feedback",
                "generic user-independent search",
                "personalized user-specific search",
                "image retrieval"
            ]
        },
        "id": 92,
        "cited_by": [
            {
                "year": "2015",
                "id": 119
            }
        ]
    },
    {
        "title": "Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection",
        "authors": [
            "Tianfu Wu",
            "Song-Chun Zhu"
        ],
        "abstract": "Many object detectors, such as AdaBoost, SVM and deformable part-based models (DPM), compute additive scoring functions at a large number of windows scanned over image pyramid, thus computational efficiency is an important consideration beside accuracy performance. In this paper, we present a framework of learning cost-sensitive decision policy which is a sequence of two-sided thresholds to execute early rejection or early acceptance based on the accumulative scores at each step. A decision policy is said to be optimal if it minimizes an empirical global risk function that sums over the loss of false negatives (FN) and false positives (FP), and the cost of computation. While the risk function is very complex due to high-order connections among the two-sided thresholds, we find its upper bound can be optimized by dynamic programming (DP) efficiently and thus say the learned policy is near-optimal. Given the loss of FN and FP and the cost in three numbers, our method can produce a policy on-the-fly for Adaboost, SVM and DPM. In experiments, we show that our decision policy outperforms state-of-the-art cascade methods significantly in terms of speed with similar accuracy performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751203",
        "reference_list": [
            {
                "year": "2007",
                "id": 203
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Trajectory",
                "Upper bound",
                "Support vector machines",
                "Detectors",
                "Additives",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic programming",
                "offalse negatives",
                "false positives",
                "empirical global risk function",
                "decision policy",
                "two-sided thresholds",
                "computational efficiency",
                "image pyramid",
                "additive scoring functions",
                "deformable part-based models",
                "SVM",
                "AdaBoost",
                "object detectors",
                "object detection",
                "learning near-optimal cost-sensitive decision policy"
            ],
            "Author Keywords": [
                "Decision Policy",
                "Cost-Sensitive Computing",
                "Risk Minimization",
                "Dynamic Programming",
                "Object Detection"
            ]
        },
        "id": 93,
        "cited_by": []
    },
    {
        "title": "NYC3DCars: A Dataset of 3D Vehicles in Geographic Context",
        "authors": [
            "Kevin Matzen",
            "Noah Snavely"
        ],
        "abstract": "Geometry and geography can play an important role in recognition tasks in computer vision. To aid in studying connections between geometry and recognition, we introduce NYC3DCars, a rich dataset for vehicle detection in urban scenes built from Internet photos drawn from the wild, focused on densely trafficked areas of New York City. Our dataset is augmented with detailed geometric and geographic information, including full camera poses derived from structure from motion, 3D vehicle annotations, and geographic information from open resources, including road segmentations and directions of travel. NYC3DCars can be used to study new questions about using geometric information in detection tasks, and to explore applications of Internet photos in understanding cities. To demonstrate the utility of our data, we evaluate the use of the geographic information in our dataset to enhance a parts-based detection method, and suggest other avenues for future exploration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751204",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2007",
                "id": 146
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 4,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicles",
                "Three-dimensional displays",
                "Cameras",
                "Roads",
                "Solid modeling",
                "Image reconstruction",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "image segmentation",
                "object recognition",
                "road vehicles"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parts-based detection method",
                "detection tasks",
                "travel directions",
                "road segmentations",
                "open resources",
                "3D vehicle annotations",
                "camera poses",
                "geometric information",
                "geographic information",
                "New York city",
                "Internet photos",
                "urban scenes",
                "vehicle detection",
                "computer vision",
                "recognition tasks",
                "geographic context",
                "3D vehicles dataset",
                "NYC3DCars"
            ],
            "Author Keywords": [
                "object detection",
                "3D models",
                "structure from motion",
                "geography"
            ]
        },
        "id": 94,
        "cited_by": [
            {
                "year": "2017",
                "id": 317
            },
            {
                "year": "2015",
                "id": 112
            },
            {
                "year": "2015",
                "id": 188
            }
        ]
    },
    {
        "title": "Unsupervised Domain Adaptation by Domain Invariant Projection",
        "authors": [
            "Mahsa Baktashmotlagh",
            "Mehrtash T. Harandi",
            "Brian C. Lovell",
            "Mathieu Salzmann"
        ],
        "abstract": "Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain specific. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More specifically, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751205",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            }
        ],
        "citation": {
            "ieee": 57,
            "other": 46,
            "total": 103
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Kernel",
                "Optimization",
                "Visualization",
                "Electronics packaging",
                "IEEE 802.11 Standards",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "object recognition",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "domain adaptation benchmark dataset",
                "visual object recognition",
                "low-dimensional latent space",
                "information extraction",
                "domain invariant projection approach",
                "feature space",
                "domain shift problem",
                "domain-invariant representations",
                "unsupervised domain adaptation method"
            ]
        },
        "id": 95,
        "cited_by": [
            {
                "year": "2017",
                "id": 62
            },
            {
                "year": "2017",
                "id": 78
            },
            {
                "year": "2017",
                "id": 377
            },
            {
                "year": "2017",
                "id": 599
            },
            {
                "year": "2015",
                "id": 460
            },
            {
                "year": "2015",
                "id": 468
            }
        ]
    },
    {
        "title": "SIFTpack: A Compact Representation for Efficient SIFT Matching",
        "authors": [
            "Alexandra Gilinsky",
            "Lihi Zelnik Manor"
        ],
        "abstract": "Computing distances between large sets of SIFT descriptors is a basic step in numerous algorithms in computer vision. When the number of descriptors is large, as is often the case, computing these distances can be extremely time consuming. In this paper we propose the SIFT pack: a compact way of storing SIFT descriptors, which enables significantly faster calculations between sets of SIFTs than the current solutions. SIFT pack can be used to represent SIFTs densely extracted from a single image or sparsely from multiple different images. We show that the SIFT pack representation saves both storage space and run time, for both finding nearest neighbors and for computing all distances between all descriptors. The usefulness of SIFT pack is also demonstrated as an alternative implementation for K-means dictionaries of visual words.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751206",
        "reference_list": [
            {
                "year": "2011",
                "id": 203
            },
            {
                "year": "2003",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Dictionaries",
                "Accuracy",
                "Arrays",
                "Visualization",
                "Vectors",
                "Standards"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "compact representation",
                "SIFT matching",
                "SIFT descriptors",
                "computer vision",
                "SIFTpack representation",
                "K-means dictionaries",
                "visual words"
            ]
        },
        "id": 96,
        "cited_by": []
    },
    {
        "title": "PhotoOCR: Reading Text in Uncontrolled Conditions",
        "authors": [
            "Alessandro Bissacco",
            "Mark Cummins",
            "Yuval Netzer",
            "Hartmut Neven"
        ],
        "abstract": "We describe Photo OCR, a system for text extraction from images. Our particular focus is reliable text extraction from smartphone imagery, with the goal of text recognition as a user input modality similar to speech recognition. Commercially available OCR performs poorly on this task. Recent progress in machine learning has substantially improved isolated character classification, we build on this progress by demonstrating a complete OCR system using these techniques. We also incorporate modern data center-scale distributed language modelling. Our approach is capable of recognizing text in a variety of challenging imaging conditions where traditional OCR systems fail, notably in the presence of substantial blur, low resolution, low contrast, high image noise and other distortions. It also operates with low latency, mean processing time is 600 ms per image. We evaluate our system on public benchmark datasets for text extraction and outperform all previously reported results, more than halving the error rate on multiple benchmarks. The system is currently in use in many applications at Google, and is available as a user input modality in Google Translate for Android.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751207",
        "reference_list": [
            {
                "year": "2011",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 100,
            "other": 58,
            "total": 158
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical character recognition software",
                "Training",
                "Image segmentation",
                "Text recognition",
                "Computational modeling",
                "Google",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "optical character recognition",
                "text detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "text reading",
                "uncontrolled conditions",
                "PhotoOCR",
                "text extraction",
                "smartphone imagery",
                "text recognition",
                "machine learning",
                "isolated character classification",
                "OCR system",
                "datacenter-scale distributed language modelling",
                "imaging conditions",
                "public benchmark datasets",
                "Google Translate",
                "Android"
            ],
            "Author Keywords": [
                "OCR",
                "deep learning",
                "text recognition",
                "scene text"
            ]
        },
        "id": 97,
        "cited_by": [
            {
                "year": "2017",
                "id": 156
            },
            {
                "year": "2017",
                "id": 533
            },
            {
                "year": "2017",
                "id": 550
            },
            {
                "year": "2015",
                "id": 134
            },
            {
                "year": "2015",
                "id": 138
            },
            {
                "year": "2015",
                "id": 318
            },
            {
                "year": "2015",
                "id": 519
            }
        ]
    },
    {
        "title": "Probabilistic Elastic Part Model for Unsupervised Face Detector Adaptation",
        "authors": [
            "Haoxiang Li",
            "Gang Hua",
            "Zhe Lin",
            "Jonathan Brandt",
            "Jianchao Yang"
        ],
        "abstract": "We propose an unsupervised detector adaptation algorithm to adapt any offline trained face detector to a specific collection of images, and hence achieve better accuracy. The core of our detector adaptation algorithm is a probabilistic elastic part (PEP) model, which is offline trained with a set of face examples. It produces a statistically aligned part based face representation, namely the PEP representation. To adapt a general face detector to a collection of images, we compute the PEP representations of the candidate detections from the general face detector, and then train a discriminative classifier with the top positives and negatives. Then we re-rank all the candidate detections with this classifier. This way, a face detector tailored to the statistics of the specific image collection is adapted from the original detector. We present extensive results on three datasets with two state-of-the-art face detectors. The significant improvement of detection accuracy over these state of-the-art face detectors strongly demonstrates the efficacy of the proposed face detector adaptation algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751208",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2007",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 19,
            "total": 51
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Detectors",
                "Feature extraction",
                "Adaptation models",
                "Training",
                "Probabilistic logic",
                "Face detection"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic elastic part model",
                "unsupervised face detector adaptation algorithm",
                "offline-trained face detector",
                "PEP model",
                "statistically-aligned part-based face representation",
                "PEP representation",
                "general face detector",
                "detection accuracy improvement",
                "candidate detection",
                "discriminative classifier",
                "image collection"
            ],
            "Author Keywords": [
                "PEP Model",
                "Detector Adaptation",
                "Face Detection"
            ]
        },
        "id": 98,
        "cited_by": [
            {
                "year": "2017",
                "id": 20
            },
            {
                "year": "2017",
                "id": 338
            },
            {
                "year": "2017",
                "id": 512
            },
            {
                "year": "2015",
                "id": 316
            },
            {
                "year": "2015",
                "id": 410
            }
        ]
    },
    {
        "title": "New Graph Structured Sparsity Model for Multi-label Image Annotations",
        "authors": [
            "Xiao Cai",
            "Feiping Nie",
            "Weidong Cai",
            "Heng Huang"
        ],
        "abstract": "In multi-label image annotations, because each image is associated to multiple categories, the semantic terms (label classes) are not mutually exclusive. Previous research showed that such label correlations can largely boost the annotation accuracy. However, all existing methods only directly apply the label correlation matrix to enhance the label inference and assignment without further learning the structural information among classes. In this paper, we model the label correlations using the relational graph, and propose a novel graph structured sparse learning model to incorporate the topological constraints of relation graph in multi-label classifications. As a result, our new method will capture and utilize the hidden class structures in relational graph to improve the annotation results. In proposed objective, a large number of structured sparsity-inducing norms are utilized, thus the optimization becomes difficult. To solve this problem, we derive an efficient optimization algorithm with proved convergence. We perform extensive experiments on six multi-label image annotation benchmark data sets. In all empirical results, our new method shows better annotation results than the state-of-the-art approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751209",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 13,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Semantics",
                "Visualization",
                "Oceans",
                "Linear programming",
                "Computational modeling",
                "Sparse matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "image classification",
                "learning (artificial intelligence)",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph structured sparsity model",
                "multilabel image annotation benchmark data sets",
                "semantic terms",
                "label classes",
                "label correlation matrix",
                "label inference",
                "label assignment",
                "relational graph",
                "graph structured sparse learning model",
                "topological constraints",
                "multilabel classifications",
                "structured sparsity-inducing norms",
                "optimization algorithm",
                "computer vision research"
            ],
            "Author Keywords": [
                "Graph Structured Sparsity",
                "Multi-Label Annotation",
                "Structured Sparsity-Inducing Norm"
            ]
        },
        "id": 99,
        "cited_by": [
            {
                "year": "2017",
                "id": 217
            }
        ]
    },
    {
        "title": "Heterogeneous Auto-similarities of Characteristics (HASC): Exploiting Relational Information for Classification",
        "authors": [
            "Marco San",
            "Marco Crocco",
            "Marco Cristani",
            "Samuele Martelli",
            "Vittorio Murino"
        ],
        "abstract": "Capturing the essential characteristics of visual objects by considering how their features are inter-related is a recent philosophy of object classification. In this paper, we embed this principle in a novel image descriptor, dubbed Heterogeneous Auto-Similarities of Characteristics (HASC). HASC is applied to heterogeneous dense features maps, encoding linear relations by co variances and nonlinear associations through information-theoretic measures such as mutual information and entropy. In this way, highly complex structural information can be expressed in a compact, scale invariant and robust manner. The effectiveness of HASC is tested on many diverse detection and classification scenarios, considering objects, textures and pedestrians, on widely known benchmarks (Caltech-101, Brodatz, Daimler Multi-Cue). In all the cases, the results obtained with standard classifiers demonstrate the superiority of HASC with respect to the most adopted local feature descriptors nowadays, such as SIFT, HOG, LBP and feature co variances. In addition, HASC sets the state-of-the-art on the Brodatz texture dataset and the Daimler Multi-Cue pedestrian dataset, without exploiting ad-hoc sophisticated classifiers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751210",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Electromagnetic interference",
                "Feature extraction",
                "Joints",
                "Histograms",
                "Noise",
                "Mutual information",
                "Entropy"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "heterogeneous auto-similarities of characteristics",
                "relational information",
                "visual objects",
                "object classification",
                "novel image descriptor",
                "HASC",
                "heterogeneous dense features maps",
                "information-theoretic measures",
                "local feature descriptors",
                "Daimler multicue pedestrian dataset"
            ]
        },
        "id": 100,
        "cited_by": []
    },
    {
        "title": "A Fully Hierarchical Approach for Finding Correspondences in Non-rigid Shapes",
        "authors": [
            "Ivan Sipiran",
            "Benjamin Bustos"
        ],
        "abstract": "This paper presents a hierarchical method for finding correspondences in non-rigid shapes. We propose a new representation for 3D meshes: the decomposition tree. This structure characterizes the recursive decomposition process of a mesh into regions of interest and key points. The internal nodes contain regions of interest (which may be recursively decomposed) and the leaf nodes contain the key points to be matched. We also propose a hierarchical matching algorithm that performs in a level-wise manner. The matching process is guided by the similarity between regions in high levels of the tree, until reaching the key points stored in the leaves. This allows us to reduce the search space of correspondences, making also the matching process efficient. We evaluate the effectiveness of our approach using the SHREC'2010 robust correspondence benchmark. In addition, we show that our results outperform the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751211",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 3,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Clustering algorithms",
                "Robustness",
                "Optimization",
                "Kernel",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image representation",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fully hierarchical approach",
                "nonrigid shapes",
                "3D meshes",
                "decomposition tree",
                "recursive decomposition process",
                "leaf nodes",
                "region of interest",
                "hierarchical matching algorithm",
                "correspondence search space",
                "SHREC'2010 robust correspondence benchmark"
            ],
            "Author Keywords": [
                "Correspondences",
                "Non-rigid shapes",
                "shape matching"
            ]
        },
        "id": 101,
        "cited_by": []
    },
    {
        "title": "Learning to Rank Using Privileged Information",
        "authors": [
            "Viktoriia Sharmanska",
            "Novi Quadrianto",
            "Christoph H. Lampert"
        ],
        "abstract": "Many computer vision problems have an asymmetric distribution of information between training and test time. In this work, we study the case where we are given additional information about the training data, which however will not be available at test time. This situation is called learning using privileged information (LUPI). We introduce two maximum-margin techniques that are able to make use of this additional source of information, and we show that the framework is applicable to several scenarios that have been studied in computer vision before. Experiments with attributes, bounding boxes, image tags and rationales as additional information in object classification show promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751212",
        "reference_list": [
            {
                "year": "2011",
                "id": 176
            },
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 44,
            "other": 20,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Support vector machines",
                "Whales",
                "Seals",
                "Optimization",
                "Computer vision",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object classification",
                "image tags",
                "bounding box",
                "maximum-margin technique",
                "LUPI",
                "learning using privileged information",
                "training data",
                "test time",
                "asymmetric information distribution",
                "computer vision problem"
            ],
            "Author Keywords": [
                "Learning to rank",
                "privileged information during training",
                "object classification"
            ]
        },
        "id": 102,
        "cited_by": []
    },
    {
        "title": "Joint Segmentation and Pose Tracking of Human in Natural Videos",
        "authors": [
            "Taegyu Lim",
            "Seunghoon Hong",
            "Bohyung Han",
            "Joon Hee Han"
        ],
        "abstract": "We propose an on-line algorithm to extract a human by foreground/background segmentation and estimate pose of the human from the videos captured by moving cameras. We claim that a virtuous cycle can be created by appropriate interactions between the two modules to solve individual problems. This joint estimation problem is divided into two sub problems, foreground/background segmentation and pose tracking, which alternate iteratively for optimization, segmentation step generates foreground mask for human pose tracking, and human pose tracking step provides fore-ground response map for segmentation. The final solution is obtained when the iterative procedure converges. We evaluate our algorithm quantitatively and qualitatively in real videos involving various challenges, and present its outstanding performance compared to the state-of-the-art techniques for segmentation and pose estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751213",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2011",
                "id": 276
            },
            {
                "year": "2009",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Motion segmentation",
                "Computational modeling",
                "Image segmentation",
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image segmentation",
                "iterative methods",
                "optimisation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video segmentation",
                "human extraction",
                "foreground-background segmentation",
                "pose estimation",
                "camera",
                "iterative method",
                "optimization",
                "human pose tracking"
            ],
            "Author Keywords": [
                "Background Subtraction",
                "Pose estimation"
            ]
        },
        "id": 103,
        "cited_by": []
    },
    {
        "title": "Characterizing Layouts of Outdoor Scenes Using Spatial Topic Processes",
        "authors": [
            "Dahua Lin",
            "Jianxiong Xiao"
        ],
        "abstract": "In this paper, we develop a generative model to describe the layouts of outdoor scenes - the spatial configuration of regions. Specifically, the layout of an image is represented as a composite of regions, each associated with a semantic topic. At the heart of this model is a novel stochastic process called Spatial Topic Process, which generates a spatial map of topics from a set of coupled Gaussian processes, thus allowing the distributions of topics to vary continuously across the image plane. A key aspect that distinguishes this model from previous ones consists in its capability of capturing dependencies across both locations and topics while allowing substantial variations in the layouts. We demonstrate the practical utility of the proposed model by testing it on scene classification, semantic segmentation, and layout hallucination.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751214",
        "reference_list": [
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2005",
                "id": 174
            },
            {
                "year": "2011",
                "id": 42
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Visualization",
                "Vectors",
                "Gaussian processes",
                "Joints",
                "Semantics",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image classification",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "outdoor scenes",
                "spatial topic process",
                "Gaussian processes",
                "scene classification",
                "semantic segmentation",
                "layout hallucination",
                "spatial configuration",
                "novel stochastic process"
            ]
        },
        "id": 104,
        "cited_by": []
    },
    {
        "title": "Image Co-segmentation via Consistent Functional Maps",
        "authors": [
            "Fan Wang",
            "Qixing Huang",
            "Leonidas J. Guibas"
        ],
        "abstract": "Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751215",
        "reference_list": [
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 18,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Optimization",
                "Image edge detection",
                "Visualization",
                "Vectors",
                "Eigenvalues and eigenfunctions",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image cosegmentation",
                "consistent functional maps",
                "unsegmented images",
                "functional maps extraction",
                "connected image pairs",
                "image appearance features",
                "reduced functional space",
                "cycle-consistency",
                "joint optimization framework",
                "network transportation",
                "information sharing",
                "iCoseg",
                "MSRC",
                "PASCAL data sets"
            ]
        },
        "id": 105,
        "cited_by": [
            {
                "year": "2017",
                "id": 234
            },
            {
                "year": "2015",
                "id": 354
            }
        ]
    },
    {
        "title": "Exemplar Cut",
        "authors": [
            "Jimei Yang",
            "Yi-Hsuan Tsai",
            "Ming-Hsuan Yang"
        ],
        "abstract": "We present a hybrid parametric and nonparametric algorithm, exemplar cut, for generating class-specific object segmentation hypotheses. For the parametric part, we train a pylon model on a hierarchical region tree as the energy function for segmentation. For the nonparametric part, we match the input image with each exemplar by using regions to obtain a score which augments the energy function from the pylon model. Our method thus generates a set of highly plausible segmentation hypotheses by solving a series of exemplar augmented graph cuts. Experimental results on the Graz and PASCAL datasets show that the proposed algorithm achieves favorable segmentation performance against the state-of-the-art methods in terms of visual quality and accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751216",
        "reference_list": [
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2011",
                "id": 173
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Poles and towers",
                "Object segmentation",
                "Computational modeling",
                "Labeling",
                "Training",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hybrid parametric and nonparametric algorithm",
                "class-specific object segmentation hypotheses",
                "hierarchical region tree",
                "energy function",
                "pylon model",
                "exemplar augmented graph cuts",
                "PASCAL datasets",
                "Graz datasets"
            ]
        },
        "id": 106,
        "cited_by": [
            {
                "year": "2015",
                "id": 178
            }
        ]
    },
    {
        "title": "Parallel Transport of Deformations in Shape Space of Elastic Surfaces",
        "authors": [
            "Qian Xie",
            "Sebastian Kurtek",
            "Huiling Le",
            "Anuj Srivastava"
        ],
        "abstract": "Statistical shape analysis develops methods for comparisons, deformations, summarizations, and modeling of shapes in given data sets. These tasks require a fundamental tool called parallel transport of tangent vectors along arbitrary paths. This tool is essential for: (1) computation of geodesic paths using either shooting or path-straightening method, (2) transferring deformations across objects, and (3) modeling of statistical variability in shapes. Using the square-root normal field (SRNF) representation of parameterized surfaces, we present a method for transporting deformations along paths in the shape space. This is difficult despite the underlying space being a vector space because the chosen (elastic) Riemannian metric is non-standard. Using a finite-basis for representing SRNFs of shapes, we derive expressions for Christoffel symbols that enable parallel transports. We demonstrate this framework using examples from shape analysis of parameterized spherical surfaces, in the three contexts mentioned above.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751217",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 7,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Vectors",
                "Manifolds",
                "Tensile stress",
                "Extraterrestrial measurements",
                "Harmonic analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "shape recognition",
                "statistical analysis",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape space deformations",
                "elastic surfaces",
                "parallel transport",
                "statistical shape analysis",
                "tangent vectors",
                "square-root normal field",
                "SRNF",
                "parameterized surfaces representation",
                "Riemannian metric",
                "Christoffel symbols",
                "parameterized spherical surfaces",
                "arbitrary paths"
            ]
        },
        "id": 107,
        "cited_by": [
            {
                "year": "2015",
                "id": 324
            }
        ]
    },
    {
        "title": "A Method of Perceptual-Based Shape Decomposition",
        "authors": [
            "Chang Ma",
            "Zhongqian Dong",
            "Tingting Jiang",
            "Yizhou Wang",
            "Wen Gao"
        ],
        "abstract": "In this paper, we propose a novel perception-based shape decomposition method which aims to decompose a shape into semantically meaningful parts. In addition to three popular perception rules (the Minima rule, the Short-cut rule and the Convexity rule) in shape decomposition, we propose a new rule named part-similarity rule to encourage consistent partition of similar parts. The problem is formulated as a quadratic ally constrained quadratic program (QCQP) problem and is solved by a trust-region method. Experiment results on MPEG-7 dataset show that we can get a more consistent shape decomposition with human perception compared with other state-of-the-art methods both qualitatively and quantitatively. Finally, we show the advantage of semantic parts over non-meaningful parts in object detection on the ETHZ dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751218",
        "reference_list": [
            {
                "year": "2011",
                "id": 38
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 6,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Skeleton",
                "Transform coding",
                "Semantics",
                "Vectors",
                "Object detection",
                "Legged locomotion"
            ],
            "INSPEC: Controlled Indexing": [
                "combinatorial mathematics",
                "object detection",
                "object recognition",
                "quadratic programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "perceptual-based shape decomposition method",
                "minima rule",
                "short-cut rule",
                "convexity rule",
                "rule named part-similarity rule",
                "quadratically constrained quadratic program",
                "QCQP problem",
                "MPEG-7 dataset",
                "trust-region method",
                "object detection",
                "object perception",
                "object recognition",
                "combinatorial optimization problem"
            ]
        },
        "id": 108,
        "cited_by": []
    },
    {
        "title": "Curvature-Aware Regularization on Riemannian Submanifolds",
        "authors": [
            "Kwang In Kim",
            "James Tompkin",
            "Christian Theobalt"
        ],
        "abstract": "One fundamental assumption in object recognition as well as in other computer vision and pattern recognition problems is that the data generation process lies on a manifold and that it respects the intrinsic geometry of the manifold. This assumption is held in several successful algorithms for diffusion and regularization, in particular, in graph-Laplacian-based algorithms. We claim that the performance of existing algorithms can be improved if we additionally account for how the manifold is embedded within the ambient space, i.e., if we consider the extrinsic geometry of the manifold. We present a procedure for characterizing the extrinsic (as well as intrinsic) curvature of a manifold M which is described by a sampled point cloud in a high-dimensional Euclidean space. Once estimated, we use this characterization in general diffusion and regularization on M, and form a new regularizer on a point cloud. The resulting re-weighted graph Laplacian demonstrates superior performance over classical graph Laplacian in semi-supervised learning and spectral clustering.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751219",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 4,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Vectors",
                "Laplace equations",
                "Surface treatment",
                "Anisotropic magnetoresistance",
                "Geometry",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "Laplace equations",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curvature-aware regularization",
                "Riemannian submanifolds",
                "object recognition",
                "computer vision",
                "pattern recognition",
                "data generation process",
                "diffusion",
                "graph-Laplacian-based algorithms",
                "high-dimensional Euclidean space",
                "point cloud",
                "re-weighted graph Laplacian",
                "semisupervised learning",
                "spectral clustering"
            ],
            "Author Keywords": [
                "Semi-supervised learning",
                "manifold",
                "regularization"
            ]
        },
        "id": 109,
        "cited_by": []
    },
    {
        "title": "Linear Sequence Discriminant Analysis: A Model-Based Dimensionality Reduction Method for Vector Sequences",
        "authors": [
            "Bing Su",
            "Xiaoqing Ding"
        ],
        "abstract": "Dimensionality reduction for vectors in sequences is challenging since labels are attached to sequences as a whole. This paper presents a model-based dimensionality reduction method for vector sequences, namely linear sequence discriminant analysis (LSDA), which attempts to find a subspace in which sequences of the same class are projected together while those of different classes are projected as far as possible. For each sequence class, an HMM is built from states of which statistics are extracted. Means of these states are linked in order to form a mean sequence, and the variance of the sequence class is defined as the sum of all variances of component states. LSDA then learns a transformation by maximizing the separability between sequence classes and at the same time minimizing the within-sequence class scatter. DTW distance between mean sequences is used to measure the separability between sequence classes. We show that the optimization problem can be approximately transformed into an eigen decomposition problem. LDA can be seen as a special case of LSDA by considering non-sequential vectors as sequences of length one. The effectiveness of the proposed LSDA is demonstrated on two individual sequence datasets from UCI machine learning repository as well as two concatenate sequence datasets: APTI Arabic printed text database and IFN/ENIT Arabic handwriting database.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751220",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 5,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Hidden Markov models",
                "Training",
                "Time series analysis",
                "Analytical models",
                "Optimization",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "handwriting recognition",
                "handwritten character recognition",
                "hidden Markov models",
                "image sequences",
                "learning (artificial intelligence)",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear sequence discriminant analysis",
                "model-based dimensionality reduction method",
                "vector sequences",
                "LSDA",
                "sequence class",
                "HMM",
                "statistics",
                "mean sequence",
                "sequence class variance",
                "separability maximization",
                "within-sequence class scatter minimization",
                "DTW distance",
                "eigen decomposition problem",
                "nonsequential vectors",
                "UCI machine learning repository",
                "concatenate sequence dataset",
                "APTI Arabic printed text database",
                "IFN-ENIT Arabic handwriting database"
            ]
        },
        "id": 110,
        "cited_by": []
    },
    {
        "title": "Frustratingly Easy NBNN Domain Adaptation",
        "authors": [
            "Tatiana Tommasi",
            "Barbara Caputo"
        ],
        "abstract": "Over the last years, several authors have signaled that state of the art categorization methods fail to perform well when trained and tested on data from different databases. The general consensus in the literature is that this issue, known as domain adaptation and/or dataset bias, is due to a distribution mismatch between data collections. Methods addressing it go from max-margin classifiers to learning how to modify the features and obtain a more robust representation. The large majority of these works use BOW feature descriptors, and learning methods based on image-to-image distance functions. Following the seminal work of [6], in this paper we challenge these two assumptions. We experimentally show that using the NBNN classifier over existing domain adaptation databases achieves always very strong performances. We build on this result, and present an NBNN-based domain adaptation algorithm that learns iteratively a class metric while inducing, for each sample, a large margin separation among classes. To the best of our knowledge, this is the first work casting the domain adaptation problem within the NBNN framework. Experiments show that our method achieves the state of the art, both in the unsupervised and semi-supervised settings.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751221",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2011",
                "id": 231
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 11,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Training",
                "Visualization",
                "Feature extraction",
                "Databases",
                "Learning systems",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "NBNN domain adaptation",
                "distribution mismatch",
                "data collections",
                "max-margin classifiers",
                "BOW feature descriptors",
                "learning methods",
                "image-to-image distance functions",
                "NBNN classifier",
                "margin separation",
                "Naive Bayes nearest neighbor method"
            ],
            "Author Keywords": [
                "Domain Adaptation",
                "Naive Bayes Nearest Neighbor"
            ]
        },
        "id": 111,
        "cited_by": [
            {
                "year": "2017",
                "id": 78
            }
        ]
    },
    {
        "title": "Video Event Understanding Using Natural Language Descriptions",
        "authors": [
            "Vignesh Ramanathan",
            "Percy Liang",
            "Li Fei-Fei"
        ],
        "abstract": "Human action and role recognition play an important part in complex event understanding. State-of-the-art methods learn action and role models from detailed spatio temporal annotations, which requires extensive human effort. In this work, we propose a method to learn such models based on natural language descriptions of the training videos, which are easier to collect and scale with the number of actions and roles. There are two challenges with using this form of weak supervision: First, these descriptions only provide a high-level summary and often do not directly mention the actions and roles occurring in a video. Second, natural language descriptions do not provide spatio temporal annotations of actions and roles. To tackle these challenges, we introduce a topic-based semantic relatedness (SR) measure between a video description and an action and role label, and incorporate it into a posterior regularization objective. Our event recognition system based on these action and role models matches the state-of-the-art method on the TRECVID-MED11 event kit, despite weaker supervision.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751222",
        "reference_list": [
            {
                "year": "2011",
                "id": 306
            },
            {
                "year": "2013",
                "id": 54
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 7,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Natural languages",
                "Training",
                "Internet",
                "Encyclopedias",
                "Instruments",
                "YouTube",
                "Atomic measurements"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "natural language processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video event understanding",
                "natural language descriptions",
                "human action recognition",
                "human role recognition",
                "spatio temporal annotations",
                "training videos",
                "semantic relatedness",
                "video description",
                "posterior regularization",
                "event recognition system",
                "TRECVID-MED11 event kit"
            ],
            "Author Keywords": [
                "video event recognition",
                "natural language descriptions"
            ]
        },
        "id": 112,
        "cited_by": [
            {
                "year": "2015",
                "id": 2
            },
            {
                "year": "2015",
                "id": 499
            },
            {
                "year": "2013",
                "id": 54
            }
        ]
    },
    {
        "title": "ACTIVE: Activity Concept Transitions in Video Event Classification",
        "authors": [
            "Chen Sun",
            "Ram Nevatia"
        ],
        "abstract": "The goal of high level event classification from videos is to assign a single, high level event label to each query video. Traditional approaches represent each video as a set of low level features and encode it into a fixed length feature vector (e.g. Bag-of-Words), which leave a big gap between low level visual features and high level events. Our paper tries to address this problem by exploiting activity concept transitions in video events (ACTIVE). A video is treated as a sequence of short clips, all of which are observations corresponding to latent activity concept variables in a Hidden Markov Model (HMM). We propose to apply Fisher Kernel techniques so that the concept transitions over time can be encoded into a compact and fixed length feature vector very efficiently. Our approach can utilize concept annotations from independent datasets, and works well even with a very small number of training samples. Experiments on the challenging NIST TRECVID Multimedia Event Detection (MED) dataset shows our approach performs favorably over the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751223",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 13,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Vectors",
                "Kernel",
                "Training",
                "Support vector machines",
                "Animals",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden Markov models",
                "image classification",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ACTIVE",
                "video event classification",
                "query video",
                "bag-of-words",
                "activity concept transitions in video events",
                "hidden Markov model",
                "HMM",
                "Fisher Kernel techniques",
                "fixed length feature vector",
                "compact length feature vector",
                "NIST TRECVID multimedia event detection"
            ]
        },
        "id": 113,
        "cited_by": [
            {
                "year": "2017",
                "id": 384
            }
        ]
    },
    {
        "title": "Analysis of Scores, Datasets, and Models in Visual Saliency Prediction",
        "authors": [
            "Ali Borji",
            "Hamed R. Tavakoli",
            "Dicky N. Sihite",
            "Laurent Itti"
        ],
        "abstract": "Significant recent progress has been made in developing high-quality saliency models. However, less effort has been undertaken on fair assessment of these models, over large standardized datasets and correctly addressing confounding factors. In this study, we pursue a critical and quantitative look at challenges (e.g., center-bias, map smoothing) in saliency modeling and the way they affect model accuracy. We quantitatively compare 32 state-of-the-art models (using the shuffled AUC score to discount center-bias) on 4 benchmark eye movement datasets, for prediction of human fixation locations and scan path sequence. We also account for the role of map smoothing. We find that, although model rankings vary, some (e.g., AWS, LG, AIM, and HouNIPS) consistently outperform other models over all datasets. Some models work well for prediction of both fixation locations and scan path sequence (e.g., Judd, GBVS). Our results show low prediction accuracy for models over emotional stimuli from the NUSEF dataset. Our last benchmark, for the first time, gauges the ability of models to decode the stimulus category from statistics of fixations, saccades, and model saliency values at fixated locations. In this test, ITTI and AIM models win over other models. Our benchmark provides a comprehensive high-level picture of the strengths and weaknesses of many popular models, and suggests future research directions in saliency modeling.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751224",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 46,
            "other": 27,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Visualization",
                "Benchmark testing",
                "Smoothing methods",
                "Analytical models",
                "Face",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "gaze tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scores",
                "visual saliency prediction",
                "high quality saliency models",
                "fair assessment",
                "standardized datasets",
                "confounding factors",
                "quantitative look",
                "saliency modeling",
                "model accuracy",
                "benchmark eye movement datasets",
                "human fixation locations",
                "scan path sequence",
                "model rankings",
                "emotional stimuli",
                "NUSEF dataset",
                "stimulus category",
                "fixations",
                "saccades",
                "model saliency values",
                "AIM models",
                "high level picture"
            ],
            "Author Keywords": [
                "visual attention",
                "saliency",
                "eye movements",
                "model benchmarking"
            ]
        },
        "id": 114,
        "cited_by": []
    },
    {
        "title": "A Color Constancy Model with Double-Opponency Mechanisms",
        "authors": [
            "Shaobing Gao",
            "Kaifu Yang",
            "Chaoyi Li",
            "Yongjie Li"
        ],
        "abstract": "The double-opponent color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. We introduce a new color constancy model by imitating the functional properties of the HVS from the retina to the double-opponent cells in V1. The idea behind the model originates from the observation that the color distribution of the responses of double-opponent cells to the input color-biased images coincides well with the light source direction. Then the true illuminant color of a scene is easily estimated by searching for the maxima of the separate RGB channels of the responses of double-opponent cells in the RGB space. Our systematical experimental evaluations on two commonly used image datasets show that the proposed model can produce competitive results in comparison to the complex state-of-the-art approaches, but with a simple implementation and without the need for training.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751225",
        "reference_list": [
            {
                "year": "2007",
                "id": 269
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 17,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Radio frequency",
                "Computational modeling",
                "Visual systems",
                "Mathematical model",
                "Retina",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color constancy model",
                "double-opponent color-sensitive cell mechanism",
                "primary visual cortex",
                "human visual system",
                "HVS",
                "input color-biased images",
                "light source direction",
                "RGB channels",
                "image datasets"
            ],
            "Author Keywords": [
                "color constancy",
                "HVS",
                "double-opponency Mechanism"
            ]
        },
        "id": 115,
        "cited_by": []
    },
    {
        "title": "Toward Guaranteed Illumination Models for Non-convex Objects",
        "authors": [
            "Yuqian Zhang",
            "Cun Mu",
            "Han-Wen Kuo",
            "John Wright"
        ],
        "abstract": "Illumination variation remains a central challenge in object detection and recognition. Existing analyses of illumination variation typically pertain to convex, Lambertian objects, and guarantee quality of approximation in an average case sense. We show that it is possible to build models for the set of images across illumination variation with worst-case performance guarantees, for nonconvex Lambertian objects. Namely, a natural verification test based on the distance to the model guarantees to accept any image which can be sufficiently well-approximated by an image of the object under some admissible lighting condition, and guarantees to reject any image that does not have a sufficiently good approximation. These models are generated by sampling illumination directions with sufficient density, which follows from a new perturbation bound for directional illuminated images in the Lambertian model. As the number of such images required for guaranteed verification may be large, we introduce a new formulation for cone preserving dimensionality reduction, which leverages tools from sparse and low-rank decomposition to reduce the complexity, while controlling the approximation error with respect to the original model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751226",
        "reference_list": [
            {
                "year": "2009",
                "id": 74
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Computational modeling",
                "Detectors",
                "Complexity theory",
                "Least squares approximations",
                "Imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "image sampling",
                "object detection",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "guaranteed illumination models",
                "nonconvex Lambertian objects",
                "object detection",
                "object recognition",
                "Lambertian objects",
                "natural verification test",
                "illumination direction sampling",
                "perturbation bound",
                "cone preserving dimensionality reduction",
                "low-rank decomposition",
                "illumination variation"
            ],
            "Author Keywords": [
                "Illumination cone model",
                "Object instance verification",
                "Lambertian surface",
                "Nonconvex object"
            ]
        },
        "id": 116,
        "cited_by": []
    },
    {
        "title": "Nonparametric Blind Super-resolution",
        "authors": [
            "Tomer Michaeli",
            "Michal Irani"
        ],
        "abstract": "Super resolution (SR) algorithms typically assume that the blur kernel is known (either the Point Spread Function 'PSF' of the camera, or some default low-pass filter, e.g. a Gaussian). However, the performance of SR methods significantly deteriorates when the assumed blur kernel deviates from the true one. We propose a general framework for \"blind\" super resolution. In particular, we show that: (i) Unlike the common belief, the PSF of the camera is the wrong blur kernel to use in SR algorithms. (ii) We show how the correct SR blur kernel can be recovered directly from the low-resolution image. This is done by exploiting the inherent recurrence property of small natural image patches (either internally within the same image, or externally in a collection of other natural images). In particular, we show that recurrence of small patches across scales of the low-res image (which forms the basis for single-image SR), can also be used for estimating the optimal blur kernel. This leads to significant improvement in SR results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751227",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2005",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 22,
            "total": 62
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Cameras",
                "Databases",
                "Estimation",
                "Image resolution",
                "Approximation methods",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image resolution",
                "image restoration",
                "optical transfer function"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric blind super-resolution algorithms",
                "point spread function",
                "PSF",
                "camera",
                "low-pass filter",
                "SR methods",
                "SR blur kernel",
                "low-resolution image",
                "inherent recurrence property",
                "natural image patches",
                "single-image SR",
                "optimal blur kernel estimation"
            ]
        },
        "id": 117,
        "cited_by": [
            {
                "year": "2017",
                "id": 26
            },
            {
                "year": "2015",
                "id": 36
            },
            {
                "year": "2015",
                "id": 57
            },
            {
                "year": "2015",
                "id": 58
            }
        ]
    },
    {
        "title": "Street View Motion-from-Structure-from-Motion",
        "authors": [
            "Bryan Klingner",
            "David Martin",
            "James Roseborough"
        ],
        "abstract": "We describe a structure-from-motion framework that handles \"generalized\" cameras, such as moving rolling-shutter cameras, and works at an unprecedented scale-billions of images covering millions of linear kilometers of roads-by exploiting a good relative pose prior along vehicle paths. We exhibit a planet-scale, appearance-augmented point cloud constructed with our framework and demonstrate its practical use in correcting the pose of a street-level image collection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751228",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2009",
                "id": 235
            },
            {
                "year": "2001",
                "id": 117
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 15,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Mathematical model",
                "Vehicles",
                "Trajectory",
                "Lenses",
                "Equations",
                "Transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "street view motion-from-structure-from-motion",
                "planet-scale",
                "appearance-augmented point cloud",
                "street-level image collection"
            ],
            "Author Keywords": [
                "geometric computer vision",
                "structure-from-motion",
                "rolling-shutter cameras"
            ]
        },
        "id": 118,
        "cited_by": [
            {
                "year": "2017",
                "id": 3
            },
            {
                "year": "2017",
                "id": 98
            },
            {
                "year": "2017",
                "id": 412
            }
        ]
    },
    {
        "title": "A Robust Analytical Solution to Isometric Shape-from-Template with Focal Length Calibration",
        "authors": [
            "Adrien Bartoli",
            "Daniel Pizarro",
            "Toby Collins"
        ],
        "abstract": "We study the uncalibrated isometric Shape-from-Template problem, that consists in estimating an isometric deformation from a template shape to an input image whose focal length is unknown. Our method is the first that combines the following features: solving for both the 3D deformation and the camera's focal length, involving only local analytical solutions (there is no numerical optimization), being robust to mismatches, handling general surfaces and running extremely fast. This was achieved through two key steps. First, an `uncalibrated' 3D deformation is computed thanks to a novel piecewise weak-perspective projection model. Second, the camera's focal length is estimated and enables upgrading the 3D deformation to metric. We use a variational framework, implemented using a smooth function basis and sampled local deformation models. The only degeneracy -which we easily detect- for focal length estimation is a flat and fronto-parallel surface. Experimental results on simulated and real datasets show that our method achieves a 3D shape accuracy slightly below state of the art methods using a precalibrated or the true focal length, and a focal length accuracy slightly below static calibration methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751229",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Equations",
                "Cameras",
                "Mathematical model",
                "Calibration"
            ],
            "INSPEC: Controlled Indexing": [
                "image sensors",
                "numerical analysis",
                "optimisation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust analytical solution",
                "focal length calibration",
                "isometric shape-from-template problem",
                "isometric deformation",
                "template shape",
                "camera focal length",
                "numerical optimization",
                "uncalibrated 3D deformation",
                "smooth function"
            ],
            "Author Keywords": [
                "reconstruction",
                "template",
                "isometric"
            ]
        },
        "id": 119,
        "cited_by": []
    },
    {
        "title": "Point-Based 3D Reconstruction of Thin Objects",
        "authors": [
            "Benjamin Ummenhofer",
            "Thomas Brox"
        ],
        "abstract": "3D reconstruction deals with the problem of finding the shape of an object from a set of images. Thin objects that have virtually no volume pose a special challenge for reconstruction with respect to shape representation and fusion of depth information. In this paper we present a dense point-based reconstruction method that can deal with this special class of objects. We seek to jointly optimize a set of depth maps by treating each pixel as a point in space. Points are pulled towards a common surface by pair wise forces in an iterative scheme. The method also handles the problem of opposed surfaces by means of penalty forces. Efficient optimization is achieved by grouping points to super pixels and a spatial hashing approach for fast neighborhood queries. We show that the approach is on a par with state-of-the-art methods for standard multi view stereo settings and gives superior results for thin objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751230",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Cameras",
                "Optimization",
                "Image resolution",
                "Three-dimensional displays",
                "Surface treatment"
            ],
            "INSPEC: Controlled Indexing": [
                "image fusion",
                "image reconstruction",
                "iterative methods",
                "optimisation",
                "shape recognition",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth point-based 3D reconstruction",
                "object shape reconstruction",
                "shape representation",
                "information fusion",
                "depth maps optimization",
                "iterative scheme",
                "spatial hashing approach",
                "multiview stereo settings"
            ]
        },
        "id": 120,
        "cited_by": []
    },
    {
        "title": "Space-Time Tradeoffs in Photo Sequencing",
        "authors": [
            "Tali Dekel",
            "Yael Moses",
            "Shai Avidan"
        ],
        "abstract": "Photo-sequencing is the problem of recovering the temporal order of a set of still images of a dynamic event, taken asynchronously by a set of uncalibrated cameras. Solving this problem is a first, crucial step for analyzing (or visualizing) the dynamic content of the scene captured by a large number of freely moving spectators. We propose a geometric based solution, followed by rank aggregation to the photo-sequencing problem. Our algorithm trades spatial certainty for temporal certainty. Whereas the previous solution proposed by [4] relies on two images taken from the same static camera to eliminate uncertainty in space, we drop the static-camera assumption and replace it with temporal information available from images taken from the same (moving) camera. Our method thus overcomes the limitation of the static-camera assumption, and scales much better with the duration of the event and the spread of cameras in space. We present successful results on challenging real data sets and large scale synthetic data (250 images).",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751231",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Trajectory",
                "Shape",
                "Noise",
                "Dynamics",
                "Sequential analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "space-time tradeoffs",
                "temporal order",
                "dynamic event images",
                "uncalibrated cameras",
                "dynamic content",
                "freely moving spectators",
                "geometric based solution",
                "rank aggregation",
                "photo-sequencing problem",
                "spatial certainty",
                "temporal certainty",
                "static camera",
                "temporal information"
            ],
            "Author Keywords": [
                "Photo Sequencing",
                "multi-view geometry",
                "dynamic events",
                "CrowdCam"
            ]
        },
        "id": 121,
        "cited_by": [
            {
                "year": "2015",
                "id": 445
            },
            {
                "year": "2015",
                "id": 495
            },
            {
                "year": "2015",
                "id": 502
            }
        ]
    },
    {
        "title": "An Enhanced Structure-from-Motion Paradigm Based on the Absolute Dual Quadric and Images of Circular Points",
        "authors": [
            "Lilian Calvet",
            "Pierre Gurdjos"
        ],
        "abstract": "This work aims at introducing a new unified Structure from Motion (SfM) paradigm in which images of circular point-pairs can be combined with images of natural points. An imaged circular point-pair encodes the 2D Euclidean structure of a world plane and can easily be derived from the image of a planar shape, especially those including circles. A classical SfM method generally runs two steps: first a projective factorization of all matched image points (into projective cameras and points) and second a camera self calibration that updates the obtained world from projective to Euclidean. This work shows how to introduce images of circular points in these two SfM steps while its key contribution is to provide the theoretical foundations for combining \"classical\" linear self-calibration constraints with additional ones derived from such images. We show that the two proposed SfM steps clearly contribute to better results than the classical approach. We validate our contributions on synthetic and real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751232",
        "reference_list": [
            {
                "year": "2009",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Equations",
                "Cameras",
                "Three-dimensional displays",
                "Image reconstruction",
                "Calibration",
                "Shape",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "image coding",
                "image matching",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "enhanced structure-from-motion paradigm",
                "absolute dual quadric",
                "structure from motion paradigm",
                "SfM paradigm",
                "circular point-pairs",
                "2D Euclidean structure encoding",
                "world plane",
                "planar shape",
                "projective factorization",
                "image point matching",
                "projective cameras",
                "camera self calibration",
                "classical linear self-calibration constraints"
            ],
            "Author Keywords": [
                "structure-from-motion",
                "circular points",
                "dual absolute quadric",
                "absolute quadratic complex",
                "factorization",
                "self-calibration"
            ]
        },
        "id": 122,
        "cited_by": []
    },
    {
        "title": "Image Guided Depth Upsampling Using Anisotropic Total Generalized Variation",
        "authors": [
            "David Ferstl",
            "Christian Reinbacher",
            "Rene Ranftl",
            "Matthias Ruether",
            "Horst Bischof"
        ],
        "abstract": "In this work we present a novel method for the challenging problem of depth image up sampling. Modern depth cameras such as Kinect or Time-of-Flight cameras deliver dense, high quality depth measurements but are limited in their lateral resolution. To overcome this limitation we formulate a convex optimization problem using higher order regularization for depth image up sampling. In this optimization an an isotropic diffusion tensor, calculated from a high resolution intensity image, is used to guide the up sampling. We derive a numerical algorithm based on a primal-dual formulation that is efficiently parallelized and runs at multiple frames per second. We show that this novel up sampling clearly outperforms state of the art approaches in terms of speed and accuracy on the widely used Middlebury 2007 datasets. Furthermore, we introduce novel datasets with highly accurate ground truth, which, for the first time, enable to benchmark depth up sampling methods using real sensor data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751233",
        "reference_list": [
            {
                "year": "2011",
                "id": 205
            },
            {
                "year": "2011",
                "id": 223
            }
        ],
        "citation": {
            "ieee": 139,
            "other": 57,
            "total": 196
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Energy resolution",
                "Optimization",
                "Tensile stress",
                "Spatial resolution",
                "Anisotropic magnetoresistance"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image resolution",
                "image sampling",
                "numerical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image guided depth upsampling",
                "anisotropic total generalized variation",
                "depth image upsampling",
                "depth cameras",
                "time of flight cameras",
                "Kinect",
                "lateral resolution",
                "convex optimization problem",
                "higher order regularization",
                "anisotropic diffusion tensor",
                "high resolution intensity image",
                "numerical algorithm",
                "benchmark depth upsampling methods",
                "real sensor data",
                "high resolution depth sensing",
                "computer vision",
                "vision"
            ],
            "Author Keywords": [
                "upsampling",
                "superresolution",
                "depth sensing",
                "anisotropic tensor",
                "total generalized variation",
                "optimization",
                "computer vision"
            ]
        },
        "id": 123,
        "cited_by": [
            {
                "year": "2017",
                "id": 330
            },
            {
                "year": "2017",
                "id": 347
            },
            {
                "year": "2015",
                "id": 40
            },
            {
                "year": "2015",
                "id": 57
            },
            {
                "year": "2015",
                "id": 385
            }
        ]
    },
    {
        "title": "Fluttering Pattern Generation Using Modified Legendre Sequence for Coded Exposure Imaging",
        "authors": [
            "Hae-Gon Jeon",
            "Joon-Young Lee",
            "Yudeog Han",
            "Seon Joo Kim",
            "In So Kweon"
        ],
        "abstract": "Finding a good binary sequence is critical in determining the performance of the coded exposure imaging, but previous methods mostly rely on a random search for finding the binary codes, which could easily fail to find good long sequences due to the exponentially growing search space. In this paper, we present a new computationally efficient algorithm for generating the binary sequence, which is especially well suited for longer sequences. We show that the concept of the low autocorrelation binary sequence that has been well exploited in the information theory community can be applied for generating the fluttering patterns of the shutter, propose a new measure of a good binary sequence, and present a new algorithm by modifying the Legendre sequence for the coded exposure imaging. Experiments using both synthetic and real data show that our new algorithm consistently generates better binary sequences for the coded exposure problem, yielding better deblurring and resolution enhancement results compared to the previous methods for generating the binary codes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751234",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 4,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Cameras",
                "Binary codes",
                "Noise",
                "Computer vision",
                "Deconvolution"
            ],
            "INSPEC: Controlled Indexing": [
                "binary codes",
                "binary sequences",
                "image coding",
                "image enhancement",
                "image resolution",
                "image restoration",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deblurring enhancement",
                "resolution enhancement",
                "information theory",
                "low-autocorrelation binary sequence",
                "computationally-efficient algorithm",
                "search space",
                "binary codes",
                "random search",
                "binary sequence generation",
                "coded exposure imaging",
                "modified Legendre sequence",
                "shutter fluttering pattern generation"
            ],
            "Author Keywords": [
                "Computational Photography",
                "Deblurring",
                "Coded Exposure"
            ]
        },
        "id": 124,
        "cited_by": [
            {
                "year": "2015",
                "id": 395
            }
        ]
    },
    {
        "title": "Towards Motion Aware Light Field Video for Dynamic Scenes",
        "authors": [
            "Salil Tambe",
            "Ashok Veeraraghavan",
            "Amit Agrawal"
        ],
        "abstract": "Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse representations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751235",
        "reference_list": [
            {
                "year": "2011",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 5,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatial resolution",
                "Cameras",
                "Apertures",
                "Image reconstruction",
                "Dictionaries",
                "PSNR"
            ],
            "INSPEC: Controlled Indexing": [
                "dictionaries",
                "image motion analysis",
                "image reconstruction",
                "image resolution",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion aware light field video",
                "dynamic scenes",
                "light field cameras",
                "spatial resolution LF video camera",
                "high resolution LF video",
                "multiplexing matrices",
                "dictionary learning",
                "sparse representations",
                "local motion-aware adaptive reconstruction algorithm",
                "graphics simulator",
                "LCoS based programmable camera"
            ],
            "Author Keywords": [
                "Light Field Video",
                "Compressive Sensing",
                "Coded Aperture",
                "Dictionary Learning",
                "Sparse Representation",
                "Motion Aware Reconstruction"
            ]
        },
        "id": 125,
        "cited_by": []
    },
    {
        "title": "Handwritten Word Spotting with Corrected Attributes",
        "authors": [
            "Jon Almaz\u00e1n",
            "Albert Gordo",
            "Alicia Forn\u00e9s",
            "Ernest Valveny"
        ],
        "abstract": "We propose an approach to multi-writer word spotting, where the goal is to find a query word in a dataset comprised of document images. We propose an attributes-based approach that leads to a low-dimensional, fixed-length representation of the word images that is fast to compute and, especially, fast to compare. This approach naturally leads to an unified representation of word images and strings, which seamlessly allows one to indistinctly perform query-by-example, where the query is an image, and query-by-string, where the query is a string. We also propose a calibration scheme to correct the attributes scores based on Canonical Correlation Analysis that greatly improves the results on a challenging dataset. We test our approach on two public datasets showing state-of-the-art results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751236",
        "reference_list": [],
        "citation": {
            "ieee": 19,
            "other": 8,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Histograms",
                "Training",
                "Calibration",
                "Computational modeling",
                "Writing",
                "Correlation"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "query processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "handwritten word spotting",
                "corrected attributes",
                "multiwriter word spotting",
                "document images",
                "attributes-based approach",
                "word images fixed-length representation",
                "query-by-string",
                "calibration scheme",
                "canonical correlation analysis"
            ],
            "Author Keywords": [
                "word spotting",
                "multi-writer",
                "attibutes",
                "cca"
            ]
        },
        "id": 126,
        "cited_by": []
    },
    {
        "title": "Exemplar-Based Graph Matching for Robust Facial Landmark Localization",
        "authors": [
            "Feng Zhou",
            "Jonathan Brandt",
            "Zhe Lin"
        ],
        "abstract": "Localizing facial landmarks is a fundamental step in facial image analysis. However, the problem is still challenging due to the large variability in pose and appearance, and the existence of occlusions in real-world face images. In this paper, we present exemplar-based graph matching (EGM), a robust framework for facial landmark localization. Compared to conventional algorithms, EGM has three advantages: (1) an affine-invariant shape constraint is learned online from similar exemplars to better adapt to the test face, (2) the optimal landmark configuration can be directly obtained by solving a graph matching problem with the learned shape constraint, (3) the graph matching problem can be optimized efficiently by linear programming. To our best knowledge, this is the first attempt to apply a graph matching technique for facial landmark localization. Experiments on several challenging datasets demonstrate the advantages of EGM over state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751237",
        "reference_list": [
            {
                "year": "2011",
                "id": 221
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 21,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Robustness",
                "Detectors",
                "Training",
                "Linear programming",
                "Active shape model"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "graph theory",
                "image matching",
                "linear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "exemplar-based graph matching problem",
                "robust facial landmark localization",
                "facial image analysis",
                "EGM",
                "affine-invariant shape constraint",
                "linear programming"
            ]
        },
        "id": 127,
        "cited_by": [
            {
                "year": "2017",
                "id": 325
            },
            {
                "year": "2015",
                "id": 408
            },
            {
                "year": "2015",
                "id": 424
            }
        ]
    },
    {
        "title": "Cascaded Shape Space Pruning for Robust Facial Landmark Detection",
        "authors": [
            "Xiaowei Zhao",
            "Shiguang Shan",
            "Xiujuan Chai",
            "Xilin Chen"
        ],
        "abstract": "In this paper, we propose a novel cascaded face shape space pruning algorithm for robust facial landmark detection. Through progressively excluding the incorrect candidate shapes, our algorithm can accurately and efficiently achieve the globally optimal shape configuration. Specifically, individual landmark detectors are firstly applied to eliminate wrong candidates for each landmark. Then, the candidate shape space is further pruned by jointly removing incorrect shape configurations. To achieve this purpose, a discriminative structure classifier is designed to assess the candidate shape configurations. Based on the learned discriminative structure classifier, an efficient shape space pruning strategy is proposed to quickly reject most incorrect candidate shapes while preserve the true shape. The proposed algorithm is carefully evaluated on a large set of real world face images. In addition, comparison results on the publicly available BioID and LFW face databases demonstrate that our algorithm outperforms some state-of-the-art algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751238",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 4,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Detectors",
                "Optimization",
                "Databases",
                "Robustness",
                "Detection algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cascaded shape space pruning algorithm",
                "robust facial landmark detection",
                "globally optimal shape configuration",
                "incorrect shape configuration removal",
                "discriminative structure classifier",
                "candidate shape configuration assessment",
                "learned discriminative structure classifier",
                "efficient shape space pruning strategy",
                "shape preservation",
                "real world face images",
                "available BioID face database",
                "LFW face database"
            ]
        },
        "id": 128,
        "cited_by": []
    },
    {
        "title": "Two-Point Gait: Decoupling Gait from Body Shape",
        "authors": [
            "Stephen Lombardi",
            "Ko Nishino",
            "Yasushi Makihara",
            "Yasushi Yagi"
        ],
        "abstract": "Human gait modeling (e.g., for person identification) largely relies on image-based representations that muddle gait with body shape. Silhouettes, for instance, inherently entangle body shape and gait. For gait analysis and recognition, decoupling these two factors is desirable. Most important, once decoupled, they can be combined for the task at hand, but not if left entangled in the first place. In this paper, we introduce Two-Point Gait, a gait representation that encodes the limb motions regardless of the body shape. Two-Point Gait is directly computed on the image sequence based on the two point statistics of optical flow fields. We demonstrate its use for exploring the space of human gait and gait recognition under large clothing variation. The results show that we can achieve state-of-the-art person recognition accuracy on a challenging dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751239",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Optical imaging",
                "Vectors",
                "Robustness",
                "Adaptive optics",
                "Clothing",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "gait analysis",
                "image representation",
                "image sequences",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "two-point gait",
                "decoupling gait",
                "body shape",
                "human gait modeling",
                "image-based representations",
                "muddle gait",
                "silhouettes",
                "gait analysis",
                "limb motions",
                "image sequence",
                "optical flow fields",
                "gait recognition",
                "large clothing variation",
                "person recognition accuracy"
            ],
            "Author Keywords": [
                "Human Gait",
                "Gait Recognition",
                "Gait Representation",
                "Two-Point Gait",
                "Two-Point Statistics"
            ]
        },
        "id": 129,
        "cited_by": []
    },
    {
        "title": "Learning People Detectors for Tracking in Crowded Scenes",
        "authors": [
            "Siyu Tang",
            "Mykhaylo Andriluka",
            "Anton Milan",
            "Konrad Schindler",
            "Stefan Roth",
            "Bernt Schiele"
        ],
        "abstract": "People tracking in crowded real-world scenes is challenging due to frequent and long-term occlusions. Recent tracking methods obtain the image evidence from object (people) detectors, but typically use off-the-shelf detectors and treat them as black box components. In this paper we argue that for best performance one should explicitly train people detectors on failure cases of the overall tracker instead. To that end, we first propose a novel joint people detector that combines a state-of-the-art single person detector with a detector for pairs of people, which explicitly exploits common patterns of person-person occlusions across multiple viewpoints that are a frequent failure case for tracking in crowded scenes. To explicitly address remaining failure modes of the tracker we explore two methods. First, we analyze typical failures of trackers and train a detector explicitly on these cases. And second, we train the detector with the people tracker in the loop, focusing on the most common tracker failures. We show that our joint multi-person detector significantly improves both detection accuracy as well as tracker performance, improving the state-of-the-art on standard benchmarks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751240",
        "reference_list": [
            {
                "year": "2007",
                "id": 97
            },
            {
                "year": "2009",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 18,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Joints",
                "Training",
                "Legged locomotion",
                "Target tracking",
                "Trajectory",
                "Tracking loops"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "people tracking",
                "crowded real-world scenes",
                "image evidence",
                "object detector",
                "black box component",
                "person-person occlusions",
                "multiple viewpoint",
                "object tracking"
            ],
            "Author Keywords": [
                "Pedestrian Detection",
                "Tracking",
                "Multiple People Tracking",
                "Occlusion Handling",
                "Structured SVM",
                "Synthetic Training Data"
            ]
        },
        "id": 130,
        "cited_by": []
    },
    {
        "title": "Efficient Pedestrian Detection by Directly Optimizing the Partial Area under the ROC Curve",
        "authors": [
            "Sakrapee Paisitkriangkrai",
            "Chunhua Shen",
            "Anton Van Den Hengel"
        ],
        "abstract": "Many typical applications of object detection operate within a prescribed false-positive range. In this situation the performance of a detector should be assessed on the basis of the area under the ROC curve over that range, rather than over the full curve, as the performance outside the range is irrelevant. This measure is labelled as the partial area under the ROC curve (pAUC). Effective cascade-based classification, for example, depends on training node classifiers that achieve the maximal detection rate at a moderate false positive rate, e.g., around 40% to 50%. We propose a novel ensemble learning method which achieves a maximal detection rate at a user-defined range of false positive rates by directly optimizing the partial AUC using structured learning. By optimizing for different ranges of false positive rates, the proposed method can be used to train either a single strong classifier or a node classifier forming part of a cascade classifier. Experimental results on both synthetic and real-world data sets demonstrate the effectiveness of our approach, and we show that it is possible to train state-of-the-art pedestrian detectors using the proposed structured ensemble learning method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751241",
        "reference_list": [
            {
                "year": "2009",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 4,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Algorithm design and analysis",
                "Support vector machines",
                "Manganese",
                "Object detection",
                "Detectors",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object detection",
                "pedestrians",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pedestrian detection",
                "partial area under the ROC curve",
                "object detection",
                "false-positive range",
                "detector performance",
                "pAUC",
                "cascade-based classification",
                "training node classifiers",
                "maximal detection rate",
                "moderate false positive rate",
                "partial AUC",
                "structured learning",
                "cascade classifier",
                "node classifier forming",
                "real-world data sets",
                "synthetic data sets",
                "structured ensemble learning method"
            ]
        },
        "id": 131,
        "cited_by": []
    },
    {
        "title": "Example-Based Facade Texture Synthesis",
        "authors": [
            "Dengxin Dai",
            "Hayko Riemenschneider",
            "Gerhard Schmitt",
            "Luc Van"
        ],
        "abstract": "There is an increased interest in the efficient creation of city models, be it virtual or as-built. We present a method for synthesizing complex, photo-realistic facade images, from a single example. After parsing the example image into its semantic components, a tiling for it is generated. Novel tilings can then be created, yielding facade textures with different dimensions or with occluded parts in painted. A genetic algorithm guides the novel facades as well as in painted parts to be consistent with the example, both in terms of their overall structure and their detailed textures. Promising results for multiple standard datasets - in particular for the different building styles they contain - demonstrate the potential of the method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751242",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 6,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Tiles",
                "Semantics",
                "Labeling",
                "Lattices",
                "Sociology",
                "Statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "buildings (structures)",
                "genetic algorithms",
                "image texture",
                "signal synthesis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "example-based facade texture synthesis",
                "city models",
                "photo-realistic facade images",
                "semantic components",
                "genetic algorithm",
                "multiple standard datasets",
                "building styles"
            ],
            "Author Keywords": [
                "Facade Modeling",
                "Facade Texture Synthesis",
                "Facade Parsing",
                "Facade Inpainting"
            ]
        },
        "id": 132,
        "cited_by": []
    },
    {
        "title": "Single-Patch Low-Rank Prior for Non-pointwise Impulse Noise Removal",
        "authors": [
            "Ruixuan Wang",
            "Emanuele Trucco"
        ],
        "abstract": "This paper introduces a `low-rank prior' for small oriented noise-free image patches: considering an oriented patch as a matrix, a low-rank matrix approximation is enough to preserve the texture details in the properly oriented patch. Based on this prior, we propose a single-patch method within a generalized joint low-rank and sparse matrix recovery framework to simultaneously detect and remove non-point wise random-valued impulse noise (e.g., very small blobs). A weighting matrix is incorporated in the framework to encode an initial estimate of the spatial noise distribution. An accelerated proximal gradient method is adapted to estimate the optimal noise-free image patches. Experiments show the effectiveness of our framework in removing non-point wise random-valued impulse noise.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751243",
        "reference_list": [
            {
                "year": "2009",
                "id": 292
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise",
                "Approximation methods",
                "Sparse matrices",
                "Noise measurement",
                "Noise reduction",
                "Image edge detection",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "gradient methods",
                "image denoising",
                "impulse noise",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single patch low rank prior",
                "nonpointwise impulse noise removal",
                "small oriented noise-free image patches",
                "low-rank matrix approximation",
                "sparse matrix recovery framework",
                "weighting matrix",
                "spatial noise distribution",
                "proximal gradient method"
            ],
            "Author Keywords": [
                "low-rank prior",
                "single-patch",
                "non-pointwise random valued impulse noise",
                "joint low-rank and sparse matrix recovery framework"
            ]
        },
        "id": 133,
        "cited_by": [
            {
                "year": "2017",
                "id": 181
            }
        ]
    },
    {
        "title": "SGTD: Structure Gradient and Texture Decorrelating Regularization for Image Decomposition",
        "authors": [
            "Qiegen Liu",
            "Jianbo Liu",
            "Pei Dong",
            "Dong Liang"
        ],
        "abstract": "This paper presents a novel structure gradient and texture decor relating regularization (SGTD) for image decomposition. The motivation of the idea is under the assumption that the structure gradient and texture components should be properly decor related for a successful decomposition. The proposed model consists of the data fidelity term, total variation regularization and the SGTD regularization. An augmented Lagrangian method is proposed to address this optimization issue, by first transforming the unconstrained problem to an equivalent constrained problem and then applying an alternating direction method to iteratively solve the sub problems. Experimental results demonstrate that the proposed method presents better or comparable performance as state-of-the-art methods do.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751244",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 6,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Decorrelation",
                "Numerical models",
                "Image decomposition",
                "Optimization",
                "Correlation",
                "Minimization",
                "TV"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SGTD",
                "structure gradient regularization",
                "texture decorrelating regularization",
                "image decomposition",
                "structure gradient components",
                "texture components",
                "data fidelity term",
                "total variation regularization",
                "augmented Lagrangian method",
                "unconstrained problem"
            ],
            "Author Keywords": [
                "Image decomposition",
                "Structural decorrelating",
                "Structure gradient"
            ]
        },
        "id": 134,
        "cited_by": []
    },
    {
        "title": "Drosophila Embryo Stage Annotation Using Label Propagation",
        "authors": [
            "Tom\u00e1 Kazmar",
            "Evgeny Z. Kvon",
            "Alexander Stark",
            "Christoph H. Lampert"
        ],
        "abstract": "In this work we propose a system for automatic classification of Drosophila embryos into developmental stages. While the system is designed to solve an actual problem in biological research, we believe that the principle underlying it is interesting not only for biologists, but also for researchers in computer vision. The main idea is to combine two orthogonal sources of information: one is a classifier trained on strongly invariant features, which makes it applicable to images of very different conditions, but also leads to rather noisy predictions. The other is a label propagation step based on a more powerful similarity measure that however is only consistent within specific subsets of the data at a time. In our biological setup, the information sources are the shape and the staining patterns of embryo images. We show experimentally that while neither of the methods can be used by itself to achieve satisfactory results, their combination achieves prediction quality comparable to human performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751245",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Embryo",
                "Training",
                "Shape",
                "DNA",
                "Training data",
                "Genomics"
            ],
            "INSPEC: Controlled Indexing": [
                "biological techniques",
                "biology computing",
                "computer vision",
                "image classification",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Drosophila embryo stage annotation",
                "label propagation",
                "automatic classification",
                "developmental stages",
                "biological research",
                "computer vision",
                "orthogonal information source",
                "strongly-invariant feature",
                "noisy predictions",
                "similarity measure",
                "specific data subset",
                "embryo image shape pattern",
                "embryo image staining pattern",
                "prediction quality",
                "human performance"
            ]
        },
        "id": 135,
        "cited_by": []
    },
    {
        "title": "Measuring Flow Complexity in Videos",
        "authors": [
            "Saad Ali"
        ],
        "abstract": "In this paper a notion of flow complexity that measures the amount of interaction among objects is introduced and an approach to compute it directly from a video sequence is proposed. The approach employs particle trajectories as the input representation of motion and maps it into a `braid' based representation. The mapping is based on the observation that 2D trajectories of particles take the form of a braid in space-time due to the intermingling among particles over time. As a result of this mapping, the problem of estimating the flow complexity from particle trajectories becomes the problem of estimating braid complexity, which in turn can be computed by measuring the topological entropy of a braid. For this purpose recently developed mathematical tools from braid theory are employed which allow rapid computation of topological entropy of braids. The approach is evaluated on a dataset consisting of open source videos depicting variations in terms of types of moving objects, scene layout, camera view angle, motion patterns, and object densities. The results show that the proposed approach is able to quantify the complexity of the flow, and at the same time provides useful insights about the sources of the complexity.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751246",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 8,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Complexity theory",
                "Generators",
                "Entropy",
                "Videos",
                "Encoding",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "entropy",
                "image motion analysis",
                "image representation",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object densities",
                "motion pattern",
                "camera view angle",
                "scene layout",
                "open source videos",
                "topological entropy",
                "braid complexity estimation",
                "2D trajectories",
                "braid based representation",
                "maps representation",
                "motion representation",
                "video sequence",
                "flow complexity measurement"
            ],
            "Author Keywords": [
                "Motion Complexity",
                "Flow Complexity",
                "Trajectory Representation",
                "Video Activity Analysis",
                "Braid Theory"
            ]
        },
        "id": 136,
        "cited_by": []
    },
    {
        "title": "Real-Time Body Tracking with One Depth Camera and Inertial Sensors",
        "authors": [
            "Thomas Helten",
            "Meinard M\u00fcller",
            "Hans-Peter Seidel",
            "Christian Theobalt"
        ],
        "abstract": "In recent years, the availability of inexpensive depth cameras, such as the Microsoft Kinect, has boosted the research in monocular full body skeletal pose tracking. Unfortunately, existing trackers often fail to capture poses where a single camera provides insufficient data, such as non-frontal poses, and all other poses with body part occlusions. In this paper, we present a novel sensor fusion approach for real-time full body tracking that succeeds in such difficult situations. It takes inspiration from previous tracking solutions, and combines a generative tracker and a discriminative tracker retrieving closest poses in a database. In contrast to previous work, both trackers employ data from a low number of inexpensive body-worn inertial sensors. These sensors provide reliable and complementary information when the monocular depth information alone is not sufficient. We also contribute by new algorithmic solutions to best fuse depth and inertial data in both trackers. One is a new visibility model to determine global body pose, occlusions and usable depth correspondences and to decide what data modality to use for discriminative tracking. We also contribute with a new inertial-based pose retrieval, and an adapted late fusion step to calculate the final body pose.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751247",
        "reference_list": [
            {
                "year": "2011",
                "id": 138
            },
            {
                "year": "2011",
                "id": 52
            },
            {
                "year": "2011",
                "id": 120
            },
            {
                "year": "2011",
                "id": 247
            },
            {
                "year": "2011",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 16,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Databases",
                "Estimation",
                "Real-time systems",
                "Optical sensors",
                "Reliability"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "pose estimation",
                "sensor fusion"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time body tracking",
                "depth camera",
                "inertial sensors",
                "Microsoft Kinect",
                "monocular full body skeletal pose tracking",
                "sensorfusion approach",
                "generative tracker",
                "discriminative tracker retrieving",
                "body-worn inertial sensors",
                "monocular depth information",
                "fuse depth",
                "inertial data",
                "global body pose",
                "discriminative tracking",
                "inertial-based pose retrieval"
            ],
            "Author Keywords": [
                "Depth tracking",
                "sensor fusion",
                "inertial sensors",
                "real-time"
            ]
        },
        "id": 137,
        "cited_by": []
    },
    {
        "title": "Constructing Adaptive Complex Cells for Robust Visual Tracking",
        "authors": [
            "Dapeng Chen",
            "Zejian Yuan",
            "Yang Wu",
            "Geng Zhang",
            "Nanning Zheng"
        ],
        "abstract": "Representation is a fundamental problem in object tracking. Conventional methods track the target by describing its local or global appearance. In this paper we present that, besides the two paradigms, the composition of local region histograms can also provide diverse and important object cues. We use cells to extract local appearance, and construct complex cells to integrate the information from cells. With different spatial arrangements of cells, complex cells can explore various contextual information at multiple scales, which is important to improve the tracking performance. We also develop a novel template-matching algorithm for object tracking, where the template is composed of temporal varying cells and has two layers to capture the target and background appearance respectively. An adaptive weight is associated with each complex cell to cope with occlusion as well as appearance variation. A fusion weight is associated with each complex cell type to preserve the global distinctiveness. Our algorithm is evaluated on 25 challenging sequences, and the results not only confirm the contribution of each component in our tracking system, but also outperform other competing trackers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751248",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2009",
                "id": 203
            },
            {
                "year": "2011",
                "id": 167
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 13,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Histograms",
                "Robustness",
                "Object tracking",
                "Search problems",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "image fusion",
                "image matching",
                "image representation",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "constructing adaptive complex cells",
                "robust visual tracking",
                "object tracking",
                "object cues",
                "novel template matching algorithm",
                "fusion weight",
                "tracking system"
            ],
            "Author Keywords": [
                "Object tracking",
                "Representation",
                "Two-layer template"
            ]
        },
        "id": 138,
        "cited_by": [
            {
                "year": "2015",
                "id": 336
            }
        ]
    },
    {
        "title": "Camera Alignment Using Trajectory Intersections in Unsynchronized Videos",
        "authors": [
            "Thomas Kuo",
            "Santhoshkumar Sunderrajan",
            "B.S. Manjunath"
        ],
        "abstract": "This paper addresses the novel and challenging problem of aligning camera views that are unsynchronized by low and/or variable frame rates using object trajectories. Unlike existing trajectory-based alignment methods, our method does not require frame-to-frame synchronization. Instead, we propose using the intersections of corresponding object trajectories to match views. To find these intersections, we introduce a novel trajectory matching algorithm based on matching Spatio-Temporal Context Graphs (STCGs). These graphs represent the distances between trajectories in time and space within a view, and are matched to an STCG from another view to find the corresponding trajectories. To the best of our knowledge, this is one of the first attempts to align views that are unsynchronized with variable frame rates. The results on simulated and real-world datasets show trajectory intersections are a viable feature for camera alignment, and that the trajectory matching method performs well in real-world scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751249",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2007",
                "id": 383
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Cameras",
                "Videos",
                "Synchronization",
                "Context",
                "Calibration"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image matching",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "trajectory matching method",
                "variable frame rates",
                "spatio-temporal context graphs",
                "trajectory matching algorithm",
                "trajectory-based alignment methods",
                "object trajectories",
                "variable frame rates",
                "unsynchronized videos",
                "trajectory intersections",
                "camera alignment"
            ],
            "Author Keywords": [
                "Camera Alignment",
                "Trajectory Matching",
                "Unsynchronized Videos"
            ]
        },
        "id": 139,
        "cited_by": []
    },
    {
        "title": "Slice Sampling Particle Belief Propagation",
        "authors": [
            "Oliver M\u00fcller",
            "Michael Ying Yang",
            "Bodo Rosenhahn"
        ],
        "abstract": "Inference in continuous label Markov random fields is a challenging task. We use particle belief propagation (PBP) for solving the inference problem in continuous label space. Sampling particles from the belief distribution is typically done by using Metropolis-Hastings (MH) Markov chain Monte Carlo (MCMC) methods which involves sampling from a proposal distribution. This proposal distribution has to be carefully designed depending on the particular model and input data to achieve fast convergence. We propose to avoid dependence on a proposal distribution by introducing a slice sampling based PBP algorithm. The proposed approach shows superior convergence performance on an image denoising toy example. Our findings are validated on a challenging relational 2D feature tracking application.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751250",
        "reference_list": [
            {
                "year": "2011",
                "id": 17
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Belief propagation",
                "Markov processes",
                "Message passing",
                "Image denoising",
                "Space exploration",
                "Convergence"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image denoising",
                "image sampling",
                "inference mechanisms",
                "Markov processes",
                "Monte Carlo methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "slice sampling particle belief propagation",
                "continuous label Markov random field",
                "PBP",
                "inference problem",
                "continuous label space",
                "belief distribution",
                "Metropolis-Hastings Markov chain Monte Carlo methods",
                "MH MCMC method",
                "proposal distribution",
                "slice sampling-based PBP algorithm",
                "image denoising toy example",
                "relational 2D feature tracking application"
            ],
            "Author Keywords": [
                "Optimization",
                "MCMC",
                "Slice Sampling",
                "Particle Belief Propagation",
                "Feature Tracking"
            ]
        },
        "id": 140,
        "cited_by": []
    },
    {
        "title": "Multi-attributed Dictionary Learning for Sparse Coding",
        "authors": [
            "Chen-Kuo Chiang",
            "Te-Feng Su",
            "Chih Yen",
            "Shang-Hong Lai"
        ],
        "abstract": "We present a multi-attributed dictionary learning algorithm for sparse coding. Considering training samples with multiple attributes, a new distance matrix is proposed by jointly incorporating data and attribute similarities. Then, an objective function is presented to learn category-dependent dictionaries that are compact (closeness of dictionary atoms based on data distance and attribute similarity), reconstructive (low reconstruction error with correct dictionary) and label-consistent (encouraging the labels of dictionary atoms to be similar). We have demonstrated our algorithm on action classification and face recognition tasks on several publicly available datasets. Experimental results with improved performance over previous dictionary learning methods are shown to validate the effectiveness of the proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751251",
        "reference_list": [
            {
                "year": "2011",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Training",
                "Linear programming",
                "Image reconstruction",
                "Face recognition",
                "Image coding",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image coding",
                "image reconstruction",
                "learning (artificial intelligence)",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiattributed dictionary learning algorithm",
                "sparse coding",
                "distance matrix",
                "attribute similarity",
                "objective function",
                "category-dependent dictionary learning",
                "data distance",
                "dictionary atoms",
                "low reconstruction error",
                "label-consistent",
                "action classification",
                "face recognition"
            ],
            "Author Keywords": [
                "Dictionary learning",
                "multiple attributes",
                "sparse coding"
            ]
        },
        "id": 141,
        "cited_by": []
    },
    {
        "title": "Semi-supervised Robust Dictionary Learning via Efficient l-Norms Minimization",
        "authors": [
            "Hua Wang",
            "Feiping Nie",
            "Weidong Cai",
            "Heng Huang"
        ],
        "abstract": "Representing the raw input of a data set by a set of relevant codes is crucial to many computer vision applications. Due to the intrinsic sparse property of real-world data, dictionary learning, in which the linear decomposition of a data point uses a set of learned dictionary bases, i.e., codes, has demonstrated state-of-the-art performance. However, traditional dictionary learning methods suffer from three weaknesses: sensitivity to noisy and outlier samples, difficulty to determine the optimal dictionary size, and incapability to incorporate supervision information. In this paper, we address these weaknesses by learning a Semi-Supervised Robust Dictionary (SSR-D). Specifically, we use the l 2,0+ -norm as the loss function to improve the robustness against outliers, and develop a new structured sparse regularization to incorporate the supervision information in dictionary learning, without incurring additional parameters. Moreover, the optimal dictionary size is automatically learned from the input data. Minimizing the derived objective function is challenging because it involves many non-smooth l 2,0+ -norm terms. We present an efficient algorithm to solve the problem with a rigorous proof of the convergence of the algorithm. Extensive experiments are presented to show the superior performance of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751252",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 6,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Robustness",
                "Vectors",
                "Noise measurement",
                "Convergence",
                "Computer vision",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semisupervised robust dictionary learning",
                "l2,0+-norms minimization",
                "computer vision applications",
                "linear data point decomposition",
                "optimal dictionary size determination",
                "supervision information",
                "SSR-D learning",
                "loss function",
                "robustness improvement",
                "structured sparse regularization",
                "objective function minimization",
                "nonsmooth l2,0+-norm terms"
            ],
            "Author Keywords": [
                "Dictionary Learning",
                "Semi-Supervised Learning",
                "Robust Dictionary Learning"
            ]
        },
        "id": 142,
        "cited_by": []
    },
    {
        "title": "Saliency and Human Fixations: State-of-the-Art and Study of Comparison Metrics",
        "authors": [
            "Nicolas Riche",
            "Matthieu Duvinage",
            "Matei Mancas",
            "Bernard Gosselin",
            "Thierry Dutoit"
        ],
        "abstract": "Visual saliency has been an increasingly active research area in the last ten years with dozens of saliency models recently published. Nowadays, one of the big challenges in the field is to find a way to fairly evaluate all of these models. In this paper, on human eye fixations, we compare the ranking of 12 state-of-the art saliency models using 12 similarity metrics. The comparison is done on Jian Li's database containing several hundreds of natural images. Based on Kendall concordance coefficient, it is shown that some of the metrics are strongly correlated leading to a redundancy in the performance metrics reported in the available benchmarks. On the other hand, other metrics provide a more diverse picture of models' overall performance. As a recommendation, three similarity metrics should be used to obtain a complete point of view of saliency model performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751253",
        "reference_list": [],
        "citation": {
            "ieee": 36,
            "other": 41,
            "total": 77
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Computational modeling",
                "Databases",
                "Correlation",
                "Benchmark testing",
                "Taxonomy",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "eye",
                "gaze tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual saliency model",
                "human eye fixation",
                "Jian Li's database",
                "natural images",
                "Kendall concordance coefficient",
                "computer vision"
            ],
            "Author Keywords": [
                "Saliency maps",
                "Human eye fixations",
                "Metrics",
                "Validation"
            ]
        },
        "id": 143,
        "cited_by": [
            {
                "year": "2015",
                "id": 21
            }
        ]
    },
    {
        "title": "Multiview Photometric Stereo Using Planar Mesh Parameterization",
        "authors": [
            "Jaesik Park",
            "Sudipta N. Sinha",
            "Yasuyuki Matsushita",
            "Yu-Wing Tai",
            "In So Kweon"
        ],
        "abstract": "We propose a method for accurate 3D shape reconstruction using uncalibrated multiview photometric stereo. A coarse mesh reconstructed using multiview stereo is first parameterized using a planar mesh parameterization technique. Subsequently, multiview photometric stereo is performed in the 2D parameter domain of the mesh, where all geometric and photometric cues from multiple images can be treated uniformly. Unlike traditional methods, there is no need for merging view-dependent surface normal maps. Our key contribution is a new photometric stereo based mesh refinement technique that can efficiently reconstruct meshes with extremely fine geometric details by directly estimating a displacement texture map in the 2D parameter domain. We demonstrate that intricate surface geometry can be reconstructed using several challenging datasets containing surfaces with specular reflections, multiple albedos and complex topologies.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751254",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 6,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Mesh generation",
                "Cameras",
                "Face",
                "Shape",
                "Stereo vision",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image reconstruction",
                "mesh generation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D shape reconstruction",
                "planar mesh parameterization technique",
                "2D parameter domain",
                "multiview photometric stereo based mesh refinement technique",
                "surface geometry",
                "specular reflections",
                "complex topologies",
                "multiple albedos",
                "photometric cues",
                "geometric cues"
            ],
            "Author Keywords": [
                "Multiview Photometric Stereo"
            ]
        },
        "id": 144,
        "cited_by": []
    },
    {
        "title": "Elastic Net Constraints for Shape Matching",
        "authors": [
            "Emanuele Rodol\u00e0",
            "Andrea Torsello",
            "Tatsuya Harada",
            "Yasuo Kuniyoshi",
            "Daniel Cremers"
        ],
        "abstract": "We consider a parametrized relaxation of the widely adopted quadratic assignment problem (QAP) formulation for minimum distortion correspondence between deformable shapes. In order to control the accuracy/sparsity trade-off we introduce a weighting parameter on the combination of two existing relaxations, namely spectral and game-theoretic. This leads to the introduction of the elastic net penalty function into shape matching problems. In combination with an efficient algorithm to project onto the elastic net ball, we obtain an approach for deformable shape matching with controllable sparsity. Experiments on a standard benchmark confirm the effectiveness of the approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751255",
        "reference_list": [
            {
                "year": "2011",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 7,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Vectors",
                "Measurement",
                "Optimization",
                "Equations",
                "Accuracy",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "combinatorial mathematics",
                "game theory",
                "image matching",
                "optimisation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "elastic net constraints",
                "parametrized relaxation",
                "quadratic assignment problem formulation",
                "QAP formulation",
                "minimum distortion correspondence",
                "weighting parameter",
                "spectral relaxations",
                "game-theoretic relaxations",
                "elastic net penalty function",
                "elastic net ball",
                "deformable shape matching problems",
                "controllable sparsity"
            ],
            "Author Keywords": [
                "shape matching",
                "graph matching",
                "regression analysis",
                "non-rigid shapes",
                "quadratic assignment problem"
            ]
        },
        "id": 145,
        "cited_by": [
            {
                "year": "2015",
                "id": 227
            }
        ]
    },
    {
        "title": "No Matter Where You Are: Flexible Graph-Guided Multi-task Learning for Multi-view Head Pose Classification under Target Motion",
        "authors": [
            "Yan Yan",
            "Elisa Ricci",
            "Ramanathan Subramanian",
            "Oswald Lanz",
            "Nicu Sebe"
        ],
        "abstract": "We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearance wise related grid partitions to derive the optimal partitioning. For pose classification, upon determining the target's position using a person tracker, the appropriate region specific classifier is invoked. Experiments confirm that FEGA-MTL achieves state-of-the-art classification with few training data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751256",
        "reference_list": [
            {
                "year": "2011",
                "id": 298
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 51,
            "total": 67
        },
        "keywords": {
            "IEEE Keywords": [
                "Head",
                "Cameras",
                "Magnetic heads",
                "Geometry",
                "Training",
                "Three-dimensional displays",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "graph theory",
                "image sensors",
                "learning (artificial intelligence)",
                "motion estimation",
                "pose estimation",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "flexible graph guided multitask learning",
                "multiview head pose classification",
                "target motion",
                "FEGA-MTL",
                "field-of-view surveillance cameras",
                "facial appearance",
                "camera perspective",
                "grid partitions",
                "camera geometry",
                "pose classification"
            ],
            "Author Keywords": [
                "Multi-Task Learning",
                "Multi-view",
                "Head Pose Classification"
            ]
        },
        "id": 146,
        "cited_by": [
            {
                "year": "2015",
                "id": 271
            }
        ]
    },
    {
        "title": "Compositional Models for Video Event Detection: A Multiple Kernel Learning Latent Variable Approach",
        "authors": [
            "Arash Vahdat",
            "Kevin Cannons",
            "Greg Mori",
            "Sangmin Oh",
            "Ilseo Kim"
        ],
        "abstract": "We present a compositional model for video event detection. A video is modeled using a collection of both global and segment-level features and kernel functions are employed for similarity comparisons. The locations of salient, discriminative video segments are treated as a latent variable, allowing the model to explicitly ignore portions of the video that are unimportant for classification. A novel, multiple kernel learning (MKL) latent support vector machine (SVM) is defined, that is used to combine and re-weight multiple feature types in a principled fashion while simultaneously operating within the latent variable framework. The compositional nature of the proposed model allows it to respond directly to the challenges of temporal clutter and intra-class variation, which are prevalent in unconstrained internet videos. Experimental results on the TRECVID Multimedia Event Detection 2011 (MED11) dataset demonstrate the efficacy of the method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751257",
        "reference_list": [
            {
                "year": "2009",
                "id": 5
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 11,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Support vector machines",
                "Training",
                "Computational modeling",
                "Feature extraction",
                "Standards",
                "Linear programming"
            ],
            "INSPEC: Controlled Indexing": [
                "clutter",
                "image classification",
                "image segmentation",
                "learning (artificial intelligence)",
                "multimedia communication",
                "support vector machines",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "TRECVID MED11 dataset",
                "TRECVID multimedia event detection 2011 dataset",
                "unconstrained Internet videos",
                "intraclass variation",
                "temporal clutter",
                "latent variable framework",
                "MKL latent SVM",
                "multiple-kernel learning latent support vector machine",
                "video classification",
                "discriminative video segment",
                "salient video segment",
                "similarity comparison",
                "kernel function",
                "segment-level feature",
                "global level feature",
                "multiple-kernel learning latent variable approach",
                "video event detection",
                "compositional model"
            ]
        },
        "id": 147,
        "cited_by": []
    },
    {
        "title": "Event Recognition in Photo Collections with a Stopwatch HMM",
        "authors": [
            "Lukas Bossard",
            "Matthieu Guillaumin",
            "Luc Van"
        ],
        "abstract": "The task of recognizing events in photo collections is central for automatically organizing images. It is also very challenging, because of the ambiguity of photos across different event classes and because many photos do not convey enough relevant information. Unfortunately, the field still lacks standard evaluation data sets to allow comparison of different approaches. In this paper, we introduce and release a novel data set of personal photo collections containing more than 61,000 images in 807 collections, annotated with 14 diverse social event classes. Casting collections as sequential data, we build upon recent and state-of-the-art work in event recognition in videos to propose a latent sub-event approach for event recognition in photo collections. However, photos in collections are sparsely sampled over time and come in bursts from which transpires the importance of specific moments for the photographers. Thus, we adapt a discriminative hidden Markov model to allow the transitions between states to be a function of the time gap between consecutive images, which we coin as Stopwatch Hidden Markov model (SHMM). In our experiments, we show that our proposed model outperforms approaches based only on feature pooling or a classical hidden Markov model. With an average accuracy of 56%, we also highlight the difficulty of the data set and the need for future advances in event recognition in photo collections.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751258",
        "reference_list": [
            {
                "year": "2007",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Visualization",
                "Videos",
                "Support vector machines",
                "Predictive models",
                "Complexity theory",
                "Markov processes"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden Markov models",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SHMM",
                "stopwatch hidden Markov model",
                "discriminative hidden Markov model",
                "latent sub-event approach",
                "sequential data",
                "social event classes",
                "personal photo collections",
                "stopwatch HMM",
                "event recognition"
            ]
        },
        "id": 148,
        "cited_by": []
    },
    {
        "title": "Nested Shape Descriptors",
        "authors": [
            "Jeffrey Byrne",
            "Jianbo Shi"
        ],
        "abstract": "In this paper, we propose a new family of binary local feature descriptors called nested shape descriptors. These descriptors are constructed by pooling oriented gradients over a large geometric structure called the Hawaiian earring, which is constructed with a nested correlation structure that enables a new robust local distance function called the nesting distance. This distance function is unique to the nested descriptor and provides robustness to outliers from order statistics. In this paper, we define the nested shape descriptor family and introduce a specific member called the seed-of-life descriptor. We perform a trade study to determine optimal descriptor parameters for the task of image matching. Finally, we evaluate performance compared to state-of-the-art local feature descriptors on the VGG-Affine image matching benchmark, showing significant performance gains. Our descriptor is the first binary descriptor to outperform SIFT on this benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751259",
        "reference_list": [
            {
                "year": "2011",
                "id": 324
            },
            {
                "year": "2011",
                "id": 326
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 7,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Robustness",
                "Spirals",
                "Image matching",
                "Benchmark testing",
                "Euclidean distance",
                "Materials"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "VGG-Affine image matching benchmark",
                "seed-of-life descriptor",
                "robust local distance function",
                "nested correlation structure",
                "Hawaiian ear-ring",
                "pooling oriented gradient",
                "binary local feature descriptor",
                "nested shape descriptor"
            ],
            "Author Keywords": [
                "local feature descriptor",
                "binary descriptor",
                "shape representation",
                "feature extraction and matching"
            ]
        },
        "id": 149,
        "cited_by": []
    },
    {
        "title": "Collaborative Active Learning of a Kernel Machine Ensemble for Recognition",
        "authors": [
            "Gang Hua",
            "Chengjiang Long",
            "Ming Yang",
            "Yan Gao"
        ],
        "abstract": "Active learning is an effective way of engaging users to interactively train models for visual recognition. The vast majority of previous works, if not all of them, focused on active learning with a single human oracle. The problem of active learning with multiple oracles in a collaborative setting has not been well explored. Moreover, most of the previous works assume that the labels provided by the human oracles are noise free, which may often be violated in reality. We present a collaborative computational model for active learning with multiple human oracles. It leads to not only an ensemble kernel machine that is robust to label noises, but also a principled label quality measure to online detect irresponsible labelers. Instead of running independent active learning processes for each individual human oracle, our model captures the inherent correlations among the labelers through shared data among them. Our simulation experiments and experiments with real crowd-sourced noisy labels demonstrated the efficacy of our model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751260",
        "reference_list": [
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2011",
                "id": 177
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 7,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Collaboration",
                "Kernel",
                "Visualization",
                "Noise",
                "Data models",
                "Labeling",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "lead compounds",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "collaborative active learning",
                "kernel machine ensemble",
                "model training",
                "visual recognition",
                "human oracles",
                "collaborative computational model",
                "ensemble kernel machine",
                "individual human oracle",
                "real crowd-sourced noisy labels"
            ],
            "Author Keywords": [
                "Active Learning",
                "Crowdsourcing",
                "Kernel Machine Ensemble",
                "Visual Recognition"
            ]
        },
        "id": 150,
        "cited_by": [
            {
                "year": "2015",
                "id": 316
            }
        ]
    },
    {
        "title": "A Max-Margin Perspective on Sparse Representation-Based Classification",
        "authors": [
            "Zhaowen Wang",
            "Jianchao Yang",
            "Nasser Nasrabadi",
            "Thomas Huang"
        ],
        "abstract": "Sparse Representation-based Classification (SRC) is a powerful tool in distinguishing signal categories which lie on different subspaces. Despite its wide application to visual recognition tasks, current understanding of SRC is solely based on a reconstructive perspective, which neither offers any guarantee on its classification performance nor provides any insight on how to design a discriminative dictionary for SRC. In this paper, we present a novel perspective towards SRC and interpret it as a margin classifier. The decision boundary and margin of SRC are analyzed in local regions where the support of sparse code is stable. Based on the derived margin, we propose a hinge loss function as the gauge for the classification performance of SRC. A stochastic gradient descent algorithm is implemented to maximize the margin of SRC and obtain more discriminative dictionaries. Experiments validate the effectiveness of the proposed approach in predicting classification performance and improving dictionary quality over reconstructive ones. Classification results competitive with other state-of-the-art sparse coding methods are reported on several data sets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751261",
        "reference_list": [
            {
                "year": "2011",
                "id": 192
            },
            {
                "year": "2011",
                "id": 89
            },
            {
                "year": "2011",
                "id": 68
            },
            {
                "year": "2011",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Approximation methods",
                "Training",
                "Support vector machines",
                "Fasteners",
                "Encoding",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image classification",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stochastic gradient descent algorithm",
                "hinge loss function",
                "sparse code",
                "local regions",
                "margin classifier",
                "SRC",
                "discriminative dictionary",
                "visual recognition tasks",
                "signal categories",
                "sparse representation-based classification",
                "max-margin perspective"
            ]
        },
        "id": 151,
        "cited_by": []
    },
    {
        "title": "Attribute Dominance: What Pops Out?",
        "authors": [
            "Naman Turakhia",
            "Devi Parikh"
        ],
        "abstract": "When we look at an image, some properties or attributes of the image stand out more than others. When describing an image, people are likely to describe these dominant attributes first. Attribute dominance is a result of a complex interplay between the various properties present or absent in the image. Which attributes in an image are more dominant than others reveals rich information about the content of the image. In this paper we tap into this information by modeling attribute dominance. We show that this helps improve the performance of vision systems on a variety of human-centric applications such as zero-shot learning, image search and generating textual descriptions of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751262",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2013",
                "id": 269
            },
            {
                "year": "2009",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 7,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Animals",
                "Vocabulary",
                "Predictive models",
                "Equations",
                "Computational modeling",
                "Training",
                "Handheld computers"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "attribute dominance modelling",
                "computer vision systems",
                "human-centric applications",
                "zero-shot learning",
                "image search",
                "image textual description generation",
                "image pop out"
            ],
            "Author Keywords": [
                "attributes",
                "attribute dominance",
                "image search",
                "zero shot learning",
                "textual description",
                "attribute based classification"
            ]
        },
        "id": 152,
        "cited_by": []
    },
    {
        "title": "Neighbor-to-Neighbor Search for Fast Coding of Feature Vectors",
        "authors": [
            "Nakamasa Inoue",
            "Koichi Shinoda"
        ],
        "abstract": "Assigning a visual code to a low-level image descriptor, which we call code assignment, is the most computationally expensive part of image classification algorithms based on the bag of visual word (BoW) framework. This paper proposes a fast computation method, Neighbor-to-Neighbor (NTN) search, for this code assignment. Based on the fact that image features from an adjacent region are usually similar to each other, this algorithm effectively reduces the cost of calculating the distance between a codeword and a feature vector. This method can be applied not only to a hard codebook constructed by vector quantization (NTN-VQ), but also to a soft codebook, a Gaussian mixture model (NTN-GMM). We evaluated this method on the PASCAL VOC 2007 classification challenge task. NTN-VQ reduced the assignment cost by 77.4% in super-vector coding, and NTN-GMM reduced it by 89.3% in Fisher-vector coding, without any significant degradation in classification performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751263",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Encoding",
                "Image coding",
                "Visualization",
                "Training",
                "Gaussian mixture model"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Gaussian processes",
                "image classification",
                "image coding",
                "vector quantisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "neighbor-to-neighbor search",
                "feature vector fast-coding",
                "visual code assignment",
                "low-level image descriptor",
                "image classification algorithm",
                "bag-of-visual word framework",
                "BoW framework",
                "fast computation method",
                "NTN search",
                "image features",
                "codeword",
                "hard codebook",
                "vector quantization",
                "NTN-VQ method",
                "soft codebook",
                "Gaussian mixture model",
                "NTN-GMM method",
                "PASCAL VOC 2007 classification challenge task",
                "super-vector coding",
                "Fisher-vector coding",
                "classification performance"
            ],
            "Author Keywords": [
                "Neighbor-To-Neighbor Search",
                "Gaussian mixture model",
                "Bag of visual word",
                "Fisher kernel",
                "Image classification"
            ]
        },
        "id": 153,
        "cited_by": []
    },
    {
        "title": "Text Localization in Natural Images Using Stroke Feature Transform and Text Covariance Descriptors",
        "authors": [
            "Weilin Huang",
            "Zhe Lin",
            "Jianchao Yang",
            "Jue Wang"
        ],
        "abstract": "In this paper, we present a new approach for text localization in natural images, by discriminating text and non-text regions at three levels: pixel, component and text line levels. Firstly, a powerful low-level filter called the Stroke Feature Transform (SFT) is proposed, which extends the widely-used Stroke Width Transform (SWT) by incorporating color cues of text pixels, leading to significantly enhanced performance on inter-component separation and intra-component connection. Secondly, based on the output of SFT, we apply two classifiers, a text component classifier and a text-line classifier, sequentially to extract text regions, eliminating the heuristic procedures that are commonly used in previous approaches. The two classifiers are built upon two novel Text Covariance Descriptors (TCDs) that encode both the heuristic properties and the statistical characteristics of text stokes. Finally, text regions are located by simply thresholding the text-line confident map. Our method was evaluated on two benchmark datasets: ICDAR 2005 and ICDAR 2011, and the corresponding F-measure values are 0.72 and 0.73, respectively, surpassing previous methods in accuracy by a large margin.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751264",
        "reference_list": [],
        "citation": {
            "ieee": 74,
            "other": 47,
            "total": 121
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Transforms",
                "Image edge detection",
                "Color",
                "Feature extraction",
                "Covariance matrices",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image colour analysis",
                "text detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ICDAR 2011",
                "ICDAR 2005",
                "text-line confident map",
                "TCD",
                "text-line classifier",
                "text component classifier",
                "intracomponent connection",
                "intercomponent separation",
                "text pixel color cues",
                "SWT",
                "stroke width transform",
                "SFT",
                "text covariance descriptors",
                "stroke feature transform",
                "natural images",
                "text localization"
            ],
            "Author Keywords": [
                "Low-level filter",
                "stroke width transform",
                "text component",
                "text covariance descriptors"
            ]
        },
        "id": 154,
        "cited_by": [
            {
                "year": "2017",
                "id": 77
            },
            {
                "year": "2017",
                "id": 156
            },
            {
                "year": "2017",
                "id": 321
            },
            {
                "year": "2017",
                "id": 519
            },
            {
                "year": "2017",
                "id": 550
            },
            {
                "year": "2015",
                "id": 519
            }
        ]
    },
    {
        "title": "A Framework for Shape Analysis via Hilbert Space Embedding",
        "authors": [
            "Sadeep Jayasumana",
            "Mathieu Salzmann",
            "Hongdong Li",
            "Mehrtash Harandi"
        ],
        "abstract": "We propose a framework for 2D shape analysis using positive definite kernels defined on Kendall's shape manifold. Different representations of 2D shapes are known to generate different nonlinear spaces. Due to the nonlinearity of these spaces, most existing shape classification algorithms resort to nearest neighbor methods and to learning distances on shape spaces. Here, we propose to map shapes on Kendall's shape manifold to a high dimensional Hilbert space where Euclidean geometry applies. To this end, we introduce a kernel on this manifold that permits such a mapping, and prove its positive definiteness. This kernel lets us extend kernel-based algorithms developed for Euclidean spaces, such as SVM, MKL and kernel PCA, to the shape manifold. We demonstrate the benefits of our approach over the state-of-the-art methods on shape classification, clustering and retrieval.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751265",
        "reference_list": [
            {
                "year": "2009",
                "id": 100
            },
            {
                "year": "2007",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 7,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Shape",
                "Manifolds",
                "Support vector machines",
                "Hilbert space",
                "Geometry",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "Hilbert spaces",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "2D shape analysis",
                "positive definite kernels",
                "Kendall shape manifold",
                "nonlinear space generation",
                "shape classification algorithms",
                "nearest neighbor methods",
                "learning distances",
                "shape spaces",
                "high dimensional Hilbert space embedding",
                "Euclidean geometry",
                "kernel-based algorithms",
                "SVM",
                "MKL",
                "kernel PCA",
                "shape clustering",
                "shape retrieval",
                "multiple kernel learning",
                "support vector machines",
                "kernel principal component analysis",
                "kernel k-means",
                "Procrustes Gaussian kernel"
            ],
            "Author Keywords": [
                "Shape manifold",
                "Positive definite kernels",
                "Mercer kernels",
                "Shape analysis"
            ]
        },
        "id": 155,
        "cited_by": [
            {
                "year": "2015",
                "id": 185
            }
        ]
    },
    {
        "title": "Offline Mobile Instance Retrieval with a Small Memory Footprint",
        "authors": [
            "Jayaguru Panda",
            "Michael S. Brown",
            "C.V. Jawahar"
        ],
        "abstract": "Existing mobile image instance retrieval applications assume a network-based usage where image features are sent to a server to query an online visual database. In this scenario, there are no restrictions on the size of the visual database. This paper, however, examines how to perform this same task offline, where the entire visual index must reside on the mobile device itself within a small memory footprint. Such solutions have applications on location recognition and product recognition. Mobile instance retrieval requires a significant reduction in the visual index size. To achieve this, we describe a set of strategies that can reduce the visual index up to 60-80 times compared to a standard instance retrieval implementation found on desktops or servers. While our proposed reduction steps affect the overall mean Average Precision (mAP), they are able to maintain a good Precision for the top K results (P K ). We argue that for such offline application, maintaining a good P K is sufficient. The effectiveness of this approach is demonstrated on several standard databases. A working application designed for a remote historical site is also presented. This application is able to reduce an 50,000 image index structure to 25 MBs while providing a precision of 97% for P 10 and 100% for P 1 .",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751266",
        "reference_list": [
            {
                "year": "2009",
                "id": 303
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2009",
                "id": 141
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 7,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Indexes",
                "Visualization",
                "Vocabulary",
                "Geometry",
                "Vectors",
                "Mobile communication"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image retrieval",
                "mobile computing",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "offline mobile image instance retrieval",
                "small memory footprint",
                "network-based usage",
                "image features",
                "online visual database querying",
                "mobile device",
                "location recognition",
                "product recognition",
                "visual index size",
                "mean average precision",
                "MAP",
                "remote historical site",
                "image index structure"
            ],
            "Author Keywords": [
                "Instance Retrieval",
                "Average Precision",
                "memory footprint"
            ]
        },
        "id": 156,
        "cited_by": []
    },
    {
        "title": "BOLD Features to Detect Texture-less Objects",
        "authors": [
            "Federico Tombari",
            "Alessandro Franchi",
            "Luigi Di"
        ],
        "abstract": "Object detection in images withstanding significant clutter and occlusion is still a challenging task whenever the object surface is characterized by poor informative content. We propose to tackle this problem by a compact and distinctive representation of groups of neighboring line segments aggregated over limited spatial supports and invariant to rotation, translation and scale changes. Peculiarly, our proposal allows for leveraging on the inherent strengths of descriptor-based approaches, i.e. robustness to occlusion and clutter and scalability with respect to the size of the model library, also when dealing with scarcely textured objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751267",
        "reference_list": [
            {
                "year": "2005",
                "id": 206
            },
            {
                "year": "2011",
                "id": 326
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 17,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Histograms",
                "Object detection",
                "Robustness",
                "Image edge detection",
                "Feature extraction",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "clutter",
                "feature extraction",
                "image representation",
                "image texture",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "BOLD features",
                "texture-less object detection",
                "clutter",
                "occlusion",
                "neighboring line segments",
                "spatial supports",
                "descriptor-based approaches",
                "scalability",
                "model library",
                "scarcely textured objects"
            ],
            "Author Keywords": [
                "object detection",
                "object recognition",
                "texture-less",
                "descriptors",
                "image features"
            ]
        },
        "id": 157,
        "cited_by": []
    },
    {
        "title": "Estimating the 3D Layout of Indoor Scenes and Its Clutter from Depth Sensors",
        "authors": [
            "Jian Zhang",
            "Chen Kan",
            "Alexander G. Schwing",
            "Raquel Urtasun"
        ],
        "abstract": "In this paper we propose an approach to jointly estimate the layout of rooms as well as the clutter present in the scene using RGB-D data. Towards this goal, we propose an effective model that is able to exploit both depth and appearance features, which are complementary. Furthermore, our approach is efficient as we exploit the inherent decomposition of additive potentials. We demonstrate the effectiveness of our approach on the challenging NYU v2 dataset and show that employing depth reduces the layout error by 6% and the clutter estimation by 13%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751268",
        "reference_list": [
            {
                "year": "2011",
                "id": 283
            },
            {
                "year": "2009",
                "id": 237
            },
            {
                "year": "2013",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 8,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Clutter",
                "Labeling",
                "Geometry",
                "Three-dimensional displays",
                "Estimation",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image sensors",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "clutter estimation",
                "layout error reduction",
                "NYU v2 dataset",
                "appearance features",
                "depth features",
                "RGB-D data",
                "depth sensors",
                "indoor scenes",
                "3D layout estimation"
            ]
        },
        "id": 158,
        "cited_by": [
            {
                "year": "2017",
                "id": 511
            },
            {
                "year": "2015",
                "id": 104
            }
        ]
    },
    {
        "title": "A Non-parametric Bayesian Network Prior of Human Pose",
        "authors": [
            "Andreas M. Lehrmann",
            "Peter V. Gehler",
            "Sebastian Nowozin"
        ],
        "abstract": "Having a sensible prior of human pose is a vital ingredient for many computer vision applications, including tracking and pose estimation. While the application of global non-parametric approaches and parametric models has led to some success, finding the right balance in terms of flexibility and tractability, as well as estimating model parameters from data has turned out to be challenging. In this work, we introduce a sparse Bayesian network model of human pose that is non-parametric with respect to the estimation of both its graph structure and its local distributions. We describe an efficient sampling scheme for our model and show its tractability for the computation of exact log-likelihoods. We empirically validate our approach on the Human 3.6M dataset and demonstrate superior performance to global models and parametric networks. We further illustrate our model's ability to represent and compose poses not present in the training set (compositionality) and describe a speed-accuracy trade-off that allows real-time scoring of poses.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751269",
        "reference_list": [
            {
                "year": "2005",
                "id": 60
            },
            {
                "year": "2003",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Kinematics",
                "Joints",
                "Bayes methods",
                "Estimation",
                "Mathematical model",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "computer vision",
                "graph theory",
                "image sampling",
                "parameter estimation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric Bayesian network",
                "human pose estimation",
                "computer vision",
                "pose tracking",
                "global nonparametric approach",
                "parametric models",
                "sparse Bayesian network model",
                "graph structure",
                "image sampling scheme",
                "exact log-likelihood computation",
                "Human 3.6M dataset",
                "model parameter estimation"
            ],
            "Author Keywords": [
                "Bayesian network",
                "pose prior",
                "non-parametric",
                "structure learning",
                "compositionality",
                "real-time"
            ]
        },
        "id": 159,
        "cited_by": []
    },
    {
        "title": "Pictorial Human Spaces: How Well Do Humans Perceive a 3D Articulated Pose?",
        "authors": [
            "Elisabeta Marinoiu",
            "Dragos Papava",
            "Cristian Sminchisescu"
        ],
        "abstract": "Human motion analysis in images and video is a central computer vision problem. Yet, there are no studies that reveal how humans perceive other people in images and how accurate they are. In this paper we aim to unveil some of the processing-as well as the levels of accuracy-involved in the 3D perception of people from images by assessing the human performance. Our contributions are: (1) the construction of an experimental apparatus that relates perception and measurement, in particular the visual and kinematic performance with respect to 3D ground truth when the human subject is presented an image of a person in a given pose, (2) the creation of a dataset containing images, articulated 2D and 3D pose ground truth, as well as synchronized eye movement recordings of human subjects, shown a variety of human body configurations, both easy and difficult, as well as their 're-enacted' 3D poses, (3) quantitative analysis revealing the human performance in 3D pose re-enactment tasks, the degree of stability in the visual fixation patterns of human subjects, and the way it correlates with different poses. We also discuss the implications of our findings for the construction of visual human sensing systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751270",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Joints",
                "Three-dimensional displays",
                "Tracking",
                "Hidden Markov models",
                "Standards",
                "Visualization",
                "Head"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "eye",
                "image motion analysis",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pictorial human spaces",
                "3D articulated pose",
                "human motion analysis",
                "central computer vision",
                "3D perception",
                "visual performance",
                "kinematic performance",
                "3D ground truth",
                "2D pose ground truth",
                "3D pose ground truth",
                "synchronized eye movement recording",
                "human subjects",
                "human body configurations",
                "3D poses",
                "quantitative analysis",
                "visual fixation patterns",
                "visual human sensing system"
            ]
        },
        "id": 160,
        "cited_by": []
    },
    {
        "title": "Co-segmentation by Composition",
        "authors": [
            "Alon Faktor",
            "Michal Irani"
        ],
        "abstract": "Given a set of images which share an object from the same semantic category, we would like to co-segment the shared object. We define 'good' co-segments to be ones which can be easily composed (like a puzzle) from large pieces of other co-segments, yet are difficult to compose from remaining image parts. These pieces must not only match well but also be statistically significant (hard to compose at random). This gives rise to co-segmentation of objects in very challenging scenarios with large variations in appearance, shape and large amounts of clutter. We further show how multiple images can collaborate and \"score\" each others' co-segments to improve the overall fidelity and accuracy of the co-segmentation. Our co-segmentation can be applied both to large image collections, as well as to very few images (where there is too little data for unsupervised learning). At the extreme, it can be applied even to a single image, to extract its co-occurring objects. Our approach obtains state-of-the-art results on benchmark datasets. We further show very encouraging co-segmentation results on the challenging PASCAL-VOC dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751271",
        "reference_list": [
            {
                "year": "2011",
                "id": 328
            },
            {
                "year": "2011",
                "id": 21
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 11,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Clutter",
                "Semantics",
                "Approximation methods",
                "Estimation",
                "Shape",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shared object co-segmentation",
                "semantic category",
                "clutter",
                "image collections",
                "PASCAL-VOC dataset",
                "co-segmentation by composition"
            ]
        },
        "id": 161,
        "cited_by": []
    },
    {
        "title": "Cosegmentation and Cosketch by Unsupervised Learning",
        "authors": [
            "Jifeng Dai",
            "Ying Nian Wu",
            "Jie Zhou",
            "Song-Chun Zhu"
        ],
        "abstract": "Co segmentation refers to the problem of segmenting multiple images simultaneously by exploiting the similarities between the foreground and background regions in these images. The key issue in co segmentation is to align common objects between these images. To address this issue, we propose an unsupervised learning framework for co segmentation, by coupling co segmentation with what we call ``co sketch''. The goal of co sketch is to automatically discover a codebook of deformable shape templates shared by the input images. These shape templates capture distinct image patterns and each template is matched to similar image patches in different images. Thus the co sketch of the images helps to align foreground objects, thereby providing crucial information for co segmentation. We present a statistical model whose energy function couples co sketch and co segmentation. We then present an unsupervised learning algorithm that performs co sketch and co segmentation by energy minimization. Experiments show that our method outperforms state of the art methods for co segmentation on the challenging MSRC and iciest datasets. We also illustrate our method on a new dataset called Coseg-Rep where co segmentation can be performed within a single image with repetitive patterns.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751272",
        "reference_list": [
            {
                "year": "2007",
                "id": 87
            },
            {
                "year": "2011",
                "id": 21
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 7,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Image color analysis",
                "Matching pursuit algorithms",
                "Couplings",
                "Unsupervised learning",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Coseg-Rep",
                "MSRC datasets",
                "iCoseg datasets",
                "energy minimization",
                "image patterns",
                "unsupervised learning framework",
                "multiple image segmention",
                "unsupervised learning",
                "cosketch",
                "cosegmentation"
            ]
        },
        "id": 162,
        "cited_by": [
            {
                "year": "2015",
                "id": 494
            }
        ]
    },
    {
        "title": "Predicting Sufficient Annotation Strength for Interactive Foreground Segmentation",
        "authors": [
            "Suyog Dutt Jain",
            "Kristen Grauman"
        ],
        "abstract": "The mode of manual annotation used in an interactive segmentation algorithm affects both its accuracy and ease-of-use. For example, bounding boxes are fast to supply, yet may be too coarse to get good results on difficult images, freehand outlines are slower to supply and more specific, yet they may be overkill for simple images. Whereas existing methods assume a fixed form of input no matter the image, we propose to predict the tradeoff between accuracy and effort. Our approach learns whether a graph cuts segmentation will succeed if initialized with a given annotation mode, based on the image's visual separability and foreground uncertainty. Using these predictions, we optimize the mode of input requested on new images a user wants segmented. Whether given a single image that should be segmented as quickly as possible, or a batch of images that must be segmented within a specified time budget, we show how to select the easiest modality that will be sufficiently strong to yield high quality segmentations. Extensive results with real users and three datasets demonstrate the impact.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751273",
        "reference_list": [
            {
                "year": "2009",
                "id": 35
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2011",
                "id": 321
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 10,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image color analysis",
                "Prediction algorithms",
                "Uncertainty",
                "Training",
                "Accuracy",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sufficient annotation strength prediction",
                "interactive foreground segmentation",
                "manual annotation mode",
                "bounding boxes",
                "freehand outlines",
                "accuracy-effort tradeoff prediction",
                "graph cut segmentation",
                "image visual separability",
                "foreground uncertainty",
                "image segmention",
                "segmentation quality"
            ]
        },
        "id": 163,
        "cited_by": [
            {
                "year": "2017",
                "id": 518
            }
        ]
    },
    {
        "title": "Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling",
        "authors": [
            "Evgeny Levinkov",
            "Mario Fritz"
        ],
        "abstract": "Semantic road labeling is a key component of systems that aim at assisted or even autonomous driving. Considering that such systems continuously operate in the real-world, unforeseen conditions not represented in any conceivable training procedure are likely to occur on a regular basis. In order to equip systems with the ability to cope with such situations, we would like to enable adaptation to such new situations and conditions at runtime. Existing adaptive methods for image labeling either require labeled data from the new condition or even operate globally on a complete test set. None of this is a desirable mode of operation for a system as described above where new images arrive sequentially and conditions may vary. We study the effect of changing test conditions on scene labeling methods based on a new diverse street scene dataset. We propose a novel approach that can operate in such conditions and is based on a sequential Bayesian model update in order to robustly integrate the arriving images into the adapting procedure.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751274",
        "reference_list": [
            {
                "year": "2011",
                "id": 278
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 7,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Roads",
                "Labeling",
                "Training",
                "Adaptation models",
                "Data models",
                "Bayes methods",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "road vehicles"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sequential Bayesian model",
                "structured scene",
                "semantic road scenes labeling",
                "adaptive methods",
                "image labeling",
                "scene labeling methods",
                "diverse street scene dataset",
                "driving assistance systems"
            ]
        },
        "id": 164,
        "cited_by": [
            {
                "year": "2017",
                "id": 374
            }
        ]
    },
    {
        "title": "Learning Graph Matching: Oriented to Category Modeling from Cluttered Scenes",
        "authors": [
            "Quanshi Zhang",
            "Xuan Song",
            "Xiaowei Shao",
            "Huijing Zhao",
            "Ryosuke Shibasaki"
        ],
        "abstract": "Although graph matching is a fundamental problem in pattern recognition, and has drawn broad interest from many fields, the problem of learning graph matching has not received much attention. In this paper, we redefine the learning of graph matching as a model learning problem. In addition to conventional training of matching parameters, our approach modifies the graph structure and attributes to generate a graphical model. In this way, the model learning is oriented toward both matching and recognition performance, and can proceed in an unsupervised fashion. Experiments demonstrate that our approach outperforms conventional methods for learning graph matching.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751275",
        "reference_list": [
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 2,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Reliability",
                "Iron",
                "Object recognition",
                "Vectors",
                "Three-dimensional displays",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph matching",
                "category modeling",
                "cluttered scenes",
                "model learning problem",
                "graph structure",
                "unsupervised fashion",
                "pattern recognition",
                "computer vision"
            ],
            "Author Keywords": [
                "Learning Graph Matching",
                "Attributed Relational Graphs",
                "Model Learning"
            ]
        },
        "id": 165,
        "cited_by": []
    },
    {
        "title": "Robust Matrix Factorization with Unknown Noise",
        "authors": [
            "Deyu Meng",
            "Fernando De La Torre"
        ],
        "abstract": "Many problems in computer vision can be posed as recovering a low-dimensional subspace from high-dimensional visual data. Factorization approaches to low-rank subspace estimation minimize a loss function between the observed measurement matrix and a bilinear factorization. Most popular loss functions include the L 1 and L 2 losses. While L 1 is optimal for Laplacian distributed noise, L 2 is optimal for Gaussian noise. However, real data is often corrupted by an unknown noise distribution, which is unlikely to be purely Gaussian or Laplacian. To address this problem, this paper proposes a low-rank matrix factorization problem with a Mixture of Gaussians (MoG) noise. The MoG model is a universal approximator for any continuous distribution, and hence is able to model a wider range of real noise distributions. The parameters of the MoG model can be estimated with a maximum likelihood method, while the subspace is computed with standard approaches. We illustrate the benefits of our approach in extensive synthetic, structure from motion, face modeling and background subtraction experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751276",
        "reference_list": [
            {
                "year": "2011",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 45,
            "other": 12,
            "total": 57
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Robustness",
                "Principal component analysis",
                "Gaussian noise",
                "Computational modeling",
                "Laplace equations"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Gaussian noise",
                "matrix decomposition",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust matrix factorization",
                "unknown noise",
                "computer vision",
                "low-dimensional subspace",
                "high-dimensional visual data",
                "factorization approaches",
                "low-rank subspace estimation",
                "loss function",
                "bilinear factorization",
                "measurement matrix",
                "L2 losses",
                "L1 losses",
                "Gaussian noise",
                "Laplacian distributed noise",
                "noise distribution",
                "low-rank matrix factorization problem",
                "mixture of Gaussians",
                "noise model",
                "MoG model",
                "universal approximator",
                "maximum likelihood method",
                "face modeling",
                "background subtraction"
            ]
        },
        "id": 166,
        "cited_by": [
            {
                "year": "2017",
                "id": 181
            },
            {
                "year": "2017",
                "id": 265
            },
            {
                "year": "2017",
                "id": 527
            },
            {
                "year": "2015",
                "id": 166
            }
        ]
    },
    {
        "title": "Correlation Adaptive Subspace Segmentation by Trace Lasso",
        "authors": [
            "Canyi Lu",
            "Jiashi Feng",
            "Zhouchen Lin",
            "Shuicheng Yan"
        ],
        "abstract": "This paper studies the subspace segmentation problem. Given a set of data points drawn from a union of subspaces, the goal is to partition them into their underlying subspaces they were drawn from. The spectral clustering method is used as the framework. It requires to find an affinity matrix which is close to block diagonal, with nonzero entries corresponding to the data point pairs from the same subspace. In this work, we argue that both sparsity and the grouping effect are important for subspace segmentation. A sparse affinity matrix tends to be block diagonal, with less connections between data points from different subspaces. The grouping effect ensures that the highly corrected data which are usually from the same subspace can be grouped together. Sparse Subspace Clustering (SSC), by using \u2113 1 -minimization, encourages sparsity for data selection, but it lacks of the grouping effect. On the contrary, Low-Rank Representation (LRR), by rank minimization, and Least Squares Regression (LSR), by \u2113 2 -regularization, exhibit strong grouping effect, but they are short in subset selection. Thus the obtained affinity matrix is usually very sparse by SSC, yet very dense by LRR and LSR. In this work, we propose the Correlation Adaptive Subspace Segmentation (CASS) method by using trace Lasso. CASS is a data correlation dependent method which simultaneously performs automatic data selection and groups correlated data together. It can be regarded as a method which adaptively balances SSC and LSR. Both theoretical and experimental results show the effectiveness of CASS.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751277",
        "reference_list": [
            {
                "year": "2011",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 48,
            "other": 31,
            "total": 79
        },
        "keywords": {
            "IEEE Keywords": [
                "Sparse matrices",
                "Correlation",
                "Vectors",
                "Noise",
                "Silicon",
                "Least squares approximations",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "correlation methods",
                "image representation",
                "image segmentation",
                "least squares approximations",
                "minimisation",
                "pattern clustering",
                "regression analysis",
                "set theory",
                "sparse matrices"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subspace segmentation problem",
                "data points",
                "spectral clustering method",
                "block diagonal",
                "nonzero entries",
                "grouping effect",
                "sparse affinity matrix",
                "block diagonal",
                "sparse subspace clustering",
                "\u21131-minimization",
                "data selection",
                "low-rank representation",
                "rank minimization",
                "least square regression",
                "\u21132-regularization",
                "subset selection",
                "SSC",
                "LRR",
                "LSR",
                "correlation adaptive subspace segmentation method",
                "CASS method",
                "trace Lasso",
                "data correlation dependent method",
                "automatic data selection"
            ]
        },
        "id": 167,
        "cited_by": []
    },
    {
        "title": "Monte Carlo Tree Search for Scheduling Activity Recognition",
        "authors": [
            "Mohamed R. Amer",
            "Sinisa Todorovic",
            "Alan Fern",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper addresses recognition of human activities with stochastic structure, characterized by variable space-time arrangements of primitive actions, and conducted by a variable number of actors. Our approach classifies the activity of interest as well as identifies the relevant foreground in the video. Each activity representation is considered as a mixture distribution of BoWs captured by a Sum-Product Network (SPN). In our approach, SPN represents a linear mixture of many bags-of-words (BoWs) where each BoW represents an important foreground part of the activity. This mixture distribution is efficiently computed by organizing the BoWs in a hierarchy, where children BoWs are nested within parent BoWs. SPN allows us to model this mixture since it consists of terminal nodes representing BoWs, product nodes, and sum nodes organized in a number of layers. The products are aimed at encoding particular configurations of primitive actions, and the sums serve to capture their alternative configurations. SPN inference amounts to parsing the SPN graph, which yields the most probable explanation (MPE) of the video foreground. SPN inference has linear complexity in the number of nodes, under fairly general conditions, enabling fast and scalable recognition. The connectivity of SPN and the parameters of BoW distributions are learned under weak supervision using a variational EM algorithm. For our evaluation, we have compiled and annotated a new Volleyball dataset. Our classification accuracy and localization results are superior to those of the state of the art on current benchmarks as well as our Volleyball datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751278",
        "reference_list": [
            {
                "year": "2011",
                "id": 61
            },
            {
                "year": "2011",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 7,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Training",
                "Planning",
                "Grammar",
                "Context",
                "Switches",
                "Monte Carlo methods"
            ],
            "INSPEC: Controlled Indexing": [
                "Monte Carlo methods",
                "object recognition",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Monte Carlo tree search",
                "scheduling activity recognition",
                "human activity recognition",
                "variable space-time arrangements",
                "sum-product network",
                "SPN",
                "bags-of-words",
                "primitive actions",
                "SPN graph",
                "most probable explanation",
                "MPE",
                "video foreground",
                "BoW distributions",
                "variational EM algorithm",
                "Volleyball dataset"
            ],
            "Author Keywords": [
                "Activity Recogition",
                "Video Parsing",
                "Event Analysis",
                "And-Or Graphs",
                "Stochastic Grammars"
            ]
        },
        "id": 168,
        "cited_by": [
            {
                "year": "2015",
                "id": 498
            }
        ]
    },
    {
        "title": "Manipulation Pattern Discovery: A Nonparametric Bayesian Approach",
        "authors": [
            "Bingbing Ni",
            "Pierre Moulin"
        ],
        "abstract": "We aim to unsupervisedly discover human's action (motion) patterns of manipulating various objects in scenarios such as assisted living. We are motivated by two key observations. First, large variation exists in motion patterns associated with various types of objects being manipulated, thus manually defining motion primitives is infeasible. Second, some motion patterns are shared among different objects being manipulated while others are object specific. We therefore propose a nonparametric Bayesian method that adopts a hierarchical Dirichlet process prior to learn representative manipulation (motion) patterns in an unsupervised manner. Taking easy-to-obtain object detection score maps and dense motion trajectories as inputs, the proposed probabilistic model can discover motion pattern groups associated with different types of objects being manipulated with a shared manipulation pattern dictionary. The size of the learned dictionary is automatically inferred. Comprehensive experiments on two assisted living benchmarks and a cooking motion dataset demonstrate superiority of our learned manipulation pattern dictionary in representing manipulation actions for recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751279",
        "reference_list": [
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2009",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Dictionaries",
                "Object detection",
                "Feature extraction",
                "Bayes methods",
                "Probabilistic logic",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "feature extraction",
                "image motion analysis",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "manipulation pattern discovery",
                "nonparametric Bayesian approach",
                "motion primitives",
                "motion pattern discovery",
                "nonparametric Bayesian method",
                "hierarchical Dirichlet process",
                "representative manipulation patterns",
                "easy-to-obtain object detection score maps",
                "dense motion trajectories",
                "shared manipulation pattern dictionary"
            ]
        },
        "id": 169,
        "cited_by": []
    },
    {
        "title": "Perspective Motion Segmentation via Collaborative Clustering",
        "authors": [
            "Zhuwen Li",
            "Jiaming Guo",
            "Loong-Fah Cheong",
            "Steven Zhiying Zhou"
        ],
        "abstract": "This paper addresses real-world challenges in the motion segmentation problem, including perspective effects, missing data, and unknown number of motions. It first formulates the 3-D motion segmentation from two perspective views as a subspace clustering problem, utilizing the epipolar constraint of an image pair. It then combines the point correspondence information across multiple image frames via a collaborative clustering step, in which tight integration is achieved via a mixed norm optimization scheme. For model selection, we propose an over-segment and merge approach, where the merging step is based on the property of the ell_1-norm of the mutual sparse representation of two over-segmented groups. The resulting algorithm can deal with incomplete trajectories and perspective effects substantially better than state-of-the-art two-frame and multi-frame methods. Experiments on a 62-clip dataset show the significant superiority of the proposed idea in both segmentation accuracy and model selection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751280",
        "reference_list": [
            {
                "year": "2009",
                "id": 52
            },
            {
                "year": "2001",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 7,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Motion segmentation",
                "Computer vision",
                "Sparse matrices",
                "Vectors",
                "Optimization",
                "Clustering algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "optimisation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "perspective motion segmentation",
                "collaborative clustering",
                "3D motion segmentation",
                "subspace clustering problem",
                "epipolar constraint",
                "point correspondence information",
                "mixed norm optimization scheme",
                "over-segment approach",
                "merge approach"
            ]
        },
        "id": 170,
        "cited_by": [
            {
                "year": "2017",
                "id": 445
            },
            {
                "year": "2015",
                "id": 365
            }
        ]
    },
    {
        "title": "Piecewise Rigid Scene Flow",
        "authors": [
            "Christoph Vogel",
            "Konrad Schindler",
            "Stefan Roth"
        ],
        "abstract": "Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixel-to-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial super pixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751281",
        "reference_list": [
            {
                "year": "2007",
                "id": 160
            },
            {
                "year": "2011",
                "id": 163
            }
        ],
        "citation": {
            "ieee": 47,
            "other": 27,
            "total": 74
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Three-dimensional displays",
                "Image segmentation",
                "Optical imaging",
                "Estimation",
                "Shape",
                "Stereo vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "image sequences",
                "parameter estimation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "piecewise rigid scene flow",
                "dense 3D scene flow estimation",
                "stereo sequences",
                "2D optical flow estimation",
                "dynamic 3D scene",
                "planar collection",
                "local segments",
                "scene flow estimation",
                "pixel-to-segment assignment",
                "3D position",
                "rigid motion parameters",
                "occlusion-sensitive data",
                "segmentation regularizers",
                "initial superpixelization",
                "motion parameter estimation",
                "shape parameter estimation",
                "real-world image sets",
                "KITTI benchmark",
                "3D scene flow methods",
                "2D motion estimates",
                "optical flow techniques"
            ]
        },
        "id": 171,
        "cited_by": [
            {
                "year": "2017",
                "id": 32
            },
            {
                "year": "2017",
                "id": 271
            },
            {
                "year": "2015",
                "id": 390
            },
            {
                "year": "2015",
                "id": 493
            }
        ]
    },
    {
        "title": "DeepFlow: Large Displacement Optical Flow with Deep Matching",
        "authors": [
            "Philippe Weinzaepfel",
            "Jerome Revaud",
            "Zaid Harchaoui",
            "Cordelia Schmid"
        ],
        "abstract": "Optical flow computation is a key component in many computer vision systems designed for tasks such as action detection or activity recognition. However, despite several major advances over the last decade, handling large displacement in optical flow remains an open problem. Inspired by the large displacement optical flow of Brox and Malik, our approach, termed Deep Flow, blends a matching algorithm with a variational approach for optical flow. We propose a descriptor matching algorithm, tailored to the optical flow problem, that allows to boost performance on fast motions. The matching algorithm builds upon a multi-stage architecture with 6 layers, interleaving convolutions and max-pooling, a construction akin to deep convolutional nets. Using dense sampling, it allows to efficiently retrieve quasi-dense correspondences, and enjoys a built-in smoothing effect on descriptors matches, a valuable asset for integration into an energy minimization framework for optical flow estimation. Deep Flow efficiently handles large displacements occurring in realistic videos, and shows competitive performance on optical flow benchmarks. Furthermore, it sets a new state-of-the-art on the MPI-Sintel dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751282",
        "reference_list": [
            {
                "year": "2005",
                "id": 96
            },
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2013",
                "id": 214
            },
            {
                "year": "2009",
                "id": 206
            }
        ],
        "citation": {
            "ieee": 210,
            "other": 132,
            "total": 342
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical imaging",
                "Integrated optics",
                "Nonlinear optics",
                "Optical filters",
                "Adaptive optics",
                "Estimation",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convolution",
                "image matching",
                "image motion analysis",
                "image retrieval",
                "image sampling",
                "image sequences",
                "minimisation",
                "smoothing methods",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large displacement optical flow",
                "deep matching",
                "optical flow computation",
                "computer vision systems",
                "large displacement handling",
                "DeepFlow algorithm",
                "variational approach",
                "descriptor matching algorithm",
                "multistage architecture",
                "interleaving convolutions",
                "max-pooling",
                "deep convolutional nets",
                "dense sampling",
                "quasidense correspondence retrieval",
                "smoothing effect",
                "energy minimization framework",
                "optical flow estimation",
                "MPI-Sintel dataset"
            ],
            "Author Keywords": [
                "optical flow",
                "large displacements",
                "dense matching",
                "non-rigid matching",
                "deep convolutional networks"
            ]
        },
        "id": 172,
        "cited_by": [
            {
                "year": "2017",
                "id": 27
            },
            {
                "year": "2017",
                "id": 42
            },
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2017",
                "id": 98
            },
            {
                "year": "2017",
                "id": 164
            },
            {
                "year": "2017",
                "id": 193
            },
            {
                "year": "2017",
                "id": 210
            },
            {
                "year": "2017",
                "id": 343
            },
            {
                "year": "2017",
                "id": 380
            },
            {
                "year": "2017",
                "id": 427
            },
            {
                "year": "2017",
                "id": 428
            },
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2017",
                "id": 491
            },
            {
                "year": "2017",
                "id": 496
            },
            {
                "year": "2017",
                "id": 498
            },
            {
                "year": "2017",
                "id": 529
            },
            {
                "year": "2015",
                "id": 108
            },
            {
                "year": "2015",
                "id": 213
            },
            {
                "year": "2015",
                "id": 272
            },
            {
                "year": "2015",
                "id": 303
            },
            {
                "year": "2015",
                "id": 307
            },
            {
                "year": "2015",
                "id": 334
            },
            {
                "year": "2015",
                "id": 365
            },
            {
                "year": "2015",
                "id": 392
            },
            {
                "year": "2015",
                "id": 447
            },
            {
                "year": "2015",
                "id": 448
            },
            {
                "year": "2015",
                "id": 449
            },
            {
                "year": "2015",
                "id": 450
            },
            {
                "year": "2015",
                "id": 493
            }
        ]
    },
    {
        "title": "Shufflets: Shared Mid-level Parts for Fast Object Detection",
        "authors": [
            "Iasonas Kokkinos"
        ],
        "abstract": "We present a method to identify and exploit structures that are shared across different object categories, by using sparse coding to learn a shared basis for the 'part' and 'root' templates of Deformable Part Models (DPMs).Our first contribution consists in using Shift-Invariant Sparse Coding (SISC) to learn mid-level elements that can translate during coding. This results in systematically better approximations than those attained using standard sparse coding. To emphasize that the learned mid-level structures are shiftable we call them shufflets.Our second contribution consists in using the resulting score to construct probabilistic upper bounds to the exact template scores, instead of taking them 'at face value' as is common in current works. We integrate shufflets in Dual- Tree Branch-and-Bound and cascade-DPMs and demonstrate that we can achieve a substantial acceleration, with practically no loss in performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751283",
        "reference_list": [
            {
                "year": "2011",
                "id": 286
            },
            {
                "year": "2011",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 1,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Approximation methods",
                "Vectors",
                "Encoding",
                "Kernel",
                "Optimization",
                "Acceleration",
                "Dictionaries"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "object detection",
                "probability",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "substantial acceleration",
                "cascade-DPM",
                "dual-tree branch-and-bound DPM",
                "probabilistic upper bounds",
                "shufflets",
                "mid-level structures",
                "standard sparse coding",
                "mid-level element",
                "SISC",
                "shift-invariant sparse coding",
                "deformable part model",
                "root templates",
                "object categories",
                "fast object detection",
                "shared mid-level part templates"
            ]
        },
        "id": 173,
        "cited_by": []
    },
    {
        "title": "To Aggregate or Not to aggregate: Selective Match Kernels for Image Search",
        "authors": [
            "Giorgos Tolias",
            "Yannis Avrithis",
            "Herv\u00e9 J\u00e9gou"
        ],
        "abstract": "This paper considers a family of metrics to compare images based on their local descriptors. It encompasses the VLAD descriptor and matching techniques such as Hamming Embedding. Making the bridge between these approaches leads us to propose a match kernel that takes the best of existing techniques by combining an aggregation procedure with a selective match kernel. Finally, the representation underpinning this kernel is approximated, providing a large scale image search both precise and scalable, as shown by our experiments on several benchmarks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751284",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2011",
                "id": 209
            }
        ],
        "citation": {
            "ieee": 50,
            "other": 31,
            "total": 81
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Kernel",
                "Visualization",
                "Mathematical model",
                "Equations",
                "Manganese",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "selective match kernels",
                "local descriptors",
                "VLAD descriptor",
                "image matching techniques",
                "Hamming embedding",
                "aggregation procedure",
                "large scale image search"
            ],
            "Author Keywords": [
                "image retrieval",
                "descriptor aggregation",
                "VLAD",
                "Hamming Embedding"
            ]
        },
        "id": 174,
        "cited_by": [
            {
                "year": "2015",
                "id": 141
            },
            {
                "year": "2015",
                "id": 167
            },
            {
                "year": "2015",
                "id": 209
            },
            {
                "year": "2013",
                "id": 377
            }
        ]
    },
    {
        "title": "NEIL: Extracting Visual Knowledge from Web Data",
        "authors": [
            "Xinlei Chen",
            "Abhinav Shrivastava",
            "Abhinav Gupta"
        ],
        "abstract": "We propose NEIL (Never Ending Image Learner), a computer program that runs 24 hours per day and 7 days per week to automatically extract visual knowledge from Internet data. NEIL uses a semi-supervised learning algorithm that jointly discovers common sense relationships (e.g., \"Corolla is a kind of/looks similar to Car\", \"Wheel is a part of Car\") and labels instances of the given visual categories. It is an attempt to develop the world's largest visual structured knowledge base with minimum human labeling effort. As of 10th October 2013, NEIL has been continuously running for 2.5 months on 200 core cluster (more than 350K CPU hours) and has an ontology of 1152 object categories, 1034 scene categories and 87 attributes. During this period, NEIL has discovered more than 1700 relationships and has labeled more than 400K visual instances.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751285",
        "reference_list": [
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2007",
                "id": 145
            },
            {
                "year": "2007",
                "id": 259
            },
            {
                "year": "2005",
                "id": 174
            }
        ],
        "citation": {
            "ieee": 107,
            "other": 68,
            "total": 175
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Detectors",
                "Semantics",
                "Data mining",
                "Computers",
                "Knowledge based systems",
                "Semisupervised learning"
            ],
            "INSPEC: Controlled Indexing": [
                "Internet",
                "knowledge acquisition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "NEIL",
                "visual knowledge extraction",
                "Web data",
                "never ending image learner",
                "semi-supervised learning algorithm",
                "common sense relationships"
            ],
            "Author Keywords": [
                "never ending learning",
                "visual knowledge base",
                "common sense relationships",
                "macro vision",
                "object detection",
                "scene classification",
                "attributes",
                "semi-supervised learning"
            ]
        },
        "id": 175,
        "cited_by": [
            {
                "year": "2017",
                "id": 87
            },
            {
                "year": "2017",
                "id": 140
            },
            {
                "year": "2017",
                "id": 201
            },
            {
                "year": "2017",
                "id": 358
            },
            {
                "year": "2017",
                "id": 442
            },
            {
                "year": "2017",
                "id": 544
            },
            {
                "year": "2015",
                "id": 1
            },
            {
                "year": "2015",
                "id": 7
            },
            {
                "year": "2015",
                "id": 111
            },
            {
                "year": "2015",
                "id": 159
            },
            {
                "year": "2015",
                "id": 168
            },
            {
                "year": "2015",
                "id": 221
            },
            {
                "year": "2015",
                "id": 223
            },
            {
                "year": "2015",
                "id": 270
            },
            {
                "year": "2015",
                "id": 283
            },
            {
                "year": "2015",
                "id": 289
            },
            {
                "year": "2015",
                "id": 304
            },
            {
                "year": "2015",
                "id": 505
            },
            {
                "year": "2015",
                "id": 509
            },
            {
                "year": "2015",
                "id": 516
            },
            {
                "year": "2013",
                "id": 217
            }
        ]
    },
    {
        "title": "Holistic Scene Understanding for 3D Object Detection with RGBD Cameras",
        "authors": [
            "Dahua Lin",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "In this paper, we tackle the problem of indoor scene understanding using RGBD data. Towards this goal, we propose a holistic approach that exploits 2D segmentation, 3D geometry, as well as contextual relations between scenes and objects. Specifically, we extend the CPMC [3] framework to 3D in order to generate candidate cuboids, and develop a conditional random field to integrate information from different sources to classify the cuboids. With this formulation, scene classification and 3D object recognition are coupled and can be jointly solved through probabilistic inference. We test the effectiveness of our approach on the challenging NYU v2 dataset. The experimental results demonstrate that through effective evidence integration and holistic reasoning, our approach achieves substantial improvement over the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751286",
        "reference_list": [],
        "citation": {
            "ieee": 69,
            "other": 22,
            "total": 91
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Context modeling",
                "Training",
                "Object detection",
                "Geometry",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image segmentation",
                "object detection",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "holistic scene",
                "3D object detection",
                "RGBD cameras",
                "RGBD data",
                "2D segmentation",
                "3D geometry",
                "CPMC framework",
                "3D object recognition",
                "probabilistic inference",
                "NYU v2 dataset"
            ]
        },
        "id": 176,
        "cited_by": [
            {
                "year": "2017",
                "id": 125
            },
            {
                "year": "2017",
                "id": 197
            },
            {
                "year": "2017",
                "id": 284
            },
            {
                "year": "2017",
                "id": 485
            },
            {
                "year": "2017",
                "id": 546
            },
            {
                "year": "2015",
                "id": 86
            },
            {
                "year": "2015",
                "id": 132
            },
            {
                "year": "2015",
                "id": 240
            }
        ]
    },
    {
        "title": "3D Scene Understanding by Voxel-CRF",
        "authors": [
            "Byung-Soo Kim",
            "Pushmeet Kohli",
            "Silvio Savarese"
        ],
        "abstract": "Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of partial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1 and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751287",
        "reference_list": [
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2005",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 15,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Semantics",
                "Labeling",
                "Solid modeling",
                "Cameras",
                "Noise measurement",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "geometry",
                "image colour analysis",
                "image reconstruction",
                "inference mechanisms",
                "random processes",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D scene understanding",
                "computer vision",
                "depth-RGB cameras",
                "scene semantics",
                "object segmentation",
                "noisy depth measurements",
                "occlusions",
                "3D scene reconstruction",
                "Voxel-CRF model",
                "conditional random field",
                "3D geometric relationships",
                "voxel-based 3D reconstruction",
                "semantic labels",
                "inference strategy",
                "NYU depth dataset",
                "object removal",
                "RGB-D images"
            ],
            "Author Keywords": [
                "Scene understanding",
                "3D reconstruction",
                "RGB-D"
            ]
        },
        "id": 177,
        "cited_by": [
            {
                "year": "2017",
                "id": 284
            },
            {
                "year": "2017",
                "id": 324
            },
            {
                "year": "2015",
                "id": 240
            }
        ]
    },
    {
        "title": "Complex 3D General Object Reconstruction from Line Drawings",
        "authors": [
            "Linjie Yang",
            "Jianzhuang Liu",
            "Xiaoou Tang"
        ],
        "abstract": "An important topic in computer vision is 3D object reconstruction from line drawings. Previous algorithms either deal with simple general objects or are limited to only manifolds (a subset of solids). In this paper, we propose a novel approach to 3D reconstruction of complex general objects, including manifolds, non-manifold solids, and nonsolids. Through developing some 3D object properties, we use the degree of freedom of objects to decompose a complex line drawing into multiple simpler line drawings that represent meaningful building blocks of a complex object. After 3D objects are reconstructed from the decomposed line drawings, they are merged to form a complex object from their touching faces, edges, and vertices. Our experiments show a number of reconstruction examples from both complex line drawings and images with line drawings superimposed. Comparisons are also given to indicate that our algorithm can deal with much more complex line drawings of general objects than previous algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751288",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Solids",
                "Three-dimensional displays",
                "Manifolds",
                "Image reconstruction",
                "Computer vision",
                "Search problems",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "complex 3D general object reconstruction",
                "computer vision",
                "complex line drawing",
                "3D object properties"
            ]
        },
        "id": 178,
        "cited_by": []
    },
    {
        "title": "Subpixel Scanning Invariant to Indirect Lighting Using Quadratic Code Length",
        "authors": [
            "Nicolas Martin",
            "Vincent Couture",
            "S\u00e9bastien Roy"
        ],
        "abstract": "We present a scanning method that recovers dense sub pixel camera-projector correspondence without requiring any photometric calibration nor preliminary knowledge of their relative geometry. Sub pixel accuracy is achieved by considering several zero-crossings defined by the difference between pairs of unstructured patterns. We use gray-level band-pass white noise patterns that increase robustness to indirect lighting and scene discontinuities. Simulated and experimental results show that our method recovers scene geometry with high sub pixel precision, and that it can handle many challenges of active reconstruction systems. We compare our results to state of the art methods such as micro phase shifting and modulated phase shifting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751289",
        "reference_list": [
            {
                "year": "2011",
                "id": 87
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Lighting",
                "Image reconstruction",
                "Frequency modulation",
                "Robots",
                "Three-dimensional displays",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "lighting",
                "optical projectors",
                "white noise"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subpixel scanning invariant method",
                "quadratic code length",
                "indirect lighting",
                "dense subpixel camera-projector correspondence recovery",
                "zero-crossings",
                "unstructured patterns",
                "gray-level band-pass white noise patterns",
                "scene discontinuities",
                "scene geometry recovery",
                "active reconstruction systems",
                "microphase shifting method",
                "modulated phase shifting method"
            ],
            "Author Keywords": [
                "active light reconstruction",
                "structured light",
                "unstructured light"
            ]
        },
        "id": 179,
        "cited_by": []
    },
    {
        "title": "Semi-dense Visual Odometry for a Monocular Camera",
        "authors": [
            "Jakob Engel",
            "J\u00fcrgen Sturm",
            "Daniel Cremers"
        ],
        "abstract": "We propose a fundamentally novel approach to real-time visual odometry for a monocular camera. It allows to benefit from the simplicity and accuracy of dense tracking - which does not depend on visual features - while running in real-time on a CPU. The key idea is to continuously estimate a semi-dense inverse depth map for the current frame, which in turn is used to track the motion of the camera using dense image alignment. More specifically, we estimate the depth of all pixels which have a non-negligible image gradient. Each estimate is represented as a Gaussian probability distribution over the inverse depth. We propagate this information over time, and update it with new measurements as new images arrive. In terms of tracking accuracy and computational speed, the proposed method compares favorably to both state-of-the-art dense and feature-based visual odometry and SLAM algorithms. As our method runs in real-time on a CPU, it is of large practical value for robotics and augmented reality applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751290",
        "reference_list": [
            {
                "year": "2011",
                "id": 295
            }
        ],
        "citation": {
            "ieee": 137,
            "other": 60,
            "total": 197
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Visualization",
                "Accuracy",
                "Real-time systems",
                "Noise",
                "Simultaneous localization and mapping",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "Gaussian distribution",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semidense visual odometry",
                "monocular camera",
                "real-time visual odometry",
                "dense tracking accuracy",
                "CPU",
                "semidense inverse depth map estimation",
                "camera motion tracking",
                "dense image alignment",
                "pixel depth estimation",
                "image gradient",
                "Gaussian probability distribution",
                "computational speed",
                "feature-based visual odometry",
                "SLAM algorithm",
                "robotics application",
                "augmented reality application"
            ],
            "Author Keywords": [
                "SLAM",
                "visual odometry",
                "monocular",
                "dense",
                "stereo"
            ]
        },
        "id": 180,
        "cited_by": [
            {
                "year": "2017",
                "id": 411
            },
            {
                "year": "2017",
                "id": 492
            },
            {
                "year": "2015",
                "id": 78
            },
            {
                "year": "2015",
                "id": 246
            }
        ]
    },
    {
        "title": "Go-ICP: Solving 3D Registration Efficiently and Globally Optimally",
        "authors": [
            "Jiaolong Yang",
            "Hongdong Li",
            "Yunde Jia"
        ],
        "abstract": "Registration is a fundamental task in computer vision. The Iterative Closest Point (ICP) algorithm is one of the widely-used methods for solving the registration problem. Based on local iteration, ICP is however well-known to suffer from local minima. Its performance critically relies on the quality of initialization, and only local optimality is guaranteed. This paper provides the very first globally optimal solution to Euclidean registration of two 3D point sets or two 3D surfaces under the L2 error. Our method is built upon ICP, but combines it with a branch-and-bound (BnB) scheme which searches the 3D motion space SE(3) efficiently. By exploiting the special structure of the underlying geometry, we derive novel upper and lower bounds for the ICP error function. The integration of local ICP and global BnB enables the new method to run efficiently in practice, and its optimality is exactly guaranteed. We also discuss extensions, addressing the issue of outlier robustness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751291",
        "reference_list": [
            {
                "year": "2009",
                "id": 166
            },
            {
                "year": "2007",
                "id": 57
            },
            {
                "year": "2005",
                "id": 163
            },
            {
                "year": "2009",
                "id": 137
            },
            {
                "year": "2007",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 65,
            "other": 43,
            "total": 108
        },
        "keywords": {
            "IEEE Keywords": [
                "Iterative closest point algorithm",
                "Three-dimensional displays",
                "Uncertainty",
                "Upper bound",
                "Erbium",
                "Convergence",
                "Standards"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image registration",
                "iterative methods",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Go-ICP algorithm",
                "3D registration",
                "computer vision",
                "iterative closest point algorithm",
                "local minima",
                "local iteration",
                "Euclidean registration",
                "3D pointsets",
                "3D surfaces",
                "L2 error",
                "branch-and-bound scheme",
                "BnB scheme",
                "3D motion space",
                "lower bounds",
                "upper bounds",
                "ICP error function"
            ],
            "Author Keywords": [
                "ICP",
                "3D registration",
                "shape matching"
            ]
        },
        "id": 181,
        "cited_by": [
            {
                "year": "2015",
                "id": 228
            },
            {
                "year": "2015",
                "id": 235
            },
            {
                "year": "2015",
                "id": 479
            }
        ]
    },
    {
        "title": "Forward Motion Deblurring",
        "authors": [
            "Shicheng Zheng",
            "Li Xu",
            "Jiaya Jia"
        ],
        "abstract": "We handle a special type of motion blur considering that cameras move primarily forward or backward. Solving this type of blur is of unique practical importance since nearly all car, traffic and bike-mounted cameras follow out-of-plane translational motion. We start with the study of geometric models and analyze the difficulty of existing methods to deal with them. We also propose a solution accounting for depth variation. Homographies associated with different 3D planes are considered and solved for in an optimization framework. Our method is verified on several natural image examples that cannot be satisfyingly dealt with by previous methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751292",
        "reference_list": [
            {
                "year": "2011",
                "id": 58
            },
            {
                "year": "2007",
                "id": 83
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Kernel",
                "Three-dimensional displays",
                "Image restoration",
                "Deconvolution",
                "Optimization",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image restoration",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "forward motion deblurring",
                "bike-mounted cameras",
                "traffic cameras",
                "car cameras",
                "geometric models",
                "depth variation",
                "homographies",
                "depth variation",
                "3D planes",
                "optimization framework"
            ],
            "Author Keywords": [
                "Deblurring",
                "Homography",
                "Forward Motion",
                "Normal"
            ]
        },
        "id": 182,
        "cited_by": []
    },
    {
        "title": "Fibonacci Exposure Bracketing for High Dynamic Range Imaging",
        "authors": [
            "Mohit Gupta",
            "Daisuke Iso",
            "Shree K. Nayar"
        ],
        "abstract": "Exposure bracketing for high dynamic range (HDR) imaging involves capturing several images of the scene at different exposures. If either the camera or the scene moves during capture, the captured images must be registered. Large exposure differences between bracketed images lead to inaccurate registration, resulting in artifacts such as ghosting (multiple copies of scene objects) and blur. We present two techniques, one for image capture (Fibonacci exposure bracketing) and one for image registration (generalized registration), to prevent such motion-related artifacts. Fibonacci bracketing involves capturing a sequence of images such that each exposure time is the sum of the previous N(N > 1) exposures. Generalized registration involves estimating motion between sums of contiguous sets of frames, instead of between individual frames. Together, the two techniques ensure that motion is always estimated between frames of the same total exposure time. This results in HDR images and videos which have both a large dynamic range and minimal motion-related artifacts. We show, by results for several real-world indoor and outdoor scenes, that the proposed approach significantly outperforms several existing bracketing schemes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751293",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Dynamic range",
                "Sensors",
                "Noise",
                "Hardware",
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image capture",
                "image registration",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Fibonacci exposure bracketing",
                "high dynamic range imaging",
                "HDR imaging",
                "camera",
                "bracketed images",
                "ghosting",
                "blur",
                "artifacts",
                "image capture",
                "image registration",
                "motion-related artifacts",
                "image sequence",
                "generalized registration",
                "motion estimation",
                "HDR videos",
                "dynamic range",
                "motion-related artifacts",
                "real-world outdoor scenes",
                "real-world indoor scenes"
            ],
            "Author Keywords": [
                "high dynamic range imaging",
                "computational imaging",
                "computational cameras",
                "outdoor imaging",
                "low light imaging",
                "high performance imaging"
            ]
        },
        "id": 183,
        "cited_by": []
    },
    {
        "title": "Compensating for Motion during Direct-Global Separation",
        "authors": [
            "Supreeth Achar",
            "Stephen T. Nuske",
            "Srinivasa G. Narasimhan"
        ],
        "abstract": "Separating the direct and global components of radiance can aid shape recovery algorithms and can provide useful information about materials in a scene. Practical methods for finding the direct and global components use multiple images captured under varying illumination patterns and require the scene, light source and camera to remain stationary during the image acquisition process. In this paper, we develop a motion compensation method that relaxes this condition and allows direct-global separation to be performed on video sequences of dynamic scenes captured by moving projector-camera systems. Key to our method is being able to register frames in a video sequence to each other in the presence of time varying, high frequency active illumination patterns. We compare our motion compensated method to alternatives such as single shot separation and frame interleaving as well as ground truth. We present results on challenging video sequences that include various types of motions and deformations in scenes that contain complex materials like fabric, skin, leaves and wax.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751294",
        "reference_list": [
            {
                "year": "2011",
                "id": 240
            },
            {
                "year": "2011",
                "id": 87
            },
            {
                "year": "2009",
                "id": 206
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 7,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Cameras",
                "Optical imaging",
                "Adaptive optics",
                "Motion compensation",
                "Video sequences",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "brightness",
                "cameras",
                "image registration",
                "image sequences",
                "motion compensation",
                "natural scenes",
                "optical projectors",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion compensation",
                "direct-global radiance component separation",
                "shape recovery algorithms",
                "multiple image capture",
                "light source",
                "image acquisition process",
                "video sequences",
                "dynamic scenes",
                "moving projector-camera systems",
                "frame registration",
                "time varying high frequency active illumination patterns",
                "complex materials"
            ]
        },
        "id": 184,
        "cited_by": []
    },
    {
        "title": "Hybrid Deep Learning for Face Verification",
        "authors": [
            "Yi Sun",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification in wild conditions. A key contribution of this work is to directly learn relational visual features, which indicate identity similarities, from raw pixels of face pairs with a hybrid deep network. The deep ConvNets in our model mimic the primary visual cortex to jointly extract local relational visual features from two face images compared with the learned filter pairs. These relational features are further processed through multiple layers to extract high-level and global features. Multiple groups of ConvNets are constructed in order to achieve robustness and characterize face similarities from different aspects. The top-layer RBM performs inference from complementary high-level features extracted from different ConvNet groups with a two-level average pooling hierarchy. The entire hybrid deep network is jointly fine-tuned to optimize for the task of face verification. Our model achieves competitive face verification performance on the LFW dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751295",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 71,
            "other": 54,
            "total": 125
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Feature extraction",
                "Training",
                "Neurons",
                "Computational modeling",
                "Visualization",
                "Face recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "Boltzmann machines",
                "face recognition",
                "feature extraction",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hybrid deep learning",
                "hybrid convolutional network",
                "ConvNet-RBM model",
                "restricted Boltzmann machine",
                "RBM modelfor",
                "raw pixels",
                "hybrid deep network",
                "deep ConvNets",
                "visual cortex",
                "learned filter pairs",
                "global features",
                "ConvNet groups",
                "face verification performance",
                "LFW dataset",
                "face recognition"
            ],
            "Author Keywords": [
                "face verification",
                "deep learning",
                "relational visual features"
            ]
        },
        "id": 185,
        "cited_by": [
            {
                "year": "2015",
                "id": 337
            },
            {
                "year": "2015",
                "id": 405
            },
            {
                "year": "2013",
                "id": 14
            }
        ]
    },
    {
        "title": "Like Father, Like Son: Facial Expression Dynamics for Kinship Verification",
        "authors": [
            "Hamdi Dibeklioglu",
            "Albert Ali Salah",
            "Theo Gevers"
        ],
        "abstract": "Kinship verification from facial appearance is a difficult problem. This paper explores the possibility of employing facial expression dynamics in this problem. By using features that describe facial dynamics and spatio-temporal appearance over smile expressions, we show that it is possible to improve the state of the art in this problem, and verify that it is indeed possible to recognize kinship by resemblance of facial expressions. The proposed method is tested on different kin relationships. On the average, 72.89% verification accuracy is achieved on spontaneous smiles.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751296",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 13,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Feature extraction",
                "Videos",
                "Databases",
                "Vectors",
                "Support vector machines",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "spatiotemporal phenomena"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial expression dynamics",
                "kinship verification",
                "facial appearance",
                "spatio-temporal appearance",
                "smile expressions"
            ],
            "Author Keywords": [
                "Kinship verification",
                "Facial expression dynamics",
                "Smile dynamics"
            ]
        },
        "id": 186,
        "cited_by": []
    },
    {
        "title": "Handling Occlusions with Franken-Classifiers",
        "authors": [
            "Markus Mathias",
            "Rodrigo Benenson",
            "Radu Timofte",
            "Luc Van Gool"
        ],
        "abstract": "Detecting partially occluded pedestrians is challenging. A common practice to maximize detection quality is to train a set of occlusion-specific classifiers, each for a certain amount and type of occlusion. Since training classifiers is expensive, only a handful are typically trained. We show that by using many occlusion-specific classifiers, we outperform previous approaches on three pedestrian datasets, INRIA, ETH, and Caltech USA. We present a new approach to train such classifiers. By reusing computations among different training stages, 16 occlusion-specific classifiers can be trained at only one tenth the cost of one full training. We show that also test time cost grows sub-linearly.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751297",
        "reference_list": [
            {
                "year": "2005",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 42,
            "other": 16,
            "total": 58
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Detectors",
                "Feature extraction",
                "Materials",
                "Standards",
                "Buildings",
                "Decision trees"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object detection",
                "pedestrians",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "occlusion handling",
                "Franken-classifiers",
                "partially occluded pedestrian detection",
                "detection quality maximization",
                "occlusion-specific classifiers",
                "training classifiers",
                "ETH pedestrian datasets",
                "INRIA pedestrian datasets",
                "Caltech USA pedestrian datasets"
            ],
            "Author Keywords": [
                "object detection",
                "pedestrian detection",
                "occlusion"
            ]
        },
        "id": 187,
        "cited_by": [
            {
                "year": "2015",
                "id": 5
            },
            {
                "year": "2015",
                "id": 212
            },
            {
                "year": "2015",
                "id": 520
            }
        ]
    },
    {
        "title": "Robust Face Landmark Estimation under Occlusion",
        "authors": [
            "Xavier P. Burgos-Artizzu",
            "Pietro Perona",
            "Piotr Doll\u00e1r"
        ],
        "abstract": "Human faces captured in real-world conditions present large variations in shape and occlusions due to differences in pose, expression, use of accessories such as sunglasses and hats and interactions with objects (e.g. food). Current face landmark estimation approaches struggle under such conditions since they fail to provide a principled way of handling outliers. We propose a novel method, called Robust Cascaded Pose Regression (RCPR) which reduces exposure to outliers by detecting occlusions explicitly and using robust shape-indexed features. We show that RCPR improves on previous landmark estimation methods on three popular face datasets (LFPW, LFW and HELEN). We further explore RCPR's performance by introducing a novel face dataset focused on occlusion, composed of 1,007 faces presenting a wide range of occlusion patterns. RCPR reduces failure cases by half on all four datasets, at the same time as it detects face occlusions with a 80/40% precision/recall.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751298",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 192,
            "other": 99,
            "total": 291
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Robustness",
                "Estimation",
                "Training",
                "Benchmark testing",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "pose estimation",
                "regression analysis",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust face landmark estimation",
                "robust cascaded pose regression",
                "RCPR",
                "face datasets",
                "LFPW",
                "LFW",
                "HELEN",
                "occlusion patterns",
                "face occlusion detection",
                "shape-indexed features"
            ],
            "Author Keywords": [
                "Face shape",
                "Face recognition",
                "Landmark estimation",
                "Pose estimation"
            ]
        },
        "id": 188,
        "cited_by": [
            {
                "year": "2017",
                "id": 171
            },
            {
                "year": "2017",
                "id": 419
            },
            {
                "year": "2017",
                "id": 419
            },
            {
                "year": "2017",
                "id": 496
            },
            {
                "year": "2015",
                "id": 171
            },
            {
                "year": "2015",
                "id": 265
            },
            {
                "year": "2015",
                "id": 408
            },
            {
                "year": "2015",
                "id": 418
            },
            {
                "year": "2015",
                "id": 476
            },
            {
                "year": "2013",
                "id": 241
            }
        ]
    },
    {
        "title": "Fingerspelling Recognition with Semi-Markov Conditional Random Fields",
        "authors": [
            "Taehwan Kim",
            "Greg Shakhnarovich",
            "Karen Livescu"
        ],
        "abstract": "Recognition of gesture sequences is in general a very difficult problem, but in certain domains the difficulty may be mitigated by exploiting the domain's ``grammar''. One such grammatically constrained gesture sequence domain is sign language. In this paper we investigate the case of finger spelling recognition, which can be very challenging due to the quick, small motions of the fingers. Most prior work on this task has assumed a closed vocabulary of finger spelled words, here we study the more natural open-vocabulary case, where the only domain knowledge is the possible finger spelled letters and statistics of their sequences. We develop a semi-Markov conditional model approach, where feature functions are defined over segments of video and their corresponding letter labels. We use classifiers of letters and linguistic hand shape features, along with expected motion profiles, to define segmental feature functions. This approach improves letter error rate (Levenshtein distance between hypothesized and correct letter sequences) from 16.3% using a hidden Markov model baseline to 11.6% using the proposed semi-Markov model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751299",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 1,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Pragmatics",
                "Artificial neural networks",
                "Visualization",
                "Assistive technology",
                "Gesture recognition",
                "Motion segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "hidden Markov models",
                "image classification",
                "image motion analysis",
                "sign language recognition",
                "statistics",
                "video signal processing",
                "vocabulary"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fingerspelling recognition",
                "semiMarkov conditional random fields",
                "gesture sequence recognition",
                "domain grammar",
                "sign language",
                "finger spelled words closed vocabulary",
                "open-vocabulary case",
                "finger spelled letters",
                "sequence statistics",
                "video segment",
                "letter classifier",
                "linguistic hand shape features",
                "motion profiles",
                "segmental feature functions",
                "letter error rate",
                "hidden Markov model baseline"
            ]
        },
        "id": 189,
        "cited_by": []
    },
    {
        "title": "Efficient Salient Region Detection with Soft Image Abstraction",
        "authors": [
            "Ming-Ming Cheng",
            "Jonathan Warrell",
            "Wen-Yan Lin",
            "Shuai Zheng",
            "Vibhav Vineet",
            "Nigel Crook"
        ],
        "abstract": "Detecting visually salient regions in images is one of the fundamental problems in computer vision. We propose a novel method to decompose an image into large scale perceptually homogeneous elements for efficient salient region detection, using a soft image abstraction representation. By considering both appearance similarity and spatial distribution of image pixels, the proposed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection. We evaluate our salient region detection approach on the largest publicly available dataset with pixel accurate annotations. The experimental results show that the proposed method outperforms 18 alternate methods, reducing the mean absolute error by 25.2% compared to the previous best result, while being computationally more efficient.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751300",
        "reference_list": [
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2009",
                "id": 104
            },
            {
                "year": "2011",
                "id": 281
            },
            {
                "year": "2011",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 145,
            "other": 124,
            "total": 269
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Visualization",
                "Abstracts",
                "Correlation",
                "Histograms",
                "Estimation",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "object detection",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visually salient region detection",
                "computer vision",
                "image decomposition",
                "large scale perceptually homogeneous elements",
                "soft image abstraction representation",
                "appearance similarity",
                "spatial distribution",
                "image pixels",
                "image details",
                "saliency values",
                "pixel accurate annotations",
                "mean absolute error"
            ],
            "Author Keywords": [
                "salient object detection",
                "visual attention",
                "image abstraction",
                "object of interest segmentation"
            ]
        },
        "id": 190,
        "cited_by": [
            {
                "year": "2017",
                "id": 477
            },
            {
                "year": "2015",
                "id": 18
            },
            {
                "year": "2015",
                "id": 24
            },
            {
                "year": "2015",
                "id": 45
            }
        ]
    },
    {
        "title": "Cross-Field Joint Image Restoration via Scale Map",
        "authors": [
            "Qiong Yan",
            "Xiaoyong Shen",
            "Li Xu",
            "Shaojie Zhuo",
            "Xiaopeng Zhang",
            "Liang Shen",
            "Jiaya Jia"
        ],
        "abstract": "Color, infrared, and flash images captured in different fields can be employed to effectively eliminate noise and other visual artifacts. We propose a two-image restoration framework considering input images in different fields, for example, one noisy color image and one dark-flashed near infrared image. The major issue in such a framework is to handle structure divergence and find commonly usable edges and smooth transition for visually compelling image reconstruction. We introduce a scale map as a competent representation to explicitly model derivative-level confidence and propose new functions and a numerical solver to effectively infer it following new structural observations. Our method is general and shows a principled way for cross-field restoration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751301",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 9,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Noise measurement",
                "Smoothing methods",
                "Image edge detection",
                "Ash",
                "Noise",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cross-field joint image restoration",
                "scale map",
                "noisy color image",
                "dark-flashed near infrared image",
                "image reconstruction",
                "numerical solver",
                "cross-field restoration"
            ],
            "Author Keywords": [
                "image restoration",
                "denoise",
                "cross-field",
                "near-infrared"
            ]
        },
        "id": 191,
        "cited_by": [
            {
                "year": "2017",
                "id": 23
            },
            {
                "year": "2017",
                "id": 347
            },
            {
                "year": "2015",
                "id": 380
            }
        ]
    },
    {
        "title": "A Joint Intensity and Depth Co-sparse Analysis Model for Depth Map Super-resolution",
        "authors": [
            "Martin Kiechle",
            "Simon Hawe",
            "Martin Kleinsteuber"
        ],
        "abstract": "High-resolution depth maps can be inferred from low-resolution depth measurements and an additional high-resolution intensity image of the same scene. To that end, we introduce a bimodal co-sparse analysis model, which is able to capture the interdependency of registered intensity and depth information. This model is based on the assumption that the co-supports of corresponding bimodal image structures are aligned when computed by a suitable pair of analysis operators. No analytic form of such operators exist and we propose a method for learning them from a set of registered training signals. This learning process is done offline and returns a bimodal analysis operator that is universally applicable to natural scenes. We use this to exploit the bimodal co-sparse analysis model as a prior for solving inverse problems, which leads to an efficient algorithm for depth map super-resolution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751302",
        "reference_list": [
            {
                "year": "2011",
                "id": 205
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 11,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Analytical models",
                "Image resolution",
                "Image reconstruction",
                "Vectors",
                "Dictionaries",
                "Signal resolution",
                "Robot sensing systems"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "image resolution",
                "inverse problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "intensity analysis model",
                "depth map superresolution",
                "low-resolution depth measurement",
                "high-resolution image intensity",
                "bimodal depth cosparse analysis model",
                "bimodal image structures",
                "learning process",
                "inverse problems",
                "image registration"
            ]
        },
        "id": 192,
        "cited_by": []
    },
    {
        "title": "Detecting Irregular Curvilinear Structures in Gray Scale and Color Imagery Using Multi-directional Oriented Flux",
        "authors": [
            "Engin T\u00fcretken",
            "Carlos Becker",
            "Przemyslaw Glowacki",
            "Fethallah Benmansour",
            "Pascal Fua"
        ],
        "abstract": "We propose a new approach to detecting irregular curvilinear structures in noisy image stacks. In contrast to earlier approaches that rely on circular models of the cross-sections, ours allows for the arbitrarily-shaped ones that are prevalent in biological imagery. This is achieved by maximizing the image gradient flux along multiple directions and radii, instead of only two with a unique radius as is usually done. This yields a more complex optimization problem for which we propose a computationally efficient solution. We demonstrate the effectiveness of our approach on a wide range of challenging gray scale and color datasets and show that it outperforms existing techniques, especially on very irregular structures.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751303",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 5,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Eigenvalues and eigenfunctions",
                "Color",
                "Vectors",
                "Biomedical imaging",
                "Biomedical measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "object detection",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "irregular curvilinear structure detection",
                "gray scale imagery",
                "color imagery",
                "multidirectional oriented flux",
                "noisy image stacks",
                "biological imagery",
                "image gradient flux maximization",
                "radii",
                "complex optimization problem",
                "color datasets",
                "gray scale datasets"
            ],
            "Author Keywords": [
                "Curvilinear networks",
                "tubular structures",
                "curvilinear structures",
                "segmentation",
                "tubularity measure",
                "image gradient flux",
                "optimally oriented flux",
                "multi-directional oriented flux"
            ]
        },
        "id": 193,
        "cited_by": [
            {
                "year": "2015",
                "id": 35
            },
            {
                "year": "2015",
                "id": 71
            }
        ]
    },
    {
        "title": "STAR3D: Simultaneous Tracking and Reconstruction of 3D Objects Using RGB-D Data",
        "authors": [
            "Carl Yuheng Ren",
            "Victor Prisacariu",
            "David Murray",
            "Ian Reid"
        ],
        "abstract": "We introduce a probabilistic framework for simultaneous tracking and reconstruction of 3D rigid objects using an RGB-D camera. The tracking problem is handled using a bag-of-pixels representation and a back-projection scheme. Surface and background appearance models are learned online, leading to robust tracking in the presence of heavy occlusion and outliers. In both our tracking and reconstruction modules, the 3D object is implicitly embedded using a 3D level-set function. The framework is initialized with a simple shape primitive model (e.g. a sphere or a cube), and the real 3D object shape is tracked and reconstructed online. Unlike existing depth-based 3D reconstruction works, which either rely on calibrated/fixed camera set up or use the observed world map to track the depth camera, our framework can simultaneously track and reconstruct small moving objects. We use both qualitative and quantitative results to demonstrate the superior performance of both tracking and reconstruction of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751304",
        "reference_list": [
            {
                "year": "2011",
                "id": 247
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Image reconstruction",
                "Solid modeling",
                "Cameras",
                "Image color analysis",
                "Real-time systems"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "image representation",
                "object tracking",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "RGB-D camera data",
                "STAR3D",
                "probabilistic framework",
                "bag-of-pixels representation",
                "back-projection scheme",
                "background appearance models",
                "heavy occlusion",
                "outliers",
                "3D level-set function",
                "shape primitive model",
                "depth-based 3D reconstruction works",
                "depth camera",
                "simultaneous tracking and reconstruction of 3D objects"
            ],
            "Author Keywords": [
                "3D Tracking",
                "3D Reconstruction",
                "Generative model"
            ]
        },
        "id": 194,
        "cited_by": [
            {
                "year": "2017",
                "id": 13
            },
            {
                "year": "2015",
                "id": 81
            }
        ]
    },
    {
        "title": "Discriminant Tracking Using Tensor Representation with Semi-supervised Improvement",
        "authors": [
            "Jin Gao",
            "Junliang Xing",
            "Weiming Hu",
            "Steve Maybank"
        ],
        "abstract": "Visual tracking has witnessed growing methods in object representation, which is crucial to robust tracking. The dominant mechanism in object representation is using image features encoded in a vector as observations to perform tracking, without considering that an image is intrinsically a matrix, or a 2^nd-order tensor. Thus approaches following this mechanism inevitably lose a lot of useful information, and therefore cannot fully exploit the spatial correlations within the 2D image ensembles. In this paper, we address an image as a 2^nd-order tensor in its original form, and find a discriminative linear embedding space approximation to the original nonlinear sub manifold embedded in the tensor space based on the graph embedding framework. We specially design two graphs for characterizing the intrinsic local geometrical structure of the tensor space, so as to retain more discriminant information when reducing the dimension along certain tensor dimensions. However, spatial correlations within a tensor are not limited to the elements along these dimensions. This means that some part of the discriminant information may not be encoded in the embedding space. We introduce a novel technique called semi-supervised improvement to iteratively adjust the embedding space to compensate for the loss of discriminant information, hence improving the performance of our tracker. Experimental results on challenging videos demonstrate the effectiveness and robustness of the proposed tracker.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751305",
        "reference_list": [
            {
                "year": "2011",
                "id": 151
            },
            {
                "year": "2011",
                "id": 79
            },
            {
                "year": "2011",
                "id": 146
            },
            {
                "year": "2007",
                "id": 111
            },
            {
                "year": "2009",
                "id": 209
            },
            {
                "year": "2007",
                "id": 194
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Vectors",
                "Training",
                "Visualization",
                "Correlation",
                "Optimization",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "graph theory",
                "image representation",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "embedding space",
                "discriminant information",
                "graph embedding framework",
                "tensor space",
                "discriminative linear embedding space approximation",
                "geometrical structure",
                "space approximation",
                "spatial correlations",
                "image features",
                "object representation",
                "visual tracking",
                "semisupervised improvement",
                "tensor representation",
                "discriminant tracking"
            ]
        },
        "id": 195,
        "cited_by": []
    },
    {
        "title": "Coherent Motion Segmentation in Moving Camera Videos Using Optical Flow Orientations",
        "authors": [
            "Manjunath Narayana",
            "Allen Hanson",
            "Erik Learned-Miller"
        ],
        "abstract": "In moving camera videos, motion segmentation is commonly performed using the image plane motion of pixels, or optical flow. However, objects that are at different depths from the camera can exhibit different optical flows even if they share the same real-world motion. This can cause a depth-dependent segmentation of the scene. Our goal is to develop a segmentation algorithm that clusters pixels that have similar real-world motion irrespective of their depth in the scene. Our solution uses optical flow orientations instead of the complete vectors and exploits the well-known property that under camera translation, optical flow orientations are independent of object depth. We introduce a probabilistic model that automatically estimates the number of observed independent motions and results in a labeling that is consistent with real-world motion in the scene. The result of our system is that static objects are correctly identified as one segment, even if they are at different depths. Color features and information from previous frames in the video sequence are used to correct occasional errors due to the orientation-based segmentation. We present results on more than thirty videos from different benchmarks. The system is particularly robust on complex background scenes containing objects at significantly different depths.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751306",
        "reference_list": [
            {
                "year": "2009",
                "id": 196
            },
            {
                "year": "2003",
                "id": 8
            },
            {
                "year": "2011",
                "id": 276
            },
            {
                "year": "2011",
                "id": 253
            },
            {
                "year": "2009",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 19,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Optical imaging",
                "Cameras",
                "Videos",
                "Image color analysis",
                "Trajectory",
                "Adaptive optics"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image motion analysis",
                "image segmentation",
                "image sequences",
                "video cameras",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coherent motion segmentation",
                "moving camera videos",
                "optical flow orientation",
                "image plane pixel motion",
                "real-world motion",
                "depth-dependent scene segmentation",
                "pixel clustering",
                "probabilistic model",
                "observed independent motions",
                "static object identification",
                "color features",
                "video sequence",
                "occasional error correction",
                "orientation-based segmentation",
                "complex background scenes"
            ],
            "Author Keywords": [
                "Motion segmentation",
                "optical flow",
                "object detection",
                "tracking"
            ]
        },
        "id": 196,
        "cited_by": [
            {
                "year": "2017",
                "id": 539
            }
        ]
    },
    {
        "title": "Minimal Basis Facility Location for Subspace Segmentation",
        "authors": [
            "Choon-Meng Lee",
            "Loong-Fah Cheong"
        ],
        "abstract": "In contrast to the current motion segmentation paradigm that assumes independence between the motion subspaces, we approach the motion segmentation problem by seeking the parsimonious basis set that can represent the data. Our formulation explicitly looks for the overlap between subspaces in order to achieve a minimal basis representation. This parsimonious basis set is important for the performance of our model selection scheme because the sharing of basis results in savings of model complexity cost. We propose the use of affinity propagation based method to determine the number of motion. The key lies in the incorporation of a global cost model into the factor graph, serving the role of model complexity. The introduction of this global cost model requires additional message update in the factor graph. We derive an efficient update for the new messages associated with this global cost model. An important step in the use of affinity propagation is the subspace hypotheses generation. We use the row-sparse convex proxy solution as an initialization strategy. We further encourage the selection of subspace hypotheses with shared basis by integrating a discount scheme that lowers the factor graph facility cost based on shared basis. We verified the model selection and classification performance of our proposed method on both the original Hopkins 155 dataset and the more balanced Hopkins 380 dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751307",
        "reference_list": [
            {
                "year": "2007",
                "id": 14
            },
            {
                "year": "2009",
                "id": 86
            },
            {
                "year": "2009",
                "id": 105
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Computer vision",
                "Cost function",
                "Message passing",
                "Complexity theory",
                "Trajectory",
                "Sparse matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "graph theory",
                "image representation",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Hopkins 380 dataset",
                "Hopkins 155 dataset",
                "factor graph facility cost",
                "initialization strategy",
                "row-sparse convex proxy solution",
                "subspace hypotheses generation",
                "affinity propagation",
                "factor graph",
                "global cost model",
                "affinity propagation based method",
                "model complexity cost",
                "model selection scheme",
                "minimal basis representation",
                "motion segmentation problem",
                "subspace segmentation",
                "minimal basis facility location"
            ],
            "Author Keywords": [
                "model selection",
                "motion segmentation",
                "subspace segmentation",
                "facility location",
                "Hopkins 155",
                "minimal basis subspace representation",
                "joint sparsity"
            ]
        },
        "id": 197,
        "cited_by": []
    },
    {
        "title": "Non-convex P-Norm Projection for Robust Sparsity",
        "authors": [
            "Mithun Das Gupta",
            "Sanjeev Kumar"
        ],
        "abstract": "In this paper, we investigate the properties of L p norm (p \u22641) within a projection framework. We start with the KKT equations of the non-linear optimization problem and then use its key properties to arrive at an algorithm for L p norm projection on the non-negative simplex. We compare with L 1 projection which needs prior knowledge of the true norm, as well as hard thresholding based sparsification proposed in recent compressed sensing literature. We show performance improvements compared to these techniques across different vision applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751308",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Mathematical model",
                "Polynomials",
                "Explosions",
                "Optimization",
                "Transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "compressed sensing",
                "computer vision",
                "concave programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonconvex P-norm projection",
                "robust sparsity",
                "Lp norm properties",
                "projection framework",
                "KKT equation",
                "nonlinear optimization problem",
                "Lp norm projection",
                "nonnegative simplex",
                "L1 projection",
                "thresholding-based sparsification",
                "compressed sensing literature",
                "vision application"
            ]
        },
        "id": 198,
        "cited_by": []
    },
    {
        "title": "Log-Euclidean Kernels for Sparse Representation and Dictionary Learning",
        "authors": [
            "Peihua Li",
            "Qilong Wang",
            "Wangmeng Zuo",
            "Lei Zhang"
        ],
        "abstract": "The symmetric positive definite (SPD) matrices have been widely used in image and vision problems. Recently there are growing interests in studying sparse representation (SR) of SPD matrices, motivated by the great success of SR for vector data. Though the space of SPD matrices is well-known to form a Lie group that is a Riemannian manifold, existing work fails to take full advantage of its geometric structure. This paper attempts to tackle this problem by proposing a kernel based method for SR and dictionary learning (DL) of SPD matrices. We disclose that the space of SPD matrices, with the operations of logarithmic multiplication and scalar logarithmic multiplication defined in the Log-Euclidean framework, is a complete inner product space. We can thus develop a broad family of kernels that satisfies Mercer's condition. These kernels characterize the geodesic distance and can be computed efficiently. We also consider the geometric structure in the DL process by updating atom matrices in the Riemannian space instead of in the Euclidean space. The proposed method is evaluated with various vision problems and shows notable performance gains over state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751309",
        "reference_list": [],
        "citation": {
            "ieee": 29,
            "other": 26,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Sparse matrices",
                "Measurement",
                "Tin",
                "Dictionaries",
                "Matrix decomposition",
                "Symmetric matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "dictionaries",
                "differential geometry",
                "image representation",
                "learning (artificial intelligence)",
                "Lie groups",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Log-Euclidean kernels",
                "sparse representation",
                "dictionary learning",
                "symmetric positive definite matrices",
                "SPD matrices",
                "image problem",
                "vision problems",
                "SR",
                "vector data",
                "Lie group",
                "Riemannian manifold",
                "geometric structure",
                "DL",
                "scalar logarithmic multiplication",
                "inner product space",
                "Mercer condition",
                "geodesic distance",
                "atom matrices",
                "Riemannian space"
            ],
            "Author Keywords": [
                "Log-Euclidean Kernels",
                "Space of Symmetric Positive Definite (SPD) Matrices",
                "Sparse Representation",
                "Dictionary Learning"
            ]
        },
        "id": 199,
        "cited_by": [
            {
                "year": "2017",
                "id": 448
            }
        ]
    },
    {
        "title": "Large-Scale Image Annotation by Efficient and Robust Kernel Metric Learning",
        "authors": [
            "Zheyun Feng",
            "Rong Jin",
            "Anil Jain"
        ],
        "abstract": "One of the key challenges in search-based image annotation models is to define an appropriate similarity measure between images. Many kernel distance metric learning (KML) algorithms have been developed in order to capture the nonlinear relationships between visual features and semantics of the images. One fundamental limitation in applying KML to image annotation is that it requires converting image annotations into binary constraints, leading to a significant information loss. In addition, most KML algorithms suffer from high computational cost due to the requirement that the learned matrix has to be positive semi-definitive (PSD). In this paper, we propose a robust kernel metric learning (RKML) algorithm based on the regression technique that is able to directly utilize image annotations. The proposed method is also computationally more efficient because PSD property is automatically ensured by regression. We provide the theoretical guarantee for the proposed algorithm, and verify its efficiency and effectiveness for image annotation by comparing it to state-of-the-art approaches for both distance metric learning and image annotation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751310",
        "reference_list": [
            {
                "year": "2009",
                "id": 39
            },
            {
                "year": "2009",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 21,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Measurement",
                "Approximation methods",
                "Training",
                "Algorithm design and analysis",
                "Semantics",
                "Approximation algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large-scale image annotation",
                "kernel metric learning efficiency",
                "robust kernel metric learning",
                "search-based image annotation model",
                "similarity measure",
                "kernel distance metric learning algorithm",
                "visual features",
                "image semantics",
                "binary constraints",
                "information loss",
                "positive semidefinitive matrix",
                "PSD matrix",
                "RKML algorithm",
                "regression technique",
                "PSD property"
            ],
            "Author Keywords": [
                "Kernel Metric Learning",
                "Image Annotation",
                "Efficient",
                "Theoretical guarantee",
                "Regression"
            ]
        },
        "id": 200,
        "cited_by": []
    },
    {
        "title": "High Quality Shape from a Single RGB-D Image under Uncalibrated Natural Illumination",
        "authors": [
            "Yudeog Han",
            "Joon-Young Lee",
            "In So Kweon"
        ],
        "abstract": "We present a novel framework to estimate detailed shape of diffuse objects with uniform albedo from a single RGB-D image. To estimate accurate lighting in natural illumination environment, we introduce a general lighting model consisting of two components: global and local models. The global lighting model is estimated from the RGB-D input using the low-dimensional characteristic of a diffuse reflectance model. The local lighting model represents spatially varying illumination and it is estimated by using the smoothly-varying characteristic of illumination. With both the global and local lighting model, we can estimate complex lighting variations in uncontrolled natural illumination conditions accurately. For high quality shape capture, a shape-from-shading approach is applied with the estimated lighting model. Since the entire process is done with a single RGB-D input, our method is capable of capturing the high quality shape details of a dynamic object under natural illumination. Experimental results demonstrate the feasibility and effectiveness of our method that dramatically improves shape details of the rough depth input.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751311",
        "reference_list": [],
        "citation": {
            "ieee": 33,
            "other": 19,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Shape",
                "Estimation",
                "Geometry",
                "Calibration",
                "Image color analysis",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "lighting",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high-quality shape detail",
                "single-RGB-D image",
                "uncalibrated natural illumination",
                "detailed shape estimation",
                "diffuse objects",
                "uniform albedo",
                "lighting accuracy estimation",
                "general lighting model",
                "global lighting model",
                "low-dimensional characteristic",
                "diffuse reflectance model",
                "local lighting model",
                "spatially-varying illumination representation",
                "smoothly-varying illumination characteristic",
                "complex lighting variation estimation",
                "uncontrolled natural illumination condition",
                "high-quality shape capture",
                "shape-from-shading approach",
                "estimated lighting model",
                "rough depth input"
            ],
            "Author Keywords": [
                "shape-from-shading",
                "shape estimation",
                "kinect"
            ]
        },
        "id": 201,
        "cited_by": [
            {
                "year": "2017",
                "id": 251
            },
            {
                "year": "2017",
                "id": 328
            },
            {
                "year": "2017",
                "id": 330
            },
            {
                "year": "2015",
                "id": 376
            }
        ]
    },
    {
        "title": "SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels",
        "authors": [
            "Jianxiong Xiao",
            "Andrew Owens",
            "Antonio Torralba"
        ],
        "abstract": "Existing scene understanding datasets contain only a limited set of views of a place, and they lack representations of complete 3D spaces. In this paper, we introduce SUN3D, a large-scale RGB-D video database with camera pose and object labels, capturing the full 3D extent of many places. The tasks that go into constructing such a dataset are difficult in isolation -- hand-labeling videos is painstaking, and structure from motion (SfM) is unreliable for large spaces. But if we combine them together, we make the dataset construction task much easier. First, we introduce an intuitive labeling tool that uses a partial reconstruction to propagate labels from one frame to another. Then we use the object labels to fix errors in the reconstruction. For this, we introduce a generalization of bundle adjustment that incorporates object-to-object correspondences. This algorithm works by constraining points for the same object from different frames to lie inside a fixed-size bounding box, parameterized by its rotation and translation. The SUN3D database, the source code for the generalized bundle adjustment, and the web-based 3D annotation tool are all available at http://sun3d.cs.princeton.edu.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751312",
        "reference_list": [
            {
                "year": "2013",
                "id": 4
            },
            {
                "year": "2013",
                "id": 29
            },
            {
                "year": "2009",
                "id": 87
            },
            {
                "year": "2009",
                "id": 186
            }
        ],
        "citation": {
            "ieee": 102,
            "other": 58,
            "total": 160
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Databases",
                "Image reconstruction",
                "Labeling",
                "Semantics",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image colour analysis",
                "image reconstruction",
                "motion estimation",
                "video databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "big spaces",
                "SfM",
                "object labels",
                "scene understanding datasets",
                "3D space representation",
                "large-scale RGB-D video database",
                "camera pose",
                "hand-labeling videos",
                "structure from motion",
                "intuitive labeling tool",
                "partial image reconstruction",
                "object-to-object correspondences",
                "fixed-size bounding box",
                "SUN3D database",
                "source code",
                "generalized bundle adjustment",
                "Web-based 3D annotation tool"
            ]
        },
        "id": 202,
        "cited_by": [
            {
                "year": "2017",
                "id": 15
            },
            {
                "year": "2017",
                "id": 17
            },
            {
                "year": "2017",
                "id": 178
            },
            {
                "year": "2017",
                "id": 412
            },
            {
                "year": "2017",
                "id": 523
            },
            {
                "year": "2015",
                "id": 193
            },
            {
                "year": "2015",
                "id": 240
            },
            {
                "year": "2015",
                "id": 246
            },
            {
                "year": "2015",
                "id": 392
            },
            {
                "year": "2013",
                "id": 4
            },
            {
                "year": "2013",
                "id": 29
            }
        ]
    },
    {
        "title": "The Interestingness of Images",
        "authors": [
            "Michael Gygli",
            "Helmut Grabner",
            "Hayko Riemenschneider",
            "Fabian Nater",
            "Luc Van Gool"
        ],
        "abstract": "We investigate human interest in photos. Based on our own and others' psychophysical experiments, we identify various cues for \"interestingness\", namely aesthetics, unusualness and general preferences. For the ranking of retrieved images, interestingness shows to be more appropriate than cues proposed earlier. Interestingness is correlated with what people believe they will remember. This is opposed to actual memorability, which is uncorrelated to both. We introduce a set of features computationally capturing the three main aspects of visual interestingness and build an interestingness predictor from them. Its performance is shown on three datasets with varying context, reflecting the prior knowledge of the viewers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751313",
        "reference_list": [
            {
                "year": "2005",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 17,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Histograms",
                "Training",
                "Databases",
                "Image color analysis",
                "Psychology",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "image processing",
                "image retrieval",
                "photography"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image interestingness",
                "human interest",
                "photos",
                "psychophysical experiments",
                "aesthetics",
                "unusualness",
                "general preferences",
                "retrieved images ranking",
                "memorability",
                "visual interestingness",
                "interestingness predictor",
                "viewer knowledge",
                "content-based image retrieval"
            ],
            "Author Keywords": [
                "Image Classification",
                "Image Retrival",
                "Human Interest",
                "Interestingness"
            ]
        },
        "id": 203,
        "cited_by": []
    },
    {
        "title": "Hierarchical Part Matching for Fine-Grained Visual Categorization",
        "authors": [
            "Lingxi Xie",
            "Qi Tian",
            "Richang Hong",
            "Shuicheng Yan",
            "Bo Zhang"
        ],
        "abstract": "As a special topic in computer vision, fine-grained visual categorization (FGVC) has been attracting growing attention these years. Different with traditional image classification tasks in which objects have large inter-class variation, the visual concepts in the fine-grained datasets, such as hundreds of bird species, often have very similar semantics. Due to the large inter-class similarity, it is very difficult to classify the objects without locating really discriminative features, therefore it becomes more important for the algorithm to make full use of the part information in order to train a robust model. In this paper, we propose a powerful flowchart named Hierarchical Part Matching (HPM) to cope with fine-grained classification tasks. We extend the Bag-of-Features (BoF) model by introducing several novel modules to integrate into image representation, including foreground inference and segmentation, Hierarchical Structure Learning (HSL), and Geometric Phrase Pooling (GPP). We verify in experiments that our algorithm achieves the state-of-the-art classification accuracy in the Caltech-UCSD-Birds-200-2011 dataset by making full use of the ground-truth part annotations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751314",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 14,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Image segmentation",
                "Vectors",
                "Inference algorithms",
                "Birds",
                "Semantics",
                "Legged locomotion"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "image matching",
                "image segmentation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical part matching",
                "fine-grained visual categorization",
                "computer vision",
                "image classification tasks",
                "inter-class variation",
                "inter-class similarity",
                "fine-grained classification tasks",
                "image representation",
                "foreground inference",
                "image segmentation",
                "hierarchical structure learning",
                "geometric phrase pooling",
                "Caltech-UCSD-birds dataset"
            ],
            "Author Keywords": [
                "Fine-Grained Visual Categorization",
                "Hierarchical Part Matching",
                "Foreground Inference and Segmentation",
                "Hierarchical Structure Learning",
                "Geometric Phrase Pooling"
            ]
        },
        "id": 204,
        "cited_by": [
            {
                "year": "2015",
                "id": 11
            }
        ]
    },
    {
        "title": "Joint Optimization for Consistent Multiple Graph Matching",
        "authors": [
            "Junchi Yan",
            "Yu Tian",
            "Hongyuan Zha",
            "Xiaokang Yang",
            "Ya Zhang",
            "Stephen M. Chu"
        ],
        "abstract": "The problem of graph matching in general is NP-hard and approaches have been proposed for its sub optimal solution, most focusing on finding the one-to-one node mapping between two graphs. A more general and challenging problem arises when one aims to find consistent mappings across a number of graphs more than two. Conventional graph pair matching methods often result in mapping inconsistency since the mapping between two graphs can either be determined by pair mapping or by an additional anchor graph. To address this issue, a novel formulation is derived which is maximized via alternating optimization. Our method enjoys several advantages: 1) the mappings are jointly optimized rather than sequentially performed by applying pair matching, allowing the global affinity information across graphs can be propagated and explored, 2) the number of concerned variables to optimize is in linear with the number of graphs, being superior to local pair matching resulting in O(n 2 ) variables, 3) the mapping consistency constraints are analytically satisfied during optimization, and 4) off-the-shelf graph pair matching solvers can be reused under the proposed framework in an `out-of-the-box' fashion. Competitive results on both the synthesized data and the real data are reported, by varying the level of deformation, outliers and edge densities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751315",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2011",
                "id": 289
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 9,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Linear programming",
                "Educational institutions",
                "Convergence",
                "Algorithm design and analysis",
                "Hypercubes",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "graph theory",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint optimization",
                "graph matching problem",
                "NP-hard problem",
                "optimal solution",
                "one-to-one node graph mapping",
                "graph pair matching methods",
                "anchor graph",
                "alternating optimization",
                "global affinity information",
                "consistency constraints mapping",
                "deformation",
                "outliers",
                "edge densities"
            ]
        },
        "id": 205,
        "cited_by": [
            {
                "year": "2017",
                "id": 427
            },
            {
                "year": "2015",
                "id": 22
            },
            {
                "year": "2015",
                "id": 450
            }
        ]
    },
    {
        "title": "Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias",
        "authors": [
            "Chen Fang",
            "Ye Xu",
            "Daniel N. Rockmore"
        ],
        "abstract": "Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyper parameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validation performance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751316",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2011",
                "id": 338
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 3,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Training",
                "Unified modeling language",
                "Silicon",
                "Training data",
                "Coherence",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "image retrieval",
                "Internet",
                "learning (artificial intelligence)",
                "search engines",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unbiased metric learning approach",
                "multiple-dataset utilization",
                "softening bias",
                "standard computer vision dataset",
                "illumination condition",
                "imaging system",
                "dataset collector preference",
                "downstream effect",
                "generalizable technique construction",
                "classification system",
                "UML approach",
                "hyperparameter variation",
                "less-biased candidate distance metrics",
                "training example",
                "multiple-biased dataset",
                "neighborhood learning",
                "structural SVM",
                "model validation",
                "weakly-labeled Web image retrieval",
                "class label",
                "search engine",
                "noisy label",
                "cross-dataset image classification experiment"
            ],
            "Author Keywords": [
                "dataset bias",
                "metric learning",
                "domain generalization",
                "web images"
            ]
        },
        "id": 206,
        "cited_by": [
            {
                "year": "2017",
                "id": 581
            },
            {
                "year": "2017",
                "id": 599
            },
            {
                "year": "2015",
                "id": 284
            }
        ]
    },
    {
        "title": "Saliency Detection via Absorbing Markov Chain",
        "authors": [
            "Bowen Jiang",
            "Lihe Zhang",
            "Huchuan Lu",
            "Chuan Yang",
            "Ming-Hsuan Yang"
        ],
        "abstract": "In this paper, we formulate saliency detection via absorbing Markov chain on an image graph model. We jointly consider the appearance divergence and spatial distribution of salient objects and the background. The virtual boundary nodes are chosen as the absorbing nodes in a Markov chain and the absorbed time from each transient node to boundary absorbing nodes is computed. The absorbed time of transient node measures its global similarity with all absorbing nodes, and thus salient objects can be consistently separated from the background when the absorbed time is used as a metric. Since the time from transient node to absorbing nodes relies on the weights on the path and their spatial distance, the background region on the center of image may be salient. We further exploit the equilibrium distribution in an ergodic Markov chain to reduce the absorbed time in the long-range smooth background regions. Extensive experiments on four benchmark datasets demonstrate robustness and efficiency of the proposed method against the state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751317",
        "reference_list": [
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2007",
                "id": 12
            },
            {
                "year": "2011",
                "id": 29
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2011",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 164,
            "other": 110,
            "total": 274
        },
        "keywords": {
            "IEEE Keywords": [
                "Markov processes",
                "Transient analysis",
                "Absorption",
                "Vectors",
                "Image edge detection",
                "Indexes",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "Markov processes",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "saliency detection",
                "image graph model",
                "spatial distribution",
                "object appearance divergence",
                "virtual boundary nodes",
                "transient node",
                "boundary absorbing nodes",
                "equilibrium distribution",
                "ergodic Markov chain",
                "long-range smooth background region"
            ],
            "Author Keywords": [
                "saliency detection",
                "object detection",
                "absorbing Markov chain"
            ]
        },
        "id": 207,
        "cited_by": [
            {
                "year": "2017",
                "id": 109
            },
            {
                "year": "2017",
                "id": 423
            },
            {
                "year": "2015",
                "id": 24
            },
            {
                "year": "2015",
                "id": 45
            },
            {
                "year": "2015",
                "id": 156
            }
        ]
    },
    {
        "title": "Semantic-Aware Co-indexing for Image Retrieval",
        "authors": [
            "Shiliang Zhang",
            "Ming Yang",
            "Xiaoyu Wang",
            "Yuanqing Lin",
            "Qi Tian"
        ],
        "abstract": "Inverted indexes in image retrieval not only allow fast access to database images but also summarize all knowledge about the database, so that their discriminative capacity largely determines the retrieval performance. In this paper, for vocabulary tree based image retrieval, we propose a semantic-aware co-indexing algorithm to jointly embed two strong cues into the inverted indexes: 1) local invariant features that are robust to delineate low-level image contents, and 2) semantic attributes from large-scale object recognition that may reveal image semantic meanings. For an initial set of inverted indexes of local features, we utilize 1000 semantic attributes to filter out isolated images and insert semantically similar images to the initial set. Encoding these two distinct cues together effectively enhances the discriminative capability of inverted indexes. Such co-indexing operations are totally off-line and introduce small computation overhead to online query cause only local features but no semantic attributes are used for query. Experiments and comparisons with recent retrieval methods on 3 datasets, i.e., UKbench, Holidays, Oxford5K, and 1.3 million images from Flickr as distractors, manifest the competitive performance of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751318",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2011",
                "id": 26
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 22,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Vocabulary",
                "Image retrieval",
                "Feature extraction",
                "Indexing"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "object recognition",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image retrieval",
                "database images",
                "vocabulary tree",
                "semantic aware coindexing algorithm",
                "inverted indexes",
                "object recognition",
                "image semantic meanings",
                "online query"
            ],
            "Author Keywords": [
                "Image retrieval",
                "co-indexing",
                "vocabulary tree",
                "object recognition"
            ]
        },
        "id": 208,
        "cited_by": []
    },
    {
        "title": "Learning the Visual Interpretation of Sentences",
        "authors": [
            "C. Lawrence Zitnick",
            "Devi Parikh",
            "Lucy Vanderwende"
        ],
        "abstract": "Sentences that describe visual scenes contain a wide variety of information pertaining to the presence of objects, their attributes and their spatial relations. In this paper we learn the visual features that correspond to semantic phrases derived from sentences. Specifically, we extract predicate tuples that contain two nouns and a relation. The relation may take several forms, such as a verb, preposition, adjective or their combination. We model a scene using a Conditional Random Field (CRF) formulation where each node corresponds to an object, and the edges to their relations. We determine the potentials of the CRF using the tuples extracted from the sentences. We generate novel scenes depicting the sentences' visual meaning by sampling from the CRF. The CRF is also used to score a set of scenes for a text-based image retrieval task. Our results show we can generate (retrieve) scenes that convey the desired semantic meaning, even when scenes (queries) are described by multiple sentences. Significant improvement is found over several baseline approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751319",
        "reference_list": [
            {
                "year": "2011",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 16,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Semantics",
                "Art",
                "Computational modeling",
                "Abstracts",
                "Feature extraction",
                "Radio access networks"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "image retrieval",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sentence visual interpretation",
                "visual scenes",
                "spatial relations",
                "visual features",
                "semantic phrases",
                "conditional random field",
                "CRF",
                "text-based image retrieval task",
                "queries",
                "baseline approaches"
            ]
        },
        "id": 209,
        "cited_by": [
            {
                "year": "2015",
                "id": 0
            },
            {
                "year": "2015",
                "id": 270
            },
            {
                "year": "2015",
                "id": 283
            },
            {
                "year": "2015",
                "id": 292
            }
        ]
    },
    {
        "title": "A Novel Earth Mover's Distance Methodology for Image Matching with Gaussian Mixture Models",
        "authors": [
            "Peihua Li",
            "Qilong Wang",
            "Lei Zhang"
        ],
        "abstract": "The similarity or distance measure between Gaussian mixture models (GMMs) plays a crucial role in content-based image matching. Though the Earth Mover's Distance (EMD) has shown its advantages in matching histogram features, its potentials in matching GMMs remain unclear and are not fully explored. To address this problem, we propose a novel EMD methodology for GMM matching. We first present a sparse representation based EMD called SR-EMD by exploiting the sparse property of the underlying problem. SR-EMD is more efficient and robust than the conventional EMD. Second, we present two novel ground distances between component Gaussians based on the information geometry. The perspective from the Riemannian geometry distinguishes the proposed ground distances from the classical entropy-or divergence-based ones. Furthermore, motivated by the success of distance metric learning of vector data, we make the first attempt to learn the EMD distance metrics between GMMs by using a simple yet effective supervised pair-wise based method. It can adapt the distance metrics between GMMs to specific classification tasks. The proposed method is evaluated on both simulated data and benchmark real databases and achieves very promising performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751320",
        "reference_list": [
            {
                "year": "2011",
                "id": 222
            },
            {
                "year": "2003",
                "id": 64
            },
            {
                "year": "2009",
                "id": 58
            },
            {
                "year": "2001",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 8,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Vectors",
                "Covariance matrices",
                "Robustness",
                "Image retrieval",
                "Noise"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "geometry",
                "image matching",
                "singular value decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "novel Earth mover's distance methodology",
                "Gaussian mixture models",
                "content-based image matching",
                "supervised pair-wise based method",
                "Riemannian geometry",
                "information geometry",
                "novel ground distances",
                "SR-EMD",
                "sparse representation",
                "novel EMD methodology",
                "histogram features",
                "EMD"
            ],
            "Author Keywords": [
                "Sparse Representation-based EMD (SR-EMD)",
                "Gaussian Mixture Model (GMM)",
                "Metric Learning for GMMs"
            ]
        },
        "id": 210,
        "cited_by": []
    },
    {
        "title": "Hierarchical Joint Max-Margin Learning of Mid and Top Level Representations for Visual Recognition",
        "authors": [
            "Hans Lobel",
            "Ren\u00e9 Vidal",
            "Alvaro Soto"
        ],
        "abstract": "Currently, Bag-of-Visual-Words (BoVW) and part-based methods are the most popular approaches for visual recognition. In both cases, a mid-level representation is built on top of low-level image descriptors and top-level classifiers use this mid-level representation to achieve visual recognition. While in current part-based approaches, mid- and top-level representations are usually jointly trained, this is not the usual case for BoVW schemes. A main reason for this is the complex data association problem related to the usual large dictionary size needed by BoVW approaches. As a further observation, typical solutions based on BoVW and part-based representations are usually limited to extensions of binary classification schemes, a strategy that ignores relevant correlations among classes. In this work we propose a novel hierarchical approach to visual recognition based on a BoVW scheme that jointly learns suitable mid- and top-level representations. Furthermore, using a max-margin learning framework, the proposed approach directly handles the multiclass case at both levels of abstraction. We test our proposed method using several popular benchmark datasets. As our main result, we demonstrate that, by coupling learning of mid- and top-level representations, the proposed approach fosters sharing of discriminative visual words among target classes, being able to achieve state-of-the-art recognition performance using far less visual words than previous approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751321",
        "reference_list": [
            {
                "year": "2005",
                "id": 77
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Dictionaries",
                "Training",
                "Vectors",
                "Encoding",
                "Support vector machines",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image recognition",
                "image representation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "top-level representations",
                "binary classification schemes",
                "part-based representations",
                "large dictionary size",
                "complex data association problem",
                "BoVW schemes",
                "low-level image descriptors",
                "mid-level representation",
                "visual recognition",
                "bag-of-visual-words",
                "hierarchical joint max-margin learning framework"
            ]
        },
        "id": 211,
        "cited_by": []
    },
    {
        "title": "Query-Adaptive Asymmetrical Dissimilarities for Visual Object Retrieval",
        "authors": [
            "Cai-Zhi Zhu",
            "Herv\u00e9 J\u00e9gou",
            "Shin'Ichi Satoh"
        ],
        "abstract": "Visual object retrieval aims at retrieving, from a collection of images, all those in which a given query object appears. It is inherently asymmetric: the query object is mostly included in the database image, while the converse is not necessarily true. However, existing approaches mostly compare the images with symmetrical measures, without considering the different roles of query and database. This paper first measure the extent of asymmetry on large-scale public datasets reflecting this task. Considering the standard bag-of-words representation, we then propose new asymmetrical dissimilarities accounting for the different inlier ratios associated with query and database images. These asymmetrical measures depend on the query, yet they are compatible with an inverted file structure, without noticeably impacting search efficiency. Our experiments show the benefit of our approach, and show that the visual object retrieval task is better treated asymmetrically, in the spirit of state-of-the-art text retrieval.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751322",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 11,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Visualization",
                "Benchmark testing",
                "Equations",
                "Standards",
                "Search problems",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "search problems",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "query-adaptive asymmetrical dissimilarities",
                "image collection",
                "query object",
                "large-scale public datasets",
                "standard bag-of-word representation",
                "query images",
                "database images",
                "asymmetrical measures",
                "inverted file structure",
                "search efficiency",
                "visual object retrieval task",
                "text retrieval"
            ],
            "Author Keywords": [
                "visual object retrieval",
                "instance search",
                "asymmetrical dissimilarity",
                "distance metric"
            ]
        },
        "id": 212,
        "cited_by": [
            {
                "year": "2015",
                "id": 130
            },
            {
                "year": "2015",
                "id": 209
            }
        ]
    },
    {
        "title": "Fine-Grained Categorization by Alignments",
        "authors": [
            "E. Gavves",
            "B. Fernando",
            "C.G.M. Snoek",
            "A.W.M. Smeulders",
            "T. Tuytelaars"
        ],
        "abstract": "The aim of this paper is fine-grained categorization without human interaction. Different from prior work, which relies on detectors for specific object parts, we propose to localize distinctive details by roughly aligning the objects using just the overall shape, since implicit to fine-grained categorization is the existence of a super-class shape shared among all classes. The alignments are then used to transfer part annotations from training images to test images (supervised alignment), or to blindly yet consistently segment the object in a number of regions (unsupervised alignment). We furthermore argue that in the distinction of fine grained sub-categories, classification-oriented encodings like Fisher vectors are better suited for describing localized information than popular matching oriented features like HOG. We evaluate the method on the CU-2011 Birds and Stanford Dogs fine-grained datasets, outperforming the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751323",
        "reference_list": [
            {
                "year": "2011",
                "id": 232
            },
            {
                "year": "2011",
                "id": 328
            },
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 321
            }
        ],
        "citation": {
            "ieee": 71,
            "other": 29,
            "total": 100
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Shape",
                "Birds",
                "Training",
                "Feature extraction",
                "Image segmentation",
                "Encoding"
            ],
            "INSPEC: Controlled Indexing": [
                "encoding",
                "image classification",
                "image coding",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fine-grained categorization",
                "training images",
                "test images",
                "unsupervised alignment",
                "supervised alignment",
                "classification-oriented encodings",
                "Fisher vectors",
                "matching oriented features",
                "HOG",
                "CU-2011 birds fine-grained datasets",
                "Stanford dogs fine-grained datasets"
            ],
            "Author Keywords": [
                "fine-grained categorization"
            ]
        },
        "id": 213,
        "cited_by": [
            {
                "year": "2017",
                "id": 141
            },
            {
                "year": "2015",
                "id": 127
            },
            {
                "year": "2015",
                "id": 267
            },
            {
                "year": "2015",
                "id": 281
            }
        ]
    },
    {
        "title": "Locally Affine Sparse-to-Dense Matching for Motion and Occlusion Estimation",
        "authors": [
            "Marius Leordeanu",
            "Andrei Zanfir",
            "Cristian Sminchisescu"
        ],
        "abstract": "Estimating a dense correspondence field between successive video frames, under large displacement, is important in many visual learning and recognition tasks. We propose a novel sparse-to-dense matching method for motion field estimation and occlusion detection. As an alternative to the current coarse-to-fine approaches from the optical flow literature, we start from the higher level of sparse matching with rich appearance and geometric constraints collected over extended neighborhoods, using an occlusion aware, locally affine model. Then, we move towards the simpler, but denser classic flow field model, with an interpolation procedure that offers a natural transition between the sparse and the dense correspondence fields. We experimentally demonstrate that our appearance features and our complex geometric constraints permit the correct motion estimation even in difficult cases of large displacements and significant appearance changes. We also propose a novel classification method for occlusion detection that works in conjunction with the sparse-to-dense matching model. We validate our approach on the newly released Sintel dataset and obtain state-of-the-art results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751324",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2011",
                "id": 289
            },
            {
                "year": "2009",
                "id": 71
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 18,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Nickel",
                "Optimization",
                "Interpolation",
                "Adaptive optics",
                "Optical imaging",
                "Estimation",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image matching",
                "image sequences",
                "interpolation",
                "learning (artificial intelligence)",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local affine sparse-to-dense matching model",
                "occlusion estimation",
                "visual learning",
                "visual recognition task",
                "motion field estimation",
                "occlusion detection",
                "optical flow literature",
                "geometric constraints",
                "interpolation procedure",
                "natural transition",
                "complex geometric constraints",
                "novel classification method",
                "Sintel dataset"
            ],
            "Author Keywords": [
                "Optical Flow",
                "Motion Field Estimation",
                "Occlusion Detection",
                "Graph Matching",
                "Feature Matching"
            ]
        },
        "id": 214,
        "cited_by": [
            {
                "year": "2015",
                "id": 307
            },
            {
                "year": "2015",
                "id": 447
            },
            {
                "year": "2015",
                "id": 493
            },
            {
                "year": "2013",
                "id": 172
            }
        ]
    },
    {
        "title": "Predicting an Object Location Using a Global Image Representation",
        "authors": [
            "Jose A. Rodriguez Serrano",
            "Diane Larlus"
        ],
        "abstract": "We tackle the detection of prominent objects in images as a retrieval task: given a global image descriptor, we find the most similar images in an annotated dataset, and transfer the object bounding boxes. We refer to this approach as data driven detection (DDD), that is an alternative to sliding windows. Previous works have used similar notions but with task-independent similarities and representations, i.e. they were not tailored to the end-goal of localization. This article proposes two contributions: (i) a metric learning algorithm and (ii) a representation of images as object probability maps, that are both optimized for detection. We show experimentally that these two contributions are crucial to DDD, do not require costly additional operations, and in some cases yield comparable or better results than state-of-the-art detectors despite conceptual simplicity and increased speed. As an application of prominent object detection, we improve fine-grained categorization by precropping images with the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751325",
        "reference_list": [
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2009",
                "id": 287
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Training",
                "Feature extraction",
                "Image representation",
                "Databases",
                "Image segmentation",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image retrieval",
                "object detection",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object location prediction",
                "global image representation",
                "image retrieval",
                "object bounding boxes",
                "data driven detection",
                "metric learning algorithm",
                "object probability map",
                "object detection",
                "DDD",
                "fine-grained categorization",
                "precropping images"
            ],
            "Author Keywords": [
                "object detection",
                "metric learning",
                "Fisher vectors",
                "image retrieval",
                "fine-grained categorization"
            ]
        },
        "id": 215,
        "cited_by": []
    },
    {
        "title": "Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model",
        "authors": [
            "Xiao Cai",
            "Feiping Nie",
            "Weidong Cai",
            "Heng Huang"
        ],
        "abstract": "Automatic image categorization has become increasingly important with the development of Internet and the growth in the size of image databases. Although the image categorization can be formulated as a typical multi-class classification problem, two major challenges have been raised by the real-world images. On one hand, though using more labeled training data may improve the prediction performance, obtaining the image labels is a time consuming as well as biased process. On the other hand, more and more visual descriptors have been proposed to describe objects and scenes appearing in images and different features describe different aspects of the visual characteristics. Therefore, how to integrate heterogeneous visual features to do the semi-supervised learning is crucial for categorizing large-scale image data. In this paper, we propose a novel approach to integrate heterogeneous features by performing multi-modal semi-supervised classification on unlabeled as well as unsegmented images. Considering each type of feature as one modality, taking advantage of the large amount of unlabeled data information, our new adaptive multi-modal semi-supervised classification (AMMSS) algorithm learns a commonly shared class indicator matrix and the weights for different modalities (image features) simultaneously.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751326",
        "reference_list": [
            {
                "year": "2007",
                "id": 14
            },
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 8,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Semisupervised learning",
                "Feature extraction",
                "Laplace equations",
                "Training",
                "Optimization",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "heterogeneous image features integration",
                "multimodal semi-supervised learning model",
                "automatic image categorization",
                "Internet",
                "image databases",
                "multiclass classification problem",
                "prediction performance",
                "visual descriptors",
                "visual characteristics",
                "large-scale image data",
                "multimodal semisupervised classification",
                "unsegmented images",
                "adaptive multimodal semisupervised classification",
                "AMMSS algorithm",
                "shared class indicator matrix"
            ],
            "Author Keywords": [
                "Multi-Modal Feature Integration",
                "Heterogeneous Data Integration",
                "Semi-Supervised Learning"
            ]
        },
        "id": 216,
        "cited_by": []
    },
    {
        "title": "Building Part-Based Object Detectors via 3D Geometry",
        "authors": [
            "Abhinav Shrivastava",
            "Abhinav Gupta"
        ],
        "abstract": "This paper proposes a novel part-based representation for modeling object categories. Our representation combines the effectiveness of deformable part-based models with the richness of geometric representation by defining parts based on consistent underlying 3D geometry. Our key hypothesis is that while the appearance and the arrangement of parts might vary across the instances of object categories, the constituent parts will still have consistent underlying 3D geometry. We propose to learn this geometry-driven deformable part-based model (gDPM) from a set of labeled RGBD images. We also demonstrate how the geometric representation of gDPM can help us leverage depth data during training and constrain the latent model learning problem. But most importantly, a joint geometric and appearance based representation not only allows us to achieve state-of-the-art results on object detection but also allows us to tackle the grand challenge of understanding 3D objects from 2D images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751327",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 232
            },
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2013",
                "id": 423
            },
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2007",
                "id": 146
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Geometry",
                "Training",
                "Deformable models",
                "Dictionaries",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image representation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "building part-based object detectors",
                "3d geometry",
                "part-based representation",
                "object categories modeling",
                "deformable part-based models",
                "geometric representation",
                "object categories",
                "geometry-driven deformable part-based model",
                "labeled RGBD images",
                "gDPM geometric representation",
                "learning problem",
                "latent model",
                "appearance based representation",
                "object detection",
                "3D objects",
                "2D images"
            ],
            "Author Keywords": [
                "object detection",
                "3D object detection",
                "deformable part models",
                "GDPM",
                "DPM",
                "geometry-driven deformable part based model",
                "3D object understanding",
                "surface normal prediction",
                "3D primitives",
                "geometry models",
                "geometry based representations"
            ]
        },
        "id": 217,
        "cited_by": [
            {
                "year": "2015",
                "id": 278
            },
            {
                "year": "2015",
                "id": 490
            }
        ]
    },
    {
        "title": "Detecting Curved Symmetric Parts Using a Deformable Disc Model",
        "authors": [
            "Tom Sie Ho Lee",
            "Sanja Fidler",
            "Sven Dickinson"
        ],
        "abstract": "Symmetry is a powerful shape regularity that's been exploited by perceptual grouping researchers in both human and computer vision to recover part structure from an image without a priori knowledge of scene content. Drawing on the concept of a medial axis, defined as the locus of centers of maximal inscribed discs that sweep out a symmetric part, we model part recovery as the search for a sequence of deformable maximal inscribed disc hypotheses generated from a multiscale super pixel segmentation, a framework proposed by LEV09. However, we learn affinities between adjacent super pixels in a space that's invariant to bending and tapering along the symmetry axis, enabling us to capture a wider class of symmetric parts. Moreover, we introduce a global cost that perceptually integrates the hypothesis space by combining a pair wise and a higher-level smoothing term, which we minimize globally using dynamic programming. The new framework is demonstrated on two datasets, and is shown to significantly outperform the baseline LEV09.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751328",
        "reference_list": [
            {
                "year": "2009",
                "id": 278
            },
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 5,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Deformable models",
                "Image edge detection",
                "Computational modeling",
                "Clustering algorithms",
                "Feature extraction",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "dynamic programming",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curved symmetric part detection",
                "deformable disc model",
                "computer vision",
                "part structure recovery",
                "scene content",
                "maximal inscribed discs",
                "medial axis",
                "deformable maximal inscribed disc hypotheses",
                "multiscale superpixel segmentation",
                "adjacent superpixels",
                "symmetry axis",
                "higher-level smoothing term",
                "dynamic programming"
            ]
        },
        "id": 218,
        "cited_by": [
            {
                "year": "2017",
                "id": 285
            },
            {
                "year": "2015",
                "id": 183
            },
            {
                "year": "2015",
                "id": 187
            }
        ]
    },
    {
        "title": "Category-Independent Object-Level Saliency Detection",
        "authors": [
            "Yangqing Jia",
            "Mei Han"
        ],
        "abstract": "It is known that purely low-level saliency cues such as frequency does not lead to a good salient object detection result, requiring high-level knowledge to be adopted for successful discovery of task-independent salient objects. In this paper, we propose an efficient way to combine such high-level saliency priors and low-level appearance models. We obtain the high-level saliency prior with the objectness algorithm to find potential object candidates without the need of category information, and then enforce the consistency among the salient regions using a Gaussian MRF with the weights scaled by diverse density that emphasizes the influence of potential foreground pixels. Our model obtains saliency maps that assign high scores for the whole salient object, and achieves state-of-the-art performance on benchmark datasets covering various foreground statistics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751329",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2011",
                "id": 21
            },
            {
                "year": "2011",
                "id": 281
            },
            {
                "year": "2009",
                "id": 281
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 17,
            "total": 60
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Image color analysis",
                "Inference algorithms",
                "Bismuth",
                "Object detection",
                "Object recognition",
                "Markov processes"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "category-independent object-level saliency detection",
                "low-level saliency cues",
                "task-independent salient object",
                "low-level appearance model",
                "objectness algorithm",
                "Gaussian MRF"
            ],
            "Author Keywords": [
                "saliency detection",
                "objectness"
            ]
        },
        "id": 219,
        "cited_by": [
            {
                "year": "2017",
                "id": 594
            },
            {
                "year": "2015",
                "id": 45
            },
            {
                "year": "2015",
                "id": 66
            }
        ]
    },
    {
        "title": "GrabCut in One Cut",
        "authors": [
            "Meng Tang",
            "Lena Gorelick",
            "Olga Veksler",
            "Yuri Boykov"
        ],
        "abstract": "Among image segmentation algorithms there are two major groups: (a) methods assuming known appearance models and (b) methods estimating appearance models jointly with segmentation. Typically, the first group optimizes appearance log-likelihoods in combination with some spacial regularization. This problem is relatively simple and many methods guarantee globally optimal results. The second group treats model parameters as additional variables transforming simple segmentation energies into high-order NP-hard functionals (Zhu-Yuille, Chan-Vese, Grab Cut, etc). It is known that such methods indirectly minimize the appearance overlap between the segments. We propose a new energy term explicitly measuring L1 distance between the object and background appearance models that can be globally maximized in one graph cut. We show that in many applications our simple term makes NP-hard segmentation functionals unnecessary. Our one cut algorithm effectively replaces approximate iterative optimization techniques based on block coordinate descent.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751330",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2009",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 45,
            "other": 33,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Error analysis",
                "Image segmentation",
                "Approximation methods",
                "Histograms",
                "Color",
                "Entropy"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "appearance log-likelihood",
                "high-order NP-hard functional",
                "object appearance model",
                "background appearance model",
                "graph cut",
                "NP-hard segmentation",
                "one cut algorithm",
                "block coordinate descent"
            ],
            "Author Keywords": [
                "segmentation",
                "graph cut",
                "color separation",
                "MRF"
            ]
        },
        "id": 220,
        "cited_by": [
            {
                "year": "2015",
                "id": 181
            }
        ]
    },
    {},
    {
        "title": "Bayesian Robust Matrix Factorization for Image and Video Processing",
        "authors": [
            "Naiyan Wang",
            "Dit-Yan Yeung"
        ],
        "abstract": "Matrix factorization is a fundamental problem that is often encountered in many computer vision and machine learning tasks. In recent years, enhancing the robustness of matrix factorization methods has attracted much attention in the research community. To benefit from the strengths of full Bayesian treatment over point estimation, we propose here a full Bayesian approach to robust matrix factorization. For the generative process, the model parameters have conjugate priors and the likelihood (or noise model) takes the form of a Laplace mixture. For Bayesian inference, we devise an efficient sampling algorithm by exploiting a hierarchical view of the Laplace distribution. Besides the basic model, we also propose an extension which assumes that the outliers exhibit spatial or temporal proximity as encountered in many computer vision applications. The proposed methods give competitive experimental results when compared with several state-of-the-art methods on some benchmark image and video processing tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751332",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 10,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Bayes methods",
                "Markov processes",
                "Noise",
                "Computer vision",
                "Vectors",
                "Approximation methods"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "Laplace transforms",
                "matrix decomposition",
                "sampling methods",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian robust matrix factorization",
                "video processing",
                "image processing",
                "Laplace mixture",
                "Bayesian inference",
                "sampling algorithm",
                "Laplace distribution"
            ]
        },
        "id": 222,
        "cited_by": [
            {
                "year": "2017",
                "id": 538
            },
            {
                "year": "2015",
                "id": 166
            }
        ]
    },
    {
        "title": "Recursive Estimation of the Stein Center of SPD Matrices and Its Applications",
        "authors": [
            "Hesamoddin Salehian",
            "Guang Cheng",
            "Baba C. Vemuri",
            "Jeffrey Ho"
        ],
        "abstract": "Symmetric positive-definite (SPD) matrices are ubiquitous in Computer Vision, Machine Learning and Medical Image Analysis. Finding the center/average of a population of such matrices is a common theme in many algorithms such as clustering, segmentation, principal geodesic analysis, etc. The center of a population of such matrices can be defined using a variety of distance/divergence measures as the minimizer of the sum of squared distances/divergences from the unknown center to the members of the population. It is well known that the computation of the Karcher mean for the space of SPD matrices which is a negatively-curved Riemannian manifold is computationally expensive. Recently, the LogDet divergence-based center was shown to be a computationally attractive alternative. However, the LogDet-based mean of more than two matrices can not be computed in closed form, which makes it computationally less attractive for large populations. In this paper we present a novel recursive estimator for center based on the Stein distance - which is the square root of the LogDet divergence - that is significantly faster than the batch mode computation of this center. The key theoretical contribution is a closed-form solution for the weighted Stein center of two SPD matrices, which is used in the recursive computation of the Stein center for a population of SPD matrices. Additionally, we show experimental evidence of the convergence of our recursive Stein center estimator to the batch mode Stein center. We present applications of our recursive estimator to K-means clustering and image indexing depicting significant time gains over corresponding algorithms that use the batch mode computations. For the latter application, we develop novel hashing functions using the Stein distance and apply it to publicly available data sets, and experimental results have shown favorable comparisons to other competing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751333",
        "reference_list": [
            {
                "year": "2011",
                "id": 305
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Clustering algorithms",
                "Sociology",
                "Covariance matrices",
                "Matrix decomposition",
                "Symmetric matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "cryptography",
                "recursive estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "recursive estimation",
                "Stein center",
                "SPD matrices",
                "symmetric positive-definite matrices",
                "computer vision",
                "machine learning",
                "medical image analysis",
                "principal geodesic analysis",
                "segmentation",
                "clustering",
                "distance-divergence measures",
                "squared distances-divergences",
                "LogDet divergence-based center",
                "LogDet-based mean",
                "Stein distance",
                "closed-form solution",
                "batch mode Stein center",
                "Stein center estimator",
                "K-means clustering",
                "image indexing",
                "batch mode computations",
                "hashing functions"
            ]
        },
        "id": 223,
        "cited_by": [
            {
                "year": "2015",
                "id": 472
            }
        ]
    },
    {
        "title": "Correntropy Induced L2 Graph for Robust Subspace Clustering",
        "authors": [
            "Canyi Lu",
            "Jinhui Tang",
            "Min Lin",
            "Liang Lin",
            "Shuicheng Yan",
            "Zhouchen Lin"
        ],
        "abstract": "In this paper, we study the robust subspace clustering problem, which aims to cluster the given possibly noisy data points into their underlying subspaces. A large pool of previous subspace clustering methods focus on the graph construction by different regularization of the representation coefficient. We instead focus on the robustness of the model to non-Gaussian noises. We propose a new robust clustering method by using the correntropy induced metric, which is robust for handling the non-Gaussian and impulsive noises. Also we further extend the method for handling the data with outlier rows/features. The multiplicative form of half-quadratic optimization is used to optimize the non-convex correntropy objective function of the proposed models. Extensive experiments on face datasets well demonstrate that the proposed methods are more robust to corruptions and occlusions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751334",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 9,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Face",
                "Noise",
                "Clustering methods",
                "Educational institutions",
                "Computer integrated manufacturing",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "concave programming",
                "face recognition",
                "graph theory",
                "impulse noise",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "correntropy-induced L2 graph",
                "robust subspace clustering problem",
                "noisy data point cluster",
                "underlying subspace",
                "subspace clustering method",
                "graph construction",
                "representation coefficient regularization",
                "nonGaussian noise",
                "impulsive noise",
                "data handling",
                "outlier rows",
                "half-quadratic optimization",
                "nonconvex correntropy objective function optimization",
                "occlusions",
                "face clustering",
                "face images"
            ]
        },
        "id": 224,
        "cited_by": []
    },
    {
        "title": "Group Sparsity and Geometry Constrained Dictionary Learning for Action Recognition from Depth Maps",
        "authors": [
            "Jiajia Luo",
            "Wei Wang",
            "Hairong Qi"
        ],
        "abstract": "Human action recognition based on the depth information provided by commodity depth sensors is an important yet challenging task. The noisy depth maps, different lengths of action sequences, and free styles in performing actions, may cause large intra-class variations. In this paper, a new framework based on sparse coding and temporal pyramid matching (TPM) is proposed for depth-based human action recognition. Especially, a discriminative class-specific dictionary learning algorithm is proposed for sparse coding. By adding the group sparsity and geometry constraints, features can be well reconstructed by the sub-dictionary belonging to the same class, and the geometry relationships among features are also kept in the calculated coefficients. The proposed approach is evaluated on two benchmark datasets captured by depth cameras. Experimental results show that the proposed algorithm repeatedly achieves superior performance to the state of the art algorithms. Moreover, the proposed dictionary learning method also outperforms classic dictionary learning approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751335",
        "reference_list": [
            {
                "year": "2011",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 64,
            "other": 37,
            "total": 101
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Joints",
                "Encoding",
                "Three-dimensional displays",
                "Geometry",
                "Feature extraction",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image coding",
                "image matching",
                "image reconstruction",
                "image sequences",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "group sparsity",
                "geometry constrained dictionary learning approach",
                "depth information",
                "commodity depth sensors",
                "noisy depth maps",
                "action sequence length",
                "intra-class variations",
                "sparse coding",
                "temporal pyramid matching",
                "depth-based human action recognition",
                "discriminative class-specific dictionary learning algorithm",
                "benchmark datasets",
                "depth cameras"
            ]
        },
        "id": 225,
        "cited_by": [
            {
                "year": "2017",
                "id": 105
            }
        ]
    },
    {
        "title": "Action and Event Recognition with Fisher Vectors on a Compact Feature Set",
        "authors": [
            "Dan Oneata",
            "Jakob Verbeek",
            "Cordelia Schmid"
        ],
        "abstract": "Action recognition in uncontrolled video is an important and challenging computer vision problem. Recent progress in this area is due to new local features and models that capture spatio-temporal structure between local features, or human-object interactions. Instead of working towards more complex models, we focus on the low-level features and their encoding. We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that for basic action recognition and localization MBH features alone are enough for state-of-the-art performance. For complex events we find that SIFT and MFCC features provide complementary cues. On all three problems we obtain state-of-the-art results, while using fewer features and less complex models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751336",
        "reference_list": [
            {
                "year": "2009",
                "id": 191
            },
            {
                "year": "2011",
                "id": 188
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 104,
            "other": 82,
            "total": 186
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Vectors",
                "Encoding",
                "Hidden Markov models",
                "Histograms",
                "Motion pictures",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image classification",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action recognition",
                "compact feature set",
                "uncontrolled video",
                "computer vision problem",
                "low-level features",
                "human-object interactions",
                "spatio-temporal structure",
                "local features",
                "Fisher vectors",
                "bag-of-word histograms",
                "linear classifiers",
                "feature-length movies",
                "complex event recognition",
                "MBH feature localization",
                "SIFT features",
                "complementary cues"
            ],
            "Author Keywords": [
                "action recognition",
                "action localization",
                "event recognition",
                "Fisher vectors",
                "bag of visual words",
                "evaluation",
                "uncontrolled realistic videos",
                "dense trajectories"
            ]
        },
        "id": 226,
        "cited_by": [
            {
                "year": "2017",
                "id": 307
            },
            {
                "year": "2017",
                "id": 466
            },
            {
                "year": "2015",
                "id": 359
            },
            {
                "year": "2015",
                "id": 499
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2013",
                "id": 443
            }
        ]
    },
    {
        "title": "Stable Hyper-pooling and Query Expansion for Event Detection",
        "authors": [
            "Matthijs Douze",
            "J\u00e9r\u00f4me Revaud",
            "Cordelia Schmid",
            "Herv\u00e9 J\u00e9gou"
        ],
        "abstract": "This paper makes two complementary contributions to event retrieval in large collections of videos. First, we propose hyper-pooling strategies that encode the frame descriptors into a representation of the video sequence in a stable manner. Our best choices compare favorably with regular pooling techniques based on k-means quantization. Second, we introduce a technique to improve the ranking. It can be interpreted either as a query expansion method or as a similarity adaptation based on the local context of the query video descriptor. Experiments on public benchmarks show that our methods are complementary and improve event retrieval results, without sacrificing efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751337",
        "reference_list": [
            {
                "year": "2011",
                "id": 337
            },
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2007",
                "id": 198
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Vectors",
                "Indexes",
                "Stability criteria",
                "Entropy",
                "Principal component analysis",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image sequences",
                "quantisation (signal)",
                "video coding",
                "video retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "frame descriptor encoding",
                "query video descriptor",
                "k-means quantization",
                "video sequence representation",
                "event retrieval",
                "event detection",
                "query expansion",
                "stable hyper-pooling"
            ]
        },
        "id": 227,
        "cited_by": []
    },
    {
        "title": "Active Learning of an Action Detector from Untrimmed Videos",
        "authors": [
            "Sunil Bandla",
            "Kristen Grauman"
        ],
        "abstract": "Collecting and annotating videos of realistic human actions is tedious, yet critical for training action recognition systems. We propose a method to actively request the most useful video annotations among a large set of unlabeled videos. Predicting the utility of annotating unlabeled video is not trivial, since any given clip may contain multiple actions of interest, and it need not be trimmed to temporal regions of interest. To deal with this problem, we propose a detection-based active learner to train action category models. We develop a voting-based framework to localize likely intervals of interest in an unlabeled clip, and use them to estimate the total reduction in uncertainty that annotating that clip would yield. On three datasets, we show our approach can learn accurate action detectors more efficiently than alternative active learning strategies that fail to accommodate the \"untrimmed\" nature of real video data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751338",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2011",
                "id": 177
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2009",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Detectors",
                "Training",
                "Entropy",
                "Uncertainty",
                "Three-dimensional displays",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active learning strategy",
                "action detector",
                "untrimmed videos",
                "video collection",
                "human actions",
                "action recognition systems",
                "unlabeled video annotation",
                "temporal regions",
                "detection-based active learner",
                "action category model",
                "voting-based framework",
                "interval-of-interest localization",
                "unlabeled clip"
            ],
            "Author Keywords": [
                "active learning",
                "action detection",
                "action localization",
                "hough",
                "entropy",
                "voting-based",
                "vatic",
                "human annotation",
                "hollywood"
            ]
        },
        "id": 228,
        "cited_by": [
            {
                "year": "2017",
                "id": 30
            }
        ]
    },
    {
        "title": "Structured Forests for Fast Edge Detection",
        "authors": [
            "Piotr Doll\u00e1r",
            "C. Lawrence Zitnick"
        ],
        "abstract": "Edge detection is a critical component of many vision systems, including object detectors and image segmentation algorithms. Patches of edges exhibit well-known forms of local structure, such as straight lines or T-junctions. In this paper we take advantage of the structure present in local image patches to learn both an accurate and computationally efficient edge detector. We formulate the problem of predicting local edge masks in a structured learning framework applied to random decision forests. Our novel approach to learning decision trees robustly maps the structured labels to a discrete space on which standard information gain measures may be evaluated. The result is an approach that obtains real time performance that is orders of magnitude faster than many competing state-of-the-art approaches, while also achieving state-of-the-art edge detection results on the BSDS500 Segmentation dataset and NYU Depth dataset. Finally, we show the potential of our approach as a general purpose edge detector by showing our learned edge models generalize well across datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751339",
        "reference_list": [
            {
                "year": "2009",
                "id": 306
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 256,
            "other": 124,
            "total": 380
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Vegetation",
                "Training",
                "Detectors",
                "Image segmentation",
                "Decision trees",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "edge detection",
                "image segmentation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "NYU depth dataset",
                "BSDS500 segmentation dataset",
                "learning decision trees",
                "structured learning framework",
                "local edge masks prediction",
                "local image patches",
                "T-junctions",
                "edge patches",
                "image segmentation algorithms",
                "vision systems",
                "fast edge detection",
                "structured forests"
            ],
            "Author Keywords": [
                "edge detection",
                "structure learning",
                "realtime vision"
            ]
        },
        "id": 229,
        "cited_by": [
            {
                "year": "2017",
                "id": 137
            },
            {
                "year": "2017",
                "id": 317
            },
            {
                "year": "2017",
                "id": 427
            },
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2015",
                "id": 31
            },
            {
                "year": "2015",
                "id": 54
            },
            {
                "year": "2015",
                "id": 64
            },
            {
                "year": "2015",
                "id": 104
            },
            {
                "year": "2015",
                "id": 128
            },
            {
                "year": "2015",
                "id": 172
            },
            {
                "year": "2015",
                "id": 178
            },
            {
                "year": "2015",
                "id": 180
            },
            {
                "year": "2015",
                "id": 188
            },
            {
                "year": "2015",
                "id": 195
            },
            {
                "year": "2015",
                "id": 225
            },
            {
                "year": "2015",
                "id": 287
            },
            {
                "year": "2015",
                "id": 300
            },
            {
                "year": "2015",
                "id": 345
            },
            {
                "year": "2015",
                "id": 375
            },
            {
                "year": "2015",
                "id": 450
            }
        ]
    },
    {
        "title": "Weakly Supervised Learning of Image Partitioning Using Decision Trees with Structured Split Criteria",
        "authors": [
            "Christoph Straehle",
            "Ullrich Koethe",
            "Fred A. Hamprecht"
        ],
        "abstract": "We propose a scheme that allows to partition an image into a previously unknown number of segments, using only minimal supervision in terms of a few must-link and cannot-link annotations. We make no use of regional data terms, learning instead what constitutes a likely boundary between segments. Since boundaries are only implicitly specified through cannot-link constraints, this is a hard and nonconvex latent variable problem. We address this problem in a greedy fashion using a randomized decision tree on features associated with interpixel edges. We use a it structured purity criterion during tree construction and also show how a backtracking strategy can be used to prevent the greedy search from ending up in poor local optima. The proposed strategy is compared with prior art on natural images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751340",
        "reference_list": [
            {
                "year": "2011",
                "id": 332
            },
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2009",
                "id": 93
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2011",
                "id": 211
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 4,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Decision trees",
                "Image edge detection",
                "Linear programming",
                "Training",
                "Labeling",
                "Prediction algorithms",
                "Silicon"
            ],
            "INSPEC: Controlled Indexing": [
                "backtracking",
                "concave programming",
                "decision trees",
                "image segmentation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image partitioning",
                "structured split criteria",
                "image segmentation",
                "nonconvex latent variable problem",
                "randomized decision tree",
                "structured purity criterion",
                "tree construction",
                "backtracking strategy",
                "supervised learning"
            ],
            "Author Keywords": [
                "segmentation",
                "edge model",
                "supervised learning",
                "region annotation",
                "decision tree",
                "structured objective function"
            ]
        },
        "id": 230,
        "cited_by": []
    },
    {
        "title": "Style-Aware Mid-level Representation for Discovering Visual Connections in Space and Time",
        "authors": [
            "Yong Jae Lee",
            "Alexei A. Efros",
            "Martial Hebert"
        ],
        "abstract": "We present a weakly-supervised visual data mining approach that discovers connections between recurring mid-level visual elements in historic (temporal) and geographic (spatial) image collections, and attempts to capture the underlying visual style. In contrast to existing discovery methods that mine for patterns that remain visually consistent throughout the dataset, our goal is to discover visual elements whose appearance changes due to change in time or location; i.e., exhibit consistent stylistic variations across the label space (date or geo-location). To discover these elements, we first identify groups of patches that are style-sensitive. We then incrementally build correspondences to find the same element across the entire dataset. Finally, we train style-aware regressors that model each element's range of stylistic differences. We apply our approach to date and geo-location prediction and show substantial improvement over several baselines that do not model visual style. We also demonstrate the method's effectiveness on the related task of fine-grained classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751341",
        "reference_list": [
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 21,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Detectors",
                "Training",
                "Databases",
                "Data mining",
                "Global Positioning System",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "style aware mid level representation",
                "discovering visual connections",
                "data mining approach",
                "geographic spatial image collections",
                "historic temporal image collections",
                "style aware regressors",
                "fine grained classification"
            ]
        },
        "id": 231,
        "cited_by": [
            {
                "year": "2015",
                "id": 112
            },
            {
                "year": "2015",
                "id": 117
            },
            {
                "year": "2015",
                "id": 162
            }
        ]
    },
    {
        "title": "Coarse-to-Fine Semantic Video Segmentation Using Supervoxel Trees",
        "authors": [
            "Aastha Jain",
            "Shuanak Chatterjee",
            "Ren\u00e9 Vidal"
        ],
        "abstract": "We propose an exact, general and efficient coarse-to-fine energy minimization strategy for semantic video segmentation. Our strategy is based on a hierarchical abstraction of the supervoxel graph that allows us to minimize an energy defined at the finest level of the hierarchy by minimizing a series of simpler energies defined over coarser graphs. The strategy is exact, i.e., it produces the same solution as minimizing over the finest graph. It is general, i.e., it can be used to minimize any energy function (e.g., unary, pair wise, and higher-order terms) with any existing energy minimization algorithm (e.g., graph cuts and belief propagation). It also gives significant speedups in inference for several datasets with varying degrees of spatio-temporal continuity. We also discuss the strengths and weaknesses of our strategy relative to existing hierarchical approaches, and the kinds of image and video data that provide the best speedups.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751342",
        "reference_list": [
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2003",
                "id": 57
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 3,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Inference algorithms",
                "Image segmentation",
                "Radio frequency",
                "Belief propagation",
                "Optimization",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "trees (mathematics)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coarse-to-fine semantic video segmentation",
                "supervoxel trees",
                "hierarchical abstraction",
                "energy minimization algorithm",
                "spatio-temporal continuity",
                "video data",
                "coarser graphs",
                "finest graph"
            ],
            "Author Keywords": [
                "Video segmentation",
                "Image segmentation",
                "hierarchical inference",
                "coarse-to-fine inference",
                "energy minimization"
            ]
        },
        "id": 232,
        "cited_by": []
    },
    {
        "title": "3DNN: Viewpoint Invariant 3D Geometry Matching for Scene Understanding",
        "authors": [
            "Scott Satkin",
            "Martial Hebert"
        ],
        "abstract": "We present a new algorithm 3DNN (3D Nearest-Neighbor), which is capable of matching an image with 3D data, independently of the viewpoint from which the image was captured. By leveraging rich annotations associated with each image, our algorithm can automatically produce precise and detailed 3D models of a scene from a single image. Moreover, we can transfer information across images to accurately label and segment objects in a scene. The true benefit of 3DNN compared to a traditional 2D nearest-neighbor approach is that by generalizing across viewpoints, we free ourselves from the need to have training examples captured from all possible viewpoints. Thus, we are able to achieve comparable results using orders of magnitude less data, and recognize objects from never-before-seen viewpoints. In this work, we describe the 3DNN algorithm and rigorously evaluate its performance for the tasks of geometry estimation and object detection/segmentation. By decoupling the viewpoint and the geometry of an image, we develop a scene matching approach which is truly 100% viewpoint invariant, yielding state-of-the-art performance on challenging data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751343",
        "reference_list": [
            {
                "year": "2009",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 7,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Three-dimensional displays",
                "Solid modeling",
                "Layout",
                "Cameras",
                "Image edge detection",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image matching",
                "image segmentation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D Nearest-Neighbor algorithm",
                "image matching",
                "scene matching approach",
                "object segmentation",
                "object detection",
                "geometry estimation",
                "2D nearest-neighbor approach",
                "3D models",
                "scene understanding",
                "viewpoint invariant 3D geometry matching",
                "3DNN algorithm"
            ],
            "Author Keywords": [
                "Computer Vision",
                "Scene Understanding",
                "Geometry Estimation",
                "3D Data",
                "Machine Learning"
            ]
        },
        "id": 233,
        "cited_by": []
    },
    {
        "title": "Local Signal Equalization for Correspondence Matching",
        "authors": [
            "Derek Bradley",
            "Thabo Beeler"
        ],
        "abstract": "Correspondence matching is one of the most common problems in computer vision, and it is often solved using photo-consistency of local regions. These approaches typically assume that the frequency content in the local region is consistent in the image pair, such that matching is performed on similar signals. However, in many practical situations this is not the case, for example with low depth of field cameras a scene point may be out of focus in one view and in-focus in the other, causing a mismatch of frequency signals. Furthermore, this mismatch can vary spatially over the entire image. In this paper we propose a local signal equalization approach for correspondence matching. Using a measure of local image frequency, we equalize local signals using an efficient scale-space image representation such that their frequency contents are optimally suited for matching. Our approach allows better correspondence matching, which we demonstrate with a number of stereo reconstruction examples on synthetic and real datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751344",
        "reference_list": [
            {
                "year": "2007",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Stereo image processing",
                "Cutoff frequency",
                "Noise",
                "Frequency-domain analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "equalisers",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local signal equalization approach",
                "correspondence matching",
                "computer vision",
                "local region photo-consistency",
                "low depth of field cameras",
                "frequency signal mismatch",
                "image frequency measurement",
                "stereo reconstruction",
                "frequency contents"
            ]
        },
        "id": 234,
        "cited_by": []
    },
    {
        "title": "Monocular Image 3D Human Pose Estimation under Self-Occlusion",
        "authors": [
            "Ibrahim Radwan",
            "Abhinav Dhall",
            "Roland Goecke"
        ],
        "abstract": "In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the Human Eva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751345",
        "reference_list": [
            {
                "year": "2009",
                "id": 240
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 4,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Vectors",
                "Joints",
                "Bones",
                "Cameras",
                "Image reconstruction",
                "Kinematics"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "monocular image 3D human pose estimation",
                "self-occlusion",
                "human body articulation",
                "hallucinated parts",
                "cluttered background",
                "pose inference",
                "anthropomorphism",
                "multiple oriented views",
                "Human Eva-I dataset",
                "image parse dataset",
                "image reconstruction"
            ],
            "Author Keywords": [
                "3D pose reconstruction",
                "pose estimation",
                "self-occlusion"
            ]
        },
        "id": 235,
        "cited_by": [
            {
                "year": "2017",
                "id": 278
            },
            {
                "year": "2015",
                "id": 220
            }
        ]
    },
    {
        "title": "Multi-view 3D Reconstruction from Uncalibrated Radially-Symmetric Cameras",
        "authors": [
            "Jae-Hak Kim",
            "Yuchao Dai",
            "Hongdong Li",
            "Xin Du",
            "Jonghyuk Kim"
        ],
        "abstract": "We present a new multi-view 3D Euclidean reconstruction method for arbitrary uncalibrated radially-symmetric cameras, which needs no calibration or any camera model parameters other than radial symmetry. It is built on the radial 1D camera model [25], a unified mathematical abstraction to different types of radially-symmetric cameras. We formulate the problem of multi-view reconstruction for radial 1D cameras as a matrix rank minimization problem. Efficient implementation based on alternating direction continuation is proposed to handle scalability issue for real-world applications. Our method applies to a wide range of omni directional cameras including both dioptric and catadioptric (central and non-central) cameras. Additionally, our method deals with complete and incomplete measurements under a unified framework elegantly. Experiments on both synthetic and real images from various types of cameras validate the superior performance of our new method, in terms of numerical accuracy and robustness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751346",
        "reference_list": [
            {
                "year": "2011",
                "id": 318
            },
            {
                "year": "2007",
                "id": 273
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Image reconstruction",
                "Mirrors",
                "Distortion measurement",
                "Mathematical model",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "matrix algebra",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview 3D reconstruction",
                "uncalibrated radially-symmetric camera",
                "radial 1D camera model",
                "unified mathematical abstraction",
                "matrix rank minimization problem",
                "omnidirectional camera",
                "dioptric camera",
                "catadioptric cameras",
                "central camera",
                "noncentral camera"
            ],
            "Author Keywords": [
                "3D reconstruction",
                "Radial 1D camera",
                "Matrix rank minimization"
            ]
        },
        "id": 236,
        "cited_by": []
    },
    {
        "title": "Corrected-Moment Illuminant Estimation",
        "authors": [
            "Graham D. Finlayson"
        ],
        "abstract": "Image colors are biased by the color of the prevailing illumination. As such the color at pixel cannot always be used directly in solving vision tasks from recognition, to tracking to general scene understanding. Illuminant estimation algorithms attempt to infer the color of the light incident in a scene and then a color cast removal step discounts the color bias due to illumination. However, despite sustained research since almost the inception of computer vision, progress has been modest. The best algorithms - now often built on top of expensive feature extraction and machine learning - are only about twice as good as the simplest approaches. This paper, in effect, will show how simple moment based algorithms - such as Gray-World - can, with the addition of a simple correction step, deliver much improved illuminant estimation performance. The corrected Gray-World algorithm maps the mean image color using a fixed (per camera) 3x3 matrix transform. More generally, our moment approach employs 1st, 2nd and higher order moments - of colors or features such as color derivatives - and these again are linearly corrected to give an illuminant estimate. The question of how to correct the moments is an important one yet we will show a simple alternating least-squares training procedure suffices. Remarkably, across the major datasets - evaluated using a 3-fold cross validation procedure - our simple corrected moment approach always delivers the best results (and the performance increment is often large compared with the prior art). Significantly, outlier performance was found to be much improved.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751347",
        "reference_list": [
            {
                "year": "2007",
                "id": 262
            },
            {
                "year": "2001",
                "id": 32
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 10,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Conferences",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image colour analysis",
                "learning (artificial intelligence)",
                "least squares approximations",
                "matrix algebra",
                "method of moments",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "corrected-moment illuminant estimation",
                "image colors",
                "illumination",
                "vision tasks",
                "light incident",
                "color cast removal step",
                "color bias",
                "computer vision",
                "feature extraction",
                "machine learning",
                "moment based algorithms",
                "illuminant estimation performance",
                "gray-world algorithm",
                "matrix transform",
                "higher order moments",
                "color derivatives",
                "least-squares training procedure",
                "3-fold cross validation procedure",
                "outlier performance"
            ],
            "Author Keywords": [
                "Color",
                "Color Constancy",
                "Illuminant Estimation",
                "Statistical Moments"
            ]
        },
        "id": 237,
        "cited_by": [
            {
                "year": "2015",
                "id": 33
            },
            {
                "year": "2015",
                "id": 42
            }
        ]
    },
    {
        "title": "Target-Driven Moire Pattern Synthesis by Phase Modulation",
        "authors": [
            "Pei-Hen Tsai",
            "Yung-Yu Chuang"
        ],
        "abstract": "This paper investigates an approach for generating two grating images so that the moire pattern of their superposition resembles the target image. Our method is grounded on the fundamental moire theorem. By focusing on the visually most dominant (1,-1)-moire component, we obtain the phase modulation constraint on the phase shifts between the two grating images. For improving visual appearance of the grating images and hiding capability the embedded image, a smoothness term is added to spread information between the two grating images and an appearance phase function is used to add irregular structures into grating images. The grating images can be printed on transparencies and the hidden image decoding can be performed optically by overlaying them together. The proposed method enables the creation of moire art and allows visual decoding without computers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751348",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Gratings",
                "Equations",
                "Visualization",
                "Phase modulation",
                "Art",
                "Optical imaging",
                "Frequency-domain analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "decoding",
                "image coding",
                "phase modulation",
                "smoothing methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "target-driven moire pattern synthesis",
                "grating images",
                "moire theorem",
                "moire component",
                "phase modulation constraint",
                "phase shifts",
                "visual appearance",
                "image hiding capability",
                "embedded image",
                "smoothness term",
                "phase function",
                "irregular structures",
                "hidden image decoding",
                "moire art",
                "visual decoding"
            ],
            "Author Keywords": [
                "fundamental moire theorem",
                "moire pattern synthesis",
                "phase modulation",
                "moire art",
                "moire cryptography"
            ]
        },
        "id": 238,
        "cited_by": []
    },
    {
        "title": "Anchored Neighborhood Regression for Fast Example-Based Super-Resolution",
        "authors": [
            "Radu Timofte",
            "Vincent De",
            "Luc Van Gool"
        ],
        "abstract": "Recently there have been significant advances in image up scaling or image super-resolution based on a dictionary of low and high resolution exemplars. The running time of the methods is often ignored despite the fact that it is a critical factor for real applications. This paper proposes fast super-resolution methods while making no compromise on quality. First, we support the use of sparse learned dictionaries in combination with neighbor embedding methods. In this case, the nearest neighbors are computed using the correlation with the dictionary atoms rather than the Euclidean distance. Moreover, we show that most of the current approaches reach top performance for the right parameters. Second, we show that using global collaborative coding has considerable speed advantages, reducing the super-resolution mapping to a precomputed projective matrix. Third, we propose the anchored neighborhood regression. That is to anchor the neighborhood embedding of a low resolution patch to the nearest atom in the dictionary and to precompute the corresponding embedding matrix. These proposals are contrasted with current state-of-the-art methods on standard images. We obtain similar or improved quality and one or two orders of magnitude speed improvements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751349",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 276,
            "other": 148,
            "total": 424
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Image resolution",
                "PSNR",
                "Encoding",
                "Training",
                "Signal resolution",
                "Interpolation"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "image reconstruction",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "neighborhood regression",
                "fast example-based super-resolution",
                "image upscaling",
                "image super-resolution",
                "sparse learned dictionaries",
                "collaborative coding",
                "super-resolution mapping reduction"
            ],
            "Author Keywords": [
                "super-resolution",
                "neighbor embedding",
                "sparse coding",
                "ridge regression",
                "anchored neighborhood regression"
            ]
        },
        "id": 239,
        "cited_by": [
            {
                "year": "2015",
                "id": 36
            },
            {
                "year": "2015",
                "id": 41
            },
            {
                "year": "2015",
                "id": 57
            },
            {
                "year": "2015",
                "id": 58
            },
            {
                "year": "2015",
                "id": 203
            }
        ]
    },
    {
        "title": "Multi-scale Topological Features for Hand Posture Representation and Analysis",
        "authors": [
            "Kaoning Hu",
            "Lijun Yin"
        ],
        "abstract": "In this paper, we propose a multi-scale topological feature representation for automatic analysis of hand posture. Such topological features have the advantage of being posture-dependent while being preserved under certain variations of illumination, rotation, personal dependency, etc. Our method studies the topology of the holes between the hand region and its convex hull. Inspired by the principle of Persistent Homology, which is the theory of computational topology for topological feature analysis over multiple scales, we construct the multi-scale Betti Numbers matrix (MSBNM) for the topological feature representation. In our experiments, we used 12 different hand postures and compared our features with three popular features (HOG, MCT, and Shape Context) on different data sets. In addition to hand postures, we also extend the feature representations to arm postures. The results demonstrate the feasibility and reliability of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751350",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Context",
                "Image resolution",
                "Accuracy",
                "Topology",
                "Robustness",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image representation",
                "matrix algebra",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hand posture matching",
                "arm postures",
                "multiscale Betti number matrix",
                "computational topology theory",
                "Persistent homology principle",
                "hole topology",
                "hand region",
                "convex hull",
                "posture-dependent preservation",
                "personal dependency",
                "multiscale topological feature representation",
                "automatic hand posture analysis",
                "hand posture representation"
            ]
        },
        "id": 240,
        "cited_by": []
    },
    {
        "title": "Sieving Regression Forest Votes for Facial Feature Detection in the Wild",
        "authors": [
            "Heng Yang",
            "Ioannis Patras"
        ],
        "abstract": "In this paper we propose a method for the localization of multiple facial features on challenging face images. In the regression forests (RF) framework, observations (patches) that are extracted at several image locations cast votes for the localization of several facial features. In order to filter out votes that are not relevant, we pass them through two types of sieves, that are organised in a cascade, and which enforce geometric constraints. The first sieve filters out votes that are not consistent with a hypothesis for the location of the face center. Several sieves of the second type, one associated with each individual facial point, filter out distant votes. We propose a method that adjusts on-the-fly the proximity threshold of each second type sieve by applying a classifier which, based on middle-level features extracted from voting maps for the facial feature in question, makes a sequence of decisions on whether the threshold should be reduced or not. We validate our proposed method on two challenging datasets with images collected from the Internet in which we obtain state of the art results without resorting to explicit facial shape models. We also show the benefits of our method for proximity threshold adjustment especially on 'difficult' face images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751351",
        "reference_list": [
            {
                "year": "2011",
                "id": 57
            },
            {
                "year": "2013",
                "id": 188
            },
            {
                "year": "2011",
                "id": 52
            },
            {
                "year": "2009",
                "id": 132
            },
            {
                "year": "2013",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 11,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Facial features",
                "Feature extraction",
                "Shape",
                "Vegetation",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image classification",
                "Internet",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sieving regression forest votes",
                "facial feature detection",
                "multiple facial feature localization",
                "face images",
                "RF framework",
                "patch extraction",
                "image locations",
                "geometric constraints",
                "sieve filters",
                "face center localization",
                "middle-level feature extraction",
                "voting maps",
                "decision sequence",
                "Internet",
                "proximity threshold adjustment",
                "classifiers"
            ],
            "Author Keywords": [
                "regression forests",
                "facial feature detection",
                "votes sieving"
            ]
        },
        "id": 241,
        "cited_by": [
            {
                "year": "2015",
                "id": 412
            },
            {
                "year": "2013",
                "id": 10
            }
        ]
    },
    {},
    {
        "title": "Markov Network-Based Unified Classifier for Face Identification",
        "authors": [
            "Wonjun Hwang",
            "Kyungshik Roh",
            "Junmo Kim"
        ],
        "abstract": "We propose a novel unifying framework using a Markov network to learn the relationship between multiple classifiers in face recognition. We assume that we have several complementary classifiers and assign observation nodes to the features of a query image and hidden nodes to the features of gallery images. We connect each hidden node to its corresponding observation node and to the hidden nodes of other neighboring classifiers. For each observation-hidden node pair, we collect a set of gallery candidates that are most similar to the observation instance, and the relationship between the hidden nodes is captured in terms of the similarity matrix between the collected gallery images. Posterior probabilities in the hidden nodes are computed by the belief-propagation algorithm. The novelty of the proposed framework is the method that takes into account the classifier dependency using the results of each neighboring classifier. We present extensive results on two different evaluation protocols, known and unknown image variation tests, using three different databases, which shows that the proposed framework always leads to good accuracy in face recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751353",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Markov random fields",
                "Face",
                "Databases",
                "Face recognition",
                "Protocols",
                "Vectors",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "face recognition",
                "image retrieval",
                "Markov processes",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Markov network-based unified classifier",
                "face identification",
                "face recognition",
                "complementary classifiers",
                "observation nodes",
                "query image",
                "gallery images",
                "similarity matrix",
                "posterior probabilities",
                "belief-propagation algorithm",
                "classifier dependency",
                "neighboring classifier"
            ],
            "Author Keywords": [
                "Face Recognition",
                "Markov Network",
                "Unified Framework"
            ]
        },
        "id": 243,
        "cited_by": []
    },
    {
        "title": "Fast High Dimensional Vector Multiplication Face Recognition",
        "authors": [
            "Oren Barkan",
            "Jonathan Weill",
            "Lior Wolf",
            "Hagai Aronowitz"
        ],
        "abstract": "This paper advances descriptor-based face recognition by suggesting a novel usage of descriptors to form an over-complete representation, and by proposing a new metric learning pipeline within the same/not-same framework. First, the Over-Complete Local Binary Patterns (OCLBP) face representation scheme is introduced as a multi-scale modified version of the Local Binary Patterns (LBP) scheme. Second, we propose an efficient matrix-vector multiplication-based recognition system. The system is based on Linear Discriminant Analysis (LDA) coupled with Within Class Covariance Normalization (WCCN). This is further extended to the unsupervised case by proposing an unsupervised variant of WCCN. Lastly, we introduce Diffusion Maps (DM) for non-linear dimensionality reduction as an alternative to the Whitened Principal Component Analysis (WPCA) method which is often used in face recognition. We evaluate the proposed framework on the LFW face recognition dataset under the restricted, unrestricted and unsupervised protocols. In all three cases we achieve very competitive results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751354",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2009",
                "id": 46
            }
        ],
        "citation": {
            "ieee": 56,
            "other": 33,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Scattering",
                "Training",
                "Vectors",
                "Accuracy",
                "Classification algorithms",
                "Face"
            ],
            "INSPEC: Controlled Indexing": [
                "covariance analysis",
                "face recognition",
                "image representation",
                "matrix algebra",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "LFW face recognition dataset",
                "WPCA",
                "whitened principal component analysis",
                "nonlinear dimensionality reduction",
                "DM",
                "diffusion maps",
                "WCCN",
                "within class covariance normalization",
                "LDA",
                "linear discriminant analysis",
                "LBP scheme",
                "OCLBP",
                "over-complete local binary pattern",
                "metric learning pipeline",
                "descriptor-based face recognition",
                "fast high dimensional vector multiplication"
            ],
            "Author Keywords": [
                "face recognition",
                "pattern recognition",
                "diffusion maps",
                "dimensionality reduction",
                "overcomplete representation",
                "high dimensional representation",
                "unsupervised labeling"
            ]
        },
        "id": 244,
        "cited_by": [
            {
                "year": "2015",
                "id": 415
            },
            {
                "year": "2015",
                "id": 476
            }
        ]
    },
    {
        "title": "A Generalized Low-Rank Appearance Model for Spatio-temporally Correlated Rain Streaks",
        "authors": [
            "Yi-Lei Chen",
            "Chiou-Ting Hsu"
        ],
        "abstract": "In this paper, we propose a novel low-rank appearance model for removing rain streaks. Different from previous work, our method needs neither rain pixel detection nor time-consuming dictionary learning stage. Instead, as rain streaks usually reveal similar and repeated patterns on imaging scene, we propose and generalize a low-rank model from matrix to tensor structure in order to capture the spatio-temporally correlated rain streaks. With the appearance model, we thus remove rain streaks from image/video (and also other high-order image structure) in a unified way. Our experimental results demonstrate competitive (or even better) visual quality and efficient run-time in comparison with state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751355",
        "reference_list": [
            {
                "year": "2009",
                "id": 272
            }
        ],
        "citation": {
            "ieee": 44,
            "other": 11,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Rain",
                "Equations",
                "Mathematical model",
                "TV",
                "Tensile stress",
                "Dictionaries",
                "Imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "tensors",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual quality",
                "high-order image structure",
                "image-video",
                "tensor structure",
                "imaging scene",
                "rain streak removal",
                "spatiotemporally-correlated rain streaks",
                "generalized low-rank appearance model"
            ]
        },
        "id": 245,
        "cited_by": [
            {
                "year": "2017",
                "id": 181
            },
            {
                "year": "2017",
                "id": 265
            },
            {
                "year": "2017",
                "id": 266
            },
            {
                "year": "2015",
                "id": 26
            },
            {
                "year": "2015",
                "id": 379
            }
        ]
    },
    {
        "title": "Salient Region Detection by UFO: Uniqueness, Focusness and Objectness",
        "authors": [
            "Peng Jiang",
            "Haibin Ling",
            "Jingyi Yu",
            "Jingliang Peng"
        ],
        "abstract": "The goal of saliency detection is to locate important pixels or regions in an image which attract humans' visual attention the most. This is a fundamental task whose output may serve as the basis for further computer vision tasks like segmentation, resizing, tracking and so forth. In this paper we propose a novel salient region detection algorithm by integrating three important visual cues namely uniqueness, focus ness and objectness (UFO). In particular, uniqueness captures the appearance-derived visual contrast, focus ness reflects the fact that salient regions are often photographed in focus, and objectness helps keep completeness of detected salient regions. While uniqueness has been used for saliency detection for long, it is new to integrate focus ness and objectness for this purpose. In fact, focus ness and objectness both provide important saliency information complementary of uniqueness. In our experiments using public benchmark datasets, we show that, even with a simple pixel level combination of the three components, the proposed approach yields significant improvement compared with previously reported methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751356",
        "reference_list": [
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2011",
                "id": 191
            },
            {
                "year": "2011",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 87,
            "other": 55,
            "total": 142
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Image color analysis",
                "Estimation",
                "Image segmentation",
                "Computer vision",
                "Visualization",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "salient region detection",
                "UFO",
                "human visual attention",
                "uniqueness focusness and objectness",
                "visual cues",
                "computer vision tasks",
                "appearance-derived visual contrast",
                "public benchmark datasets"
            ]
        },
        "id": 246,
        "cited_by": []
    },
    {
        "title": "Estimating the Material Properties of Fabric from Video",
        "authors": [
            "Katherine L. Bouman",
            "Bei Xiao",
            "Peter Battaglia",
            "William T. Freeman"
        ],
        "abstract": "Passively estimating the intrinsic material properties of deformable objects moving in a natural environment is essential for scene understanding. We present a framework to automatically analyze videos of fabrics moving under various unknown wind forces, and recover two key material properties of the fabric: stiffness and area weight. We extend features previously developed to compactly represent static image textures to describe video textures, such as fabric motion. A discriminatively trained regression model is then used to predict the physical properties of fabric from these features. The success of our model is demonstrated on a new, publicly available database of fabric videos with corresponding measured ground truth material properties. We show that our predictions are well correlated with ground truth measurements of stiffness and density for the fabrics. Our contributions include: (a) a database that can be used for training and testing algorithms for passively predicting fabric properties from video, (b) an algorithm for predicting the material properties of fabric from a video, and (c) a perceptual study of humans' ability to estimate the material properties of fabric from videos and images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751357",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 13,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Fabrics",
                "Material properties",
                "Databases",
                "Force",
                "Computational modeling",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "fabrics",
                "image texture",
                "regression analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fabric material properties estimation",
                "intrinsic material properties",
                "scene understanding",
                "wind forces",
                "key material properties",
                "stiffness",
                "area weight",
                "weight",
                "video textures",
                "asfabric motion",
                "discriminatively trained regression model",
                "publicly available database",
                "offabric videos",
                "fabric properties prediction",
                "computer vision"
            ]
        },
        "id": 247,
        "cited_by": [
            {
                "year": "2017",
                "id": 460
            },
            {
                "year": "2015",
                "id": 79
            },
            {
                "year": "2015",
                "id": 84
            }
        ]
    },
    {
        "title": "Multiple Non-rigid Surface Detection and Registration",
        "authors": [
            "Yi Wu",
            "Yoshihisa Ijiri",
            "Ming-Hsuan Yang"
        ],
        "abstract": "Detecting and registering nonrigid surfaces are two important research problems for computer vision. Much work has been done with the assumption that there exists only one instance in the image. In this work, we propose an algorithm that detects and registers multiple nonrigid instances of given objects in a cluttered image. Specifically, after we use low level feature points to obtain the initial matches between templates and the input image, a novel high-order affinity graph is constructed to model the consistency of local topology. A hierarchical clustering approach is then used to locate the nonrigid surfaces. To remove the outliers in the cluster, we propose a deterministic annealing approach based on the Thin Plate Spline (TPS) model. The proposed method achieves high accuracy even when the number of outliers is nineteen times larger than the inliers. As the matches may appear sparsely in each instance, we propose a TPS based match growing approach to propagate the matches. Finally, an approach that fuses feature and appearance information is proposed to register each nonrigid surface. Extensive experiments and evaluations demonstrate that the proposed algorithm achieves promising results in detecting and registering multiple non-rigid surfaces in a cluttered scene.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751358",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Annealing",
                "Deformable models",
                "Nickel",
                "Optimization",
                "Robustness",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image registration",
                "splines (mathematics)",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple nonrigid surface detection",
                "multiple nonrigid surface registration",
                "computer vision",
                "nonrigid instances",
                "low level feature points",
                "high order affinity graph",
                "local topology",
                "hierarchical clustering",
                "deterministic annealing",
                "thin plate spline",
                "TPS model",
                "TPS based match growing",
                "cluttered scene"
            ]
        },
        "id": 248,
        "cited_by": []
    },
    {
        "title": "Discriminative Label Propagation for Multi-object Tracking with Sporadic Appearance Features",
        "authors": [
            "K.C. Amit Kumar",
            "Christophe De Vleeschouwer"
        ],
        "abstract": "Given a set of plausible detections, detected at each time instant independently, we investigate how to associate them across time. This is done by propagating labels on a set of graphs that capture how the spatio-temporal and the appearance cues promote the assignment of identical or distinct labels to a pair of nodes. The graph construction is driven by the locally linear embedding (LLE) of either the spatio-temporal or the appearance features associated to the detections. Interestingly, the neighborhood of a node in each appearance graph is defined to include all nodes for which the appearance feature is available (except the ones that coexist at the same time). This allows to connect the nodes that share the same appearance even if they are temporally distant, which gives our framework the uncommon ability to exploit the appearance features that are available only sporadically along the sequence of detections. Once the graphs have been defined, the multi-object tracking is formulated as the problem of finding a label assignment that is consistent with the constraints captured by each of the graphs. This results into a difference of convex program that can be efficiently solved. Experiments are performed on a basketball and several well-known pedestrian datasets in order to validate the effectiveness of the proposed solution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751359",
        "reference_list": [
            {
                "year": "2011",
                "id": 17
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Feature extraction",
                "Target tracking",
                "Reliability",
                "Color",
                "Equations",
                "Trajectory"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "feature extraction",
                "graph theory",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative label propagation",
                "multiobject tracking",
                "sporadic appearance feature",
                "plausible detections",
                "graph set",
                "spatiotemporal cue",
                "appearance cue",
                "identical label assignment",
                "distinct label assignment",
                "graph construction",
                "locally-linear embedding",
                "LLE",
                "appearance graph",
                "node neighborhood",
                "detection sequence",
                "convex program",
                "basketball dataset",
                "pedestrian dataset"
            ]
        },
        "id": 249,
        "cited_by": []
    },
    {
        "title": "Online Motion Segmentation Using Dynamic Label Propagation",
        "authors": [
            "Ali Elqursh",
            "Ahmed Elgammal"
        ],
        "abstract": "The vast majority of work on motion segmentation adopts the affine camera model due to its simplicity. Under the affine model, the motion segmentation problem becomes that of subspace separation. Due to this assumption, such methods are mainly offline and exhibit poor performance when the assumption is not satisfied. This is made evident in state-of-the-art methods that relax this assumption by using piecewise affine spaces and spectral clustering techniques to achieve better results. In this paper, we formulate the problem of motion segmentation as that of manifold separation. We then show how label propagation can be used in an online framework to achieve manifold separation. The performance of our framework is evaluated on a benchmark dataset and achieves competitive performance while being online.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751360",
        "reference_list": [
            {
                "year": "2001",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Motion segmentation",
                "Manifolds",
                "Computer vision",
                "Cameras",
                "Streaming media",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "affine transforms",
                "cameras",
                "image motion analysis",
                "image segmentation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online motion segmentation",
                "dynamic label propagation",
                "affine camera model",
                "subspace separation",
                "piecewise affine spaces",
                "spectral clustering techniques",
                "manifold separation",
                "label propagation",
                "online framework",
                "benchmark dataset"
            ]
        },
        "id": 250,
        "cited_by": [
            {
                "year": "2017",
                "id": 539
            },
            {
                "year": "2015",
                "id": 364
            }
        ]
    },
    {
        "title": "A Unified Rolling Shutter and Motion Blur Model for 3D Visual Registration",
        "authors": [
            "Maxime Meilland",
            "Tom Drummond",
            "Andrew I. Comport"
        ],
        "abstract": "Motion blur and rolling shutter deformations both inhibit visual motion registration, whether it be due to a moving sensor or a moving target. Whilst both deformations exist simultaneously, no models have been proposed to handle them together. Furthermore, neither deformation has been considered previously in the context of monocular full-image 6 degrees of freedom registration or RGB-D structure and motion. As will be shown, rolling shutter deformation is observed when a camera moves faster than a single pixel in parallax between subsequent scan-lines. Blur is a function of the pixel exposure time and the motion vector. In this paper a complete dense 3D registration model will be derived to account for both motion blur and rolling shutter deformations simultaneously. Various approaches will be compared with respect to ground truth and live real-time performance will be demonstrated for complex scenarios where both blur and shutter deformations are dominant.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751361",
        "reference_list": [
            {
                "year": "2009",
                "id": 235
            },
            {
                "year": "2003",
                "id": 130
            },
            {
                "year": "2011",
                "id": 295
            },
            {
                "year": "2011",
                "id": 199
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 2,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Solid modeling",
                "Three-dimensional displays",
                "Real-time systems",
                "Mathematical model",
                "Deformable models",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image motion analysis",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unified rolling shutter",
                "motion blur model",
                "3D visual registration",
                "rolling shutter deformations",
                "visual motion registration",
                "moving sensor",
                "moving target",
                "monocular full-image context",
                "RGB-D structure",
                "RGB-D motion",
                "red-green-blue-depth",
                "ground truth",
                "blur deformations"
            ],
            "Author Keywords": [
                "Rolling Shutter",
                "Motion Blur",
                "Registration",
                "Tracking",
                "Dense",
                "Real-time"
            ]
        },
        "id": 251,
        "cited_by": [
            {
                "year": "2015",
                "id": 62
            },
            {
                "year": "2015",
                "id": 252
            }
        ]
    },
    {
        "title": "Shortest Paths with Curvature and Torsion",
        "authors": [
            "Petter Strandmark",
            "Johannes Ul\u00e9n",
            "Fredrik Kahl",
            "Leo Grady"
        ],
        "abstract": "This paper describes a method of finding thin, elongated structures in images and volumes. We use shortest paths to minimize very general functionals of higher-order curve properties, such as curvature and torsion. Our globally optimal method uses line graphs and its runtime is polynomial in the size of the discretization, often in the order of seconds on a single computer. To our knowledge, we are the first to perform experiments in three dimensions with curvature and torsion regularization. The largest graphs we process have almost one hundred billion arcs. Experiments on medical images and in multi-view reconstruction show the significance and practical usefulness of regularization based on curvature while torsion is still only tractable for small-scale problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751362",
        "reference_list": [
            {
                "year": "2003",
                "id": 134
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Three-dimensional displays",
                "Image segmentation",
                "Arrays",
                "Arteries",
                "Biomedical imaging",
                "Splines (mathematics)"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "differential geometry",
                "graph theory",
                "image reconstruction",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shortest paths",
                "curvature regularization",
                "torsion regularization",
                "elongated structures",
                "higher-order curve properties",
                "line graphs",
                "polynomial runtime",
                "medical images",
                "multiview reconstruction"
            ]
        },
        "id": 252,
        "cited_by": []
    },
    {
        "title": "Bounded Labeling Function for Global Segmentation of Multi-part Objects with Geometric Constraints",
        "authors": [
            "Masoud S. Nosrati",
            "Shawn Andrews",
            "Ghassan Hamarneh"
        ],
        "abstract": "The inclusion of shape and appearance priors have proven useful for obtaining more accurate and plausible segmentations, especially for complex objects with multiple parts. In this paper, we augment the popular Mum ford-Shah model to incorporate two important geometrical constraints, termed containment and detachment, between different regions with a specified minimum distance between their boundaries. Our method is able to handle multiple instances of multi-part objects defined by these geometrical constraints using a single labeling function while maintaining global optimality. We demonstrate the utility and advantages of these two constraints and show that the proposed convex continuous method is superior to other state-of-the-art methods, including its discrete counterpart, in terms of memory usage, and metrication errors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751363",
        "reference_list": [
            {
                "year": "2009",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Labeling",
                "Optimization",
                "Level set",
                "Standards",
                "Vectors",
                "Biomedical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bounded labeling function",
                "global segmentation",
                "multipart objects",
                "geometric constraints",
                "Mumford-Shah model",
                "containment",
                "detachment",
                "single labeling function",
                "global optimality",
                "convex continuous method",
                "memory usage",
                "metrication errors"
            ],
            "Author Keywords": [
                "Global segmentation",
                "geometric constraints",
                "convex optimization",
                "functional lifting",
                "containment",
                "detachment",
                "microscopy",
                "histology"
            ]
        },
        "id": 253,
        "cited_by": []
    },
    {
        "title": "Randomized Ensemble Tracking",
        "authors": [
            "Qinxun Bai",
            "Zheng Wu",
            "Stan Sclaroff",
            "Margrit Betke",
            "Camille Monnier"
        ],
        "abstract": "We propose a randomized ensemble algorithm to model the time-varying appearance of an object for visual tracking. In contrast with previous online methods for updating classifier ensembles in tracking-by-detection, the weight vector that combines weak classifiers is treated as a random variable and the posterior distribution for the weight vector is estimated in a Bayesian manner. In essence, the weight vector is treated as a distribution that reflects the confidence among the weak classifiers used to construct and adapt the classifier ensemble. The resulting formulation models the time-varying discriminative ability among weak classifiers so that the ensembled strong classifier can adapt to the varying appearance, backgrounds, and occlusions. The formulation is tested in a tracking-by-detection implementation. Experiments on 28 challenging benchmark videos demonstrate that the proposed method can achieve results comparable to and often better than those of state-of-the-art approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751364",
        "reference_list": [
            {
                "year": "2011",
                "id": 172
            },
            {
                "year": "2011",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 30,
            "other": 22,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Bayes methods",
                "Reliability",
                "Visualization",
                "Boosting",
                "Target tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "image classification",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "randomized ensemble tracking algorithm",
                "time-varying appearance",
                "visual object tracking",
                "weak classifiers",
                "random variable",
                "posterior distribution",
                "Bayesian manner",
                "tracking-by-detection implementation",
                "strong classifier"
            ],
            "Author Keywords": [
                "classifier ensemble",
                "online learning",
                "tracking-by-detection"
            ]
        },
        "id": 254,
        "cited_by": [
            {
                "year": "2017",
                "id": 269
            }
        ]
    },
    {
        "title": "Discriminatively Trained Templates for 3D Object Detection: A Real Time Scalable Approach",
        "authors": [
            "Reyes Rios-Cabrera",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, we propose a challenging new dataset made of 12 objects, for future competing methods on monocular color images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751365",
        "reference_list": [],
        "citation": {
            "ieee": 36,
            "other": 19,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Training",
                "Support vector machines",
                "US Department of Transportation",
                "Clutter",
                "Testing",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminatively trained templates",
                "real time scalable approach",
                "multiple specific 3D object detection",
                "template-based approach",
                "LINE2D-LINEMOD representation",
                "frame rates",
                "single CPU core",
                "RGBD images",
                "monocular color images"
            ],
            "Author Keywords": [
                "Object Detection",
                "3D"
            ]
        },
        "id": 255,
        "cited_by": [
            {
                "year": "2017",
                "id": 13
            },
            {
                "year": "2017",
                "id": 406
            },
            {
                "year": "2017",
                "id": 433
            },
            {
                "year": "2015",
                "id": 490
            }
        ]
    },
    {
        "title": "Joint Deep Learning for Pedestrian Detection",
        "authors": [
            "Wanli Ouyang",
            "Xiaogang Wang"
        ],
        "abstract": "Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture. By establishing automatic, mutual interaction among components, the deep model achieves a 9% reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751366",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2007",
                "id": 252
            },
            {
                "year": "2009",
                "id": 3
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2005",
                "id": 11
            },
            {
                "year": "2011",
                "id": 256
            },
            {
                "year": "2013",
                "id": 15
            }
        ],
        "citation": {
            "ieee": 156,
            "other": 88,
            "total": 244
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Feature extraction",
                "Support vector machines",
                "Image edge detection",
                "Image color analysis",
                "Training",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)",
                "object detection",
                "pedestrians",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pedestrian detection",
                "feature extraction",
                "deformation handling",
                "occlusion handling",
                "classification",
                "joint deep learning framework",
                "deep model",
                "largest Caltech benchmark dataset"
            ],
            "Author Keywords": [
                "Pedestrian Detection",
                "object detection",
                "deep learning",
                "feature learning",
                "deep neural network",
                "convolutional neural network",
                "deformation",
                "occlusion"
            ]
        },
        "id": 256,
        "cited_by": [
            {
                "year": "2017",
                "id": 274
            },
            {
                "year": "2017",
                "id": 367
            },
            {
                "year": "2015",
                "id": 9
            },
            {
                "year": "2015",
                "id": 46
            },
            {
                "year": "2015",
                "id": 211
            },
            {
                "year": "2015",
                "id": 212
            },
            {
                "year": "2015",
                "id": 348
            },
            {
                "year": "2015",
                "id": 375
            },
            {
                "year": "2013",
                "id": 15
            }
        ]
    },
    {
        "title": "Detecting Avocados to Zucchinis: What Have We Done, and Where Are We Going?",
        "authors": [
            "Olga Russakovsky",
            "Jia Deng",
            "Zhiheng Huang",
            "Alexander C. Berg",
            "Li Fei-Fei"
        ],
        "abstract": "The growth of detection datasets and the multiple directions of object detection research provide both an unprecedented need and a great opportunity for a thorough evaluation of the current state of the field of categorical object detection. In this paper we strive to answer two key questions. First, where are we currently as a field: what have we done right, what still needs to be improved? Second, where should we be going in designing the next generation of object detectors? Inspired by the recent work of Hoiem et al. on the standard PASCAL VOC detection dataset, we perform a large-scale study on the Image Net Large Scale Visual Recognition Challenge (ILSVRC) data. First, we quantitatively demonstrate that this dataset provides many of the same detection challenges as the PASCAL VOC. Due to its scale of 1000 object categories, ILSVRC also provides an excellent test bed for understanding the performance of detectors as a function of several key properties of the object classes. We conduct a series of analyses looking at how different detection methods perform on a number of image-level and object-class-level properties such as texture, color, deformation, and clutter. We learn important lessons of the current object detection methods and propose a number of insights for designing the next generation object detectors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751367",
        "reference_list": [
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2009",
                "id": 4
            },
            {
                "year": "2005",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 12,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Clutter",
                "Object detection",
                "Detectors",
                "Detection algorithms",
                "Standards",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "next generation object detectors",
                "clutter",
                "deformation",
                "color",
                "texture",
                "object-class-level properties",
                "image-level",
                "object class",
                "object categories",
                "ILSVRC data",
                "image net large scale visual recognition challenge data",
                "PASCAL VOC detection dataset",
                "categorical object detection",
                "object detection research",
                "zucchinis",
                "avocado detection"
            ]
        },
        "id": 257,
        "cited_by": []
    },
    {
        "title": "Ensemble Projection for Semi-supervised Image Classification",
        "authors": [
            "Dengxin Dai",
            "Luc Van Gool"
        ],
        "abstract": "This paper investigates the problem of semi-supervised classification. Unlike previous methods to regularize classifying boundaries with unlabeled data, our method learns a new image representation from all available data (labeled and unlabeled) and performs plain supervised learning with the new feature. In particular, an ensemble of image prototype sets are sampled automatically from the available data, to represent a rich set of visual categories/attributes. Discriminative functions are then learned on these prototype sets, and image are represented by the concatenation of their projected values onto the prototypes (similarities to them) for further classification. Experiments on four standard datasets show three interesting phenomena: (1) our method consistently outperforms previous methods for semi-supervised image classification, (2) our method lets itself combine well with these methods, and (3) our method works well for self-taught image classification where unlabeled data are not coming from the same distribution as labeled ones, but rather from a random collection of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751368",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2011",
                "id": 248
            },
            {
                "year": "2009",
                "id": 64
            },
            {
                "year": "2011",
                "id": 181
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 9,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Prototypes",
                "Hafnium",
                "Visualization",
                "Image representation",
                "Training",
                "Skeleton",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image representation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "plain supervised learning",
                "Ensemble Projection",
                "random collection",
                "image prototype sets",
                "image representation",
                "semi-supervised image classification"
            ],
            "Author Keywords": [
                "Semi-supervised Learning",
                "Semi-supervised Image Classification",
                "Ensemble Projection",
                "Ensemble Learning",
                "High-level Image Feature Learning"
            ]
        },
        "id": 258,
        "cited_by": [
            {
                "year": "2015",
                "id": 199
            }
        ]
    },
    {
        "title": "Latent Task Adaptation with Large-Scale Hierarchies",
        "authors": [
            "Yangqing Jia",
            "Trevor Darrell"
        ],
        "abstract": "Recent years have witnessed the success of large-scale image classification systems that are able to identify objects among thousands of possible labels. However, it is yet unclear how general classifiers such as ones trained on Image Net can be optimally adapted to specific tasks, each of which only covers a semantically related subset of all the objects in the world. It is inefficient and sub optimal to retrain classifiers whenever a new task is given, and is inapplicable when tasks are not given explicitly, but implicitly specified as a set of image queries. In this paper we propose a novel probabilistic model that jointly identifies the underlying task and performs prediction with a linear-time probabilistic inference algorithm, given a set of query images from a latent task. We present efficient ways to estimate parameters for the model, and an open-source toolbox to train classifiers distributedly at a large scale. Empirical results based on the Image Net data showed significant performance increase over several baseline algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751369",
        "reference_list": [
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 263
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Testing",
                "Training",
                "Accuracy",
                "Probabilistic logic",
                "Adaptation models",
                "Context",
                "Psychology"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "parameter estimation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "baseline algorithms",
                "open-source toolbox",
                "parameter estimation",
                "query images",
                "linear-time probabilistic inference algorithm",
                "probabilistic model",
                "semantically related subset",
                "ImageNet",
                "general classifiers",
                "large-scale image classification systems",
                "large-scale hierarchies",
                "latent task adaptation"
            ],
            "Author Keywords": [
                "object recognition",
                "image classification",
                "large scale",
                "optimization"
            ]
        },
        "id": 259,
        "cited_by": [
            {
                "year": "2017",
                "id": 318
            }
        ]
    },
    {
        "title": "Learning Coupled Feature Spaces for Cross-Modal Matching",
        "authors": [
            "Kaiye Wang",
            "Ran He",
            "Wei Wang",
            "Liang Wang",
            "Tieniu Tan"
        ],
        "abstract": "Cross-modal matching has recently drawn much attention due to the widespread existence of multimodal data. It aims to match data from different modalities, and generally involves two basic problems: the measure of relevance and coupled feature selection. Most previous works mainly focus on solving the first problem. In this paper, we propose a novel coupled linear regression framework to deal with both problems. Our method learns two projection matrices to map multimodal data into a common feature space, in which cross-modal data matching can be performed. And in the learning procedure, the ell_21-norm penalties are imposed on the two projection matrices separately, which leads to select relevant and discriminative features from coupled feature spaces simultaneously. A trace norm is further imposed on the projected data as a low-rank constraint, which enhances the relevance of different modal data with connections. We also present an iterative algorithm based on half-quadratic minimization to solve the proposed regularized linear regression problem. The experimental results on two challenging cross-modal datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751370",
        "reference_list": [
            {
                "year": "2011",
                "id": 318
            },
            {
                "year": "2011",
                "id": 134
            }
        ],
        "citation": {
            "ieee": 56,
            "other": 44,
            "total": 100
        },
        "keywords": {
            "IEEE Keywords": [
                "Minimization",
                "Iterative methods",
                "Linear programming",
                "Principal component analysis",
                "Linear regression",
                "Correlation",
                "Face recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coupled feature spaces",
                "cross-modal matching",
                "multimodal data",
                "coupled feature selection",
                "coupled linear regression framework",
                "cross-modal data matching",
                "trace norm",
                "low-rank constraint",
                "different modal data",
                "iterative algorithm",
                "half-quadratic minimization",
                "regularized linear regression problem",
                "cross-modal datasets"
            ]
        },
        "id": 260,
        "cited_by": [
            {
                "year": "2017",
                "id": 429
            },
            {
                "year": "2015",
                "id": 457
            },
            {
                "year": "2015",
                "id": 475
            }
        ]
    },
    {
        "title": "CoDeL: A Human Co-detection and Labeling Framework",
        "authors": [
            "Jianping Shi",
            "Renjie Liao",
            "Jiaya Jia"
        ],
        "abstract": "We propose a co-detection and labeling (CoDeL) framework to identify persons that contain self-consistent appearance in multiple images. Our CoDeL model builds upon the deformable part-based model to detect human hypotheses and exploits cross-image correspondence via a matching classifier. Relying on a Gaussian process, this matching classifier models the similarity of two hypotheses and efficiently captures the relative importance contributed by various visual features, reducing the adverse effect of scattered occlusion. Further, the detector and matching classifier together make our model fit into a semi-supervised co-training framework, which can get enhanced results with a small amount of labeled training data. Our CoDeL model achieves decent performance on existing and new benchmark datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751371",
        "reference_list": [
            {
                "year": "2007",
                "id": 252
            },
            {
                "year": "2009",
                "id": 92
            },
            {
                "year": "2009",
                "id": 3
            },
            {
                "year": "2009",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Face",
                "Training",
                "Training data",
                "Feature extraction",
                "Labeling",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image classification",
                "image matching",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "matching classifier models",
                "Gaussian process",
                "cross-image correspondence",
                "human hypotheses detection",
                "deformable part-based model",
                "human codetection and labeling framework",
                "CoDeL"
            ],
            "Author Keywords": [
                "Human co-detection",
                "human identity labeling",
                "co-training",
                "HCD dataset"
            ]
        },
        "id": 261,
        "cited_by": [
            {
                "year": "2015",
                "id": 431
            }
        ]
    },
    {
        "title": "How Related Exemplars Help Complex Event Detection in Web Videos?",
        "authors": [
            "Yi Yang",
            "Zhigang Ma",
            "Zhongwen Xu",
            "Shuicheng Yan",
            "Alexander G. Hauptmann"
        ],
        "abstract": "Compared to visual concepts such as actions, scenes and objects, complex event is a higher level abstraction of longer video sequences. For example, a \"marriage proposal\" event is described by multiple objects (e.g., ring, faces), scenes (e.g., in a restaurant, outdoor) and actions (e.g., kneeling down). The positive exemplars which exactly convey the precise semantic of an event are hard to obtain. It would be beneficial to utilize the related exemplars for complex event detection. However, the semantic correlations between related exemplars and the target event vary substantially as relatedness assessment is subjective. Two related exemplars can be about completely different events, e.g., in the TRECVID MED dataset, both bicycle riding and equestrianism are labeled as related to \"attempting a bike trick\" event. To tackle the subjectiveness of human assessment, our algorithm automatically evaluates how positive the related exemplars are for the detection of an event and uses them on an exemplar-specific basis. Experiments demonstrate that our algorithm is able to utilize related exemplars adaptively, and the algorithm gains good performance for complex event detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751372",
        "reference_list": [
            {
                "year": "2013",
                "id": 429
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 14,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Event detection",
                "Proposals",
                "Video sequences",
                "Feature extraction",
                "Visualization",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "complex event detection",
                "web videos",
                "video sequences",
                "marriage proposal",
                "positive exemplars",
                "semantic correlations",
                "TRECVID MED dataset",
                "equestrianism",
                "bicycle riding",
                "bike trick event",
                "human assessment"
            ]
        },
        "id": 262,
        "cited_by": []
    },
    {
        "title": "Pose Estimation and Segmentation of People in 3D Movies",
        "authors": [
            "Karteek Alahari",
            "Guillaume Seguin",
            "Josef Sivic",
            "Ivan Laptev"
        ],
        "abstract": "We seek to obtain a pixel-wise segmentation and pose estimation of multiple people in a stereoscopic video. This involves challenges such as dealing with unconstrained stereoscopic video, non-stationary cameras, and complex indoor and outdoor dynamic scenes. The contributions of our work are two-fold: First, we develop a segmentation model incorporating person detection, pose estimation, as well as colour, motion, and disparity cues. Our new model explicitly represents depth ordering and occlusion. Second, we introduce a stereoscopic dataset with frames extracted from feature-length movies \"Street Dance 3D\" and \"Pina\". The dataset contains 2727 realistic stereo pairs and includes annotation of human poses, person bounding boxes, and pixel-wise segmentations for hundreds of people. The dataset is composed of indoor and outdoor scenes depicting multiple people with frequent occlusions. We demonstrate results on our new challenging dataset, as well as on the H2view dataset from (Sheasby et al. ACCV 2012).",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751373",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Stereo image processing",
                "Image color analysis",
                "Motion pictures",
                "Image segmentation",
                "Joints",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "feature extraction",
                "image colour analysis",
                "image motion analysis",
                "image representation",
                "image segmentation",
                "image sensors",
                "pose estimation",
                "stereo image processing",
                "visual perception"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human pose estimation",
                "people segmentation",
                "pixel-wise segmentation",
                "stereoscopic video dataset",
                "unconstrained stereoscopic video",
                "non-stationary camera",
                "complex outdoor dynamic scene",
                "complex indoor dynamic scene",
                "person detection",
                "image motion analysis",
                "image colour analysis",
                "occlusion",
                "feature-length movie extraction",
                "H2view dataset",
                "StreetDance 3D movie",
                "Pina",
                "person bounding box"
            ],
            "Author Keywords": [
                "Pose estimation",
                "Segmentation"
            ]
        },
        "id": 263,
        "cited_by": []
    },
    {
        "title": "A Unified Probabilistic Approach Modeling Relationships between Attributes and Objects",
        "authors": [
            "Xiaoyang Wang",
            "Qiang Ji"
        ],
        "abstract": "This paper proposes a unified probabilistic model to model the relationships between attributes and objects for attribute prediction and object recognition. As a list of semantically meaningful properties of objects, attributes generally relate to each other statistically. In this paper, we propose a unified probabilistic model to automatically discover and capture both the object-dependent and object-independent attribute relationships. The model utilizes the captured relationships to benefit both attribute prediction and object recognition. Experiments on four benchmark attribute datasets demonstrate the effectiveness of the proposed unified model for improving attribute prediction as well as object recognition in both standard and zero-shot learning cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751374",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2011",
                "id": 177
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 155
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 168
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 9,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Mathematical model",
                "Training",
                "Object recognition",
                "Testing",
                "Support vector machines",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object recognition",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unified probabilistic approach modeling relationships",
                "attribute prediction",
                "object recognition",
                "object-independent attribute relationships"
            ]
        },
        "id": 264,
        "cited_by": [
            {
                "year": "2017",
                "id": 443
            },
            {
                "year": "2015",
                "id": 279
            },
            {
                "year": "2015",
                "id": 465
            }
        ]
    },
    {
        "title": "Fast Neighborhood Graph Search Using Cartesian Concatenation",
        "authors": [
            "Jing Wang",
            "Jingdong Wang",
            "Gang Zeng",
            "Rui Gan",
            "Shipeng Li",
            "Baining Guo"
        ],
        "abstract": "In this paper, we propose a new data structure for approximate nearest neighbor search. This structure augments the neighborhood graph with a bridge graph. We propose to exploit Cartesian concatenation to produce a large set of vectors, called bridge vectors, from several small sets of subvectors. Each bridge vector is connected with a few reference vectors near to it, forming a bridge graph. Our approach finds nearest neighbors by simultaneously traversing the neighborhood graph and the bridge graph in the best-first strategy. The success of our approach stems from two factors: the exact nearest neighbor search over a large number of bridge vectors can be done quickly, and the reference vectors connected to a bridge (reference) vector near the query are also likely to be near the query. Experimental results on searching over large scale datasets (SIFT, GISTand HOG) show that our approach outperforms state-of-the-art ANN search algorithms in terms of efficiency and accuracy. The combination of our approach with the IVFADC system [18] also shows superior performance over the BIGANN dataset of 1 billion SIFT features compared with the best previously published result.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751375",
        "reference_list": [
            {
                "year": "2003",
                "id": 159
            },
            {
                "year": "2007",
                "id": 1
            },
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2011",
                "id": 206
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Bridges",
                "Accuracy",
                "Artificial neural networks",
                "Indexes",
                "Approximation algorithms",
                "Quantization (signal)"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "data structures",
                "graph theory",
                "learning (artificial intelligence)",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast neighborhood graph search",
                "Cartesian concatenation",
                "data structure",
                "neighborhood graph",
                "bridge graph",
                "bridge vector",
                "reference vectors",
                "GIST",
                "SIFT",
                "HOG",
                "ANN search algorithms",
                "IV-FADC system",
                "BIGANN dataset",
                "approximate nearest neighbor search",
                "machine learning"
            ]
        },
        "id": 265,
        "cited_by": [
            {
                "year": "2015",
                "id": 3
            }
        ]
    },
    {
        "title": "Codemaps - Segment, Classify and Search Objects Locally",
        "authors": [
            "Zhenyang Li",
            "Efstratios Gavves",
            "Koen E.A. van de Sande",
            "Cees G.M. Snoek",
            "Arnold W.M. Smeulders"
        ],
        "abstract": "In this paper we aim for segmentation and classification of objects. We propose codemaps that are a joint formulation of the classification score and the local neighborhood it belongs to in the image. We obtain the codemap by reordering the encoding, pooling and classification steps over lattice elements. Other than existing linear decompositions who emphasize only the efficiency benefits for localized search, we make three novel contributions. As a preliminary, we provide a theoretical generalization of the sufficient mathematical conditions under which image encodings and classification becomes locally decomposable. As first novelty we introduce l 2 normalization for arbitrarily shaped image regions, which is fast enough for semantic segmentation using our Fisher codemaps. Second, using the same lattice across images, we propose kernel pooling which embeds nonlinearities into codemaps for object classification by explicit or approximate feature mappings. Results demonstrate that l 2 normalized Fisher codemaps improve the state-of-the-art in semantic segmentation for PASCAL VOC. For object classification the addition of nonlinearities brings us on par with the state-of-the-art, but is 3x faster. Because of the codemaps' inherent efficiency, we can reach significant speed-ups for localized search as well. We exploit the efficiency gain for our third novelty: object segment retrieval using a single query image only.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751376",
        "reference_list": [
            {
                "year": "2009",
                "id": 30
            },
            {
                "year": "2009",
                "id": 5
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image segmentation",
                "Encoding",
                "Semantics",
                "Vectors",
                "Lattices",
                "Image coding"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image coding",
                "image retrieval",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object segmentation",
                "object classification",
                "object local search",
                "classification score",
                "local neighborhood",
                "encoding step reordering",
                "pooling step reordering",
                "classification step reordering",
                "lattice elements",
                "linear decomposition",
                "theoretical generalization",
                "mathematical condition",
                "image encoding",
                "l2 normalization",
                "arbitrarily-shaped image regions",
                "semantic segmentation",
                "kernel pooling",
                "feature mapping",
                "l2 normalized Fisher codemaps",
                "PASCAL VOC",
                "codemap inherent efficiency",
                "efficiency gain",
                "object segment retrieval",
                "single-query image"
            ]
        },
        "id": 266,
        "cited_by": []
    },
    {
        "title": "Support Surface Prediction in Indoor Scenes",
        "authors": [
            "Ruiqi Guo",
            "Derek Hoiem"
        ],
        "abstract": "In this paper, we present an approach to predict the extent and height of supporting surfaces such as tables, chairs, and cabinet tops from a single RGBD image. We define support surfaces to be horizontal, planar surfaces that can physically support objects and humans. Given a RGBD image, our goal is to localize the height and full extent of such surfaces in 3D space. To achieve this, we created a labeling tool and annotated 1449 images with rich, complete 3D scene models in NYU dataset. We extract ground truth from the annotated dataset and developed a pipeline for predicting floor space, walls, the height and full extent of support surfaces. Finally we match the predicted extent with annotated scenes in training scenes and transfer the the support surface configuration from training scenes. We evaluate the proposed approach in our dataset and demonstrate its effectiveness in understanding scenes in 3D space.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751377",
        "reference_list": [
            {
                "year": "2009",
                "id": 10
            },
            {
                "year": "2009",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 12,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Training",
                "Shape",
                "Layout",
                "Labeling",
                "Pipelines"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "support surface prediction",
                "indoor scenes",
                "RGBD image",
                "3D space",
                "3D scene models",
                "NYU dataset",
                "pipeline",
                "floor space",
                "walls"
            ],
            "Author Keywords": [
                "RGBD",
                "scene understanding",
                "image parsing",
                "support surfaces"
            ]
        },
        "id": 267,
        "cited_by": [
            {
                "year": "2017",
                "id": 93
            },
            {
                "year": "2015",
                "id": 86
            },
            {
                "year": "2015",
                "id": 117
            },
            {
                "year": "2015",
                "id": 147
            },
            {
                "year": "2015",
                "id": 242
            }
        ]
    },
    {
        "title": "Incorporating Cloud Distribution in Sky Representation",
        "authors": [
            "Kuan-Chuan Peng",
            "Tsuhan Chen"
        ],
        "abstract": "Most sky models only describe the cloudiness of the overall sky by a single category or parameter such as sky index, which does not account for the distribution of the clouds across the sky. To capture variable cloudiness, we extend the concept of sky index to a random field indicating the level of cloudiness of each sky pixel in our proposed sky representation based on the Igawa sky model. We formulate the problem of solving the sky index of every sky pixel as a labeling problem, where an approximate solution can be efficiently found. Experimental results show that our proposed sky model has better expressiveness, stability with respect to variation in camera parameters, and geo-location estimation in outdoor images compared to the uniform sky index model. Potential applications of our proposed sky model include sky image rendering, where sky images can be generated with an arbitrary cloud distribution at any time and any location, previously impossible with traditional sky models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751378",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Indexes",
                "Clouds",
                "Computational modeling",
                "Image reconstruction",
                "Data models",
                "Cameras",
                "Silicon"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "clouds",
                "geophysical image processing",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cloud distribution",
                "sky representation",
                "random field",
                "Igawa sky model",
                "labeling problem",
                "camera parameters",
                "geo-location estimation",
                "outdoor images",
                "uniform sky index model",
                "sky image rendering"
            ]
        },
        "id": 268,
        "cited_by": []
    },
    {
        "title": "Spoken Attributes: Mixing Binary and Relative Attributes to Say the Right Thing",
        "authors": [
            "Amir Sadovnik",
            "Andrew Gallagher",
            "Devi Parikh",
            "Tsuhan Chen"
        ],
        "abstract": "In recent years, there has been a great deal of progress in describing objects with attributes. Attributes have proven useful for object recognition, image search, face verification, image description, and zero-shot learning. Typically, attributes are either binary or relative: they describe either the presence or absence of a descriptive characteristic, or the relative magnitude of the characteristic when comparing two exemplars. However, prior work fails to model the actual way in which humans use these attributes in descriptive statements of images. Specifically, it does not address the important interactions between the binary and relative aspects of an attribute. In this work we propose a spoken attribute classifier which models a more natural way of using an attribute in a description. For each attribute we train a classifier which captures the specific way this attribute should be used. We show that as a result of using this model, we produce descriptions about images of people that are more natural and specific than past systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751379",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2009",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Glass",
                "Support vector machines",
                "Vectors",
                "Feature extraction",
                "Computer vision",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "image classification",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relative attributes",
                "binary attributes",
                "object recognition",
                "image search",
                "face verification",
                "image description",
                "zero-shot learning",
                "image descriptive statements",
                "descriptive characteristic",
                "spoken attribute classifier",
                "computer vision"
            ],
            "Author Keywords": [
                "attributes",
                "relative attributes",
                "visual attributes"
            ]
        },
        "id": 269,
        "cited_by": [
            {
                "year": "2015",
                "id": 269
            },
            {
                "year": "2013",
                "id": 152
            }
        ]
    },
    {
        "title": "Image Segmentation with Cascaded Hierarchical Models and Logistic Disjunctive Normal Networks",
        "authors": [
            "Mojtaba Seyedhosseini",
            "Mehdi Sajjadi",
            "Tolga Tasdizen"
        ],
        "abstract": "Contextual information plays an important role in solving vision problems such as image segmentation. However, extracting contextual information and using it in an effective way remains a difficult problem. To address this challenge, we propose a multi-resolution contextual framework, called cascaded hierarchical model (CHM), which learns contextual information in a hierarchical framework for image segmentation. At each level of the hierarchy, a classifier is trained based on down sampled input images and outputs of previous levels. Our model then incorporates the resulting multi-resolution contextual information into a classifier to segment the input image at original resolution. We repeat this procedure by cascading the hierarchical framework to improve the segmentation accuracy. Multiple classifiers are learned in the CHM, therefore, a fast and accurate classifier is required to make the training tractable. The classifier also needs to be robust against over fitting due to the large number of parameters learned during training. We introduce a novel classification scheme, called logistic disjunctive normal networks (LDNN), which consists of one adaptive layer of feature detectors implemented by logistic sigmoid functions followed by two fixed layers of logical units that compute conjunctions and disjunctions, respectively. We demonstrate that LDNN outperforms state-of-the-art classifiers and can be used in the CHM to improve object segmentation performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751380",
        "reference_list": [
            {
                "year": "2009",
                "id": 29
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 11,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Training",
                "Mathematical model",
                "Equations",
                "Image segmentation",
                "Image resolution",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image resolution",
                "image segmentation",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "cascaded hierarchical models",
                "logistic disjunctive normal networks",
                "vision problems",
                "contextual information extraction",
                "multiresolution contextual framework",
                "multiresolution contextual information",
                "feature detectors",
                "logistic sigmoid functions",
                "object segmentation"
            ],
            "Author Keywords": [
                "Contextual information",
                "Hierarchical models",
                "Image segmentation",
                "Disjunctive normal form"
            ]
        },
        "id": 270,
        "cited_by": [
            {
                "year": "2017",
                "id": 252
            },
            {
                "year": "2017",
                "id": 252
            },
            {
                "year": "2015",
                "id": 35
            }
        ]
    },
    {
        "title": "Semantic Segmentation without Annotating Segments",
        "authors": [
            "Wei Xia",
            "Csaba Domokos",
            "Jian Dong",
            "Loong-Fah Cheong",
            "Shuicheng Yan"
        ],
        "abstract": "Numerous existing object segmentation frameworks commonly utilize the object bounding box as a prior. In this paper, we address semantic segmentation assuming that object bounding boxes are provided by object detectors, but no training data with annotated segments are available. Based on a set of segment hypotheses, we introduce a simple voting scheme to estimate shape guidance for each bounding box. The derived shape guidance is used in the subsequent graph-cut-based figure-ground segmentation. The final segmentation result is obtained by merging the segmentation results in the bounding boxes. We conduct an extensive analysis of the effect of object bounding box accuracy. Comprehensive experiments on both the challenging PASCAL VOC object segmentation dataset and GrabCut-50 image segmentation dataset show that the proposed approach achieves competitive results compared to previous detection or bounding box prior based methods, as well as other state-of-the-art semantic segmentation methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751381",
        "reference_list": [
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2009",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 4,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image segmentation",
                "Detectors",
                "Semantics",
                "Training",
                "Accuracy",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic segmentation",
                "object segmentation frameworks",
                "object detectors",
                "voting scheme",
                "shape guidance estimation",
                "graph-cut-based figure-ground segmentation",
                "object bounding box accuracy",
                "PASCAL VOC object segmentation dataset",
                "GrabCut-50 image segmentation dataset"
            ]
        },
        "id": 271,
        "cited_by": [
            {
                "year": "2017",
                "id": 372
            },
            {
                "year": "2015",
                "id": 182
            },
            {
                "year": "2015",
                "id": 194
            }
        ]
    },
    {
        "title": "Progressive Multigrid Eigensolvers for Multiscale Spectral Segmentation",
        "authors": [
            "Michael Maire",
            "Stella X. Yu"
        ],
        "abstract": "We reexamine the role of multiscale cues in image segmentation using an architecture that constructs a globally coherent scale-space output representation. This characteristic is in contrast to many existing works on bottom-up segmentation, which prematurely compress information into a single scale. The architecture is a standard extension of Normalized Cuts from an image plane to an image pyramid, with cross-scale constraints enforcing consistency in the solution while allowing emergence of coarse-to-fine detail. We observe that multiscale processing, in addition to improving segmentation quality, offers a route by which to speed computation. We make a significant algorithmic advance in the form of a custom multigrid eigensolver for constrained Angular Embedding problems possessing coarse-to-fine structure. Multiscale Normalized Cuts is a special case. Our solver builds atop recent results on randomized matrix approximation, using a novel interpolation operation to mold its computational strategy according to cross-scale constraints in the problem definition. Applying our solver to multiscale segmentation problems demonstrates speedup by more than an order of magnitude. This speedup is at the algorithmic level and carries over to any implementation target.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751382",
        "reference_list": [
            {
                "year": "2009",
                "id": 306
            },
            {
                "year": "2011",
                "id": 272
            },
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Interpolation",
                "Sparse matrices",
                "Convergence",
                "Image segmentation",
                "Approximation algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "image representation",
                "image segmentation",
                "interpolation",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "progressive multigrid eigensolvers",
                "multiscale spectral segmentation",
                "image segmentation",
                "multiscale cues",
                "globally coherent scale-space output representation",
                "bottom-up segmentation",
                "image plane",
                "image pyramid",
                "cross-scale constraints",
                "multiscale processing",
                "constrained angular embedding problems",
                "multiscale normalized cuts",
                "randomized matrix approximation",
                "interpolation operation",
                "multiscale segmentation problems",
                "algorithmic level"
            ]
        },
        "id": 272,
        "cited_by": []
    },
    {
        "title": "Video Segmentation by Tracking Many Figure-Ground Segments",
        "authors": [
            "Fuxin Li",
            "Taeyoung Kim",
            "Ahmad Humayun",
            "David Tsai",
            "James M. Rehg"
        ],
        "abstract": "We propose an unsupervised video segmentation approach by simultaneously tracking multiple holistic figure-ground segments. Segment tracks are initialized from a pool of segment proposals generated from a figure-ground segmentation algorithm. Then, online non-local appearance models are trained incrementally for each track using a multi-output regularized least squares formulation. By using the same set of training examples for all segment tracks, a computational trick allows us to track hundreds of segment tracks efficiently, as well as perform optimal online updates in closed-form. Besides, a new composite statistical inference approach is proposed for refining the obtained segment tracks, which breaks down the initial segment proposals and recombines for better ones by utilizing high-order statistic estimates from the appearance model and enforcing temporal consistency. For evaluating the algorithm, a dataset, SegTrack v2, is collected with about 1,000 frames with pixel-level annotations. The proposed framework outperforms state-of-the-art approaches in the dataset, showing its efficiency and robustness to challenges in different video sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751383",
        "reference_list": [
            {
                "year": "2009",
                "id": 106
            },
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2011",
                "id": 253
            },
            {
                "year": "2009",
                "id": 99
            },
            {
                "year": "2009",
                "id": 186
            }
        ],
        "citation": {
            "ieee": 120,
            "other": 49,
            "total": 169
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Motion segmentation",
                "Training",
                "Predictive models",
                "Target tracking",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "higher order statistics",
                "image segmentation",
                "image sequences",
                "inference mechanisms",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised video segmentation approach",
                "multiple holistic figure-ground segment tracking",
                "online nonlocal appearance models",
                "multioutput regularized least squares formulation",
                "composite statistical inference approach",
                "high-order statistic estimates",
                "appearance model",
                "temporal consistency",
                "SegTrack v2",
                "video sequences"
            ],
            "Author Keywords": [
                "Video Segmentation",
                "CPMC",
                "tracking segments",
                "appearance model",
                "composite statistical inference",
                "CSI"
            ]
        },
        "id": 273,
        "cited_by": [
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2017",
                "id": 148
            },
            {
                "year": "2017",
                "id": 175
            },
            {
                "year": "2017",
                "id": 228
            },
            {
                "year": "2017",
                "id": 228
            },
            {
                "year": "2017",
                "id": 379
            },
            {
                "year": "2017",
                "id": 534
            },
            {
                "year": "2015",
                "id": 352
            },
            {
                "year": "2015",
                "id": 360
            },
            {
                "year": "2015",
                "id": 361
            },
            {
                "year": "2015",
                "id": 364
            },
            {
                "year": "2015",
                "id": 492
            },
            {
                "year": "2015",
                "id": 511
            },
            {
                "year": "2015",
                "id": 524
            }
        ]
    },
    {
        "title": "Transfer Feature Learning with Joint Distribution Adaptation",
        "authors": [
            "Mingsheng Long",
            "Jianmin Wang",
            "Guiguang Ding",
            "Jiaguang Sun",
            "Philip S. Yu"
        ],
        "abstract": "Transfer learning is established as an effective technology in computer vision for leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robust for substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751384",
        "reference_list": [
            {
                "year": "2011",
                "id": 236
            }
        ],
        "citation": {
            "ieee": 137,
            "other": 73,
            "total": 210
        },
        "keywords": {
            "IEEE Keywords": [
                "Equations",
                "Face",
                "Joints",
                "Principal component analysis",
                "Robustness",
                "Optimization",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image classification",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "transfer feature learning approach",
                "joint distribution adaptation",
                "computer vision",
                "source domain",
                "target domain",
                "marginal distribution",
                "conditional distribution",
                "JDA",
                "principled dimensionality reduction procedure",
                "feature representation",
                "substantial distribution difference",
                "cross-domain image classification problem"
            ],
            "Author Keywords": [
                "Transfer learning",
                "feature learning",
                "joint distribution adaptation"
            ]
        },
        "id": 274,
        "cited_by": [
            {
                "year": "2017",
                "id": 210
            },
            {
                "year": "2017",
                "id": 620
            },
            {
                "year": "2015",
                "id": 460
            }
        ]
    },
    {
        "title": "Manifold Based Face Synthesis from Sparse Samples",
        "authors": [
            "Hongteng Xu",
            "Hongyuan Zha"
        ],
        "abstract": "Data sparsity has been a thorny issue for manifold-based image synthesis, and in this paper we address this critical problem by leveraging ideas from transfer learning. Specifically, we propose methods based on generating auxiliary data in the form of synthetic samples using transformations of the original sparse samples. To incorporate the auxiliary data, we propose a weighted data synthesis method, which adaptively selects from the generated samples for inclusion during the manifold learning process via a weighted iterative algorithm. To demonstrate the feasibility of the proposed method, we apply it to the problem of face image synthesis from sparse samples. Compared with existing methods, the proposed method shows encouraging results with good performance improvements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751385",
        "reference_list": [
            {
                "year": "2007",
                "id": 190
            },
            {
                "year": "2007",
                "id": 204
            },
            {
                "year": "2011",
                "id": 331
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Face",
                "Nickel",
                "Noise measurement",
                "Vectors",
                "Image generation"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "manifold-based face synthesis",
                "sparse samples",
                "data sparsity",
                "manifold-based image synthesis",
                "transfer learning",
                "auxiliary data generation",
                "weighted data synthesis method",
                "adaptive selection",
                "inclusion",
                "manifold learning process",
                "weighted iterative algorithm"
            ],
            "Author Keywords": [
                "face synthesis",
                "manifold",
                "sparse samples",
                "transfer learning"
            ]
        },
        "id": 275,
        "cited_by": [
            {
                "year": "2015",
                "id": 483
            }
        ]
    },
    {
        "title": "Robust Dictionary Learning by Error Source Decomposition",
        "authors": [
            "Zhuoyuan Chen",
            "Ying Wu"
        ],
        "abstract": "Sparsity models have recently shown great promise in many vision tasks. Using a learned dictionary in sparsity models can in general outperform predefined bases in clean data. In practice, both training and testing data may be corrupted and contain noises and outliers. Although recent studies attempted to cope with corrupted data and achieved encouraging results in testing phase, how to handle corruption in training phase still remains a very difficult problem. In contrast to most existing methods that learn the dictionary from clean data, this paper is targeted at handling corruptions and outliers in training data for dictionary learning. We propose a general method to decompose the reconstructive residual into two components: a non-sparse component for small universal noises and a sparse component for large outliers, respectively. In addition, further analysis reveals the connection between our approach and the ``partial'' dictionary learning approach, updating only part of the prototypes (or informative code words) with remaining (or noisy code words) fixed. Experiments on synthetic data as well as real applications have shown satisfactory performance of this new robust dictionary learning approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751386",
        "reference_list": [
            {
                "year": "2011",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Robustness",
                "Laplace equations",
                "Encoding",
                "Gaussian noise",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "dictionaries",
                "image reconstruction",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust dictionary learning approach",
                "error source decomposition",
                "sparsity models",
                "reconstructive residual",
                "nonsparse component",
                "sparse component"
            ],
            "Author Keywords": [
                "robust statistics",
                "machine learning"
            ]
        },
        "id": 276,
        "cited_by": []
    },
    {
        "title": "Inferring \"Dark Matter\" and \"Dark Energy\" from Videos",
        "authors": [
            "Dan Xie",
            "Sinisa Todorovic",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper presents an approach to localizing functional objects in surveillance videos without domain knowledge about semantic object classes that may appear in the scene. Functional objects do not have discriminative appearance and shape, but they affect behavior of people in the scene. For example, they \"attract\" people to approach them for satisfying certain needs (e.g., vending machines could quench thirst), or \"repel\" people to avoid them (e.g., grass lawns). Therefore, functional objects can be viewed as \"dark matter\", emanating \"dark energy\" that affects people's trajectories in the video. To detect \"dark matter\" and infer their \"dark energy\" field, we extend the Lagrangian mechanics. People are treated as particle-agents with latent intents to approach \"dark matter\" and thus satisfy their needs, where their motions are subject to a composite \"dark energy\" field of all functional objects in the scene. We make the assumption that people take globally optimal paths toward the intended \"dark matter\" while avoiding latent obstacles. A Bayesian framework is used to probabilistically model: people's trajectories and intents, constraint map of the scene, and locations of functional objects. A data-driven Markov Chain Monte Carlo (MCMC) process is used for inference. Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in localizing functional objects and predicting people's trajectories in unobserved parts of the video footage.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751387",
        "reference_list": [
            {
                "year": "2011",
                "id": 78
            },
            {
                "year": "2011",
                "id": 61
            },
            {
                "year": "2011",
                "id": 131
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Videos",
                "Force",
                "Dark energy",
                "Proposals",
                "Probabilistic logic",
                "Surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "Markov processes",
                "Monte Carlo methods",
                "object recognition",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surveillance videos",
                "semantic object classes",
                "functional objects",
                "dark matter",
                "dark energy",
                "Lagrangian mechanics",
                "particle agents",
                "Bayesian framework",
                "data driven Markov Chain Monte Carlo process",
                "video footage"
            ]
        },
        "id": 277,
        "cited_by": [
            {
                "year": "2017",
                "id": 316
            },
            {
                "year": "2017",
                "id": 389
            }
        ]
    },
    {
        "title": "Video Co-segmentation for Meaningful Action Extraction",
        "authors": [
            "Jiaming Guo",
            "Zhuwen Li",
            "Loong-Fah Cheong",
            "Steven Zhiying Zhou"
        ],
        "abstract": "Given a pair of videos having a common action, our goal is to simultaneously segment this pair of videos to extract this common action. As a preprocessing step, we first remove background trajectories by a motion-based figure ground segmentation. To remove the remaining background and those extraneous actions, we propose the trajectory co saliency measure, which captures the notion that trajectories recurring in all the videos should have their mutual saliency boosted. This requires a trajectory matching process which can compare trajectories with different lengths and not necessarily spatiotemporally aligned, and yet be discriminative enough despite significant intra-class variation in the common action. We further leverage the graph matching to enforce geometric coherence between regions so as to reduce feature ambiguity and matching errors. Finally, to classify the trajectories into common action and action outliers, we formulate the problem as a binary labeling of a Markov Random Field, in which the data term is measured by the trajectory co-saliency and the smoothness term is measured by the spatiotemporal consistency between trajectories. To evaluate the performance of our framework, we introduce a dataset containing clips that have animal actions as well as human actions. Experimental results show that the proposed method performs well in common action extraction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751388",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2011",
                "id": 200
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 3,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Spatiotemporal phenomena",
                "Labeling",
                "Motion segmentation",
                "Coherence",
                "Histograms",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image segmentation",
                "Markov processes",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video cosegmentation",
                "meaningful action extraction",
                "motion-based figure ground segmentation",
                "trajectory cosaliency measure",
                "trajectory matching process",
                "graph matching",
                "geometric coherence",
                "feature ambiguity reduction",
                "matching error reduction",
                "binary labeling",
                "Markov random field",
                "common action extraction"
            ]
        },
        "id": 278,
        "cited_by": [
            {
                "year": "2017",
                "id": 227
            },
            {
                "year": "2015",
                "id": 494
            }
        ]
    },
    {
        "title": "Flattening Supervoxel Hierarchies by the Uniform Entropy Slice",
        "authors": [
            "Chenliang Xu",
            "Spencer Whitt",
            "Jason J. Corso"
        ],
        "abstract": "Supervoxel hierarchies provide a rich multiscale decomposition of a given video suitable for subsequent processing in video analysis. The hierarchies are typically computed by an unsupervised process that is susceptible to under-segmentation at coarse levels and over-segmentation at fine levels, which make it a challenge to adopt the hierarchies for later use. In this paper, we propose the first method to overcome this limitation and flatten the hierarchy into a single segmentation. Our method, called the uniform entropy slice, seeks a selection of supervoxels that balances the relative level of information in the selected supervoxels based on some post hoc feature criterion such as object-ness. For example, with this criterion, in regions nearby objects, our method prefers finer supervoxels to capture the local details, but in regions away from any objects we prefer coarser supervoxels. We formulate the uniform entropy slice as a binary quadratic program and implement four different feature criteria, both unsupervised and supervised, to drive the flattening. Although we apply it only to supervoxel hierarchies in this paper, our method is generally applicable to segmentation tree hierarchies. Our experiments demonstrate both strong qualitative performance and superior quantitative performance to state of the art baselines on benchmark internet videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751389",
        "reference_list": [
            {
                "year": "2009",
                "id": 106
            },
            {
                "year": "2011",
                "id": 253
            },
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2011",
                "id": 81
            },
            {
                "year": "2011",
                "id": 167
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 12,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Benchmark testing",
                "Image segmentation",
                "Motion segmentation",
                "Vectors",
                "Image coding",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "entropy",
                "image segmentation",
                "quadratic programming",
                "trees (mathematics)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "benchmark Internet videos",
                "segmentation tree hierarchy",
                "supervised feature criteria",
                "unsupervised feature criteria",
                "binary quadratic program",
                "hoc feature criterion",
                "supervoxel selection",
                "fine level",
                "coarse level",
                "unsupervised process",
                "video analysis",
                "video multiscale decomposition",
                "entropy slice uniformity",
                "supervoxel hierarchy flattening"
            ]
        },
        "id": 279,
        "cited_by": [
            {
                "year": "2015",
                "id": 352
            },
            {
                "year": "2015",
                "id": 358
            },
            {
                "year": "2015",
                "id": 511
            }
        ]
    },
    {
        "title": "From Actemes to Action: A Strongly-Supervised Representation for Detailed Action Understanding",
        "authors": [
            "Weiyu Zhang",
            "Menglong Zhu",
            "Konstantinos G. Derpanis"
        ],
        "abstract": "This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, key point labels (e.g., position) across space time are used in a data-driven training process to discover patches that are highly clustered in the space time key point configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of key points and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localization as additional output, which sheds further light into detailed action understanding.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751390",
        "reference_list": [
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2011",
                "id": 325
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 16,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatiotemporal phenomena",
                "Training",
                "Trajectory",
                "Visualization",
                "Semantics",
                "Cameras",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "image classification",
                "image representation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatiotemporal localization",
                "action classification",
                "sparse nonoverlapping detections",
                "sliding volume scheme",
                "action label",
                "consumer videos",
                "human action dataset",
                "spacetime key-point configuration space",
                "data-driven training process",
                "space time",
                "patch classifiers discovery",
                "action-related work",
                "actemes",
                "unconstrained video settings",
                "action understanding",
                "human actions analyzing",
                "supervised representation"
            ],
            "Author Keywords": [
                "action classification",
                "action detection"
            ]
        },
        "id": 280,
        "cited_by": [
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2017",
                "id": 461
            }
        ]
    },
    {
        "title": "From Semi-supervised to Transfer Counting of Crowds",
        "authors": [
            "Chen Change Loy",
            "Shaogang Gong",
            "Tao Xiang"
        ],
        "abstract": "Regression-based techniques have shown promising results for people counting in crowded scenes. However, most existing techniques require expensive and laborious data annotation for model training. In this study, we propose to address this problem from three perspectives: (1) Instead of exhaustively annotating every single frame, the most informative frames are selected for annotation automatically and actively. (2) Rather than learning from only labelled data, the abundant unlabelled data are exploited. (3) Labelled data from other scenes are employed to further alleviate the burden for data annotation. All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning, by exploiting the underlying geometric structure of crowd patterns via manifold analysis. Extensive experiments validate the effectiveness of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751391",
        "reference_list": [],
        "citation": {
            "ieee": 24,
            "other": 13,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Data models",
                "Feature extraction",
                "Labeling",
                "Training",
                "Computational modeling",
                "Laplace equations"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "learning (artificial intelligence)",
                "natural scenes",
                "regression analysis",
                "video retrieval",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "crowded scenes",
                "people counting",
                "frame selection",
                "frame annotation",
                "unlabelled data",
                "labelled data",
                "data annotation",
                "unified active semisupervised regression framework",
                "transfer learning",
                "crowd pattern geometric structure",
                "manifold analysis"
            ],
            "Author Keywords": [
                "visual surveillance",
                "crowd counting",
                "person counting",
                "semi-supervised",
                "regression"
            ]
        },
        "id": 281,
        "cited_by": [
            {
                "year": "2017",
                "id": 541
            },
            {
                "year": "2015",
                "id": 363
            },
            {
                "year": "2015",
                "id": 466
            }
        ]
    },
    {
        "title": "Learning to Share Latent Tasks for Action Recognition",
        "authors": [
            "Qiang Zhou",
            "Gang Wang",
            "Kui Jia",
            "Qi Zhao"
        ],
        "abstract": "Sharing knowledge for multiple related machine learning tasks is an effective strategy to improve the generalization performance. In this paper, we investigate knowledge sharing across categories for action recognition in videos. The motivation is that many action categories are related, where common motion pattern are shared among them (e.g. diving and high jump share the jump motion). We propose a new multi-task learning method to learn latent tasks shared across categories, and reconstruct a classifier for each category from these latent tasks. Compared to previous methods, our approach has two advantages: (1) The learned latent tasks correspond to basic motion patterns instead of full actions, thus enhancing discrimination power of the classifiers. (2) Categories are selected to share information with a sparsity regularizer, avoiding falsely forcing all categories to share knowledge. Experimental results on multiple public data sets show that the proposed approach can effectively transfer knowledge between different action categories to improve the performance of conventional single task learning methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751392",
        "reference_list": [
            {
                "year": "2011",
                "id": 168
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 9,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Optimization",
                "Videos",
                "Pattern recognition",
                "Data models",
                "Learning systems",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "image motion analysis",
                "image reconstruction",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "latent task sharing",
                "action recognition",
                "multiple-related machine learning tasks",
                "generalization performance",
                "knowledge sharing",
                "videos",
                "jump motion",
                "multitask learning method",
                "classifier reconstruction",
                "basic motion pattern",
                "classifier discrimination power enhancement",
                "sparsity regularizer",
                "action categories"
            ],
            "Author Keywords": [
                "Latent Task",
                "Action Recognition"
            ]
        },
        "id": 282,
        "cited_by": []
    },
    {
        "title": "Large-Scale Video Hashing via Structure Learning",
        "authors": [
            "Guangnan Ye",
            "Dong Liu",
            "Jun Wang",
            "Shih-Fu Chang"
        ],
        "abstract": "Recently, learning based hashing methods have become popular for indexing large-scale media data. Hashing methods map high-dimensional features to compact binary codes that are efficient to match and robust in preserving original similarity. However, most of the existing hashing methods treat videos as a simple aggregation of independent frames and index each video through combining the indexes of frames. The structure information of videos, e.g., discriminative local visual commonality and temporal consistency, is often neglected in the design of hash functions. In this paper, we propose a supervised method that explores the structure learning techniques to design efficient hash functions. The proposed video hashing method formulates a minimization problem over a structure-regularized empirical loss. In particular, the structure regularization exploits the common local visual patterns occurring in video frames that are associated with the same semantic class, and simultaneously preserves the temporal consistency over successive frames from the same video. We show that the minimization objective can be efficiently solved by an Accelerated Proximal Gradient (APG) method. Extensive experiments on two large video benchmark datasets (up to around 150K video clips with over 12 million frames) show that the proposed method significantly outperforms the state-of-the-art hashing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751393",
        "reference_list": [
            {
                "year": "2011",
                "id": 206
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 8,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Approximation methods",
                "Indexing",
                "Semantics",
                "Binary codes",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "cryptography",
                "gradient methods",
                "learning (artificial intelligence)",
                "minimisation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large-scale video hashing",
                "structure learning",
                "learning based hashing methods",
                "high-dimensional features",
                "compact binary codes",
                "discriminative local visual commonality",
                "temporal consistency",
                "hash functions",
                "supervised method",
                "structure learning techniques",
                "structure-regularized empirical loss",
                "minimization problem",
                "structure regularization",
                "local visual patterns",
                "video frames",
                "accelerated proximal gradient method"
            ],
            "Author Keywords": [
                "large-scale",
                "video hashing",
                "video structure"
            ]
        },
        "id": 283,
        "cited_by": [
            {
                "year": "2015",
                "id": 123
            }
        ]
    },
    {
        "title": "Finding Actors and Actions in Movies",
        "authors": [
            "P. Bojanowski",
            "F. Bach",
            "I. Laptev",
            "J. Ponce",
            "C. Schmid",
            "J. Sivic"
        ],
        "abstract": "We address the problem of learning a joint model of actors and actions in movies using weak supervision provided by scripts. Specifically, we extract actor/action pairs from the script and use them as constraints in a discriminative clustering framework. The corresponding optimization problem is formulated as a quadratic program under linear constraints. People in video are represented by automatically extracted and tracked faces together with corresponding motion features. First, we apply the proposed framework to the task of learning names of characters in the movie and demonstrate significant improvements over previous methods used for this task. Second, we explore the joint actor/action constraint and show its advantage for weakly supervised action learning. We validate our method in the challenging setting of localizing and recognizing characters and their actions in feature length movies Casablanca and American Beauty.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751394",
        "reference_list": [
            {
                "year": "2009",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 19,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion pictures",
                "Feature extraction",
                "Joints",
                "Kernel",
                "Tracking",
                "Optimization",
                "Facial features"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "gesture recognition",
                "image motion analysis",
                "image representation",
                "quadratic programming",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "actor-action joint model",
                "actor-action pair extraction",
                "discriminative clustering framework",
                "optimization problem",
                "quadratic program",
                "linear constraints",
                "video representation",
                "people representation",
                "face extraction",
                "face tracking",
                "motion features",
                "character name learning",
                "joint actor-action constraint",
                "weakly-supervised action learning",
                "character localization",
                "character recognition",
                "feature length movie",
                "Casablanca",
                "American Beauty"
            ]
        },
        "id": 284,
        "cited_by": [
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2017",
                "id": 224
            },
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2017",
                "id": 552
            },
            {
                "year": "2015",
                "id": 498
            },
            {
                "year": "2015",
                "id": 505
            }
        ]
    },
    {
        "title": "Hierarchical Data-Driven Descent for Efficient Optimal Deformation Estimation",
        "authors": [
            "Yuandong Tian",
            "Srinivasa G. Narasimhan"
        ],
        "abstract": "Real-world surfaces such as clothing, water and human body deform in complex ways. The image distortions observed are high-dimensional and non-linear, making it hard to estimate these deformations accurately. The recent data-driven descent approach applies Nearest Neighbor estimators iteratively on a particular distribution of training samples to obtain a globally optimal and dense deformation field between a template and a distorted image. In this work, we develop a hierarchical structure for the Nearest Neighbor estimators, each of which can have only a local image support. We demonstrate in both theory and practice that this algorithm has several advantages over the non-hierarchical version: it guarantees global optimality with significantly fewer training samples, is several orders faster, provides a metric to decide whether a given image is ``hard'' (or ``easy'') requiring more (or less) samples, and can handle more complex scenes that include both global motion and local deformation. The proposed algorithm successfully tracks a broad range of non-rigid scenes including water, clothing, and medical images, and compares favorably against several other deformation estimation and tracking approaches that do not provide optimality guarantees.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751395",
        "reference_list": [
            {
                "year": "2007",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Estimation",
                "Complexity theory",
                "Accuracy",
                "Cascading style sheets",
                "Clothing",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "data reduction",
                "deformation",
                "distortion",
                "iterative methods",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dimensionality reduction",
                "template image",
                "tracking approach",
                "medical images",
                "clothing",
                "water",
                "nonrigid scenes",
                "global motion",
                "local image support",
                "hierarchical structure",
                "dense deformation estimation",
                "training sample distribution",
                "nearest neighbor estimators",
                "image distortions",
                "optimal deformation estimation",
                "hierarchical data-driven descent approach"
            ],
            "Author Keywords": [
                "Hierarchical Model",
                "Data-Driven Approach",
                "Globally Optimal Solution",
                "Theoretical Analysis",
                "Nonconvex Optimization"
            ]
        },
        "id": 285,
        "cited_by": []
    },
    {
        "title": "Orderless Tracking through Model-Averaged Posterior Estimation",
        "authors": [
            "Seunghoon Hong",
            "Suha Kwak",
            "Bohyung Han"
        ],
        "abstract": "We propose a novel offline tracking algorithm based on model-averaged posterior estimation through patch matching across frames. Contrary to existing online and offline tracking methods, our algorithm is not based on temporally-ordered estimates of target state but attempts to select easy-to-track frames first out of the remaining ones without exploiting temporal coherency of target. The posterior of the selected frame is estimated by propagating densities from the already tracked frames in a recursive manner. The density propagation across frames is implemented by an efficient patch matching technique, which is useful for our algorithm since it does not require motion smoothness assumption. Also, we present a hierarchical approach, where a small set of key frames are tracked first and non-key frames are handled by local key frames. Our tracking algorithm is conceptually well-suited for the sequences with abrupt motion, shot changes, and occlusion. We compare our tracking algorithm with existing techniques in real videos with such challenges and illustrate its superior performance qualitatively and quantitatively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751396",
        "reference_list": [
            {
                "year": "2011",
                "id": 233
            },
            {
                "year": "2011",
                "id": 203
            },
            {
                "year": "2011",
                "id": 196
            },
            {
                "year": "2005",
                "id": 92
            },
            {
                "year": "2007",
                "id": 110
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 6,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Videos",
                "Density functional theory",
                "Computational modeling",
                "Estimation",
                "Hidden Markov models"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image matching",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "orderless tracking",
                "model averaged posterior estimation",
                "novel offline tracking algorithm",
                "offline tracking methods",
                "online tracking methods",
                "density propagation",
                "patch matching technique"
            ],
            "Author Keywords": [
                "offline tracking",
                "Bayesian model averaging"
            ]
        },
        "id": 286,
        "cited_by": [
            {
                "year": "2015",
                "id": 354
            }
        ]
    },
    {
        "title": "The Way They Move: Tracking Multiple Targets with Similar Appearance",
        "authors": [
            "Caglayan Dicle",
            "Octavia I. Camps",
            "Mario Sznaier"
        ],
        "abstract": "We introduce a computationally efficient algorithm for multi-object tracking by detection that addresses four main challenges: appearance similarity among targets, missing data due to targets being out of the field of view or occluded behind other objects, crossing trajectories, and camera motion. The proposed method uses motion dynamics as a cue to distinguish targets with similar appearance, minimize target mis-identification and recover missing data. Computational efficiency is achieved by using a Generalized Linear Assignment (GLA) coupled with efficient procedures to recover missing data and estimate the complexity of the underlying dynamics. The proposed approach works with track lets of arbitrary length and does not assume a dynamical model a priori, yet it captures the overall motion dynamics of the targets. Experiments using challenging videos show that this framework can handle complex target motions, non-stationary cameras and long occlusions, on scenarios where appearance cues are not available or poor.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751397",
        "reference_list": [
            {
                "year": "2011",
                "id": 17
            },
            {
                "year": "2009",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 78,
            "other": 37,
            "total": 115
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Trajectory",
                "Heuristic algorithms",
                "Dynamics",
                "IP networks",
                "Complexity theory"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "object detection",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonstationary cameras",
                "complex target motions",
                "target misidentification minimization",
                "missing data recovery",
                "generalized linear assignment",
                "computational efficiency",
                "multiple target tracking"
            ],
            "Author Keywords": [
                "Multitarget tracking",
                "tracking",
                "motion dynamics",
                "Hankel",
                "Rank Estimation",
                "Generalized Linear Assignment"
            ]
        },
        "id": 287,
        "cited_by": [
            {
                "year": "2017",
                "id": 31
            },
            {
                "year": "2017",
                "id": 508
            },
            {
                "year": "2015",
                "id": 487
            },
            {
                "year": "2015",
                "id": 488
            },
            {
                "year": "2015",
                "id": 525
            }
        ]
    },
    {
        "title": "Active MAP Inference in CRFs for Efficient Semantic Segmentation",
        "authors": [
            "Gemma Roig",
            "Xavier Boix",
            "Roderick De Nijs",
            "Sebastian Ramos",
            "Koljia Kuhnlenz",
            "Luc Van Gool"
        ],
        "abstract": "Most MAP inference algorithms for CRFs optimize an energy function knowing all the potentials. In this paper, we focus on CRFs where the computational cost of instantiating the potentials is orders of magnitude higher than MAP inference. This is often the case in semantic image segmentation, where most potentials are instantiated by slow classifiers fed with costly features. We introduce Active MAP inference 1) to on-the-fly select a subset of potentials to be instantiated in the energy function, leaving the rest of the parameters of the potentials unknown, and 2) to estimate the MAP labeling from such incomplete energy function. Results for semantic segmentation benchmarks, namely PASCAL VOC 2010 and MSRC-21, show that Active MAP inference achieves similar levels of accuracy but with major efficiency gains.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751398",
        "reference_list": [
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2011",
                "id": 1
            },
            {
                "year": "2011",
                "id": 272
            },
            {
                "year": "2011",
                "id": 24
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 5,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Semantics",
                "Computational modeling",
                "Silicon",
                "Inference algorithms",
                "Image segmentation",
                "Random variables"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "inference mechanisms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active MAP inference",
                "CRF",
                "conditional random fields",
                "energy function",
                "computational cost",
                "semantic image segmentation",
                "MAP labeling estimation",
                "PASCAL VOC 2010 benchmarks",
                "MSRC-21 benchmarks"
            ]
        },
        "id": 288,
        "cited_by": []
    },
    {
        "title": "Potts Model, Parametric Maxflow and K-Submodular Functions",
        "authors": [
            "Igor Gridchyn",
            "Vladimir Kolmogorov"
        ],
        "abstract": "The problem of minimizing the Potts energy function frequently occurs in computer vision applications. One way to tackle this NP-hard problem was proposed by Kovtun [19, 20]. It identifies a part of an optimal solution by running k maxflow computations, where k is the number of labels. The number of \"labeled\" pixels can be significant in some applications, e.g. 50-93% in our tests for stereo. We show how to reduce the runtime to O(log k) maxflow computations (or one parametric maxflow computation). Furthermore, the output of our algorithm allows to speed-up the subsequent alpha expansion for the unlabeled part, or can be used as it is for time-critical applications. To derive our technique, we generalize the algorithm of Felzenszwalb et al. [7] for Tree Metrics. We also show a connection to k-sub modular functions from combinatorial optimization, and discuss k-sub modular relaxations for general energy functions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751399",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 9,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Measurement",
                "Minimization",
                "Approximation algorithms",
                "Computer vision",
                "NP-hard problem",
                "Recycling"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "minimisation",
                "Potts model",
                "stereo image processing",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Potts model",
                "k-submodular functions",
                "Potts energy function minimization problem",
                "computer vision application",
                "NP-hard problem",
                "k-maxflow computation",
                "labeled pixel number",
                "stereo",
                "O(log k) maxflow computation",
                "parametric maxflow computation",
                "subsequent alpha expansion",
                "tree metrics",
                "combinatorial optimization",
                "k-submodular relaxation"
            ]
        },
        "id": 289,
        "cited_by": []
    },
    {
        "title": "Proportion Priors for Image Sequence Segmentation",
        "authors": [
            "Claudia Nieuwenhuis",
            "Evgeny Strekalovskiy",
            "Daniel Cremers"
        ],
        "abstract": "We propose a convex multilabel framework for image sequence segmentation which allows to impose proportion priors on object parts in order to preserve their size ratios across multiple images. The key idea is that for strongly deformable objects such as a gymnast the size ratio of respective regions (head versus torso, legs versus full body, etc.) is typically preserved. We propose different ways to impose such priors in a Bayesian framework for image segmentation. We show that near-optimal solutions can be computed using convex relaxation techniques. Extensive qualitative and quantitative evaluations demonstrate that the proportion priors allow for highly accurate segmentations, avoiding seeping-out of regions and preserving semantically relevant small-scale structures such as hands or feet. They naturally apply to multiple object instances such as players in sports scenes, and they can relate different objects instead of object parts, e.g. organs in medical imaging. The algorithm is efficient and easily parallelized leading to proportion-consistent segmentations at runtimes around one second.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751400",
        "reference_list": [
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2011",
                "id": 284
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image color analysis",
                "Image sequences",
                "Optimization",
                "Upper bound",
                "Shape",
                "Bayes methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image sequence segmentation",
                "convex multilabel framework",
                "proportion priors",
                "object parts",
                "size ratio preservation",
                "gymnast",
                "Bayesian framework",
                "convex relaxation technique",
                "small-scale structure preservation",
                "hands",
                "feet",
                "multiple-object instances",
                "players",
                "sports scenes",
                "medical imaging",
                "organs",
                "proportion-consistent segmentation"
            ],
            "Author Keywords": [
                "segmentation",
                "optimization",
                "sequences",
                "continuous",
                "proportion",
                "prior",
                "size shape",
                "multi-label",
                "constraint",
                "video"
            ]
        },
        "id": 290,
        "cited_by": [
            {
                "year": "2015",
                "id": 197
            },
            {
                "year": "2015",
                "id": 198
            },
            {
                "year": "2015",
                "id": 361
            }
        ]
    },
    {
        "title": "Tree Shape Priors with Connectivity Constraints Using Convex Relaxation on General Graphs",
        "authors": [
            "Jan St\u00fchmer",
            "Peter Schr\u00f6der",
            "Daniel Cremers"
        ],
        "abstract": "In this work we propose a novel method to include a connectivity prior into image segmentation that is based on a binary labeling of a directed graph, in this case a geodesic shortest path tree. Specifically we make two contributions: First, we construct a geodesic shortest path tree with a distance measure that is related to the image data and the bending energy of each path in the tree. Second, we include a connectivity prior in our segmentation model, that allows to segment not only a single elongated structure, but instead a whole connected branching tree. Because both our segmentation model and the connectivity constraint are convex a global optimal solution can be found. To this end, we generalize a recent primal-dual algorithm for continuous convex optimization to an arbitrary graph structure. To validate our method we present results on data from medical imaging in angiography and retinal blood vessel segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751401",
        "reference_list": [
            {
                "year": "2011",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Biomedical imaging",
                "Topology",
                "Labeling",
                "Blood vessels",
                "Approximation algorithms",
                "Convex functions"
            ],
            "INSPEC: Controlled Indexing": [
                "blood vessels",
                "computerised tomography",
                "differential geometry",
                "directed graphs",
                "eye",
                "image segmentation",
                "medical image processing",
                "relaxation theory",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "binary labeling",
                "directed graph",
                "geodesic shortest path tree",
                "distance measure",
                "image data",
                "bending energy",
                "connectivity prior",
                "image segmentation model",
                "single elongated structure",
                "connected branching tree",
                "convex connectivity constraint",
                "global optimal solution",
                "primal-dual algorithm",
                "continuous convex optimization",
                "arbitrary graph structure",
                "medical imaging",
                "angiography",
                "retinal blood vessel segmentation",
                "tree shape priors",
                "convex relaxation",
                "general graphs"
            ],
            "Author Keywords": [
                "Segmentation",
                "Optimization",
                "Medical Imaging"
            ]
        },
        "id": 291,
        "cited_by": []
    },
    {
        "title": "Revisiting the PnP Problem: A Fast, General and Optimal Solution",
        "authors": [
            "Yinqiang Zheng",
            "Yubin Kuang",
            "Shigeki Sugimoto",
            "Kalle \u00c5str\u00f6m",
            "Masatoshi Okutomi"
        ],
        "abstract": "In this paper, we revisit the classical perspective-n-point (PnP) problem, and propose the first non-iterative O(n) solution that is fast, generally applicable and globally optimal. Our basic idea is to formulate the PnP problem into a functional minimization problem and retrieve all its stationary points by using the Gr\"obner basis technique. The novelty lies in a non-unit quaternion representation to parameterize the rotation and a simple but elegant formulation of the PnP problem into an unconstrained optimization problem. Interestingly, the polynomial system arising from its first-order optimality condition assumes two-fold symmetry, a nice property that can be utilized to improve speed and numerical stability of a Grobner basis solver. Experiment results have demonstrated that, in terms of accuracy, our proposed solution is definitely better than the state-of-the-art O(n) methods, and even comparable with the reprojection error minimization method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751402",
        "reference_list": [
            {
                "year": "2011",
                "id": 48
            }
        ],
        "citation": {
            "ieee": 50,
            "other": 28,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Polynomials",
                "Three-dimensional displays",
                "Cameras",
                "Accuracy",
                "Quaternions",
                "Optimization",
                "Minimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image processing",
                "minimisation",
                "polynomials"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "PnP Problem",
                "first noniterative O(n) solution",
                "functional minimization problem",
                "Gr\"obner basis technique",
                "unconstrained optimization problem",
                "polynomial system",
                "first-order optimality condition",
                "two-fold symmetry",
                "Grobner basis solver",
                "perspective-n-point problem"
            ]
        },
        "id": 292,
        "cited_by": []
    },
    {
        "title": "Direct Optimization of Frame-to-Frame Rotation",
        "authors": [
            "Laurent Kneip",
            "Simon Lynen"
        ],
        "abstract": "This work makes use of a novel, recently proposed epipolar constraint for computing the relative pose between two calibrated images. By enforcing the coplanarity of epipolar plane normal vectors, it constrains the three degrees of freedom of the relative rotation between two camera views directly-independently of the translation. The present paper shows how the approach can be extended to n points, and translated into an efficient eigenvalue minimization over the three rotational degrees of freedom. Each iteration in the non-linear optimization has constant execution time, independently of the number of features. Two global optimization approaches are proposed. The first one consists of an efficient Levenberg-Marquardt scheme with randomized initial value, which already leads to stable and accurate results. The second scheme consists of a globally optimal branch-and-bound algorithm based on a bound on the eigenvalue variation derived from symmetric eigenvalue-perturbation theory. Analysis of the cost function reveals insights into the nature of a specific relative pose problem, and outlines the complexity under different conditions. The algorithm shows state-of-the-art performance w.r.t. essential-matrix based solutions, and a frame-to-frame application to a video sequence immediately leads to an alternative, real-time visual odometry solution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751403",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 5,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Eigenvalues and eigenfunctions",
                "Vectors",
                "Transmission line matrix methods",
                "Cost function",
                "Symmetric matrices",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "image sequences",
                "iterative methods",
                "matrix algebra",
                "minimisation",
                "nonlinear programming",
                "pose estimation",
                "tree searching",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "direct optimization",
                "frame-to-frame rotation",
                "epipolar constraint",
                "image calibration",
                "epipolar plane normal vector coplanarity",
                "relative rotation",
                "camera views",
                "eigenvalue minimization",
                "three-rotational degrees-of-freedom",
                "iteration",
                "nonlinear optimization",
                "constant execution time",
                "global optimization approach",
                "Levenberg-Marquardt scheme",
                "randomized initial value",
                "globally optimal branch-and-bound algorithm",
                "eigenvalue variation",
                "symmetric eigenvalue-perturbation theory",
                "cost function analysis",
                "specific relative pose problem",
                "essential-matrix-based solution",
                "frame-to-frame application",
                "video sequence",
                "real-time visual odometry solution"
            ],
            "Author Keywords": [
                "Geometric Vision",
                "Relative Pose Computation"
            ]
        },
        "id": 293,
        "cited_by": []
    },
    {
        "title": "PM-Huber: PatchMatch with Huber Regularization for Stereo Matching",
        "authors": [
            "Philipp Heise",
            "Sebastian Klose",
            "Brian Jensen",
            "Alois Knoll"
        ],
        "abstract": "Most stereo correspondence algorithms match support windows at integer-valued disparities and assume a constant disparity value within the support window. The recently proposed Patch Match stereo algorithm by Bleyer et al. overcomes this limitation of previous algorithms by directly estimating planes. This work presents a method that integrates the Patch Match stereo algorithm into a variational smoothing formulation using quadratic relaxation. The resulting algorithm allows the explicit regularization of the disparity and normal gradients using the estimated plane parameters. Evaluation of our method in the Middlebury benchmark shows that our method outperforms the traditional integer-valued disparity strategy as well as the original algorithm and its variants in sub-pixel accurate disparity estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751404",
        "reference_list": [
            {
                "year": "2011",
                "id": 295
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 14,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Stereo vision",
                "Smoothing methods",
                "Mathematical model",
                "Benchmark testing",
                "Equations",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "smoothing methods",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "PM-Huber",
                "Huber regularization",
                "integer-valued disparities",
                "stereo correspondence algorithms",
                "stereo matching algorithms",
                "match support windows",
                "PatchMatch stereo algorithm",
                "variational smoothing formulation",
                "quadratic relaxation",
                "estimated plane parameters",
                "Middlebury benchmark",
                "integer-valued disparity strategy",
                "sub-pixel accurate disparity estimation"
            ],
            "Author Keywords": [
                "subpixel stereo matching",
                "second-order prior",
                "variational formulation",
                "quadratic relaxation",
                "PatchMatch"
            ]
        },
        "id": 294,
        "cited_by": [
            {
                "year": "2017",
                "id": 475
            },
            {
                "year": "2015",
                "id": 87
            },
            {
                "year": "2015",
                "id": 97
            },
            {
                "year": "2015",
                "id": 98
            },
            {
                "year": "2015",
                "id": 229
            },
            {
                "year": "2015",
                "id": 447
            }
        ]
    },
    {
        "title": "Extrinsic Camera Calibration without a Direct View Using Spherical Mirror",
        "authors": [
            "Amit Agrawal"
        ],
        "abstract": "We consider the problem of estimating the extrinsic parameters (pose) of a camera with respect to a reference 3D object without a direct view. Since the camera does not view the object directly, previous approaches have utilized reflections in a planar mirror to solve this problem. However, a planar mirror based approach requires a minimum of three reflections and has degenerate configurations where estimation fails. In this paper, we show that the pose can be obtained using a single reflection in a spherical mirror of known radius. This makes our approach simpler and easier in practice. In addition, unlike planar mirrors, the spherical mirror based approach does not have any degenerate configurations, leading to a robust algorithm. While a planar mirror reflection results in a virtual perspective camera, a spherical mirror reflection results in a non-perspective axial camera. The axial nature of rays allows us to compute the axis (direction of sphere center) and few pose parameters in a linear fashion. We then derive an analytical solution to obtain the distance to the sphere center and remaining pose parameters and show that it corresponds to solving a 16th degree equation. We present comparisons with a recent method that use planar mirrors and show that our approach recovers more accurate pose in the presence of noise. Extensive simulations and results on real data validate our algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751405",
        "reference_list": [
            {
                "year": "2009",
                "id": 234
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 3,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Mirrors",
                "Cameras",
                "Three-dimensional displays",
                "Calibration",
                "Transmission line matrix methods",
                "Equations",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "mirrors",
                "parameter estimation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "extrinsic camera calibration",
                "spherical mirror single reflection",
                "reference 3D object",
                "planar mirror based approach",
                "virtual perspective camera",
                "planar mirror reflection",
                "nonperspective axial camera",
                "pose parameter estimation"
            ],
            "Author Keywords": [
                "calibration",
                "spherical mirror",
                "axial",
                "catadioptirc",
                "mirrors"
            ]
        },
        "id": 295,
        "cited_by": []
    },
    {
        "title": "Robust Non-parametric Data Fitting for Correspondence Modeling",
        "authors": [
            "Wen-Yan Lin",
            "Ming-Ming Cheng",
            "Shuai Zheng",
            "Jiangbo Lu",
            "Nigel Crook"
        ],
        "abstract": "We propose a generic method for obtaining nonparametric image warps from noisy point correspondences. Our formulation integrates a huber function into a motion coherence framework. This makes our fitting function especially robust to piecewise correspondence noise (where an image section is consistently mismatched). By utilizing over parameterized curves, we can generate realistic nonparametric image warps from very noisy correspondence. We also demonstrate how our algorithm can be used to help stitch images taken from a panning camera by warping the images onto a virtual push-broom camera imaging plane.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751406",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Noise",
                "Splines (mathematics)",
                "Noise measurement",
                "Coherence",
                "Minimization",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "curve fitting",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust nonparametric data fitting",
                "correspondence modeling",
                "generic method",
                "nonparametric image warps",
                "noisy point correspondence",
                "huber function",
                "motion coherence framework",
                "fitting function",
                "piecewise correspondence noise",
                "image section",
                "parameterized curves",
                "image stitching",
                "panning camera",
                "virtual push-broom camera imaging plane"
            ],
            "Author Keywords": [
                "matching",
                "warping",
                "curve fitting",
                "spline",
                "non-parametric"
            ]
        },
        "id": 296,
        "cited_by": []
    },
    {
        "title": "Deblurring by Example Using Dense Correspondence",
        "authors": [
            "Yoav Hacohen",
            "Eli Shechtman",
            "Dani Lischinski"
        ],
        "abstract": "This paper presents a new method for deblurring photos using a sharp reference example that contains some shared content with the blurry photo. Most previous deblurring methods that exploit information from other photos require an accurately registered photo of the same static scene. In contrast, our method aims to exploit reference images where the shared content may have undergone substantial photometric and non-rigid geometric transformations, as these are the kind of reference images most likely to be found in personal photo albums. Our approach builds upon a recent method for example-based deblurring using non-rigid dense correspondence (NRDC) [HaCohen et al. 2011] and extends it in two ways. First, we suggest exploiting information from the reference image not only for blur kernel estimation, but also as a powerful local prior for the non-blind deconvolution step. Second, we introduce a simple yet robust technique for spatially varying blur estimation, rather than assuming spatially uniform blur. Unlike the above previous method, which has proven successful only with simple deblurring scenarios, we demonstrate that our method succeeds on a variety of real-world examples. We provide quantitative and qualitative evaluation of our method and show that it outperforms the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751407",
        "reference_list": [
            {
                "year": "2011",
                "id": 58
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 14,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Deconvolution",
                "Cameras",
                "Estimation",
                "Mathematical model",
                "Image reconstruction",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "deconvolution",
                "image registration",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photo deblurring",
                "sharp reference example",
                "photo registration",
                "deblurring method",
                "static scene",
                "shared content",
                "substantial photometric transformation",
                "nonrigid geometric transformation",
                "personal photo albums",
                "example-based deblurring",
                "nonrigid dense correspondence",
                "NRDC",
                "reference image",
                "blur kernel estimation",
                "nonblind deconvolution step",
                "spatial varying blur estimation",
                "deblurring scenario"
            ],
            "Author Keywords": [
                "deblurring",
                "image restoration"
            ]
        },
        "id": 297,
        "cited_by": [
            {
                "year": "2017",
                "id": 112
            }
        ]
    },
    {
        "title": "On One-Shot Similarity Kernels: Explicit Feature Maps and Properties",
        "authors": [
            "Stefanos Zafeiriou",
            "Irene Kotsia"
        ],
        "abstract": "Kernels have been a common tool of machine learning and computer vision applications for modeling non-linearities and/or the design of robust Robustness may refer to either the presence of outliers and noise or to the robustness to a class of transformations (e.g., translation). similarity measures between objects. Arguably, the class of positive semi-definite (psd) kernels, widely known as Mercer's Kernels, constitutes one of the most well-studied cases. For every psd kernel there exists an associated feature map to an arbitrary dimensional Hilbert space mathcal H, the so-called feature space. The main reason behind psd kernels' popularity is the fact that classification/regression techniques (such as Support Vector Machines (SVMs)) and component analysis algorithms (such as Kernel Principal Component Analysis (KPCA)) can be devised in mathcal H, without an explicit definition of the feature map, only by using the kernel (the so-called kernel trick). Recently, due to the development of very efficient solutions for large scale linear SVMs and for incremental linear component analysis, the research towards finding feature map approximations for classes of kernels has attracted significant interest. In this paper, we attempt the derivation of explicit feature maps of a recently proposed class of kernels, the so-called one-shot similarity kernels. We show that for this class of kernels either there exists an explicit representation in feature space or the kernel can be expressed in such a form that allows for exact incremental learning. We theoretically explore the properties of these kernels and show how these kernels can be used for the development of robust visual tracking, recognition and deformable fitting algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751408",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Face",
                "Approximation methods",
                "Principal component analysis",
                "Support vector machines",
                "Approximation algorithms",
                "Face recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Hilbert spaces",
                "learning (artificial intelligence)",
                "principal component analysis",
                "regression analysis",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "one-shot similarity kernels",
                "explicit feature map approximation",
                "machine learning",
                "computer vision",
                "robust similarity measure design",
                "Mercer kernels",
                "positive semidefinite kernels",
                "arbitrary dimensional Hilbert space",
                "feature space",
                "psd kernel popularity",
                "classification-regression techniques",
                "support vector machines",
                "SVMs",
                "component analysis algorithms",
                "kernel principal component analysis",
                "KPCA",
                "kernel trick",
                "large scale linear SVMs",
                "incremental linear component analysis",
                "exact incremental learning",
                "robust visual tracking",
                "deformable fitting algorithms",
                "visual recognition"
            ],
            "Author Keywords": [
                "face recognition",
                "kernel learning",
                "face tracking"
            ]
        },
        "id": 298,
        "cited_by": []
    },
    {
        "title": "Facial Action Unit Event Detection by Cascade of Tasks",
        "authors": [
            "Xiaoyu Ding",
            "Wen-Sheng Chu",
            "Fernando De La Torre",
            "Jeffery F. Cohn",
            "Qiao Wang"
        ],
        "abstract": "Automatic facial Action Unit (AU) detection from video is a long-standing problem in facial expression analysis. AU detection is typically posed as a classification problem between frames or segments of positive examples and negative ones, where existing work emphasizes the use of different features or classifiers. In this paper, we propose a method called Cascade of Tasks (CoT) that combines the use of different tasks (i.e., frame, segment and transition) for AU event detection. We train CoT in a sequential manner embracing diversity, which ensures robustness and generalization to unseen data. In addition to conventional frame based metrics that evaluate frames independently, we propose a new event-based metric to evaluate detection performance at event-level. We show how the CoT method consistently outperforms state-of-the-art approaches in both frame-based and event-based metrics, across three public datasets that differ in complexity: CK+, FERA and RU-FACS.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751409",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 5,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Gold",
                "Detectors",
                "Feature extraction",
                "Support vector machines",
                "Measurement",
                "Training",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial action unit event detection",
                "AU detection",
                "facial expression analysis",
                "Cascade of Tasks",
                "CoT",
                "public datasets"
            ]
        },
        "id": 299,
        "cited_by": [
            {
                "year": "2015",
                "id": 404
            }
        ]
    },
    {
        "title": "Similarity Metric Learning for Face Recognition",
        "authors": [
            "Qiong Cao",
            "Yiming Ying",
            "Peng Li"
        ],
        "abstract": "Recently, there is a considerable amount of efforts devoted to the problem of unconstrained face verification, where the task is to predict whether pairs of images are from the same person or not. This problem is challenging and difficult due to the large variations in face images. In this paper, we develop a novel regularization framework to learn similarity metrics for unconstrained face verification. We formulate its objective function by incorporating the robustness to the large intra-personal variations and the discriminative power of novel similarity metrics. In addition, our formulation is a convex optimization problem which guarantees the existence of its global solution. Experiments show that our proposed method achieves the state-of-the-art results on the challenging Labeled Faces in the Wild (LFW) database [10].",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751410",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 73,
            "other": 30,
            "total": 103
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Face",
                "Principal component analysis",
                "Robustness",
                "Learning systems",
                "Vectors",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "face recognition",
                "learning (artificial intelligence)",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "similarity metric learning",
                "face recognition",
                "unconstrained face verification",
                "regularization framework",
                "objective function",
                "large intra-personal variations",
                "convex optimization problem",
                "global solution",
                "Labeled Faces in the Wild database",
                "LFW database"
            ],
            "Author Keywords": [
                "unconstrained face recognition",
                "metric learning",
                "convex optimization"
            ]
        },
        "id": 300,
        "cited_by": [
            {
                "year": "2015",
                "id": 462
            },
            {
                "year": "2015",
                "id": 476
            }
        ]
    },
    {
        "title": "Random Faces Guided Sparse Many-to-One Encoder for Pose-Invariant Face Recognition",
        "authors": [
            "Yizhe Zhang",
            "Ming Shao",
            "Edward K. Wong",
            "Yun Fu"
        ],
        "abstract": "One of the most challenging task in face recognition is to identify people with varied poses. Namely, the test faces have significantly different poses compared with the registered faces. In this paper, we propose a high-level feature learning scheme to extract pose-invariant identity feature for face recognition. First, we build a single-hidden-layer neural network with sparse constraint, to extract pose-invariant feature in a supervised fashion. Second, we further enhance the discriminative capability of the proposed feature by using multiple random faces as the target values for multiple encoders. By enforcing the target values to be unique for input faces over different poses, the learned high-level feature that is represented by the neurons in the hidden layer is pose free and only relevant to the identity information. Finally, we conduct face identification on CMU Multi-PIE, and verification on Labeled Faces in the Wild (LFW) databases, where identification rank-1 accuracy and face verification accuracy with ROC curve are reported. These experiments demonstrate that our model is superior to other state-of-the-art approaches on handling pose variations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751411",
        "reference_list": [
            {
                "year": "2011",
                "id": 118
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 11,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Face recognition",
                "Training",
                "Three-dimensional displays",
                "Biological neural networks",
                "Analytical models",
                "Transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "neural nets",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "random faces guided sparse many-to-one encoder",
                "pose-invariant face recognition",
                "high-level feature learning scheme",
                "pose-invariant identity feature extraction",
                "single-hidden-layer neural network",
                "multiple random face",
                "identity information",
                "face identification",
                "CMU MultiPIE",
                "LFW database",
                "Labeled Faces in the Wild database",
                "ROC curve",
                "pose variation handling"
            ],
            "Author Keywords": [
                "Face Recognition",
                "Pose-Invariant",
                "Sparse",
                "Many-to-One Encoder",
                "Random Faces"
            ]
        },
        "id": 301,
        "cited_by": [
            {
                "year": "2015",
                "id": 428
            }
        ]
    },
    {
        "title": "Breaking the Chain: Liberation from the Temporal Markov Assumption for Tracking Human Poses",
        "authors": [
            "Ryan Tokola",
            "Wongun Choi",
            "Silvio Savarese"
        ],
        "abstract": "We present an approach to multi-target tracking that has expressive potential beyond the capabilities of chain-shaped hidden Markov models, yet has significantly reduced complexity. Our framework, which we call tracking-by-selection, is similar to tracking-by-detection in that it separates the tasks of detection and tracking, but it shifts temporal reasoning from the tracking stage to the detection stage. The core feature of tracking-by-selection is that it reasons about path hypotheses that traverse the entire video instead of a chain of single-frame object hypotheses. A traditional chain-shaped tracking-by-detection model is only able to promote consistency between one frame and the next. In tracking-by-selection, path hypotheses exist across time, and encouraging long-term temporal consistency is as simple as rewarding path hypotheses with consistent image features. One additional advantage of tracking-by-selection is that it results in a dramatically simplified model that can be solved exactly. We adapt an existing tracking-by-detection model to the tracking-by-selection framework, and show improved performance on a challenging dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751412",
        "reference_list": [
            {
                "year": "2011",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 3,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Joints",
                "Adaptation models",
                "Target tracking",
                "Elbow",
                "Trajectory"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden Markov models",
                "pose estimation",
                "target tracking",
                "temporal reasoning",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporal Markov assumption",
                "human pose tracking",
                "multitarget tracking",
                "chain-shaped hidden Markov models",
                "tracking-by-selection",
                "detection task",
                "temporal reasoning",
                "video",
                "chain-shaped tracking-by-detection model",
                "long-term temporal consistency",
                "path hypothesis rewarding"
            ],
            "Author Keywords": [
                "tracking",
                "pose estimation"
            ]
        },
        "id": 302,
        "cited_by": [
            {
                "year": "2015",
                "id": 224
            }
        ]
    },
    {
        "title": "Exploiting Reflection Change for Automatic Reflection Removal",
        "authors": [
            "Yu Li",
            "Michael S. Brown"
        ],
        "abstract": "This paper introduces an automatic method for removing reflection interference when imaging a scene behind a glass surface. Our approach exploits the subtle changes in the reflection with respect to the background in a small set of images taken at slightly different view points. Key to this idea is the use of SIFT-flow to align the images such that a pixel-wise comparison can be made across the input set. Gradients with variation across the image set are assumed to belong to the reflected scenes while constant gradients are assumed to belong to the desired background scene. By correctly labelling gradients belonging to reflection or background, the background scene can be separated from the reflection interference. Unlike previous approaches that exploit motion, our approach does not make any assumptions regarding the background or reflected scenes' geometry, nor requires the reflection to be static. This makes our approach practical for use in casual imaging scenarios. Our approach is straight forward and produces good results compared with existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751413",
        "reference_list": [
            {
                "year": "2005",
                "id": 3
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 41,
            "other": 8,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Image reconstruction",
                "Interference",
                "Glass",
                "Cameras",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sequences",
                "interference suppression",
                "reflection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "reflected scene geometry",
                "background scene",
                "image set",
                "SIFT-flow",
                "glass surface",
                "reflection interference removal",
                "automatic reflection removal"
            ]
        },
        "id": 303,
        "cited_by": [
            {
                "year": "2017",
                "id": 254
            },
            {
                "year": "2017",
                "id": 341
            },
            {
                "year": "2017",
                "id": 413
            }
        ]
    },
    {
        "title": "Shape Index Descriptors Applied to Texture-Based Galaxy Analysis",
        "authors": [
            "Kim Steenstrup Pedersen",
            "Kristoffer Stensbo-Smidt",
            "Andrew Zirm",
            "Christian Igel"
        ],
        "abstract": "A texture descriptor based on the shape index and the accompanying curvedness measure is proposed, and it is evaluated for the automated analysis of astronomical image data. A representative sample of images of low-red shift galaxies from the Sloan Digital Sky Survey (SDSS) serves as a test bed. The goal of applying texture descriptors to these data is to extract novel information about galaxies, information which is often lost in more traditional analysis. In this study, we build a regression model for predicting a spectroscopic quantity, the specific star-formation rate (sSFR). As texture features we consider multi-scale gradient orientation histograms as well as multi-scale shape index histograms, which lead to a new descriptor. Our results show that we can successfully predict spectroscopic quantities from the texture in optical multi-band images. We successfully recover the observed bi-modal distribution of galaxies into quiescent and star-forming. The state-of-the-art for predicting the sSFR is a color-based physical model. We significantly improve its accuracy by augmenting the model with texture information. This study is the first step towards enabling the quantification of physical galaxy properties from imaging data alone.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751414",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 5,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Shape",
                "Indexes",
                "Imaging",
                "Image color analysis",
                "Noise",
                "Apertures"
            ],
            "INSPEC: Controlled Indexing": [
                "astronomy computing",
                "galaxies",
                "gradient methods",
                "image colour analysis",
                "image texture",
                "regression analysis",
                "spectroscopy"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape index descriptors",
                "texture-based galaxy analysis",
                "texture descriptor",
                "astronomical image data",
                "low-red shift galaxies",
                "Sloan Digital Sky Survey",
                "SDSS",
                "regression model",
                "spectroscopic quantity",
                "specific star-formation rate",
                "sSFR",
                "multiscale gradient orientation histograms",
                "multiscale shape index histograms",
                "optical multiband images",
                "bimodal distribution",
                "color-based physical model"
            ],
            "Author Keywords": [
                "Texture",
                "Image Features",
                "Analysis of Galaxy Images"
            ]
        },
        "id": 304,
        "cited_by": []
    },
    {
        "title": "Robust Tucker Tensor Decomposition for Effective Image Representation",
        "authors": [
            "Miao Zhang",
            "Chris Ding"
        ],
        "abstract": "Many tensor based algorithms have been proposed for the study of high dimensional data in a large variety of computer vision and machine learning applications. However, most of the existing tensor analysis approaches are based on Frobenius norm, which makes them sensitive to outliers, because they minimize the sum of squared errors and enlarge the influence of both outliers and large feature noises. In this paper, we propose a robust Tucker tensor decomposition model (RTD) to suppress the influence of outliers, which uses L 1 -norm loss function. Yet, the optimization on L1-norm based tensor analysis is much harder than standard tensor decomposition. In this paper, we propose a simple and efficient algorithm to solve our RTD model. Moreover, tensor factorization-based image storage needs much less space than PCA based methods. We carry out extensive experiments to evaluate the proposed algorithm, and verify the robustness against image occlusions. Both numerical and visual results show that our RTD model is consistently better against the existence of outliers than previous tensor and PCA methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751415",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Robustness",
                "Algorithm design and analysis",
                "Principal component analysis",
                "Vectors",
                "Matrix decomposition",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "matrix decomposition",
                "minimisation",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "outlier noise",
                "feature noise",
                "RTD model",
                "L1-norm loss function",
                "L1-norm based tensor analysis",
                "standard tensor decomposition",
                "tensor factorization-based image storage",
                "PCA-based method",
                "image occlusion",
                "squared error sum minimization",
                "Frobenius norm",
                "tensor analysis approach",
                "machine learning application",
                "computer vision",
                "high-dimensional data",
                "tensor-based algorithm",
                "effective image representation",
                "robust tucker tensor decomposition model"
            ]
        },
        "id": 305,
        "cited_by": []
    },
    {
        "title": "Interactive Markerless Articulated Hand Motion Tracking Using RGB and Depth Data",
        "authors": [
            "Srinath Sridhar",
            "Antti Oulasvirta",
            "Christian Theobalt"
        ],
        "abstract": "Tracking the articulated 3D motion of the hand has important applications, for example, in human-computer interaction and teleoperation. We present a novel method that can capture a broad range of articulated hand motions at interactive rates. Our hybrid approach combines, in a voting scheme, a discriminative, part-based pose retrieval method with a generative pose estimation method based on local optimization. Color information from a multi-view RGB camera setup along with a person-specific hand model are used by the generative method to find the pose that best explains the observed images. In parallel, our discriminative pose estimation method uses fingertips detected on depth data to estimate a complete or partial pose of the hand by adopting a part-based pose retrieval strategy. This part-based strategy helps reduce the search space drastically in comparison to a global pose retrieval strategy. Quantitative results show that our method achieves state-of-the-art accuracy on challenging sequences and a near-real time performance of 10 fps on a desktop computer.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751416",
        "reference_list": [
            {
                "year": "2011",
                "id": 138
            },
            {
                "year": "2009",
                "id": 189
            },
            {
                "year": "2011",
                "id": 265
            },
            {
                "year": "2011",
                "id": 120
            },
            {
                "year": "2001",
                "id": 161
            },
            {
                "year": "2011",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 34,
            "total": 74
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Tracking",
                "Three-dimensional displays",
                "Estimation",
                "Cameras",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "human computer interaction",
                "image colour analysis",
                "image motion analysis",
                "image retrieval",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interactive markerless articulated hand motion tracking",
                "depth data",
                "articulated 3D motion",
                "human-computer interaction",
                "teleoperation",
                "articulated hand motions",
                "interactive rates",
                "part-based pose retrieval method",
                "generative pose estimation method",
                "local optimization",
                "color information",
                "multiview RGB camera",
                "person-specific hand model",
                "discriminative pose estimation method",
                "fingertips detection",
                "pose estimation",
                "part-based pose retrieval strategy",
                "search space",
                "global pose retrieval strategy",
                "desktop computer",
                "near-real time performance"
            ],
            "Author Keywords": [
                "hand tracking",
                "RGB",
                "depth",
                "time-of-flight",
                "3D pose estimation",
                "sum of gaussians",
                "markerless tracking",
                "articulation"
            ]
        },
        "id": 306,
        "cited_by": [
            {
                "year": "2017",
                "id": 267
            },
            {
                "year": "2015",
                "id": 91
            },
            {
                "year": "2015",
                "id": 208
            },
            {
                "year": "2015",
                "id": 370
            }
        ]
    },
    {
        "title": "Video Motion for Every Visible Point",
        "authors": [
            "Susanna Ricco",
            "Carlo Tomasi"
        ],
        "abstract": "Dense motion of image points over many video frames can provide important information about the world. However, occlusions and drift make it impossible to compute long motion paths by merely concatenating optical flow vectors between consecutive frames. Instead, we solve for entire paths directly, and flag the frames in which each is visible. As in previous work, we anchor each path to a unique pixel which guarantees an even spatial distribution of paths. Unlike earlier methods, we allow paths to be anchored in any frame. By explicitly requiring that at least one visible path passes within a small neighborhood of every pixel, we guarantee complete coverage of all visible points in all frames. We achieve state-of-the-art results on real sequences including both rigid and non-rigid motions with significant occlusions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751417",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Optimization",
                "Optical imaging",
                "Motion estimation",
                "Estimation",
                "Vectors",
                "Subspace constraints"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video motion",
                "image point dense motion",
                "video frames",
                "occlusions",
                "drift",
                "motion path",
                "optical flow vectors",
                "consecutive frames",
                "frame flagging",
                "path spatial distribution",
                "pixel neighborhood",
                "real sequence",
                "rigid motion",
                "nonrigid motion"
            ]
        },
        "id": 307,
        "cited_by": [
            {
                "year": "2015",
                "id": 334
            }
        ]
    },
    {
        "title": "EVSAC: Accelerating Hypotheses Generation by Modeling Matching Scores with Extreme Value Theory",
        "authors": [
            "Victor Fragoso",
            "Pradeep Sen",
            "Sergio Rodriguez",
            "Matthew Turk"
        ],
        "abstract": "Algorithms based on RANSAC that estimate models using feature correspondences between images can slow down tremendously when the percentage of correct correspondences (inliers) is small. In this paper, we present a probabilistic parametric model that allows us to assign confidence values for each matching correspondence and therefore accelerates the generation of hypothesis models for RANSAC under these conditions. Our framework leverages Extreme Value Theory to accurately model the statistics of matching scores produced by a nearest-neighbor feature matcher. Using a new algorithm based on this model, we are able to estimate accurate hypotheses with RANSAC at low inlier ratios significantly faster than previous state-of-the-art approaches, while still performing comparably when the number of inliers is large. We present results of homography and fundamental matrix estimation experiments for both SIFT and SURF matches that demonstrate that our method leads to accurate and fast model estimations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751418",
        "reference_list": [
            {
                "year": "2009",
                "id": 216
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 9,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Estimation",
                "Probabilistic logic",
                "Acceleration",
                "Measurement",
                "Accuracy",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "matrix algebra",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SURF",
                "SIFT",
                "matrix estimation",
                "nearest-neighbor feature matcher",
                "probabilistic parametric model",
                "RANSAC",
                "extreme value theory",
                "matching score modeling",
                "accelerating hypotheses generation",
                "EVSAC"
            ],
            "Author Keywords": [
                "extreme value theory",
                "ransac",
                "robust estimation"
            ]
        },
        "id": 308,
        "cited_by": []
    },
    {
        "title": "PixelTrack: A Fast Adaptive Algorithm for Tracking Non-rigid Objects",
        "authors": [
            "Stefan Duffner",
            "Christophe Garcia"
        ],
        "abstract": "In this paper, we present a novel algorithm for fast tracking of generic objects in videos. The algorithm uses two components: a detector that makes use of the generalised Hough transform with pixel-based descriptors, and a probabilistic segmentation method based on global models for foreground and background. These components are used for tracking in a combined way, and they adapt each other in a co-training manner. Through effective model adaptation and segmentation, the algorithm is able to track objects that undergo rigid and non-rigid deformations and considerable shape and appearance variations. The proposed tracking method has been thoroughly evaluated on challenging standard videos, and outperforms state-of-the-art tracking methods designed for the same task. Finally, the proposed models allow for an extremely efficient implementation, and thus tracking is very fast.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751419",
        "reference_list": [
            {
                "year": "2009",
                "id": 196
            },
            {
                "year": "2011",
                "id": 10
            },
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2011",
                "id": 167
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 27,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Image segmentation",
                "Videos",
                "Detectors",
                "Image color analysis",
                "Robustness",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "Hough transforms",
                "image segmentation",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast adaptive algorithm",
                "PixelTrack",
                "generic object tracking",
                "generalised Hough transform",
                "pixel-based descriptors",
                "probabilistic segmentation method"
            ],
            "Author Keywords": [
                "object tracking"
            ]
        },
        "id": 309,
        "cited_by": [
            {
                "year": "2017",
                "id": 576
            },
            {
                "year": "2015",
                "id": 336
            },
            {
                "year": "2015",
                "id": 341
            }
        ]
    },
    {
        "title": "Unifying Nuclear Norm and Bilinear Factorization Approaches for Low-Rank Matrix Decomposition",
        "authors": [
            "Ricardo Cabral",
            "Fernando De la Torre",
            "Jo\u00e3o P. Costeira",
            "Alexandre Bernardino"
        ],
        "abstract": "Low rank models have been widely used for the representation of shape, appearance or motion in computer vision problems. Traditional approaches to fit low rank models make use of an explicit bilinear factorization. These approaches benefit from fast numerical methods for optimization and easy kernelization. However, they suffer from serious local minima problems depending on the loss function and the amount/type of missing data. Recently, these low-rank models have alternatively been formulated as convex problems using the nuclear norm regularizer, unlike factorization methods, their numerical solvers are slow and it is unclear how to kernelize them or to impose a rank a priori. This paper proposes a unified approach to bilinear factorization and nuclear norm regularization, that inherits the benefits of both. We analyze the conditions under which these approaches are equivalent. Moreover, based on this analysis, we propose a new optimization algorithm and a \"rank continuation'' strategy that outperform state-of-the-art approaches for Robust PCA, Structure from Motion and Photometric Stereo with outliers and missing data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751420",
        "reference_list": [
            {
                "year": "2011",
                "id": 318
            },
            {
                "year": "2011",
                "id": 310
            },
            {
                "year": "2011",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 41,
            "other": 18,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Computer vision",
                "Robustness",
                "Algorithm design and analysis",
                "Optimization",
                "Principal component analysis",
                "Numerical models"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "matrix decomposition",
                "optimisation",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bilinear factorization",
                "nuclear norm approach",
                "low-rank matrix decomposition",
                "nuclear norm regularization",
                "optimization algorithm",
                "rank continuation",
                "robust PCA",
                "photometric stereo"
            ]
        },
        "id": 310,
        "cited_by": [
            {
                "year": "2017",
                "id": 447
            },
            {
                "year": "2017",
                "id": 453
            },
            {
                "year": "2015",
                "id": 450
            }
        ]
    },
    {
        "title": "Coupled Dictionary and Feature Space Learning with Applications to Cross-Domain Image Synthesis and Recognition",
        "authors": [
            "De-An Huang",
            "Yu-Chiang Frank Wang"
        ],
        "abstract": "Cross-domain image synthesis and recognition are typically considered as two distinct tasks in the areas of computer vision and pattern recognition. Therefore, it is not clear whether approaches addressing one task can be easily generalized or extended for solving the other. In this paper, we propose a unified model for coupled dictionary and feature space learning. The proposed learning model not only observes a common feature space for associating cross-domain image data for recognition purposes, the derived feature space is able to jointly update the dictionaries in each image domain for improved representation. This is why our method can be applied to both cross-domain image synthesis and recognition problems. Experiments on a variety of synthesis and recognition tasks such as single image super-resolution, cross-view action recognition, and sketch-to-photo face recognition would verify the effectiveness of our proposed learning model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751421",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2011",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 53,
            "other": 27,
            "total": 80
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Image recognition",
                "Image generation",
                "Face recognition",
                "Training",
                "Data models",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "feature extraction",
                "gesture recognition",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coupled dictionary-feature space learning",
                "cross-domain image synthesis",
                "cross-domain image recognition",
                "computer vision",
                "pattern recognition",
                "unified model",
                "improved image representation",
                "single-image super-resolution",
                "cross-view action recognition",
                "sketch-to-photo face recognition"
            ]
        },
        "id": 311,
        "cited_by": [
            {
                "year": "2015",
                "id": 70
            },
            {
                "year": "2015",
                "id": 428
            },
            {
                "year": "2015",
                "id": 460
            }
        ]
    },
    {
        "title": "Multi-view Normal Field Integration for 3D Reconstruction of Mirroring Objects",
        "authors": [
            "Michael Weinmann",
            "Aljosa Osep",
            "Roland Ruiters",
            "Reinhard Klein"
        ],
        "abstract": "In this paper, we present a novel, robust multi-view normal field integration technique for reconstructing the full 3D shape of mirroring objects. We employ a turntable-based setup with several cameras and displays. These are used to display illumination patterns which are reflected by the object surface. The pattern information observed in the cameras enables the calculation of individual volumetric normal fields for each combination of camera, display and turntable angle. As the pattern information might be blurred depending on the surface curvature or due to non-perfect mirroring surface characteristics, we locally adapt the decoding to the finest still resolvable pattern resolution. In complex real-world scenarios, the normal fields contain regions without observations due to occlusions and outliers due to interreflections and noise. Therefore, a robust reconstruction using only normal information is challenging. Via a non-parametric clustering of normal hypotheses derived for each point in the scene, we obtain both the most likely local surface normal and a local surface consistency estimate. This information is utilized in an iterative min-cut based variational approach to reconstruct the surface geometry.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751422",
        "reference_list": [
            {
                "year": "2003",
                "id": 78
            },
            {
                "year": "2005",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Cameras",
                "Three-dimensional displays",
                "Light sources",
                "Lighting",
                "Geometry",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "decoding",
                "image coding",
                "image reconstruction",
                "image resolution",
                "iterative methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D reconstruction",
                "mirroring objects",
                "robust multiview normal field integration technique",
                "full 3D shape",
                "turntable-based setup",
                "cameras",
                "illumination patterns",
                "object surface",
                "pattern information",
                "volumetric normal fields",
                "surface curvature",
                "nonperfect mirroring surface characteristics",
                "decoding",
                "pattern resolution",
                "outliers",
                "occlusions",
                "nonparametric clustering",
                "local surface consistency estimate",
                "local surface normal",
                "iterative min-cut based variational approach",
                "surface geometry reconstruction"
            ],
            "Author Keywords": [
                "3D reconstruction",
                "multi-view normal field integration",
                "mirroring objects"
            ]
        },
        "id": 312,
        "cited_by": [
            {
                "year": "2015",
                "id": 249
            }
        ]
    },
    {
        "title": "Discovering Object Functionality",
        "authors": [
            "Bangpeng Yao",
            "Jiayuan Ma",
            "Li Fei-Fei"
        ],
        "abstract": "Object functionality refers to the quality of an object that allows humans to perform some specific actions. It has been shown in psychology that functionality (affordance) is at least as essential as appearance in object recognition by humans. In computer vision, most previous work on functionality either assumes exactly one functionality for each object, or requires detailed annotation of human poses and objects. In this paper, we propose a weakly supervised approach to discover all possible object functionalities. Each object functionality is represented by a specific type of human-object interaction. Our method takes any possible human-object interaction into consideration, and evaluates image similarity in 3D rather than 2D in order to cluster human-object interactions more coherently. Experimental results on a dataset of people interacting with musical instruments show the effectiveness of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751423",
        "reference_list": [
            {
                "year": "2007",
                "id": 145
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 9,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Instruments",
                "Estimation",
                "Object detection",
                "Detectors",
                "Computer vision",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "man-machine systems",
                "musical instruments",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object quality",
                "psychology",
                "functionality affordance",
                "object recognition",
                "computer vision",
                "human pose annotation",
                "object annotation",
                "weakly supervised approach",
                "human-object interaction",
                "image similarity evaluation",
                "human-object interaction clustering",
                "musical instruments",
                "object functionality discovery",
                "human actions"
            ]
        },
        "id": 313,
        "cited_by": [
            {
                "year": "2017",
                "id": 309
            },
            {
                "year": "2015",
                "id": 79
            }
        ]
    },
    {
        "title": "Bird Part Localization Using Exemplar-Based Models with Enforced Pose and Subcategory Consistency",
        "authors": [
            "Jiongxin Liu",
            "Peter N. Belhumeur"
        ],
        "abstract": "In this paper, we propose a novel approach for bird part localization, targeting fine-grained categories with wide variations in appearance due to different poses (including aspect and orientation) and subcategories. As it is challenging to represent such variations across a large set of diverse samples with tractable parametric models, we turn to individual exemplars. Specifically, we extend the exemplar-based models in [4] by enforcing pose and subcategory consistency at the parts. During training, we build pose-specific detectors scoring part poses across subcategories, and subcategory-specific detectors scoring part appearance across poses. At the testing stage, likely exemplars are matched to the image, suggesting part locations whose pose and subcategory consistency are well-supported by the image cues. From these hypotheses, part configuration can be predicted with very high accuracy. Experimental results demonstrate significant performance gains from our method on an extensive dataset: CUB-200-2011 [30], for both localization and classification tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751424",
        "reference_list": [
            {
                "year": "2011",
                "id": 57
            },
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 232
            },
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2009",
                "id": 132
            },
            {
                "year": "2011",
                "id": 91
            },
            {
                "year": "2011",
                "id": 321
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 10,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Birds",
                "Shape",
                "Feature extraction",
                "Computational modeling",
                "Training",
                "Complexity theory"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object detection",
                "zoology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bird part localization",
                "exemplar based models",
                "subcategory consistency",
                "fine-grained categories",
                "pose specific detectors",
                "image matching",
                "image cues",
                "object detection"
            ],
            "Author Keywords": [
                "Part localization",
                "Fine-grained classification"
            ]
        },
        "id": 314,
        "cited_by": [
            {
                "year": "2017",
                "id": 319
            },
            {
                "year": "2017",
                "id": 338
            }
        ]
    },
    {
        "title": "Person Re-identification by Salience Matching",
        "authors": [
            "Rui Zhao",
            "Wanli Ouyang",
            "Xiaogang Wang"
        ],
        "abstract": "Human salience is distinctive and reliable information in matching pedestrians across disjoint camera views. In this paper, we exploit the pair wise salience distribution relationship between pedestrian images, and solve the person re-identification problem by proposing a salience matching strategy. To handle the misalignment problem in pedestrian images, patch matching is adopted and patch salience is estimated. Matching patches with inconsistent salience brings penalty. Images of the same person are recognized by minimizing the salience matching cost. Furthermore, our salience matching is tightly integrated with patch matching in a unified structural Rank SVM learning framework. The effectiveness of our approach is validated on the VIPeR dataset and the CUHK Campus dataset. It outperforms the state-of-the-art methods on both datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751425",
        "reference_list": [
            {
                "year": "2013",
                "id": 55
            },
            {
                "year": "2013",
                "id": 393
            },
            {
                "year": "2007",
                "id": 179
            }
        ],
        "citation": {
            "ieee": 170,
            "other": 68,
            "total": 238
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Visualization",
                "Training",
                "Image color analysis",
                "Measurement",
                "Vectors",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human salience",
                "pedestrian matching",
                "disjoint camera view",
                "pair wise salience distribution relationship",
                "pedestrian images",
                "person re-identification problem",
                "salience matching strategy",
                "misalignment problem",
                "patch matching",
                "patch salience estimation",
                "person images",
                "unified structural rank SVM learning framework",
                "VIPeR dataset",
                "CUHK Campus dataset"
            ],
            "Author Keywords": [
                "Person re-identification",
                "salience matching",
                "salience",
                "saliency",
                "patch matching",
                "pedestrian matching"
            ]
        },
        "id": 315,
        "cited_by": [
            {
                "year": "2017",
                "id": 255
            },
            {
                "year": "2017",
                "id": 258
            },
            {
                "year": "2017",
                "id": 301
            },
            {
                "year": "2017",
                "id": 339
            },
            {
                "year": "2017",
                "id": 497
            },
            {
                "year": "2017",
                "id": 567
            },
            {
                "year": "2015",
                "id": 124
            },
            {
                "year": "2015",
                "id": 357
            },
            {
                "year": "2015",
                "id": 411
            },
            {
                "year": "2015",
                "id": 417
            },
            {
                "year": "2015",
                "id": 420
            },
            {
                "year": "2015",
                "id": 425
            },
            {
                "year": "2015",
                "id": 431
            },
            {
                "year": "2015",
                "id": 437
            },
            {
                "year": "2015",
                "id": 522
            },
            {
                "year": "2013",
                "id": 55
            }
        ]
    },
    {
        "title": "Prime Object Proposals with Randomized Prim's Algorithm",
        "authors": [
            "Santiago Manen",
            "Matthieu Guillaumin",
            "Luc Van Gool"
        ],
        "abstract": "Generic object detection is the challenging task of proposing windows that localize all the objects in an image, regardless of their classes. Such detectors have recently been shown to benefit many applications such as speeding-up class-specific object detection, weakly supervised learning of object detectors and object discovery. In this paper, we introduce a novel and very efficient method for generic object detection based on a randomized version of Prim's algorithm. Using the connectivity graph of an image's super pixels, with weights modelling the probability that neighbouring super pixels belong to the same object, the algorithm generates random partial spanning trees with large expected sum of edge weights. Object localizations are proposed as bounding-boxes of those partial trees. Our method has several benefits compared to the state-of-the-art. Thanks to the efficiency of Prim's algorithm, it samples proposals very quickly: 1000 proposals are obtained in about 0.7s. With proposals bound to super pixel boundaries yet diversified by randomization, it yields very high detection rates and windows that tightly fit objects. In extensive experiments on the challenging PASCAL VOC 2007 and 2012 and SUN2012 benchmark datasets, we show that our method improves over state-of-the-art competitors for a wide range of evaluation scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751426",
        "reference_list": [
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2011",
                "id": 130
            },
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2011",
                "id": 43
            },
            {
                "year": "2011",
                "id": 191
            },
            {
                "year": "2011",
                "id": 238
            }
        ],
        "citation": {
            "ieee": 60,
            "other": 50,
            "total": 110
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Object detection",
                "Image edge detection",
                "Image color analysis",
                "Image segmentation",
                "Heuristic algorithms",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object detection",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SUN2012 benchmark datasets",
                "PASCAL VOC 2012",
                "PASCAL VOC 2007",
                "randomization",
                "object localizations",
                "random partial spanning trees",
                "image superpixels",
                "connectivity graph",
                "object discovery",
                "object detectors",
                "supervised learning",
                "class-specific object detection",
                "generic object detection",
                "randomized Prim algorithm",
                "prime object proposals"
            ],
            "Author Keywords": [
                "Object Detection",
                "Object Proposal"
            ]
        },
        "id": 316,
        "cited_by": [
            {
                "year": "2017",
                "id": 193
            },
            {
                "year": "2015",
                "id": 31
            },
            {
                "year": "2015",
                "id": 128
            },
            {
                "year": "2015",
                "id": 172
            },
            {
                "year": "2015",
                "id": 225
            },
            {
                "year": "2015",
                "id": 276
            },
            {
                "year": "2015",
                "id": 287
            },
            {
                "year": "2015",
                "id": 354
            }
        ]
    },
    {
        "title": "Mining Multiple Queries for Image Retrieval: On-the-Fly Learning of an Object-Specific Mid-level Representation",
        "authors": [
            "Basura Fernando",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this paper we present a new method for object retrieval starting from multiple query images. The use of multiple queries allows for a more expressive formulation of the query object including, e.g., different viewpoints and/or viewing conditions. This, in turn, leads to more diverse and more accurate retrieval results. When no query images are available to the user, they can easily be retrieved from the internet using a standard image search engine. In particular, we propose a new method based on pattern mining. Using the minimal description length principle, we derive the most suitable set of patterns to describe the query object, with patterns corresponding to local feature configurations. This results in a powerful object-specific mid-level image representation. The archive can then be searched efficiently for similar images based on this representation, using a combination of two inverted file systems. Since the patterns already encode local spatial information, good results on several standard image retrieval datasets are obtained even without costly re-ranking based on geometric verification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751427",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2009",
                "id": 118
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2007",
                "id": 67
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 14,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Image retrieval",
                "Computational modeling",
                "Standards",
                "Histograms",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "feature extraction",
                "image representation",
                "image retrieval",
                "Internet",
                "object detection",
                "search engines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric verification",
                "standard image retrieval datasets",
                "local spatial information encoding",
                "inverted file systems",
                "object-specific mid-level image representation",
                "local feature configurations",
                "pattern mining",
                "standard image search engine",
                "Internet",
                "query images",
                "object retrieval",
                "object-specific mid-level representation",
                "on-the-fly learning",
                "query mining",
                "image retrieval"
            ],
            "Author Keywords": [
                "multiple query object retrieval",
                "image retrieval",
                "feature configurations",
                "mid-level image representation",
                "mid-level patterns"
            ]
        },
        "id": 317,
        "cited_by": []
    },
    {
        "title": "A General Two-Step Approach to Learning-Based Hashing",
        "authors": [
            "Guosheng Lin",
            "Chunhua Shen",
            "David Suter",
            "Anton van den Hengel"
        ],
        "abstract": "Most existing approaches to hashing apply a single form of hash function, and an optimization process which is typically deeply coupled to this specific form. This tight coupling restricts the flexibility of the method to respond to the data, and can result in complex optimization problems that are difficult to solve. Here we propose a flexible yet simple framework that is able to accommodate different types of loss functions and hash functions. This framework allows a number of existing approaches to hashing to be placed in context, and simplifies the development of new problem-specific hashing methods. Our framework decomposes the hashing learning problem into two steps: hash bit learning and hash function learning based on the learned bits. The first step can typically be formulated as binary quadratic problems, and the second step can be accomplished by training standard binary classifiers. Both problems have been extensively studied in the literature. Our extensive experiments demonstrate that the proposed framework is effective, flexible and outperforms the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751428",
        "reference_list": [],
        "citation": {
            "ieee": 51,
            "other": 30,
            "total": 81
        },
        "keywords": {
            "IEEE Keywords": [
                "Binary codes",
                "Optimization",
                "Support vector machines",
                "Kernel",
                "Hamming distance",
                "Training",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "cryptography",
                "learning (artificial intelligence)",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "general two step approach",
                "learning based hashing",
                "hash function",
                "optimization process",
                "complex optimization problems",
                "hashing learning problem",
                "hash bit learning",
                "hash function learning",
                "binary quadratic problems"
            ],
            "Author Keywords": [
                "binary codes",
                "hashing",
                "image retrieval"
            ]
        },
        "id": 318,
        "cited_by": [
            {
                "year": "2015",
                "id": 116
            },
            {
                "year": "2015",
                "id": 122
            },
            {
                "year": "2015",
                "id": 467
            }
        ]
    },
    {
        "title": "Modeling Occlusion by Discriminative AND-OR Structures",
        "authors": [
            "Bo Li",
            "Wenze Hu",
            "Tianfu Wu",
            "Song-Chun Zhu"
        ],
        "abstract": "Occlusion presents a challenge for detecting objects in real world applications. To address this issue, this paper models object occlusion with an AND-OR structure which (i) represents occlusion at semantic part level, and (ii) captures the regularities of different occlusion configurations (i.e., the different combinations of object part visibilities). This paper focuses on car detection on street. Since annotating part occlusion on real images is time-consuming and error-prone, we propose to learn the the AND-OR structure automatically using synthetic images of CAD models placed at different relative positions. The model parameters are learned from real images under the latent structural SVM (LSSVM) framework. In inference, an efficient dynamic programming (DP) algorithm is utilized. In experiments, we test our method on both car detection and car view estimation. Experimental results show that (i) Our CAD simulation strategy is capable of generating occlusion patterns for real scenarios, (ii) The proposed AND-OR structure model is effective for modeling occlusions, which outperforms the deformable part-based model (DPM) DPM, voc5 in car detection on both our self-collected street parking dataset and the Pascal VOC 2007 car dataset pascal-voc-2007}, (iii) The learned model is on-par with the state-of-the-art methods on car view estimation tested on two public datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751429",
        "reference_list": [
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2007",
                "id": 146
            },
            {
                "year": "2009",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 3,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Design automation",
                "Vectors",
                "Deformable models",
                "Computational modeling",
                "Training",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "automobiles",
                "CAD",
                "dynamic programming",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Pascal VOC 2007 car dataset",
                "street parking dataset",
                "DPM",
                "deformable part-based model",
                "occlusion pattern generation",
                "CAD simulation strategy",
                "car view estimation",
                "DP algorithm",
                "dynamic programming algorithm",
                "LSSVM framework",
                "latent structural SVM",
                "synthetic images",
                "car detection",
                "occlusion configuration",
                "object occlusion modeling",
                "object detection",
                "AND-OR structure discrimination"
            ],
            "Author Keywords": [
                "Occlusion Modeling",
                "Car Detection",
                "AND-OR structure",
                "CAD simulation"
            ]
        },
        "id": 319,
        "cited_by": []
    },
    {
        "title": "An Adaptive Descriptor Design for Object Recognition in the Wild",
        "authors": [
            "Zhenyu Guo",
            "Z. Jane Wang"
        ],
        "abstract": "Digital images nowadays show large appearance variabilities on picture styles, in terms of color tone, contrast, vignetting, and etc. These `picture styles' are directly related to the scene radiance, image pipeline of the camera, and post processing functions (e.g., photography effect filters). Due to the complexity and nonlinearity of these factors, popular gradient-based image descriptors generally are not invariant to different picture styles, which could degrade the performance for object recognition. Given that images shared online or created by individual users are taken with a wide range of devices and may be processed by various post processing functions, to find a robust object recognition system is useful and challenging. In this paper, we investigate the influence of picture styles on object recognition by making a connection between image descriptors and a pixel mapping function g, and accordingly propose an adaptive approach based on a g-incorporated kernel descriptor and multiple kernel learning, without estimating or specifying the image styles used in training and testing. We conduct experiments on the Domain Adaptation data set, the Oxford Flower data set, and several variants of the Flower data set by introducing popular photography effects through post-processing. The results demonstrate that the proposed method consistently yields recognition improvements over standard descriptors in all studied cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751430",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2005",
                "id": 191
            },
            {
                "year": "2011",
                "id": 321
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Object recognition",
                "Standards",
                "Training",
                "Photography",
                "Testing",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "digital photography",
                "image recognition",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Oxford Flower data set",
                "domain adaptation data set",
                "image style estimation",
                "image style specification",
                "multiple-kernel learning",
                "g-incorporated kernel descriptor",
                "adaptive approach",
                "pixel mapping function",
                "robust object recognition system",
                "gradient-based image descriptors",
                "photography effect filters",
                "post processing functions",
                "camera image pipeline",
                "scene radiance",
                "vignetting",
                "contrast",
                "color tone",
                "picture styles",
                "appearance variabilities",
                "digital images",
                "adaptive descriptor design"
            ],
            "Author Keywords": [
                "image descriptor",
                "multiple kernel learning",
                "domain adaptation"
            ]
        },
        "id": 320,
        "cited_by": []
    },
    {
        "title": "Coherent Object Detection with 3D Geometric Context from a Single Image",
        "authors": [
            "Jiyan Pan",
            "Takeo Kanade"
        ],
        "abstract": "Objects in a real world image cannot have arbitrary appearance, sizes and locations due to geometric constraints in 3D space. Such a 3D geometric context plays an important role in resolving visual ambiguities and achieving coherent object detection. In this paper, we develop a RANSAC-CRF framework to detect objects that are geometrically coherent in the 3D world. Different from existing methods, we propose a novel generalized RANSAC algorithm to generate global 3D geometry hypotheses from local entities such that outlier suppression and noise reduction is achieved simultaneously. In addition, we evaluate those hypotheses using a CRF which considers both the compatibility of individual objects under global 3D geometric context and the compatibility between adjacent objects under local 3D geometric context. Experiment results show that our approach compares favorably with the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751431",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Geometry",
                "Context",
                "Gravity",
                "Object detection",
                "Cameras",
                "Noise"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coherent object detection",
                "3D geometric context",
                "real world image",
                "visual ambiguities",
                "novel generalized RANSAC CRF algorithm",
                "global 3D geometry",
                "outlier suppression",
                "noise reduction",
                "global 3D geometric context",
                "local 3D geometric context"
            ],
            "Author Keywords": [
                "object detection",
                "3D geometric context"
            ]
        },
        "id": 321,
        "cited_by": []
    },
    {
        "title": "Write a Classifier: Zero-Shot Learning Using Purely Textual Descriptions",
        "authors": [
            "Mohamed Elhoseiny",
            "Babak Saleh",
            "Ahmed Elgammal"
        ],
        "abstract": "The main question we address in this paper is how to use purely textual description of categories with no training images to learn visual classifiers for these categories. We propose an approach for zero-shot learning of object categories where the description of unseen categories comes in the form of typical text such as an encyclopedia entry, without the need to explicitly defined attributes. We propose and investigate two baseline formulations, based on regression and domain adaptation. Then, we propose a new constrained optimization formulation that combines a regression function and a knowledge transfer function with additional constraints to predict the classifier parameters for new classes. We applied the proposed approach on two fine-grained categorization datasets, and the results indicate successful classifier prediction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751432",
        "reference_list": [],
        "citation": {
            "ieee": 45,
            "other": 22,
            "total": 67
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Training",
                "Optimization",
                "Correlation",
                "Transfer functions",
                "Semantics",
                "Birds"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "object recognition",
                "optimisation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "zero-shot learning",
                "purely textual descriptions",
                "object categories",
                "domain adaptation",
                "regression adaptation",
                "constrained optimization formulation",
                "regression function",
                "knowledge transfer function",
                "fine-grained categorization datasets"
            ],
            "Author Keywords": [
                "Zero shot learning",
                "object recognition",
                "fine grained object recognition",
                "computer vision",
                "domain adaptation"
            ]
        },
        "id": 322,
        "cited_by": [
            {
                "year": "2017",
                "id": 129
            },
            {
                "year": "2017",
                "id": 318
            },
            {
                "year": "2017",
                "id": 366
            },
            {
                "year": "2017",
                "id": 443
            },
            {
                "year": "2015",
                "id": 273
            },
            {
                "year": "2015",
                "id": 282
            },
            {
                "year": "2015",
                "id": 470
            },
            {
                "year": "2015",
                "id": 474
            },
            {
                "year": "2015",
                "id": 512
            }
        ]
    },
    {
        "title": "Random Forests of Local Experts for Pedestrian Detection",
        "authors": [
            "Javier Mar\u00edn",
            "David V\u00e1zquez",
            "Antonio M. L\u00f3pez",
            "Jaume Amores",
            "Bastian Leibe"
        ],
        "abstract": "Pedestrian detection is one of the most challenging tasks in computer vision, and has received a lot of attention in the last years. Recently, some authors have shown the advantages of using combinations of part/patch-based detectors in order to cope with the large variability of poses and the existence of partial occlusions. In this paper, we propose a pedestrian detection method that efficiently combines multiple local experts by means of a Random Forest ensemble. The proposed method works with rich block-based representations such as HOG and LBP, in such a way that the same features are reused by the multiple local experts, so that no extra computational cost is needed with respect to a holistic method. Furthermore, we demonstrate how to integrate the proposed approach with a cascaded architecture in order to achieve not only high accuracy but also an acceptable efficiency. In particular, the resulting detector operates at five frames per second using a laptop machine. We tested the proposed method with well-known challenging datasets such as Caltech, ETH, Daimler, and INRIA. The method proposed in this work consistently ranks among the top performers in all the datasets, being either the best method or having a small difference with the best one.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751433",
        "reference_list": [
            {
                "year": "2007",
                "id": 252
            },
            {
                "year": "2009",
                "id": 4
            }
        ],
        "citation": {
            "ieee": 41,
            "other": 20,
            "total": 61
        },
        "keywords": {
            "IEEE Keywords": [
                "Vegetation",
                "Training",
                "Standards",
                "Radio frequency",
                "Support vector machines",
                "Vectors",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image representation",
                "pedestrians",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "part-patch-based detector combination",
                "pose variability",
                "partial occlusion existence",
                "pedestrian detection method",
                "multiple-local expert combination",
                "random forest ensemble",
                "block-based representations",
                "HOG",
                "LBP",
                "feature reuse",
                "laptop machine",
                "Caltech dataset",
                "ETH dataset",
                "Daimler dataset",
                "INRIA dataset"
            ],
            "Author Keywords": [
                "Pedestrian detection",
                "Random Forest",
                "Local Experts",
                "HOG",
                "LBP"
            ]
        },
        "id": 323,
        "cited_by": [
            {
                "year": "2015",
                "id": 458
            }
        ]
    },
    {
        "title": "Visual Reranking through Weakly Supervised Multi-graph Learning",
        "authors": [
            "Cheng Deng",
            "Rongrong Ji",
            "Wei Liu",
            "Dacheng Tao",
            "Xinbo Gao"
        ],
        "abstract": "Visual reranking has been widely deployed to refine the quality of conventional content-based image retrieval engines. The current trend lies in employing a crowd of retrieved results stemming from multiple feature modalities to boost the overall performance of visual reranking. However, a major challenge pertaining to current reranking methods is how to take full advantage of the complementary property of distinct feature modalities. Given a query image and one feature modality, a regular visual reranking framework treats the top-ranked images as pseudo positive instances which are inevitably noisy, difficult to reveal this complementary property, and thus lead to inferior ranking performance. This paper proposes a novel image reranking approach by introducing a Co-Regularized Multi-Graph Learning (Co-RMGL) framework, in which the intra-graph and inter-graph constraints are simultaneously imposed to encode affinities in a single graph and consistency across different graphs. Moreover, weakly supervised learning driven by image attributes is performed to denoise the pseudo-labeled instances, thereby highlighting the unique strength of individual feature modality. Meanwhile, such learning can yield a few anchors in graphs that vitally enable the alignment and fusion of multiple graphs. As a result, an edge weight matrix learned from the fused graph automatically gives the ordering to the initially retrieved results. We evaluate our approach on four benchmark image retrieval datasets, demonstrating a significant performance gain over the state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751434",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2011",
                "id": 81
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 19,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Vectors",
                "Supervised learning",
                "Labeling",
                "Image edge detection",
                "Noise measurement",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "feature extraction",
                "image denoising",
                "image fusion",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual reranking",
                "weakly supervised multigraph learning",
                "content-based image retrieval engines",
                "multiple feature modalities",
                "distinct feature modalities",
                "query image",
                "image reranking approach",
                "co-regularized multigraph learning",
                "Co-RMGL framework",
                "intra-graph constraints",
                "inter-graph constraints",
                "pseudo-labeled instances",
                "multiple graph alignment",
                "multiple graph fusion",
                "edge weight matrix",
                "image retrieval datasets"
            ],
            "Author Keywords": [
                "Visual reranking",
                "multi-graph learning",
                "weakly-supervised learning",
                "graph anchor",
                "attribute"
            ]
        },
        "id": 324,
        "cited_by": []
    },
    {
        "title": "Domain Adaptive Classification",
        "authors": [
            "Fatemeh Mirrashed",
            "Mohammad Rastegari"
        ],
        "abstract": "We propose an unsupervised domain adaptation method that exploits intrinsic compact structures of categories across different domains using binary attributes. Our method directly optimizes for classification in the target domain. The key insight is finding attributes that are discriminative across categories and predictable across domains. We achieve a performance that significantly exceeds the state-of-the-art results on standard benchmarks. In fact, in many cases, our method reaches the same-domain performance, the upper bound, in unsupervised domain adaptation scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751435",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Training",
                "Binary codes",
                "Kernel",
                "Adaptation models",
                "Support vector machines",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "domain adaptive classification",
                "unsupervised domain adaptation method",
                "intrinsic compact structures",
                "binary attributes",
                "same-domain performance",
                "unsupervised domain adaptation scenarios"
            ]
        },
        "id": 325,
        "cited_by": []
    },
    {
        "title": "Supervised Binary Hash Code Learning with Jensen Shannon Divergence",
        "authors": [
            "Lixin Fan"
        ],
        "abstract": "This paper proposes to learn binary hash codes within a statistical learning framework, in which an upper bound of the probability of Bayes decision errors is derived for different forms of hash functions and a rigorous proof of the convergence of the upper bound is presented. Consequently, minimizing such an upper bound leads to consistent performance improvements of existing hash code learning algorithms, regardless of whether original algorithms are unsupervised or supervised. This paper also illustrates a fast hash coding method that exploits simple binary tests to achieve orders of magnitude improvement in coding speed as compared to projection based methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751436",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2011",
                "id": 324
            },
            {
                "year": "2011",
                "id": 326
            },
            {
                "year": "2011",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Binary codes",
                "Upper bound",
                "Convergence",
                "Vectors",
                "Training",
                "Nickel",
                "Linear programming"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "binary codes",
                "decision theory",
                "error statistics",
                "file organisation",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "supervised binary hash code learning algorithm",
                "Jensen Shannon divergence",
                "statistical learning framework",
                "Bayes decision error probability",
                "hash functions",
                "upper bound",
                "unsupervised learning",
                "binary tests",
                "projection based methods"
            ],
            "Author Keywords": [
                "binary code",
                "indexing",
                "matching",
                "approximate nearest neighbor search",
                "Jensen Shannon Divergence",
                "randomized tree"
            ]
        },
        "id": 326,
        "cited_by": []
    },
    {
        "title": "Model Recommendation with Virtual Probes for Egocentric Hand Detection",
        "authors": [
            "Cheng Li",
            "Kris M. Kitani"
        ],
        "abstract": "Egocentric cameras can be used to benefit such tasks as analyzing fine motor skills, recognizing gestures and learning about hand-object manipulation. To enable such technology, we believe that the hands must detected on the pixel-level to gain important information about the shape of the hands and fingers. We show that the problem of pixel-wise hand detection can be effectively solved, by posing the problem as a model recommendation task. As such, the goal of a recommendation system is to recommend the n-best hand detectors based on the probe set - a small amount of labeled data from the test distribution. This requirement of a probe set is a serious limitation in many applications, such as ego-centric hand detection, where the test distribution may be continually changing. To address this limitation, we propose the use of virtual probes which can be automatically extracted from the test distribution. The key idea is that many features, such as the color distribution or relative performance between two detectors, can be used as a proxy to the probe set. In our experiments we show that the recommendation paradigm is well-equipped to handle complex changes in the appearance of the hands in first-person vision. In particular, we show how our system is able to generalize to new scenarios by testing our model across multiple users.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751437",
        "reference_list": [
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2009",
                "id": 156
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 9,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Probes",
                "Detectors",
                "Feature extraction",
                "Computational modeling",
                "Imaging",
                "Image color analysis",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "feature extraction",
                "object detection",
                "palmprint recognition",
                "recommender systems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color distribution",
                "test distribution",
                "n-best hand detectors",
                "pixel-wise hand detection",
                "pixel-level detection",
                "hand-object manipulation",
                "gesture recognition",
                "motor skill analysis",
                "egocentric cameras",
                "egocentric hand detection",
                "virtual probes",
                "model recommendation system"
            ]
        },
        "id": 327,
        "cited_by": [
            {
                "year": "2015",
                "id": 217
            }
        ]
    },
    {
        "title": "Geometric Registration Based on Distortion Estimation",
        "authors": [
            "Wei Zeng",
            "Mayank Goswami",
            "Feng Luo",
            "Xianfeng Gu"
        ],
        "abstract": "Surface registration plays a fundamental role in many applications in computer vision and aims at finding a one-to-one correspondence between surfaces. Conformal mapping based surface registration methods conformally map 2D/3D surfaces onto 2D canonical domains and perform the matching on the 2D plane. This registration framework reduces dimensionality, and the result is intrinsic to Riemannian metric and invariant under isometric deformation. However, conformal mapping will be affected by inconsistent boundaries and non-isometric deformations of surfaces. In this work, we quantify the effects of boundary variation and non-isometric deformation to conformal mappings, and give the theoretical upper bounds for the distortions of conformal mappings under these two factors. Besides giving the thorough theoretical proofs of the theorems, we verified them by concrete experiments using 3D human facial scans with dynamic expressions and varying boundaries. Furthermore, we used the distortion estimates for reducing search range in feature matching of surface registration applications. The experimental results are consistent with the theoretical predictions and also demonstrate the performance improvements in feature tracking.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751438",
        "reference_list": [
            {
                "year": "2007",
                "id": 268
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Conformal mapping",
                "Measurement",
                "Three-dimensional displays",
                "Surface morphology",
                "Surface treatment",
                "Estimation",
                "Upper bound"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "feature extraction",
                "image matching",
                "image registration",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric registration",
                "distortion estimation",
                "computer vision",
                "one-to-one correspondence",
                "surface registration methods",
                "3D surface mapping",
                "2D canonical domains",
                "2D plane matching",
                "dimensionality reduction",
                "Riemannian metric",
                "isometric deformation",
                "inconsistent boundaries",
                "nonisometric surface deformations",
                "boundary variation",
                "upper bounds",
                "conformal mapping distortions",
                "3D human facial scans",
                "dynamic expressions",
                "distortion estimates",
                "search range reduction",
                "feature matching",
                "performance improvements",
                "feature tracking",
                "2D surface mapping"
            ]
        },
        "id": 328,
        "cited_by": []
    },
    {
        "title": "Multi-view Object Segmentation in Space and Time",
        "authors": [
            "Abdelaziz Djelouah",
            "Jean-S\u00e9bastien Franco",
            "Edmond Boyer",
            "Fran\u00e7ois Le Clerc",
            "Patrick P\u00e9rez"
        ],
        "abstract": "In this paper, we address the problem of object segmentation in multiple views or videos when two or more viewpoints of the same scene are available. We propose a new approach that propagates segmentation coherence information in both space and time, hence allowing evidences in one image to be shared over the complete set. To this aim the segmentation is cast as a single efficient labeling problem over space and time with graph cuts. In contrast to most existing multi-view segmentation methods that rely on some form of dense reconstruction, ours only requires a sparse 3D sampling to propagate information between viewpoints. The approach is thoroughly evaluated on standard multi-view datasets, as well as on videos. With static views, results compete with state of the art methods but they are achieved with significantly fewer viewpoints. With multiple videos, we report results that demonstrate the benefit of segmentation propagation through temporal cues.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751439",
        "reference_list": [
            {
                "year": "2009",
                "id": 34
            },
            {
                "year": "2007",
                "id": 89
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 7,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Image segmentation",
                "Labeling",
                "Image color analysis",
                "Videos",
                "Computational modeling",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense reconstruction",
                "cues",
                "temporal cues",
                "segmentation propagation",
                "static views",
                "standard multiview datasets",
                "3D sampling",
                "multiview segmentation methods",
                "efficient labeling problem",
                "segmentation coherence information"
            ],
            "Author Keywords": [
                "segmentation",
                "multi-view segmentation"
            ]
        },
        "id": 329,
        "cited_by": []
    },
    {
        "title": "Pedestrian Parsing via Deep Decompositional Network",
        "authors": [
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "We propose a new Deep Decompositional Network (DDN) for parsing pedestrian images into semantic regions, such as hair, head, body, arms, and legs, where the pedestrians can be heavily occluded. Unlike existing methods based on template matching or Bayesian inference, our approach directly maps low-level visual features to the label maps of body parts with DDN, which is able to accurately estimate complex pose variations with good robustness to occlusions and background clutters. DDN jointly estimates occluded regions and segments body parts by stacking three types of hidden layers: occlusion estimation layers, completion layers, and decomposition layers. The occlusion estimation layers estimate a binary mask, indicating which part of a pedestrian is invisible. The completion layers synthesize low-level features of the invisible part from the original features and the occlusion mask. The decomposition layers directly transform the synthesized visual features to label maps. We devise a new strategy to pre-train these hidden layers, and then fine-tune the entire network using the stochastic gradient descent. Experimental results show that our approach achieves better segmentation accuracy than the state-of-the-art methods on pedestrian images with or without occlusions. Another important contribution of this paper is that it provides a large scale benchmark human parsing dataset that includes 3,673 annotated samples collected from 171 surveillance videos. It is 20 times larger than existing public datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751440",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2013",
                "id": 357
            },
            {
                "year": "2009",
                "id": 4
            },
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 19,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Training",
                "Shape",
                "Transforms",
                "Vectors",
                "Noise",
                "Clutter"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "neural nets",
                "pedestrians",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pedestrian image parsing",
                "deep decompositional network",
                "DDN",
                "template matching",
                "Bayesian inference",
                "low-level visual features",
                "occlusion estimation layers",
                "completion layers",
                "decomposition layers",
                "binary mask",
                "stochastic gradient descent",
                "video surveillance"
            ],
            "Author Keywords": [
                "pedestrian parsing",
                "deep learning"
            ]
        },
        "id": 330,
        "cited_by": [
            {
                "year": "2015",
                "id": 153
            },
            {
                "year": "2013",
                "id": 15
            },
            {
                "year": "2013",
                "id": 357
            }
        ]
    },
    {
        "title": "Dynamic Structured Model Selection",
        "authors": [
            "David Weiss",
            "Benjamin Sapp",
            "Ben Taskar"
        ],
        "abstract": "In many cases, the predictive power of structured models for for complex vision tasks is limited by a trade-off between the expressiveness and the computational tractability of the model. However, choosing this trade-off statically a priori is sub optimal, as images and videos in different settings vary tremendously in complexity. On the other hand, choosing the trade-off dynamically requires knowledge about the accuracy of different structured models on any given example. In this work, we propose a novel two-tier architecture that provides dynamic speed/accuracy trade-offs through a simple type of introspection. Our approach, which we call dynamic structured model selection (DMS), leverages typically intractable features in structured learning problems in order to automatically determine' which of several models should be used at test-time in order to maximize accuracy under a fixed budgetary constraint. We demonstrate DMS on two sequential modeling vision tasks, and we establish a new state-of-the-art in human pose estimation in video with an implementation that is roughly 23\u00d7 faster than the previous standard implementation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751441",
        "reference_list": [
            {
                "year": "2011",
                "id": 334
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Predictive models",
                "Accuracy",
                "Videos",
                "Training",
                "Prediction algorithms",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic structured model selection",
                "DMS",
                "structured learning problems",
                "fixed budgetary constraint",
                "sequential modeling vision tasks",
                "human pose estimation"
            ],
            "Author Keywords": [
                "structured prediction",
                "pose estimation"
            ]
        },
        "id": 331,
        "cited_by": []
    },
    {
        "title": "From Point to Set: Extend the Learning of Distance Metrics",
        "authors": [
            "Pengfei Zhu",
            "Lei Zhang",
            "Wangmeng Zuo",
            "David Zhang"
        ],
        "abstract": "Most of the current metric learning methods are proposed for point-to-point distance (PPD) based classification. In many computer vision tasks, however, we need to measure the point-to-set distance (PSD) and even set-to-set distance (SSD) for classification. In this paper, we extend the PPD based Mahalanobis distance metric learning to PSD and SSD based ones, namely point-to-set distance metric learning (PSDML) and set-to-set distance metric learning (SSDML), and solve them under a unified optimization framework. First, we generate positive and negative sample pairs by computing the PSD and SSD between training samples. Then, we characterize each sample pair by its covariance matrix, and propose a covariance kernel based discriminative function. Finally, we tackle the PSDML and SSDML problems by using standard support vector machine solvers, making the metric learning very efficient for multiclass visual classification tasks. Experiments on gender classification, digit recognition, object categorization and face recognition show that the proposed metric learning methods can effectively enhance the performance of PSD and SSD based classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751442",
        "reference_list": [
            {
                "year": "2011",
                "id": 186
            },
            {
                "year": "2011",
                "id": 197
            },
            {
                "year": "2009",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 29,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Support vector machines",
                "Training",
                "Learning systems",
                "Covariance matrices",
                "Face recognition",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "covariance matrices",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "point-to-set distance",
                "set-to-set distance",
                "PPD based Mahalanobis distance metric learning",
                "PSD",
                "SSD",
                "point-to-point distance",
                "covariance matrix",
                "point-to-set distance metric learning",
                "PSDML",
                "set-to-set distance metric learning",
                "SSDML",
                "support vector machine solvers",
                "multiclass visual classification",
                "gender classification",
                "digit recognition",
                "object categorization",
                "face recognition"
            ]
        },
        "id": 332,
        "cited_by": [
            {
                "year": "2017",
                "id": 75
            }
        ]
    },
    {
        "title": "Class-Specific Simplex-Latent Dirichlet Allocation for Image Classification",
        "authors": [
            "Mandar Dixit",
            "Nikhil Rasiwasia",
            "Nuno Vasconcelos"
        ],
        "abstract": "An extension of the latent Dirichlet allocation (LDA), denoted class-specific-simplex LDA (css-LDA), is proposed for image classification. An analysis of the supervised LDA models currently used for this task shows that the impact of class information on the topics discovered by these models is very weak in general. This implies that the discovered topics are driven by general image regularities, rather than the semantic regularities of interest for classification. To address this, we introduce a model that induces supervision in topic discovery, while retaining the original flexibility of LDA to account for unanticipated structures of interest. The proposed css-LDA is an LDA model with class supervision at the level of image features. In css-LDA topics are discovered per class, i.e. a single set of topics shared across classes is replaced by multiple class-specific topic sets. This model can be used for generative classification using the Bayes decision rule or even extended to discriminative classification with support vector machines (SVMs). A css-LDA model can endow an image with a vector of class and topic specific count statistics that are similar to the Bag-of-words (BoW) histogram. SVM-based discriminants can be learned for classes in the space of these histograms. The effectiveness of css-LDA model in both generative and discriminative classification frameworks is demonstrated through an extensive experimental evaluation, involving multiple benchmark datasets, where it is shown to outperform all existing LDA based image classification approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751443",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Semantics",
                "Histograms",
                "Accuracy",
                "Support vector machines",
                "Visualization",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image classification",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "class-specific simplex-latent Dirichlet allocation",
                "image classification",
                "Bayes decision rule",
                "topic discovery",
                "unanticipated structures of interest",
                "css-LDA",
                "generative classification",
                "support vector machines",
                "SVM-based discriminants",
                "count statistics",
                "bag-of-words histogram",
                "BoW histogram"
            ],
            "Author Keywords": [
                "Bag of Words",
                "Latent Dirichlet Allocation",
                "Topic Supervision"
            ]
        },
        "id": 333,
        "cited_by": []
    },
    {
        "title": "Mining Motion Atoms and Phrases for Complex Action Recognition",
        "authors": [
            "Limin Wang",
            "Yu Qiao",
            "Xiaoou Tang"
        ],
        "abstract": "This paper proposes motion atom and phrase as a mid-level temporal ``part'' for representing and classifying complex action. Motion atom is defined as an atomic part of action, and captures the motion information of action video in a short temporal scale. Motion phrase is a temporal composite of multiple motion atoms with an AND/OR structure, which further enhances the discriminative ability of motion atoms by incorporating temporal constraints in a longer scale. Specifically, given a set of weakly labeled action videos, we firstly design a discriminative clustering method to automatically discover a set of representative motion atoms. Then, based on these motion atoms, we mine effective motion phrases with high discriminative and representative power. We introduce a bottom-up phrase construction algorithm and a greedy selection method for this mining task. We examine the classification performance of the motion atom and phrase based representation on two complex action datasets: Olympic Sports and UCF50. Experimental results show that our method achieves superior performance over recent published methods on both datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751444",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 11,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Training",
                "Hidden Markov models",
                "Support vector machines",
                "Image segmentation",
                "Correlation",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "image classification",
                "pattern clustering",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "UCF50",
                "Olympic sports",
                "complex action dataset",
                "mining task",
                "greedy selection method",
                "bottom-up phrase construction algorithm",
                "effective motion phrase",
                "discriminative clustering method",
                "action video",
                "motion information",
                "complex action classification",
                "midlevel temporal part",
                "complex action recognition",
                "phrases mining",
                "motion atoms mining"
            ],
            "Author Keywords": [
                "action recognition",
                "mid-level representation"
            ]
        },
        "id": 334,
        "cited_by": []
    },
    {
        "title": "Learning Maximum Margin Temporal Warping for Action Recognition",
        "authors": [
            "Jiang Wang",
            "Ying Wu"
        ],
        "abstract": "Temporal misalignment and duration variation in video actions largely influence the performance of action recognition, but it is very difficult to specify effective temporal alignment on action sequences. To address this challenge, this paper proposes a novel discriminative learning-based temporal alignment method, called maximum margin temporal warping (MMTW), to align two action sequences and measure their matching score. Based on the latent structure SVM formulation, the proposed MMTW method is able to learn a phantom action template to represent an action class for maximum discrimination against other classes. The recognition of this action class is based on the associated learned alignment of the input action. Extensive experiments on five benchmark datasets have demonstrated that this MMTW model is able to significantly promote the accuracy and robustness of action recognition under temporal misalignment and variations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751445",
        "reference_list": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2011",
                "id": 61
            },
            {
                "year": "2009",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 12,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Phantoms",
                "Hidden Markov models",
                "Joints",
                "Training data",
                "Support vector machines",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image sequences",
                "learning (artificial intelligence)",
                "object recognition",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "maximum margin temporal warping learning",
                "action recognition",
                "video actions",
                "action sequences",
                "discriminative learning-based temporal alignment method",
                "MMTW method",
                "matching score",
                "latent structure SVM formulation",
                "phantom action template",
                "support vector machines"
            ],
            "Author Keywords": [
                "Action Recognition",
                "Dynamic Temporal Warpping",
                "Temporal Model",
                "Depth Camera"
            ]
        },
        "id": 335,
        "cited_by": []
    },
    {
        "title": "Combining the Right Features for Complex Event Recognition",
        "authors": [
            "Kevin Tang",
            "Bangpeng Yao",
            "Li Fei-Fei",
            "Daphne Koller"
        ],
        "abstract": "In this paper, we tackle the problem of combining features extracted from video for complex event recognition. Feature combination is an especially relevant task in video data, as there are many features we can extract, ranging from image features computed from individual frames to video features that take temporal information into account. To combine features effectively, we propose a method that is able to be selective of different subsets of features, as some features or feature combinations may be uninformative for certain classes. We introduce a hierarchical method for combining features based on the AND/OR graph structure, where nodes in the graph represent combinations of different sets of features. Our method automatically learns the structure of the AND/OR graph using score-based structure learning, and we introduce an inference procedure that is able to efficiently compute structure scores. We present promising results and analysis on the difficult and large-scale 2011 TRECVID Multimedia Event Detection dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751446",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2007",
                "id": 36
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 14,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Feature extraction",
                "TV",
                "Histograms",
                "Animals",
                "Image color analysis",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "complex event recognition",
                "features extraction",
                "video data",
                "video features",
                "temporal information",
                "hierarchical method",
                "AND-OR graph structure",
                "score-based structure learning",
                "inference procedure",
                "TRECVID multimedia event detection dataset"
            ],
            "Author Keywords": [
                "Complex Event Recognition",
                "Feature Combination"
            ]
        },
        "id": 336,
        "cited_by": []
    },
    {
        "title": "Space-Time Robust Representation for Action Recognition",
        "authors": [
            "Nicolas Ballas",
            "Yi Yang",
            "Zhen-Zhong Lan",
            "Bertrand Delezoide",
            "Fran\u00e7oise Pr\u00eateux",
            "Alexander Hauptmann"
        ],
        "abstract": "We address the problem of action recognition in unconstrained videos. We propose a novel content driven pooling that leverages space-time context while being robust toward global space-time transformations. Being robust to such transformations is of primary importance in unconstrained videos where the action localizations can drastically shift between frames. Our pooling identifies regions of interest using video structural cues estimated by different saliency functions. To combine the different structural information, we introduce an iterative structure learning algorithm, WSVM (weighted SVM), that determines the optimal saliency layout of an action model through a sparse regularizer. A new optimization method is proposed to solve the WSVM' highly non-smooth objective function. We evaluate our approach on standard action datasets (KTH, UCF50 and HMDB). Most noticeably, the accuracy of our algorithm reaches 51.8% on the challenging HMDB dataset which outperforms the state-of-the-art of 7.3% relatively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751447",
        "reference_list": [
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2011",
                "id": 316
            },
            {
                "year": "2011",
                "id": 76
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 11,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Support vector machines",
                "Context",
                "Feature extraction",
                "Motion segmentation",
                "Encoding",
                "Trajectory"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image recognition",
                "image representation",
                "iterative methods",
                "learning (artificial intelligence)",
                "optimisation",
                "support vector machines",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action recognition",
                "unconstrained videos",
                "global space-time transformations",
                "content driven pooling",
                "action localizations",
                "video structural cues",
                "saliency functions",
                "iterative structure learning algorithm",
                "WSVM",
                "weighted SVM",
                "sparse regularizer",
                "optimization method",
                "nonsmooth objective function",
                "action datasets",
                "HMDB dataset",
                "space-time robust video representation"
            ],
            "Author Keywords": [
                "action recognition",
                "pooling",
                "WSVM",
                "sparse regularization",
                "saliency"
            ]
        },
        "id": 337,
        "cited_by": [
            {
                "year": "2015",
                "id": 362
            }
        ]
    },
    {
        "title": "YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition",
        "authors": [
            "Sergio Guadarrama",
            "Niveda Krishnamoorthy",
            "Girish Malkarnenkar",
            "Subhashini Venugopalan",
            "Raymond Mooney",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "abstract": "Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities ``in-the-wild''. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from Web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects, we also use a Web-scale language model to ``fill in'' novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751448",
        "reference_list": [],
        "citation": {
            "ieee": 69,
            "other": 68,
            "total": 137
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Semantics",
                "Accuracy",
                "YouTube",
                "Visualization",
                "Support vector machines",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "natural language processing",
                "object recognition",
                "social networking (online)",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "YouTube2Text",
                "arbitrary activity description",
                "arbitrary activity recognition",
                "zero-shot recognition",
                "large-scale object recognition",
                "video activity",
                "out-of-domain actions",
                "pretrained model",
                "pragmatic standpoint",
                "semantic hierarchy learning",
                "generalization level",
                "Web-scale natural language corpora",
                "actor-action-object combination penalization",
                "verbs",
                "training set",
                "large YouTube corpus",
                "short-sentence description generation",
                "video clips"
            ],
            "Author Keywords": [
                "Large-scale activity recognition",
                "Describing Activities in videos",
                "Recognizing activities in videos",
                "semantic hierarchies",
                "zero-shot learning"
            ]
        },
        "id": 338,
        "cited_by": [
            {
                "year": "2017",
                "id": 10
            },
            {
                "year": "2017",
                "id": 440
            },
            {
                "year": "2015",
                "id": 270
            },
            {
                "year": "2015",
                "id": 289
            },
            {
                "year": "2015",
                "id": 506
            },
            {
                "year": "2015",
                "id": 512
            },
            {
                "year": "2013",
                "id": 54
            }
        ]
    },
    {
        "title": "Abnormal Event Detection at 150 FPS in MATLAB",
        "authors": [
            "Cewu Lu",
            "Jianping Shi",
            "Jiaya Jia"
        ],
        "abstract": "Speedy abnormal event detection meets the growing demand to process an enormous number of surveillance videos. Based on inherent redundancy of video structures, we propose an efficient sparse combination learning framework. It achieves decent performance in the detection phase without compromising result quality. The short running time is guaranteed because the new method effectively turns the original complicated problem to one in which only a few costless small-scale least square optimization steps are involved. Our method reaches high detection rates on benchmark datasets at a speed of 140-150 frames per second on average when computing on an ordinary desktop PC using MATLAB.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751449",
        "reference_list": [
            {
                "year": "2011",
                "id": 307
            }
        ],
        "citation": {
            "ieee": 90,
            "other": 57,
            "total": 147
        },
        "keywords": {
            "IEEE Keywords": [
                "Silicon",
                "Videos",
                "Testing",
                "Training",
                "Surveillance",
                "Training data",
                "MATLAB"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "least squares approximations",
                "object detection",
                "optimisation",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "speedy abnormal event detection",
                "MATLAB",
                "surveillance videos",
                "efficient sparse combination learning framework",
                "detection phase",
                "short running time",
                "small-scale least square optimization steps",
                "ordinary desktop PC"
            ],
            "Author Keywords": [
                "abnormal event detection",
                "real-time",
                "surveillance video",
                "dictionary learning"
            ]
        },
        "id": 339,
        "cited_by": [
            {
                "year": "2017",
                "id": 35
            },
            {
                "year": "2017",
                "id": 305
            },
            {
                "year": "2017",
                "id": 381
            }
        ]
    },
    {
        "title": "Dynamic Pooling for Complex Event Recognition",
        "authors": [
            "Weixin Li",
            "Qian Yu",
            "Ajay Divakaran",
            "Nuno Vasconcelos"
        ],
        "abstract": "The problem of adaptively selecting pooling regions for the classification of complex video events is considered. Complex events are defined as events composed of several characteristic behaviors, whose temporal configuration can change from sequence to sequence. A dynamic pooling operator is defined so as to enable a unified solution to the problems of event specific video segmentation, temporal structure modeling, and event detection. Video is decomposed into segments, and the segments most informative for detecting a given event are identified, so as to dynamically determine the pooling operator most suited for each sequence. This dynamic pooling is implemented by treating the locations of characteristic segments as hidden information, which is inferred, on a sequence-by-sequence basis, via a large-margin classification rule with latent variables. Although the feasible set of segment selections is combinatorial, it is shown that a globally optimal solution to the inference problem can be obtained efficiently, through the solution of a series of linear programs. Besides the coarse-level location of segments, a finer model of video structure is implemented by jointly pooling features of segment-tuples. Experimental evaluation demonstrates that the resulting event detector has state-of-the-art performance on challenging video datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751450",
        "reference_list": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2007",
                "id": 209
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 9,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Detectors",
                "Visualization",
                "Support vector machines",
                "Histograms",
                "Vectors",
                "Animals"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image recognition",
                "image segmentation",
                "image sequences",
                "linear programming",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "complex event recognition",
                "pooling region adaptive selection",
                "complex video event classification",
                "video dataset",
                "jointly segment-tuple pooling features",
                "video structure",
                "coarse-level segment location",
                "linear programs",
                "inference problem",
                "combinatorial segment selection set",
                "latent variables",
                "large-margin classification rule",
                "sequence-by-sequence basis",
                "hidden information",
                "event detection",
                "temporal structure modeling",
                "specific video segmentation",
                "dynamic pooling operator",
                "temporal configuration"
            ],
            "Author Keywords": [
                "video analysis",
                "pooling",
                "complex event",
                "activity recognition"
            ]
        },
        "id": 340,
        "cited_by": [
            {
                "year": "2017",
                "id": 76
            }
        ]
    },
    {
        "title": "Relative Attributes for Large-Scale Abandoned Object Detection",
        "authors": [
            "Quanfu Fan",
            "Prasad Gabbur",
            "Sharath Pankanti"
        ],
        "abstract": "Effective reduction of false alarms in large-scale video surveillance is rather challenging, especially for applications where abnormal events of interest rarely occur, such as abandoned object detection. We develop an approach to prioritize alerts by ranking them, and demonstrate its great effectiveness in reducing false positives while keeping good detection accuracy. Our approach benefits from a novel representation of abandoned object alerts by relative attributes, namely static ness, foreground ness and abandonment. The relative strengths of these attributes are quantified using a ranking function[19] learnt on suitably designed low-level spatial and temporal features. These attributes of varying strengths are not only powerful in distinguishing abandoned objects from false alarms such as people and light artifacts, but also computationally efficient for large-scale deployment. With these features, we apply a linear ranking algorithm to sort alerts according to their relevance to the end-user. We test the effectiveness of our approach on both public data sets and large ones collected from the real world.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751451",
        "reference_list": [
            {
                "year": "2011",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 9,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Feature extraction",
                "Tracking",
                "Cameras",
                "Object detection",
                "Robustness",
                "Video surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large-scale abandoned object detection",
                "large-scale video surveillance",
                "ranking function",
                "low-level spatial features",
                "low-level temporal features",
                "linear ranking algorithm"
            ],
            "Author Keywords": [
                "abandoned object detection",
                "large-scale deployment",
                "video surveillance",
                "alert ranking",
                "relative attributes"
            ]
        },
        "id": 341,
        "cited_by": [
            {
                "year": "2015",
                "id": 269
            }
        ]
    },
    {
        "title": "Action Recognition and Localization by Hierarchical Space-Time Segments",
        "authors": [
            "Shugao Ma",
            "Jianming Zhang",
            "Nazli Ikizler-Cinbis",
            "Stan Sclaroff"
        ],
        "abstract": "We propose Hierarchical Space-Time Segments as a new representation for action recognition and localization. This representation has a two-level hierarchy. The first level comprises the root space-time segments that may contain a human body. The second level comprises multi-grained space-time segments that contain parts of the root. We present an unsupervised method to generate this representation from video, which extracts both static and non-static relevant space-time segments, and also preserves their hierarchical and temporal relationships. Using simple linear SVM on the resultant bag of hierarchical space-time segments representation, we attain better than, or comparable to, state-of-the-art action recognition performance on two challenging benchmark datasets and at the same time produce good action localization results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751452",
        "reference_list": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2009",
                "id": 118
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2011",
                "id": 253
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 20,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Color",
                "Tracking",
                "Vegetation",
                "Trajectory",
                "Shape",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "image representation",
                "support vector machines",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action recognition",
                "action localization",
                "hierarchical space-time segments",
                "two-level hierarchy",
                "unsupervised method",
                "video representation",
                "nonstatic relevant space-time segments",
                "static space-time segments",
                "temporal relationships",
                "hierarchical relationships",
                "linear SVM",
                "hierarchical space-time segments representation"
            ],
            "Author Keywords": [
                "action recognition",
                "action localization",
                "space-time representation"
            ]
        },
        "id": 342,
        "cited_by": [
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2017",
                "id": 609
            },
            {
                "year": "2015",
                "id": 353
            },
            {
                "year": "2015",
                "id": 366
            },
            {
                "year": "2015",
                "id": 508
            }
        ]
    },
    {
        "title": "The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection",
        "authors": [
            "Mihai Zanfir",
            "Marius Leordeanu",
            "Cristian Sminchisescu"
        ],
        "abstract": "Human action recognition under low observational latency is receiving a growing interest in computer vision due to rapidly developing technologies in human-robot interaction, computer gaming and surveillance. In this paper we propose a fast, simple, yet powerful non-parametric Moving Pose (MP) framework for low-latency human action and activity recognition. Central to our methodology is a moving pose descriptor that considers both pose information as well as differential quantities (speed and acceleration) of the human body joints within a short time window around the current frame. The proposed descriptor is used in conjunction with a modified kNN classifier that considers both the temporal location of a particular frame within the action sequence as well as the discrimination power of its moving pose descriptor compared to other frames in the training set. The resulting method is non-parametric and enables low-latency recognition, one-shot learning, and action detection in difficult unsegmented sequences. Moreover, the framework is real-time, scalable, and outperforms more sophisticated approaches on challenging benchmarks like MSR-Action3D or MSR-DailyActivities3D.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751453",
        "reference_list": [
            {
                "year": "2009",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 116,
            "other": 59,
            "total": 175
        },
        "keywords": {
            "IEEE Keywords": [
                "Joints",
                "Three-dimensional displays",
                "Training",
                "Kinematics",
                "Acceleration",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image motion analysis",
                "image sequences",
                "learning (artificial intelligence)",
                "nonparametric statistics",
                "object detection",
                "object recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D kinematics descriptor",
                "low-latency action recognition",
                "low-latency action detection",
                "human action recognition",
                "computer vision",
                "human-robot interaction",
                "surveillance",
                "nonparametric Moving Pose framework",
                "nonparametric MP framework",
                "low-latency human action recognition",
                "low-latency human activity recognition",
                "pose information",
                "human body joints",
                "short time window",
                "modified kNN classifier",
                "temporal location",
                "action sequence",
                "moving pose descriptor",
                "one-shot learning",
                "unsegmented sequences"
            ],
            "Author Keywords": [
                "action recognition",
                "action detection",
                "moving pose descriptor",
                "RGB-D cameras"
            ]
        },
        "id": 343,
        "cited_by": [
            {
                "year": "2017",
                "id": 151
            },
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2017",
                "id": 611
            },
            {
                "year": "2015",
                "id": 510
            },
            {
                "year": "2015",
                "id": 521
            }
        ]
    },
    {
        "title": "Beyond Hard Negative Mining: Efficient Detector Learning via Block-Circulant Decomposition",
        "authors": [
            "Jo\u00e3o F. Henriques",
            "Jo\u00e3o Carreira",
            "Rui Caseiro",
            "Jorge Batista"
        ],
        "abstract": "Competitive sliding window detectors require vast training sets. Since a pool of natural images provides a nearly endless supply of negative samples, in the form of patches at different scales and locations, training with all the available data is considered impractical. A staple of current approaches is hard negative mining, a method of selecting relevant samples, which is nevertheless expensive. Given that samples at slightly different locations have overlapping support, there seems to be an enormous amount of duplicated work. It is natural, then, to ask whether these redundancies can be eliminated. In this paper, we show that the Gram matrix describing such data is block-circulant. We derive a transformation based on the Fourier transform that block-diagonalizes the Gram matrix, at once eliminating redundancies and partitioning the learning problem. This decomposition is valid for any dense features and several learning algorithms, and takes full advantage of modern parallel architectures. Surprisingly, it allows training with all the potential samples in sets of thousands of images. By considering the full set, we generate in a single shot the optimal solution, which is usually obtained only after several rounds of hard negative mining. We report speed gains on Caltech Pedestrians and INRIA Pedestrians of over an order of magnitude, allowing training on a desktop computer in a couple of minutes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751454",
        "reference_list": [
            {
                "year": "2013",
                "id": 383
            },
            {
                "year": "2011",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 14,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Matrix decomposition",
                "Support vector machines",
                "Fourier transforms",
                "Vectors",
                "Detectors",
                "Redundancy"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "learning (artificial intelligence)",
                "matrix decomposition",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hard negative mining",
                "detector learning algorithm",
                "block-circulant decomposition",
                "competitive sliding window detectors",
                "training sets",
                "natural images",
                "Gram matrix",
                "Fourier transform",
                "parallel architectures",
                "Caltech pedestrians",
                "INRIA pedestrians",
                "desktop computer"
            ],
            "Author Keywords": [
                "detection",
                "hard negative mining",
                "support vector machine",
                "filtering",
                "fourier transform",
                "circulant matrix",
                "block-diagonalization"
            ]
        },
        "id": 344,
        "cited_by": [
            {
                "year": "2017",
                "id": 256
            },
            {
                "year": "2015",
                "id": 318
            },
            {
                "year": "2015",
                "id": 339
            },
            {
                "year": "2015",
                "id": 449
            },
            {
                "year": "2015",
                "id": 481
            }
        ]
    },
    {
        "title": "From Large Scale Image Categorization to Entry-Level Categories",
        "authors": [
            "Vicente Ordonez",
            "Jia Deng",
            "Yejin Choi",
            "Alexander C. Berg",
            "Tamara L. Berg"
        ],
        "abstract": "Entry level categories - the labels people will use to name an object - were originally defined and studied by psychologists in the 1980s. In this paper we study entry-level categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word \"naturalness\" mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751455",
        "reference_list": [],
        "citation": {
            "ieee": 19,
            "other": 13,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Birds",
                "Vocabulary",
                "Predictive models",
                "Training",
                "Noise measurement",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image entry-level categories prediction",
                "image retrieval",
                "natural language image description",
                "human-focused application",
                "Web text",
                "visual recognition prediction",
                "large scale image categorization"
            ],
            "Author Keywords": [
                "categorization",
                "categories",
                "prediction",
                "images",
                "language",
                "entry-level",
                "retrieval"
            ]
        },
        "id": 345,
        "cited_by": [
            {
                "year": "2017",
                "id": 211
            },
            {
                "year": "2015",
                "id": 129
            },
            {
                "year": "2015",
                "id": 289
            }
        ]
    },
    {
        "title": "Fast Subspace Search via Grassmannian Based Hashing",
        "authors": [
            "Xu Wang",
            "Stefan Atev",
            "John Wright",
            "Gilad Lerman"
        ],
        "abstract": "The problem of efficiently deciding which of a database of models is most similar to a given input query arises throughout modern computer vision. Motivated by applications in recognition, image retrieval and optimization, there has been significant recent interest in the variant of this problem in which the database models are linear subspaces and the input is either a point or a subspace. Current approaches to this problem have poor scaling in high dimensions, and may not guarantee sub linear query complexity. We present a new approach to approximate nearest subspace search, based on a simple, new locality sensitive hash for subspaces. Our approach allows point-to-subspace query for a database of subspaces of arbitrary dimension d, in a time that depends sub linearly on the number of subspaces in the database. The query complexity of our algorithm is linear in the ambient dimension D, allowing it to be directly applied to high-dimensional imagery data. Numerical experiments on model problems in image repatching and automatic face recognition confirm the advantages of our algorithm in terms of both speed and accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751456",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Approximation algorithms",
                "Search problems",
                "Computational modeling",
                "Complexity theory",
                "Vectors",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Grassmannian based hashing",
                "fast subspace search",
                "computer vision",
                "image retrieval",
                "locality sensitive hash",
                "point-to-subspace query",
                "query complexity",
                "high-dimensional imagery data",
                "image repatching",
                "automatic face recognition"
            ],
            "Author Keywords": [
                "Subspace Search",
                "Locality Sensitive Hashing",
                "Grassmannian Based Hashing"
            ]
        },
        "id": 346,
        "cited_by": []
    },
    {
        "title": "Finding the Best from the Second Bests - Inhibiting Subjective Bias in Evaluation of Visual Tracking Algorithms",
        "authors": [
            "Yu Pang",
            "Haibin Ling"
        ],
        "abstract": "Evaluating visual tracking algorithms, or trackers for short, is of great importance in computer vision. However, it is hard to fairly compare trackers due to many parameters need to be tuned in the experimental configurations. On the other hand, when introducing a new tracker, a recent trend is to validate it by comparing it with several existing ones. Such an evaluation may have subjective biases towards the new tracker which typically performs the best. This is mainly due to the difficulty to optimally tune all its competitors and sometimes the selected testing sequences. By contrast, little subjective bias exists towards the second best ones in the contest. This observation inspires us with a novel perspective towards inhibiting subjective bias in evaluating trackers by analyzing the results between the second bests. In particular, we first collect all tracking papers published in major computer vision venues in recent years. From these papers, after filtering out potential biases in various aspects, we create a dataset containing many records of comparison results between various visual trackers. Using these records, we derive performance rankings of the involved trackers by four different methods. The first two methods model the dataset as a graph and then derive the rankings over the graph, one by a rank aggregation algorithm and the other by a PageRank-like solution. The other two methods take the records as generated from sports contests and adopt widely used Elo's and Glicko's rating systems to derive the rankings. The experimental results are presented and may serve as a reference for related research.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751457",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2011",
                "id": 151
            },
            {
                "year": "2011",
                "id": 79
            },
            {
                "year": "2011",
                "id": 146
            },
            {
                "year": "2011",
                "id": 167
            },
            {
                "year": "2011",
                "id": 139
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 13,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Conferences",
                "Computer vision",
                "Target tracking",
                "Positron emission tomography",
                "Market research",
                "Surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Elo rating systems",
                "Glicko rating systems",
                "PageRank-like solution",
                "rank aggregation algorithm",
                "visual trackers",
                "dataset",
                "testing sequences",
                "computer vision",
                "visual tracking algorithms",
                "subjective bias inhibition",
                "visual tracking algorithms evaluation"
            ]
        },
        "id": 347,
        "cited_by": [
            {
                "year": "2017",
                "id": 33
            },
            {
                "year": "2015",
                "id": 208
            },
            {
                "year": "2015",
                "id": 345
            },
            {
                "year": "2015",
                "id": 346
            }
        ]
    },
    {
        "title": "Line Assisted Light Field Triangulation and Stereo Matching",
        "authors": [
            "Zhan Yu",
            "Xinqing Guo",
            "Haibing Ling",
            "Andrew Lumsdaine",
            "Jingyi Yu"
        ],
        "abstract": "Light fields are image-based representations that use densely sampled rays as a scene description. In this paper, we explore geometric structures of 3D lines in ray space for improving light field triangulation and stereo matching. The triangulation problem aims to fill in the ray space with continuous and non-overlapping simplices anchored at sampled points (rays). Such a triangulation provides a piecewise-linear interpolant useful for light field super-resolution. We show that the light field space is largely bilinear due to 3D line segments in the scene, and direct triangulation of these bilinear subspaces leads to large errors. We instead present a simple but effective algorithm to first map bilinear subspaces to line constraints and then apply Constrained Delaunay Triangulation (CDT). Based on our analysis, we further develop a novel line-assisted graph-cut (LAGC) algorithm that effectively encodes 3D line constraints into light field stereo matching. Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform state-of-the-art solutions in accuracy and visual quality.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751458",
        "reference_list": [],
        "citation": {
            "ieee": 55,
            "other": 18,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Geometry",
                "Cameras",
                "Image edge detection",
                "Image segmentation",
                "Image resolution",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image coding",
                "image matching",
                "image representation",
                "image resolution",
                "image sampling",
                "image segmentation",
                "interpolation",
                "mesh generation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "line assisted light field triangulation problem",
                "densely sampled ray space",
                "scene description",
                "3D line geometric structure",
                "piecewise-linear interpolant",
                "light field super-resolution",
                "3D line segmentation",
                "constrained Delaunay triangulation",
                "CDT",
                "line-assisted graph-cut algorithm",
                "LAGC algorithm",
                "3D line constraints encoding",
                "light field stereo matching",
                "image-based representation"
            ],
            "Author Keywords": [
                "Light Field Triangulation",
                "Light Field Stereo Matching"
            ]
        },
        "id": 348,
        "cited_by": [
            {
                "year": "2017",
                "id": 1
            },
            {
                "year": "2017",
                "id": 253
            },
            {
                "year": "2017",
                "id": 486
            },
            {
                "year": "2015",
                "id": 385
            },
            {
                "year": "2015",
                "id": 389
            }
        ]
    },
    {
        "title": "A Flexible Scene Representation for 3D Reconstruction Using an RGB-D Camera",
        "authors": [
            "Diego Thomas",
            "Akihiro Sugimoto"
        ],
        "abstract": "Updating a global 3D model with live RGB-D measurements has proven to be successful for 3D reconstruction of indoor scenes. Recently, a Truncated Signed Distance Function (TSDF) volumetric model and a fusion algorithm have been introduced (KinectFusion), showing significant advantages such as computational speed and accuracy of the reconstructed scene. This algorithm, however, is expensive in memory when constructing and updating the global model. As a consequence, the method is not well scalable to large scenes. We propose a new flexible 3D scene representation using a set of planes that is cheap in memory use and, nevertheless, achieves accurate reconstruction of indoor scenes from RGB-D image sequences. Projecting the scene onto different planes reduces significantly the size of the scene representation and thus it allows us to generate a global textured 3D model with lower memory requirement while keeping accuracy and easiness to update with live RGB-D measurements. Experimental results demonstrate that our proposed flexible 3D scene representation achieves accurate reconstruction, while keeping the scalability for large indoor scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751459",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Solid modeling",
                "Image reconstruction",
                "Image color analysis",
                "Color",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image fusion",
                "image reconstruction",
                "image representation",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "flexible scene representation",
                "3D reconstruction",
                "RGB-D camera",
                "truncated signed distance function",
                "TSDF volumetric model",
                "fusion algorithm",
                "KinectFusion",
                "flexible 3D scene representation",
                "image sequences"
            ]
        },
        "id": 349,
        "cited_by": []
    },
    {
        "title": "Automatic Registration of RGB-D Scans via Salient Directions",
        "authors": [
            "Bernhard Zeisl",
            "Kevin K\u00f6ser",
            "Marc Pollefeys"
        ],
        "abstract": "We address the problem of wide-baseline registration of RGB-D data, such as photo-textured laser scans without any artificial targets or prediction on the relative motion. Our approach allows to fully automatically register scans taken in GPS-denied environments such as urban canyon, industrial facilities or even indoors. We build upon image features which are plenty, localized well and much more discriminative than geometry features, however, they suffer from viewpoint distortions and request for normalization. We utilize the principle of salient directions present in the geometry and propose to extract (several) directions from the distribution of surface normals or other cues such as observable symmetries. Compared to previous work we pose no requirements on the scanned scene (like containing large textured planes) and can handle arbitrary surface shapes. Rendering the whole scene from these repeatable directions using an orthographic camera generates textures which are identical up to 2D similarity transformations. This ambiguity is naturally handled by 2D features and allows to find stable correspondences among scans. For geometric pose estimation from tentative matches we propose a fast and robust 2 point sample consensus scheme integrating an early rejection phase. We evaluate our approach on different challenging real world scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751460",
        "reference_list": [
            {
                "year": "2007",
                "id": 287
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Three-dimensional displays",
                "Geometry",
                "Estimation",
                "Cameras",
                "Lasers",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "geometry",
                "image colour analysis",
                "image registration",
                "image texture",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "early rejection phase",
                "2 point sample consensus scheme",
                "tentative matches",
                "geometric pose estimation",
                "2D features",
                "2D similarity transformations",
                "orthographic camera",
                "arbitrary surface shapes",
                "scanned scene",
                "observable symmetries",
                "surface normals",
                "salient directions",
                "normalization request",
                "viewpoint distortions",
                "image features",
                "GPS-denied environments",
                "photo-textured laser scans",
                "RGB-D data",
                "wide-baseline registration"
            ],
            "Author Keywords": [
                "point cloud registration",
                "RGB-D registration",
                "3D reconstruction"
            ]
        },
        "id": 350,
        "cited_by": [
            {
                "year": "2015",
                "id": 237
            }
        ]
    },
    {
        "title": "Real-Time Solution to the Absolute Pose Problem with Unknown Radial Distortion and Focal Length",
        "authors": [
            "Zuzana Kukelova",
            "Martin Bujnak",
            "Tomas Pajdla"
        ],
        "abstract": "The problem of determining the absolute position and orientation of a camera from a set of 2D-to-3D point correspondences is one of the most important problems in computer vision with a broad range of applications. In this paper we present a new solution to the absolute pose problem for camera with unknown radial distortion and unknown focal length from five 2D-to-3D point correspondences. Our new solver is numerically more stable, more accurate, and significantly faster than the existing state-of-the-art minimal four point absolute pose solvers for this problem. Moreover, our solver results in less solutions and can handle larger radial distortions. The new solver is straightforward and uses only simple concepts from linear algebra. Therefore it is simpler than the state-of-the-art Groebner basis solvers. We compare our new solver with the existing state-of-the-art solvers and show its usefulness on synthetic and real datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751461",
        "reference_list": [
            {
                "year": "2007",
                "id": 97
            },
            {
                "year": "2007",
                "id": 276
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 9,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Mathematical model",
                "Transmission line matrix methods",
                "Noise",
                "Three-dimensional displays",
                "Polynomials"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "linear algebra",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time solution",
                "absolute pose problem",
                "unknown radial distortion",
                "unknown focal length",
                "absolute position determination",
                "absolute orientation determination",
                "2D-to-3D point correspondence set",
                "computer vision",
                "camera",
                "absolute pose solvers",
                "radial distortion",
                "linear algebra",
                "Groebner basis solvers"
            ],
            "Author Keywords": [
                "absolute pose",
                "radial distortion",
                "focal length",
                "3D reconstruction"
            ]
        },
        "id": 351,
        "cited_by": [
            {
                "year": "2017",
                "id": 65
            },
            {
                "year": "2017",
                "id": 244
            },
            {
                "year": "2015",
                "id": 244
            },
            {
                "year": "2015",
                "id": 257
            }
        ]
    },
    {
        "title": "Rectangling Stereographic Projection for Wide-Angle Image Visualization",
        "authors": [
            "Che-Han Chang",
            "Min-Chun Hu",
            "Wen-Huang Cheng",
            "Yung-Yu Chuang"
        ],
        "abstract": "This paper proposes a new projection model for mapping a hemisphere to a plane. Such a model can be useful for viewing wide-angle images. Our model consists of two steps. In the first step, the hemisphere is projected onto a swung surface constructed by a circular profile and a rounded rectangular trajectory. The second step maps the projected image on the swung surface onto the image plane through the perspective projection. We also propose a method for automatically determining proper parameters for the projection model based on image content. The proposed model has several advantages. It is simple, efficient and easy to control. Most importantly, it makes a better compromise between distortion minimization and line preserving than popular projection models, such as stereographic and Pannini projections. Experiments and analysis demonstrate the effectiveness of our model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751462",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Computational modeling",
                "Equations",
                "Mathematical model",
                "Visualization",
                "Shape",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "data visualisation",
                "minimisation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "rectangling stereographic projection",
                "wide-angle image visualization",
                "projection model",
                "swung surface",
                "circular profile",
                "rounded rectangular trajectory",
                "projected image",
                "image plane",
                "perspective projection",
                "image content",
                "distortion minimization",
                "line preserving",
                "Pannini projection"
            ]
        },
        "id": 352,
        "cited_by": [
            {
                "year": "2017",
                "id": 498
            }
        ]
    },
    {
        "title": "Accurate Blur Models vs. Image Priors in Single Image Super-resolution",
        "authors": [
            "Netalee Efrat",
            "Daniel Glasner",
            "Alexander Apartsin",
            "Boaz Nadler",
            "Anat Levin"
        ],
        "abstract": "Over the past decade, single image Super-Resolution (SR) research has focused on developing sophisticated image priors, leading to significant advances. Estimating and incorporating the blur model, that relates the high-res and low-res images, has received much less attention, however. In particular, the reconstruction constraint, namely that the blurred and down sampled high-res output should approximately equal the low-res input image, has been either ignored or applied with default fixed blur models. In this work, we examine the relative importance of the image prior and the reconstruction constraint. First, we show that an accurate reconstruction constraint combined with a simple gradient regularization achieves SR results almost as good as those of state-of-the-art algorithms with sophisticated image priors. Second, we study both empirically and theoretically the sensitivity of SR algorithms to the blur model assumed in the reconstruction constraint. We find that an accurate blur model is more important than a sophisticated image prior. Finally, using real camera data, we demonstrate that the default blur models of various SR algorithms may differ from the camera blur, typically leading to over-smoothed results. Our findings highlight the importance of accurately estimating camera blur in reconstructing raw lowers images acquired by an actual camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751463",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2005",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 21,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Cameras",
                "Image reconstruction",
                "Sensitivity",
                "Algorithm design and analysis",
                "Noise",
                "Interpolation"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction",
                "image resolution",
                "image restoration",
                "image sampling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image priors",
                "single-image super-resolution",
                "single-image SR",
                "reconstruction constraint",
                "blurred downsampled high-resolution output image",
                "low-resolution input image",
                "gradient regularization",
                "empirical analysis",
                "SR algorithm sensitivity",
                "default blur models",
                "raw low-resolution image reconstruction",
                "camera blur estimation"
            ]
        },
        "id": 353,
        "cited_by": [
            {
                "year": "2015",
                "id": 58
            }
        ]
    },
    {
        "title": "Learning Slow Features for Behaviour Analysis",
        "authors": [
            "Lazaros Zafeiriou",
            "Mihalis A. Nicolaou",
            "Stefanos Zafeiriou",
            "Symeon Nikitidis",
            "Maja Pantic"
        ],
        "abstract": "A recently introduced latent feature learning technique for time varying dynamic phenomena analysis is the so called Slow Feature Analysis (SFA). SFA is a deterministic component analysis technique for multi-dimensional sequences that by minimizing the variance of the first order time derivative approximation of the input signal finds uncorrelated projections that extract slowly-varying features ordered by their temporal consistency and constancy. In this paper, we propose a number of extensions in both the deterministic and the probabilistic SFA optimization frameworks. In particular, we derive a novel deterministic SFA algorithm that is able to identify linear projections that extract the common slowest varying features of two or more sequences. In addition, we propose an Expectation Maximization (EM) algorithm to perform inference in a probabilistic formulation of SFA and similarly extend it in order to handle two and more time varying data sequences. Moreover, we demonstrate that the probabilistic SFA (EMSFA) algorithm that discovers the common slowest varying latent space of multiple sequences can be combined with dynamic time warping techniques for robust sequence time alignment. The proposed SFA algorithms were applied for facial behavior analysis demonstrating their usefulness and appropriateness for this task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751464",
        "reference_list": [
            {
                "year": "2011",
                "id": 109
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 1,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Probabilistic logic",
                "Covariance matrices",
                "Optimization",
                "Feature extraction",
                "Mathematical model",
                "Algorithm design and analysis",
                "Eigenvalues and eigenfunctions"
            ],
            "INSPEC: Controlled Indexing": [
                "deterministic algorithms",
                "expectation-maximisation algorithm",
                "face recognition",
                "feature extraction",
                "image sequences",
                "learning (artificial intelligence)",
                "optimisation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "slow feature analysis learning",
                "latent feature learning technique",
                "time varying dynamic phenomena analysis",
                "deterministic component analysis technique",
                "multidimensional sequences",
                "first order time derivative approximation",
                "input signal",
                "probabilistic SFA optimization frameworks",
                "deterministic SFA algorithm",
                "linear projections",
                "slow varying feature extraction",
                "expectation maximization algorithm",
                "probabilistic formulation",
                "time varying data sequences",
                "EMSFA algorithm",
                "dynamic time warping techniques",
                "robust sequence time alignment",
                "facial behavior analysis"
            ],
            "Author Keywords": [
                "Slow Feature Analysis",
                "Component Analysis"
            ]
        },
        "id": 354,
        "cited_by": []
    },
    {
        "title": "Fast Face Detector Training Using Tailored Views",
        "authors": [
            "Kristina Scherbaum",
            "James Petterson",
            "Rogerio S. Feris",
            "Volker Blanz",
            "Hans-Peter Seidel"
        ],
        "abstract": "Face detection is an important task in computer vision and often serves as the first step for a variety of applications. State-of-the-art approaches use efficient learning algorithms and train on large amounts of manually labeled imagery. Acquiring appropriate training images, however, is very time-consuming and does not guarantee that the collected training data is representative in terms of data variability. Moreover, available data sets are often acquired under controlled settings, restricting, for example, scene illumination or 3D head pose to a narrow range. This paper takes a look into the automated generation of adaptive training samples from a 3D morphable face model. Using statistical insights, the tailored training data guarantees full data variability and is enriched by arbitrary facial attributes such as age or body weight. Moreover, it can automatically adapt to environmental constraints, such as illumination or viewing angle of recorded video footage from surveillance cameras. We use the tailored imagery to train a new many-core implementation of Viola Jones' AdaBoost object detection framework. The new implementation is not only faster but also enables the use of multiple feature channels such as color features at training time. In our experiments we trained seven view-dependent face detectors and evaluate these on the Face Detection Data Set and Benchmark (FDDB). Our experiments show that the use of tailored training imagery outperforms state-of-the-art approaches on this challenging dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751465",
        "reference_list": [
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2011",
                "id": 77
            },
            {
                "year": "2007",
                "id": 288
            },
            {
                "year": "2009",
                "id": 305
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Training",
                "Three-dimensional displays",
                "Image color analysis",
                "Training data",
                "Solid modeling",
                "Face detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "feature extraction",
                "image colour analysis",
                "learning (artificial intelligence)",
                "statistical analysis",
                "video cameras",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast face detector training",
                "computer vision",
                "learning algorithm",
                "manually-labeled imagery",
                "training images",
                "controlled settings",
                "scene illumination",
                "3D head pose",
                "adaptive training sample automated generation",
                "3D morphable face model",
                "statistical insight",
                "tailored training data",
                "full-data variability",
                "arbitrary facial attribute",
                "environmental constraint",
                "viewing angle",
                "recorded video footage",
                "surveillance cameras",
                "Viola Jones AdaBoost object detection framework",
                "multiple-feature channels",
                "color features",
                "training time",
                "view-dependent face detectors",
                "face detection data set and benchmark",
                "FDDB"
            ],
            "Author Keywords": [
                "Face detection",
                "face reconstruction",
                "morphable face model",
                "ada boost"
            ]
        },
        "id": 355,
        "cited_by": []
    },
    {
        "title": "Simultaneous Clustering and Tracklet Linking for Multi-face Tracking in Videos",
        "authors": [
            "Baoyuan Wu",
            "Siwei Lyu",
            "Bao-Gang Hu",
            "Qiang Ji"
        ],
        "abstract": "We describe a novel method that simultaneously clusters and associates short sequences of detected faces (termed as face track lets) in videos. The rationale of our method is that face track let clustering and linking are related problems that can benefit from the solutions of each other. Our method is based on a hidden Markov random field model that represents the joint dependencies of cluster labels and track let linking associations. We provide an efficient algorithm based on constrained clustering and optimal matching for the simultaneous inference of cluster labels and track let associations. We demonstrate significant improvements on the state-of-the-art results in face tracking and clustering performances on several video datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751466",
        "reference_list": [
            {
                "year": "2011",
                "id": 197
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 11,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Joining processes",
                "Hidden Markov models",
                "Tracking",
                "Videos",
                "Clustering algorithms",
                "Optimization",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "hidden Markov models",
                "image matching",
                "image sequences",
                "object tracking",
                "pattern clustering",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "simultaneous clustering",
                "multiface tracking",
                "short detected face sequences",
                "face tracklet clustering",
                "face tracklet linking",
                "hidden Markov random field model",
                "cluster labels",
                "tracklet linking associations",
                "constrained clustering",
                "optimal matching",
                "cluster labels simultaneous inference",
                "video datasets"
            ]
        },
        "id": 356,
        "cited_by": [
            {
                "year": "2017",
                "id": 554
            },
            {
                "year": "2015",
                "id": 464
            }
        ]
    },
    {
        "title": "A Deep Sum-Product Architecture for Robust Facial Attributes Analysis",
        "authors": [
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Recent works have shown that facial attributes are useful in a number of applications such as face recognition and retrieval. However, estimating attributes in images with large variations remains a big challenge. This challenge is addressed in this paper. Unlike existing methods that assume the independence of attributes during their estimation, our approach captures the interdependencies of local regions for each attribute, as well as the high-order correlations between different attributes, which makes it more robust to occlusions and misdetection of face regions. First, we have modeled region interdependencies with a discriminative decision tree, where each node consists of a detector and a classifier trained on a local region. The detector allows us to locate the region, while the classifier determines the presence or absence of an attribute. Second, correlations of attributes and attribute predictors are modeled by organizing all of the decision trees into a large sum-product network (SPN), which is learned by the EM algorithm and yields the most probable explanation (MPE) of the facial attributes in terms of the region's localization and classification. Experimental results on a large data set with 22,400 images show the effectiveness of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751467",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2013",
                "id": 330
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 18,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Face",
                "Training",
                "Decision trees",
                "Robustness",
                "Detectors",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "face recognition",
                "image classification",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep sum-product architecture",
                "robust facial attribute analysis",
                "image attribute estimation",
                "local region interdependencies",
                "high-order correlations",
                "face region misdetection robustness",
                "occlusion robustness",
                "discriminative decision tree",
                "classifier",
                "detector",
                "attribute correlations",
                "attribute predictors",
                "sum-product network",
                "SPN",
                "EM algorithm",
                "most probable explanation",
                "MPE",
                "region localization",
                "region classification"
            ],
            "Author Keywords": [
                "face recognition",
                "attributes",
                "deep learning"
            ]
        },
        "id": 357,
        "cited_by": [
            {
                "year": "2015",
                "id": 405
            },
            {
                "year": "2015",
                "id": 416
            },
            {
                "year": "2013",
                "id": 330
            }
        ]
    },
    {
        "title": "Hidden Factor Analysis for Age Invariant Face Recognition",
        "authors": [
            "Dihong Gong",
            "Zhifeng Li",
            "Dahua Lin",
            "Jianzhuang Liu",
            "Xiaoou Tang"
        ],
        "abstract": "Age invariant face recognition has received increasing attention due to its great potential in real world applications. In spite of the great progress in face recognition techniques, reliably recognizing faces across ages remains a difficult task. The facial appearance of a person changes substantially over time, resulting in significant intra-class variations. Hence, the key to tackle this problem is to separate the variation caused by aging from the person-specific features that are stable. Specifically, we propose a new method, called Hidden Factor Analysis (HFA). This method captures the intuition above through a probabilistic model with two latent factors: an identity factor that is age-invariant and an age factor affected by the aging process. Then, the observed appearance can be modeled as a combination of the components generated based on these factors. We also develop a learning algorithm that jointly estimates the latent factors and the model parameters using an EM procedure. Extensive experiments on two well-known public domain face aging datasets: MORPH (the largest public face aging database) and FGNET, clearly show that the proposed method achieves notable improvement over state-of-the-art algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751468",
        "reference_list": [
            {
                "year": "2009",
                "id": 79
            },
            {
                "year": "2007",
                "id": 210
            },
            {
                "year": "2005",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 22,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Face recognition",
                "Aging",
                "Computational modeling",
                "Mathematical model",
                "Vectors",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "face recognition",
                "learning (artificial intelligence)",
                "probability",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hidden factor analysis",
                "age invariant face recognition",
                "facial appearance",
                "intra-class variations",
                "HFA",
                "probabilistic model",
                "identity factor",
                "age factor",
                "learning algorithm",
                "EM procedure",
                "public domain face aging datasets",
                "MORPH dataset",
                "FGNET dataset",
                "expectation-maximisation algorithm"
            ],
            "Author Keywords": [
                "Age invariance",
                "face recognition"
            ]
        },
        "id": 358,
        "cited_by": [
            {
                "year": "2015",
                "id": 443
            }
        ]
    },
    {
        "title": "A Learning-Based Approach to Reduce JPEG Artifacts in Image Matting",
        "authors": [
            "Inchang Choi",
            "Sunyeong Kim",
            "Michael S. Brown",
            "Yu-Wing Tai"
        ],
        "abstract": "Single image matting techniques assume high-quality input images. The vast majority of images on the web and in personal photo collections are encoded using JPEG compression. JPEG images exhibit quantization artifacts that adversely affect the performance of matting algorithms. To address this situation, we propose a learning-based post-processing method to improve the alpha mattes extracted from JPEG images. Our approach learns a set of sparse dictionaries from training examples that are used to transfer details from high-quality alpha mattes to alpha mattes corrupted by JPEG compression. Three different dictionaries are defined to accommodate different object structure (long hair, short hair, and sharp boundaries). A back-projection criteria combined within an MRF framework is used to automatically select the best dictionary to apply on the object's local boundary. We demonstrate that our method can produces superior results over existing state-of-the-art matting algorithms on a variety of inputs and compression levels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751469",
        "reference_list": [
            {
                "year": "2003",
                "id": 118
            },
            {
                "year": "2009",
                "id": 113
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Image coding",
                "Transform coding",
                "Hair",
                "Image reconstruction",
                "Training",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning-based approach",
                "JPEG artifacts",
                "single image matting techniques",
                "JPEG compression",
                "learning-based post-processing method",
                "alpha mattes",
                "sparse dictionaries",
                "object structure",
                "back-projection criteria",
                "MRF framework"
            ],
            "Author Keywords": [
                "Matting",
                "Jpeg Deblocking",
                "Learning"
            ]
        },
        "id": 359,
        "cited_by": [
            {
                "year": "2015",
                "id": 174
            }
        ]
    },
    {
        "title": "A New Image Quality Metric for Image Auto-denoising",
        "authors": [
            "Xiangfei Kong",
            "Kuan Li",
            "Qingxiong Yang",
            "Liu Wenyin",
            "Ming-Hsuan Yang"
        ],
        "abstract": "This paper proposes a new non-reference image quality metric that can be adopted by the state-of-the-art image/ video denoising algorithms for auto-denoising. The proposed metric is extremely simple and can be implemented in four lines of Matlab code. The basic assumption employed by the proposed metric is that the noise should be independent of the original image. A direct measurement of this dependence is, however, impractical due to the relatively low accuracy of existing denoising method. The proposed metric thus aims at maximizing the structure similarity between the input noisy image and the estimated image noise around homogeneous regions and the structure similarity between the input noisy image and the denoised image around highly-structured regions, and is computed as the linear correlation coefficient of the two corresponding structure similarity maps. Numerous experimental results demonstrate that the proposed metric not only outperforms the current state-of-the-art non-reference quality metric quantitatively and qualitatively, but also better maintains temporal coherence when used for video denoising.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751470",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 11,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise measurement",
                "PSNR",
                "Noise reduction",
                "Noise level",
                "Correlation"
            ],
            "INSPEC: Controlled Indexing": [
                "image denoising"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image quality metric",
                "image autodenoising",
                "video denoising algorithms",
                "image denoising algorithms",
                "Matlab code",
                "structure similarity",
                "linear correlation coefficient",
                "structure similarity maps",
                "nonreference quality metric"
            ]
        },
        "id": 360,
        "cited_by": []
    },
    {
        "title": "Joint Noise Level Estimation from Personal Photo Collections",
        "authors": [
            "Yichang Shih",
            "Vivek Kwatra",
            "Troy Chinen",
            "Hui Fang",
            "Sergey Ioffe"
        ],
        "abstract": "Personal photo albums are heavily biased towards faces of people, but most state-of-the-art algorithms for image denoising and noise estimation do not exploit facial information. We propose a novel technique for jointly estimating noise levels of all face images in a photo collection. Photos in a personal album are likely to contain several faces of the same people. While some of these photos would be clean and high quality, others may be corrupted by noise. Our key idea is to estimate noise levels by comparing multiple images of the same content that differ predominantly in their noise content. Specifically, we compare geometrically and photo metrically aligned face images of the same person. Our estimation algorithm is based on a probabilistic formulation that seeks to maximize the joint probability of estimated noise levels across all images. We propose an approximate solution that decomposes this joint maximization into a two-stage optimization. The first stage determines the relative noise between pairs of images by pooling estimates from corresponding patch pairs in a probabilistic fashion. The second stage then jointly optimizes for all absolute noise parameters by conditioning them upon relative noise levels, which allows for a pair wise factorization of the probability distribution. We evaluate our noise estimation method using quantitative experiments to measure accuracy on synthetic data. Additionally, we employ the estimated noise levels for automatic denoising using \"BM3D\", and evaluate the quality of denoising on real-world photos through a user study.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751471",
        "reference_list": [
            {
                "year": "2009",
                "id": 284
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise level",
                "Estimation",
                "Joints",
                "Image color analysis",
                "Noise measurement",
                "Colored noise"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "face recognition",
                "image denoising"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint noise level estimation",
                "personal photo collections",
                "personal photo albums",
                "image denoising",
                "facial information",
                "noise content",
                "face images",
                "probabilistic formulation",
                "probabilistic fashion",
                "probability distribution",
                "noise estimation method"
            ],
            "Author Keywords": [
                "Photo collections",
                "image noise estimation and denoising"
            ]
        },
        "id": 361,
        "cited_by": []
    },
    {
        "title": "Latent Data Association: Bayesian Model Selection for Multi-target Tracking",
        "authors": [
            "Aleksandr V. Segal",
            "Ian Reid"
        ],
        "abstract": "We propose a novel parametrization of the data association problem for multi-target tracking. In our formulation, the number of targets is implicitly inferred together with the data association, effectively solving data association and model selection as a single inference problem. The novel formulation allows us to interpret data association and tracking as a single Switching Linear Dynamical System (SLDS). We compute an approximate posterior solution to this problem using a dynamic programming/message passing technique. This inference-based approach allows us to incorporate richer probabilistic models into the tracking system. In particular, we incorporate inference over inliers/outliers and track termination times into the system. We evaluate our approach on publicly available datasets and demonstrate results competitive with, and in some cases exceeding the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751472",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 7,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Radar tracking",
                "Detectors",
                "Trajectory",
                "Data models",
                "Probabilistic logic"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "dynamic programming",
                "message passing",
                "sensor fusion",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "inference-based approach",
                "message passing technique",
                "dynamic programming",
                "SLDS",
                "switching linear dynamical system",
                "single inference problem",
                "data association problem",
                "multitarget tracking",
                "Bayesian model selection",
                "latent data association"
            ],
            "Author Keywords": [
                "multi-target tracking",
                "data association",
                "bayesian inference",
                "hybrid inference",
                "model selection",
                "message passing"
            ]
        },
        "id": 362,
        "cited_by": [
            {
                "year": "2015",
                "id": 340
            },
            {
                "year": "2015",
                "id": 524
            }
        ]
    },
    {
        "title": "Initialization-Insensitive Visual Tracking through Voting with Salient Local Features",
        "authors": [
            "Kwang Moo Yi",
            "Hawook Jeong",
            "Byeongho Heo",
            "Hyung Jin Chang",
            "Jin Young Choi"
        ],
        "abstract": "In this paper we propose an object tracking method in case of inaccurate initializations. To track objects accurately in such situation, the proposed method uses \"motion saliency\" and \"descriptor saliency\" of local features and performs tracking based on generalized Hough transform (GHT). The proposed motion saliency of a local feature emphasizes features having distinctive motions, compared to the motions which are not from the target object. The descriptor saliency emphasizes features which are likely to be of the object in terms of its feature descriptors. Through these saliencies, the proposed method tries to \"learn and find\" the target object rather than looking for what was given at initialization, giving robust results even with inaccurate initializations. Also, our tracking result is obtained by combining the results of each local feature of the target and the surroundings with GHT voting, thus is robust against severe occlusions as well. The proposed method is compared against nine other methods, with nine image sequences, and hundred random initializations. The experimental results show that our method outperforms all other compared methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751473",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Robustness",
                "Feature extraction",
                "Adaptation models",
                "Optical sensors",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Hough transforms",
                "image motion analysis",
                "image sequences",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "initialization-insensitive visual tracking",
                "salient local features",
                "object tracking method",
                "motion saliency",
                "descriptor saliency",
                "generalized Hough transform",
                "distinctive motion",
                "feature descriptor",
                "target object",
                "GHT voting",
                "occlusions",
                "image sequence",
                "random initialization"
            ],
            "Author Keywords": [
                "Visual Tracking",
                "Initialization",
                "Saliency",
                "Local Feature",
                "Generalized Hough Transform"
            ]
        },
        "id": 363,
        "cited_by": []
    },
    {
        "title": "Pose-Configurable Generic Tracking of Elongated Objects",
        "authors": [
            "Daniel Wesierski",
            "Patrick Horain"
        ],
        "abstract": "Elongated objects have various shapes and can shift, rotate, change scale, and be rigid or deform by flexing, articulating, and vibrating, with examples as varied as a glass bottle, a robotic arm, a surgical suture, a finger pair, a tram, and a guitar string. This generally makes tracking of poses of elongated objects very challenging. We describe a unified, configurable framework for tracking the pose of elongated objects, which move in the image plane and extend over the image region. Our method strives for simplicity, versatility, and efficiency. The object is decomposed into a chained assembly of segments of multiple parts that are arranged under a hierarchy of tailored spatio-temporal constraints. In this hierarchy, segments can rescale independently while their elasticity is controlled with global orientations and local distances. While the trend in tracking is to design complex, structure-free algorithms that update object appearance on-line, we show that our tracker, with the novel but remarkably simple, structured organization of parts with constant appearance, reaches or improves state-of-the-art performance. Most importantly, our model can be easily configured to track exact pose of arbitrary, elongated objects in the image plane. The tracker can run up to 100 fps on a desktop PC, yet the computation time scales linearly with the number of object parts. To our knowledge, this is the first approach to generic tracking of elongated objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751474",
        "reference_list": [
            {
                "year": "2011",
                "id": 172
            },
            {
                "year": "2011",
                "id": 10
            },
            {
                "year": "2011",
                "id": 334
            },
            {
                "year": "2005",
                "id": 197
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Image segmentation",
                "Robustness",
                "Image color analysis",
                "Histograms",
                "Shape",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "object tracking",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "constant appearance",
                "object appearance",
                "complex structure-free algorithms",
                "local distances",
                "global orientations",
                "spatio-temporal constraints",
                "image plane",
                "elongated objects",
                "pose-configurable generic tracking"
            ],
            "Author Keywords": [
                "visual object tracking",
                "tracking elongated objects",
                "tracking longitudinal objects",
                "generic tracking",
                "configurable tracking"
            ]
        },
        "id": 364,
        "cited_by": []
    },
    {
        "title": "Conservation Tracking",
        "authors": [
            "Martin Schiegg",
            "Philipp Hanslovsky",
            "Bernhard X. Kausler",
            "Lars Hufnagel",
            "Fred A. Hamprecht"
        ],
        "abstract": "The quality of any tracking-by-assignment hinges on the accuracy of the foregoing target detection / segmentation step. In many kinds of images, errors in this first stage are unavoidable. These errors then propagate to, and corrupt, the tracking result. Our main contribution is the first probabilistic graphical model that can explicitly account for over- and under segmentation errors even when the number of tracking targets is unknown and when they may divide, as in cell cultures. The tracking model we present implements global consistency constraints for the number of targets comprised by each detection and is solved to global optimality on reasonably large 2D+t and 3D+t datasets. In addition, we empirically demonstrate the effectiveness of a post processing that allows to establish target identity even across occlusion / under segmentation. The usefulness and efficiency of this new tracking method is demonstrated on three different and challenging 2D+t and 3D+t datasets from developmental biology.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751475",
        "reference_list": [
            {
                "year": "2011",
                "id": 17
            },
            {
                "year": "2009",
                "id": 95
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 18,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Graphical models",
                "Probabilistic logic",
                "Image segmentation",
                "Three-dimensional displays",
                "Correlation",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "biological techniques",
                "biology computing",
                "cellular biophysics",
                "graph theory",
                "image segmentation",
                "probability",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fruit flies",
                "Drosophila embryo",
                "undersegmentation error",
                "oversegmentation error",
                "developmental biology",
                "occlusion",
                "3D+t dataset",
                "2D+t dataset",
                "global optimality",
                "global consistency constraints",
                "tracking model",
                "target tracking",
                "probabilistic graphical model",
                "target detection-segmentation step accuracy",
                "tracking-by-assignment quality",
                "conservation tracking"
            ],
            "Author Keywords": [
                "Tracking",
                "Graphical Model",
                "Factor Graph",
                "Segmentation",
                "Integer Linear Programming"
            ]
        },
        "id": 365,
        "cited_by": [
            {
                "year": "2017",
                "id": 493
            },
            {
                "year": "2017",
                "id": 506
            }
        ]
    },
    {
        "title": "Partial Enumeration and Curvature Regularization",
        "authors": [
            "Carl Olsson",
            "Johannes Ul\u00e9n",
            "Yuri Boykov",
            "Vladimir Kolmogorov"
        ],
        "abstract": "Energies with high-order non-sub modular interactions have been shown to be very useful in vision due to their high modeling power. Optimization of such energies, however, is generally NP-hard. A naive approach that works for small problem instances is exhaustive search, that is, enumeration of all possible labelings of the underlying graph. We propose a general minimization approach for large graphs based on enumeration of labelings of certain small patches. This partial enumeration technique reduces complex high-order energy formulations to pair wise Constraint Satisfaction Problems with unary costs (uCSP), which can be efficiently solved using standard methods like TRW-S. Our approach outperforms a number of existing state-of-the-art algorithms on well known difficult problems (e.g. curvature regularization, stereo, deconvolution), it gives near global minimum and better speed. Our main application of interest is curvature regularization. In the context of segmentation, our partial enumeration technique allows to evaluate curvature directly on small patches using a novel integral geometry approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751476",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Optimization",
                "Standards",
                "Labeling",
                "Approximation algorithms",
                "Approximation methods",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "constraint satisfaction problems",
                "graph theory",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "integral geometry approach",
                "partial enumeration technique",
                "uCSP",
                "constraint satisfaction problems",
                "underlying graph",
                "optimization",
                "NP-hard problem",
                "curvature regularization"
            ]
        },
        "id": 366,
        "cited_by": [
            {
                "year": "2015",
                "id": 44
            }
        ]
    },
    {
        "title": "Total Variation Regularization for Functions with Values in a Manifold",
        "authors": [
            "Jan Lellmann",
            "Evgeny Strekalovskiy",
            "Sabrina Koetter",
            "Daniel Cremers"
        ],
        "abstract": "While total variation is among the most popular regularizers for variational problems, its extension to functions with values in a manifold is an open problem. In this paper, we propose the first algorithm to solve such problems which applies to arbitrary Riemannian manifolds. The key idea is to reformulate the variational problem as a multilabel optimization problem with an infinite number of labels. This leads to a hard optimization problem which can be approximately solved using convex relaxation techniques. The framework can be easily adapted to different manifolds including spheres and three-dimensional rotations, and allows to obtain accurate solutions even with a relatively coarse discretization. With numerous examples we demonstrate that the proposed framework can be applied to variational models that incorporate chromaticity values, normal fields, or camera trajectories.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751477",
        "reference_list": [
            {
                "year": "2009",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 18,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Noise reduction",
                "Vectors",
                "Optimization",
                "Labeling",
                "Computational modeling",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "convex programming",
                "image denoising"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "total variation regularization",
                "variational problems",
                "arbitrary Riemannian manifolds",
                "multilabel optimization problem",
                "optimization problem",
                "convex relaxation techniques",
                "three-dimensional rotations",
                "coarse discretization",
                "variational models",
                "chromaticity values",
                "camera trajectories",
                "denoising"
            ],
            "Author Keywords": [
                "variational methods",
                "total variation",
                "denoising",
                "manifold",
                "tensor",
                "angular data",
                "rotation group"
            ]
        },
        "id": 367,
        "cited_by": [
            {
                "year": "2017",
                "id": 124
            }
        ]
    },
    {
        "title": "Matching Dry to Wet Materials",
        "authors": [
            "Yaser Yacoob"
        ],
        "abstract": "When a translucent liquid is spilled over a rough surface it causes a significant change in the visual appearance of the surface. This wetting phenomenon is easily detected by humans, and an early model was devised by the physicist Andres Jonas Angstrom nearly a century ago. In this paper we investigate the problem of determining if a wet/dry relationship between two image patches explains the differences in their visual appearance. Water tends to be the typical liquid involved and therefore it is the main objective. At the same time, we consider the general problem where the liquid has some of the characteristics of water (i.e., a similar refractive index), but has an unknown spectral absorption profile (e.g., coffee, tea, wine, etc.). We report on several experiments using our own images, a publicly available dataset, and images downloaded from the web.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751478",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Liquids",
                "Absorption",
                "Optical surface waves",
                "Surface treatment",
                "Rough surfaces",
                "Surface roughness"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "refractive index",
                "wetting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dry-to-wet materials matching",
                "translucent liquid",
                "wetting phenomenon",
                "wet-dry relationship",
                "image patches",
                "refractive index",
                "spectral absorption profile",
                "coffee",
                "tea",
                "wine",
                "downloaded images"
            ],
            "Author Keywords": [
                "Wet surfaces",
                "Dry surfaces",
                "spectral absorption"
            ]
        },
        "id": 368,
        "cited_by": []
    },
    {
        "title": "Unsupervised Visual Domain Adaptation Using Subspace Alignment",
        "authors": [
            "Basura Fernando",
            "Amaury Habrard",
            "Marc Sebban",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this paper, we introduce a new domain adaptation (DA) algorithm where the source and target domains are represented by subspaces described by eigenvectors. In this context, our method seeks a domain adaptation solution by learning a mapping function which aligns the source subspace with the target one. We show that the solution of the corresponding optimization problem can be obtained in a simple closed form, leading to an extremely fast algorithm. We use a theoretical result to tune the unique hyper parameter corresponding to the size of the subspaces. We run our method on various datasets and show that, despite its intrinsic simplicity, it outperforms state of the art DA methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751479",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            }
        ],
        "citation": {
            "ieee": 203,
            "other": 94,
            "total": 297
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Context",
                "Manifolds",
                "Principal component analysis",
                "Support vector machines",
                "Covariance matrices",
                "Eigenvalues and eigenfunctions"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "image classification",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised visual domain adaptation",
                "subspace alignment",
                "DA algorithm",
                "eigenvectors",
                "mapping function",
                "optimization problem"
            ],
            "Author Keywords": [
                "domain adaptation",
                "subspace alignment",
                "object recognition"
            ]
        },
        "id": 369,
        "cited_by": [
            {
                "year": "2017",
                "id": 62
            },
            {
                "year": "2017",
                "id": 78
            },
            {
                "year": "2017",
                "id": 213
            },
            {
                "year": "2017",
                "id": 291
            },
            {
                "year": "2017",
                "id": 377
            },
            {
                "year": "2017",
                "id": 532
            },
            {
                "year": "2017",
                "id": 599
            },
            {
                "year": "2017",
                "id": 620
            },
            {
                "year": "2015",
                "id": 273
            },
            {
                "year": "2015",
                "id": 373
            },
            {
                "year": "2015",
                "id": 404
            },
            {
                "year": "2015",
                "id": 454
            },
            {
                "year": "2015",
                "id": 460
            },
            {
                "year": "2015",
                "id": 468
            }
        ]
    },
    {
        "title": "Segmentation Driven Object Detection with Fisher Vectors",
        "authors": [
            "Ramazan Gokberk Cinbis",
            "Jakob Verbeek",
            "Cordelia Schmid"
        ],
        "abstract": "We present an object detection system based on the Fisher vector (FV) image representation computed over SIFT and color descriptors. For computational and storage efficiency, we use a recent segmentation-based method to generate class-independent object detection hypotheses, in combination with data compression techniques. Our main contribution is a method to produce tentative object segmentation masks to suppress background clutter in the features. Re-weighting the local image features based on these masks is shown to improve object detection significantly. We also exploit contextual features in the form of a full-image FV descriptor, and an inter-category rescoring mechanism. Our experiments on the VOC 2007 and 2010 datasets show that our detector improves over the current state-of-the-art detection results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751480",
        "reference_list": [
            {
                "year": "2009",
                "id": 30
            },
            {
                "year": "2009",
                "id": 125
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 25,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Image segmentation",
                "Feature extraction",
                "Vectors",
                "Training",
                "Image color analysis",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image segmentation",
                "object detection",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "segmentation driven object detection",
                "Fisher vectors",
                "object detection system",
                "FV image representation",
                "SIFT",
                "color descriptors",
                "segmentation-based method",
                "class-independent object detection hypotheses",
                "data compression techniques",
                "tentative object segmentation masks",
                "background clutter suppression",
                "intercategory rescoring mechanism",
                "VOC"
            ],
            "Author Keywords": [
                "object detection",
                "fisher vectors"
            ]
        },
        "id": 370,
        "cited_by": [
            {
                "year": "2015",
                "id": 5
            },
            {
                "year": "2015",
                "id": 128
            },
            {
                "year": "2015",
                "id": 158
            },
            {
                "year": "2015",
                "id": 287
            },
            {
                "year": "2015",
                "id": 296
            }
        ]
    },
    {
        "title": "Saliency Detection via Dense and Sparse Reconstruction",
        "authors": [
            "Xiaohui Li",
            "Huchuan Lu",
            "Lihe Zhang",
            "Xiang Ruan",
            "Ming-Hsuan Yang"
        ],
        "abstract": "In this paper, we propose a visual saliency detection algorithm from the perspective of reconstruction errors. The image boundaries are first extracted via super pixels as likely cues for background templates, from which dense and sparse appearance models are constructed. For each image region, we first compute dense and sparse reconstruction errors. Second, the reconstruction errors are propagated based on the contexts obtained from K-means clustering. Third, pixel-level saliency is computed by an integration of multi-scale reconstruction errors and refined by an object-biased Gaussian model. We apply the Bayes formula to integrate saliency measures based on dense and sparse reconstruction errors. Experimental results show that the proposed algorithm performs favorably against seventeen state-of-the-art methods in terms of precision and recall. In addition, the proposed algorithm is demonstrated to be more effective in highlighting salient objects uniformly and robust to background noise.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751481",
        "reference_list": [
            {
                "year": "2011",
                "id": 115
            }
        ],
        "citation": {
            "ieee": 191,
            "other": 117,
            "total": 308
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Image segmentation",
                "Measurement uncertainty",
                "Bayes methods",
                "Computational modeling",
                "Databases",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "Gaussian processes",
                "image reconstruction",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual saliency detection algorithm",
                "sparse reconstruction",
                "image region",
                "sparse reconstruction errors",
                "dense reconstruction errors",
                "K-means clustering",
                "Bayes formula",
                "object-biased Gaussian model",
                "pixel-level saliency"
            ]
        },
        "id": 371,
        "cited_by": [
            {
                "year": "2017",
                "id": 21
            },
            {
                "year": "2017",
                "id": 22
            },
            {
                "year": "2017",
                "id": 423
            },
            {
                "year": "2017",
                "id": 477
            },
            {
                "year": "2015",
                "id": 24
            },
            {
                "year": "2015",
                "id": 45
            },
            {
                "year": "2015",
                "id": 156
            }
        ]
    },
    {
        "title": "Bayesian Joint Topic Modelling for Weakly Supervised Object Localisation",
        "authors": [
            "Zhiyuan Shi",
            "Timothy M. Hospedales",
            "Tao Xiang"
        ],
        "abstract": "We address the problem of localisation of objects as bounding boxes in images with weak labels. This weakly supervised object localisation problem has been tackled in the past using discriminative models where each object class is localised independently from other classes. We propose a novel framework based on Bayesian joint topic modelling. Our framework has three distinctive advantages over previous works: (1) All object classes and image backgrounds are modelled jointly together in a single generative model so that \"explaining away\" inference can resolve ambiguity and lead to better learning and localisation. (2) The Bayesian formulation of the model enables easy integration of prior knowledge about object appearance to compensate for limited supervision. (3) Our model can be learned with a mixture of weakly labelled and unlabelled data, allowing the large volume of unlabelled images on the Internet to be exploited for learning. Extensive experiments on the challenging VOC dataset demonstrate that our approach outperforms the state-of-the-art competitors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751482",
        "reference_list": [
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2011",
                "id": 43
            },
            {
                "year": "2011",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 6,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayes methods",
                "Joints",
                "Data models",
                "Semisupervised learning",
                "Detectors",
                "Computational modeling",
                "Supervised learning"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "inference mechanisms",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian joint topic modelling",
                "weakly-supervised object localisation problem",
                "bounding box",
                "discriminative model",
                "object class independent localization",
                "image background model",
                "single-generative model",
                "explaining away inference",
                "object appearance",
                "weakly-labelled data",
                "weakly-unlabelled data",
                "Internet",
                "VOC dataset"
            ],
            "Author Keywords": [
                "Bayesian",
                "Joint Topic Modelling",
                "Weakly Supervised"
            ]
        },
        "id": 372,
        "cited_by": [
            {
                "year": "2015",
                "id": 136
            },
            {
                "year": "2015",
                "id": 354
            }
        ]
    },
    {
        "title": "Parsing IKEA Objects: Fine Pose Estimation",
        "authors": [
            "Joseph J. Lim",
            "Hamed Pirsiavash",
            "Antonio Torralba"
        ],
        "abstract": "We address the problem of localizing and estimating the fine-pose of objects in the image with exact 3D models. Our main focus is to unify contributions from the 1970s with recent advances in object detection: use local keypoint detectors to find candidate poses and score global alignment of each candidate pose to the image. Moreover, we also provide a new dataset containing fine-aligned objects with their exactly matched 3D models, and a set of models for widely used objects. We also evaluate our algorithm both on object detection and fine pose estimation, and show that our method outperforms state-of-the art algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751483",
        "reference_list": [],
        "citation": {
            "ieee": 57,
            "other": 21,
            "total": 78
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Image edge detection",
                "Estimation",
                "Design automation",
                "Computational modeling",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "object detection",
                "pose estimation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "IKEA object parsing",
                "objects fine-pose localization",
                "object fine-pose estimation",
                "object detection",
                "local keypoint detectors",
                "global alignment",
                "candidate pose",
                "fine-aligned objects",
                "exact 3D model matching"
            ]
        },
        "id": 373,
        "cited_by": [
            {
                "year": "2017",
                "id": 6
            },
            {
                "year": "2017",
                "id": 167
            },
            {
                "year": "2017",
                "id": 548
            },
            {
                "year": "2015",
                "id": 52
            },
            {
                "year": "2015",
                "id": 103
            },
            {
                "year": "2015",
                "id": 226
            },
            {
                "year": "2015",
                "id": 277
            },
            {
                "year": "2015",
                "id": 291
            },
            {
                "year": "2015",
                "id": 299
            },
            {
                "year": "2015",
                "id": 373
            }
        ]
    },
    {
        "title": "Active Visual Recognition with Expertise Estimation in Crowdsourcing",
        "authors": [
            "Chengjiang Long",
            "Gang Hua",
            "Ashish Kapoor"
        ],
        "abstract": "We present a noise resilient probabilistic model for active learning of a Gaussian process classifier from crowds, i.e., a set of noisy labelers. It explicitly models both the overall label noises and the expertise level of each individual labeler in two levels of flip models. Expectation propagation is adopted for efficient approximate Bayesian inference of our probabilistic model for classification, based on which, a generalized EM algorithm is derived to estimate both the global label noise and the expertise of each individual labeler. The probabilistic nature of our model immediately allows the adoption of the prediction entropy and estimated expertise for active selection of data sample to be labeled, and active selection of high quality labelers to label the data, respectively. We apply the proposed model for three visual recognition tasks, i.e., object category recognition, gender recognition, and multi-modal activity recognition, on three datasets with real crowd-sourced labels from Amazon Mechanical Turk. The experiments clearly demonstrated the efficacy of the proposed model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751484",
        "reference_list": [
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2009",
                "id": 135
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 6,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise",
                "Feature extraction",
                "Noise measurement",
                "Bayes methods",
                "Joints",
                "Probabilistic logic",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "Gaussian processes",
                "image recognition",
                "inference mechanisms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Amazon Mechanical Turk",
                "crowd-sourced labels",
                "multimodal activity recognition",
                "gender recognition",
                "object category recognition",
                "high-quality labeler active selection",
                "data sample active selection",
                "prediction entropy",
                "global label noise estimation",
                "generalized EM algorithm",
                "classification",
                "Bayesian inference",
                "expectation propagation",
                "flip model",
                "noisy labelers",
                "Gaussian process classifier",
                "active learning",
                "noise resilient probabilistic model",
                "crowdsourcing",
                "expertise estimation",
                "active visual recognition"
            ],
            "Author Keywords": [
                "Active Learning",
                "Crowdsourcing",
                "Visual Recognition",
                "Gaussian Processes",
                "Expectation Propagation"
            ]
        },
        "id": 374,
        "cited_by": [
            {
                "year": "2015",
                "id": 316
            },
            {
                "year": "2015",
                "id": 331
            }
        ]
    },
    {
        "title": "A Scalable Unsupervised Feature Merging Approach to Efficient Dimensionality Reduction of High-Dimensional Visual Data",
        "authors": [
            "Lingqiao Liu",
            "Lei Wang"
        ],
        "abstract": "To achieve a good trade-off between recognition accuracy and computational efficiency, it is often needed to reduce high-dimensional visual data to medium-dimensional ones. For this task, even applying a simple full-matrix-based linear projection causes significant computation and memory use. When the number of visual data is large, how to efficiently learn such a projection could even become a problem. The recent feature merging approach offers an efficient way to reduce the dimensionality, which only requires a single scan of features to perform reduction. However, existing merging algorithms do not scale well with high-dimensional data, especially in the unsupervised case. To address this problem, we formulate unsupervised feature merging as a PCA problem imposed with a special structure constraint. By exploiting its connection with k-means, we transform this constrained PCA problem into a feature clustering problem. Moreover, we employ the hashing technique to improve its scalability. These produce a scalable feature merging algorithm for our dimensionality reduction task. In addition, we develop an extension of this method by leveraging the neighborhood structure in the data to further improve dimensionality reduction performance. In further, we explore the incorporation of bipolar merging - a variant of merging function which allows the subtraction operation - into our algorithms. Through three applications in visual recognition, we demonstrate that our methods can not only achieve good dimensionality reduction performance with little computational cost but also help to create more powerful representation at both image level and local feature level.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751485",
        "reference_list": [
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Merging",
                "Principal component analysis",
                "Kernel",
                "Clustering algorithms",
                "Scalability",
                "Indexes",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "data reduction",
                "image recognition",
                "matrix algebra",
                "principal component analysis",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "principal component analysis problem",
                "local feature level",
                "image level",
                "visual recognition",
                "subtraction operation",
                "bipolar merging",
                "neighborhood structure",
                "constrained PCA problem",
                "k-means",
                "special structure constraint",
                "full-matrix-based linear projection",
                "high-dimensional visual data reduction",
                "dimensionality reduction",
                "scalable unsupervised feature merging approach"
            ]
        },
        "id": 375,
        "cited_by": [
            {
                "year": "2015",
                "id": 314
            }
        ]
    },
    {
        "title": "Training Deformable Part Models with Decorrelated Features",
        "authors": [
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "In this paper, we show how to train a deformable part model (DPM) fast-typically in less than 20 minutes, or four times faster than the current fastest method-while maintaining high average precision on the PASCAL VOC datasets. At the core of our approach is \"latent LDA,\" a novel generalization of linear discriminant analysis for learning latent variable models. Unlike latent SVM, latent LDA uses efficient closed-form updates and does not require an expensive search for hard negative examples. Our approach also acts as a springboard for a detailed experimental study of DPM training. We isolate and quantify the impact of key training factors for the first time (e.g., How important are discriminative SVM filters? How important is joint parameter estimation? How many negative images are needed for training?). Our findings yield useful insights for researchers working with Markov random fields and part-based models, and have practical implications for speeding up tasks such as model selection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751486",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 1,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Support vector machines",
                "Vectors",
                "Computational modeling",
                "Deformable models",
                "Covariance matrices",
                "Acceleration"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Markov processes",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable part model",
                "decorrelated features",
                "latent LDA",
                "linear discriminant analysis",
                "PASCAL VOC datasets",
                "latent variable models",
                "DPM training",
                "Markov random fields",
                "part-based models",
                "object detection"
            ]
        },
        "id": 376,
        "cited_by": [
            {
                "year": "2015",
                "id": 103
            }
        ]
    },
    {
        "title": "Quantize and Conquer: A Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval",
        "authors": [
            "Yannis Avrithis"
        ],
        "abstract": "Inspired by the close relation between nearest neighbor search and clustering in high-dimensional spaces as well as the success of one helping to solve the other, we introduce a new paradigm where both problems are solved simultaneously. Our solution is recursive, not in the size of input data but in the number of dimensions. One result is a clustering algorithm that is tuned to small codebooks but does not need all data in memory at the same time and is practically constant in the data size. As a by-product, a tree structure performs either exact or approximate quantization on trained centroids, the latter being not very precise but extremely fast. A lesser contribution is a new indexing scheme for image retrieval that exploits multiple small codebooks to provide an arbitrarily fine partition of the descriptor space. Large scale experiments on public datasets exhibit state of the art performance and remarkable generalization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751487",
        "reference_list": [
            {
                "year": "2013",
                "id": 174
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Quantization (signal)",
                "Image retrieval",
                "Clustering algorithms",
                "Zirconium",
                "Nearest neighbor searches",
                "Indexing",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "indexing",
                "pattern clustering",
                "search problems",
                "tree data structures",
                "vector quantisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dimensionality-recursive solution",
                "vector quantization",
                "image retrieval",
                "nearest neighbor search",
                "clustering algorithm",
                "high-dimensional spaces",
                "data size",
                "tree structure",
                "approximate quantization",
                "exact quantization",
                "indexing scheme",
                "descriptor space"
            ],
            "Author Keywords": [
                "clustering",
                "vector quantization",
                "nearest neighbor search",
                "image retrieval"
            ]
        },
        "id": 377,
        "cited_by": []
    },
    {
        "title": "Learning Hash Codes with Listwise Supervision",
        "authors": [
            "Jun Wang",
            "Wei Liu",
            "Andy X. Sun",
            "Yu-Gang Jiang"
        ],
        "abstract": "Hashing techniques have been intensively investigated in the design of highly efficient search engines for large-scale computer vision applications. Compared with prior approximate nearest neighbor search approaches like tree-based indexing, hashing-based search schemes have prominent advantages in terms of both storage and computational efficiencies. Moreover, the procedure of devising hash functions can be easily incorporated into sophisticated machine learning tools, leading to data-dependent and task-specific compact hash codes. Therefore, a number of learning paradigms, ranging from unsupervised to supervised, have been applied to compose appropriate hash functions. However, most of the existing hash function learning methods either treat hash function design as a classification problem or generate binary codes to satisfy pair wise supervision, and have not yet directly optimized the search accuracy. In this paper, we propose to leverage list wise supervision into a principled hash function learning framework. In particular, the ranking information is represented by a set of rank triplets that can be used to assess the quality of ranking. Simple linear projection-based hash functions are solved efficiently through maximizing the ranking quality over the training data. We carry out experiments on large image datasets with size up to one million and compare with the state-of-the-art hashing techniques. The extensive results corroborate that our learned hash codes via list wise supervision can provide superior search accuracy without incurring heavy computational overhead.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751488",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 10,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Loss measurement",
                "Search problems",
                "Binary codes",
                "Semantics",
                "Vectors",
                "Tensile stress",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "binary codes",
                "computer vision",
                "file organisation",
                "image classification",
                "image coding",
                "search problems",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image datasets",
                "ranking quality",
                "linear projection-based hash functions",
                "rank triplets",
                "ranking information",
                "principled hash function learning framework",
                "binary codes",
                "data-dependent compact hash codes",
                "task-specific compact hash codes",
                "sophisticated machine learning tools",
                "hashing-based search schemes",
                "tree-based indexing",
                "approximate nearest neighbor search approaches",
                "large-scale computer vision",
                "listwise supervision",
                "hash code learning technique"
            ]
        },
        "id": 378,
        "cited_by": [
            {
                "year": "2015",
                "id": 123
            },
            {
                "year": "2015",
                "id": 214
            },
            {
                "year": "2015",
                "id": 426
            },
            {
                "year": "2015",
                "id": 463
            }
        ]
    },
    {
        "title": "Image Retrieval Using Textual Cues",
        "authors": [
            "Anand Mishra",
            "Karteek Alahari",
            "C.V. Jawahar"
        ],
        "abstract": "We present an approach for the text-to-image retrieval problem based on textual content present in images. Given the recent developments in understanding text in images, an appealing approach to address this problem is to localize and recognize the text, and then query the database, as in a text retrieval problem. We show that such an approach, despite being based on state-of-the-art methods, is insufficient, and propose a method, where we do not rely on an exact localization and recognition pipeline. We take a query-driven search approach, where we find approximate locations of characters in the text query, and then impose spatial constraints to generate a ranked list of images in the database. The retrieval performance is evaluated on public scene text datasets as well as three large datasets, namely IIIT scene text retrieval, Sports-10K and TV series-1M, we introduce.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751489",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2011",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 13,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Vocabulary",
                "Text recognition",
                "TV",
                "Indexes",
                "Vectors",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "search problems",
                "text analysis",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "textual cues",
                "text-to-image retrieval problem",
                "text recognition",
                "query-driven search approach",
                "text query",
                "spatial constraints",
                "public scene text datasets",
                "IIIT scene text retrieval",
                "TV series-1M",
                "Sports-10K"
            ]
        },
        "id": 379,
        "cited_by": []
    },
    {
        "title": "Scene Collaging: Analysis and Synthesis of Natural Images with Semantic Layers",
        "authors": [
            "Phillip Isola",
            "Ce Liu"
        ],
        "abstract": "To quickly synthesize complex scenes, digital artists often collage together visual elements from multiple sources: for example, mountains from New Zealand behind a Scottish castle with wisps of Saharan sand in front. In this paper, we propose to use a similar process in order to parse a scene. We model a scene as a collage of warped, layered objects sampled from labeled, reference images. Each object is related to the rest by a set of support constraints. Scene parsing is achieved through analysis-by-synthesis. Starting with a dataset of labeled exemplar scenes, we retrieve a dictionary of candidate object segments that match a query image. We then combine elements of this set into a \"scene collage\" that explains the query image. Beyond just assigning object labels to pixels, scene collaging produces a lot more information such as the number of each type of object in the scene, how they support one another, the ordinal depth of each object, and, to some degree, occluded content. We exploit this representation for several applications: image editing, random scene synthesis, and image-to-anaglyph.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751490",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 5,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Dictionaries",
                "Context",
                "Visualization",
                "Grammar",
                "Semantics",
                "Buildings"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "signal synthesis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene collaging",
                "natural image synthesis",
                "natural image analysis",
                "semantic layers",
                "complex scene synthesis",
                "Scottish castle",
                "Saharan sand wisps",
                "New Zealand",
                "reference images",
                "scene parsing",
                "analysis-by-synthesis",
                "labeled exemplar scenes",
                "candidate object segment dictionary",
                "query image matching",
                "image editing",
                "random scene synthesis",
                "image-to-anaglyph",
                "warped layered objects"
            ]
        },
        "id": 380,
        "cited_by": [
            {
                "year": "2015",
                "id": 106
            },
            {
                "year": "2015",
                "id": 291
            }
        ]
    },
    {
        "title": "Understanding High-Level Semantics by Modeling Traffic Patterns",
        "authors": [
            "Hongyi Zhang",
            "Andreas Geiger",
            "Raquel Urtasun"
        ],
        "abstract": "In this paper, we are interested in understanding the semantics of outdoor scenes in the context of autonomous driving. Towards this goal, we propose a generative model of 3D urban scenes which is able to reason not only about the geometry and objects present in the scene, but also about the high-level semantics in the form of traffic patterns. We found that a small number of patterns is sufficient to model the vast majority of traffic scenes and show how these patterns can be learned. As evidenced by our experiments, this high-level reasoning significantly improves the overall scene estimation as well as the vehicle-to-lane association when compared to state-of-the-art approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751491",
        "reference_list": [
            {
                "year": "2009",
                "id": 237
            },
            {
                "year": "2009",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 8,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Vehicles",
                "Geometry",
                "Splines (mathematics)",
                "Roads",
                "Semantics",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "inference mechanisms",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high-level semantic understanding",
                "autonomous driving",
                "traffic patterns",
                "3D urban scenes",
                "traffic scenes",
                "vehicle-to-lane association",
                "high-level reasoning"
            ]
        },
        "id": 381,
        "cited_by": [
            {
                "year": "2017",
                "id": 460
            },
            {
                "year": "2015",
                "id": 303
            }
        ]
    },
    {
        "title": "Efficient 3D Scene Labeling Using Fields of Trees",
        "authors": [
            "Olaf K\u00e4hler",
            "Ian Reid"
        ],
        "abstract": "We address the problem of 3D scene labeling in a structured learning framework. Unlike previous work which uses structured Support Vector Machines, we employ the recently described Decision Tree Field and Regression Tree Field frameworks, which learn the unary and binary terms of a Conditional Random Field from training data. We show this has significant advantages in terms of inference speed, while maintaining similar accuracy. We also demonstrate empirically the importance for overall labeling accuracy of features that make use of prior knowledge about the coarse scene layout such as the location of the ground plane. We show how this coarse layout can be estimated by our framework automatically, and that this information can be used to bootstrap improved accuracy in the detailed labeling.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751492",
        "reference_list": [
            {
                "year": "2011",
                "id": 283
            },
            {
                "year": "2011",
                "id": 295
            },
            {
                "year": "2011",
                "id": 211
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 8,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Three-dimensional displays",
                "Vegetation",
                "Vectors",
                "Image segmentation",
                "Feature extraction",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "feature extraction",
                "learning (artificial intelligence)",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "efficient 3D scene labeling",
                "structured learning framework",
                "structured support vector machines",
                "decision tree field framework",
                "regression tree field framework",
                "conditional random field unary term",
                "conditional random field binary term",
                "inference speed",
                "feature labeling accuracy",
                "coarse scene layout",
                "ground plane location",
                "detailed labeling"
            ]
        },
        "id": 382,
        "cited_by": [
            {
                "year": "2015",
                "id": 250
            }
        ]
    },
    {
        "title": "Multi-channel Correlation Filters",
        "authors": [
            "Hamed Kiani Galoogahi",
            "Terence Sim",
            "Simon Lucey"
        ],
        "abstract": "Modern descriptors like HOG and SIFT are now commonly used in vision for pattern detection within image and video. From a signal processing perspective, this detection process can be efficiently posed as a correlation/ convolution between a multi-channel image and a multi-channel detector/filter which results in a single channel response map indicating where the pattern (e.g. object) has occurred. In this paper, we propose a novel framework for learning a multi-channel detector/filter efficiently in the frequency domain, both in terms of training time and memory footprint, which we refer to as a multichannel correlation filter. To demonstrate the effectiveness of our strategy, we evaluate it across a number of visual detection/ localization tasks where we: (i) exhibit superior performance to current state of the art correlation filters, and (ii) superior computational and memory efficiencies compared to state of the art spatial detectors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751493",
        "reference_list": [
            {
                "year": "2009",
                "id": 276
            }
        ],
        "citation": {
            "ieee": 69,
            "other": 33,
            "total": 102
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Detectors",
                "Frequency-domain analysis",
                "Training",
                "Equations",
                "Vectors",
                "Linear systems"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "filtering theory",
                "frequency-domain analysis",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multichannel correlation filters",
                "HOG descriptor",
                "SIFT descriptor",
                "pattern detection",
                "video",
                "signal processing perspective",
                "detection process",
                "multichannel image",
                "multichannel detector-filter learning",
                "single-channel response map",
                "frequency domain",
                "training time",
                "memory footprint",
                "visual detection-localization tasks",
                "computational efficiency",
                "memory efficiency",
                "computer vision"
            ],
            "Author Keywords": [
                "multi channel features",
                "correlation filter learning",
                "pattern recognition"
            ]
        },
        "id": 383,
        "cited_by": [
            {
                "year": "2017",
                "id": 118
            },
            {
                "year": "2017",
                "id": 119
            },
            {
                "year": "2017",
                "id": 575
            },
            {
                "year": "2015",
                "id": 339
            },
            {
                "year": "2015",
                "id": 343
            },
            {
                "year": "2015",
                "id": 481
            },
            {
                "year": "2013",
                "id": 344
            }
        ]
    },
    {
        "title": "Learning CRFs for Image Parsing with Adaptive Subgradient Descent",
        "authors": [
            "Honghui Zhang",
            "Jingdong Wang",
            "Ping Tan",
            "Jinglu Wang",
            "Long Quan"
        ],
        "abstract": "We propose an adaptive sub gradient descent method to efficiently learn the parameters of CRF models for image parsing. To balance the learning efficiency and performance of the learned CRF models, the parameter learning is iteratively carried out by solving a convex optimization problem in each iteration, which integrates a proximal term to preserve the previously learned information and the large margin preference to distinguish bad labeling and the ground truth labeling. A solution of sub gradient descent updating form is derived for the convex optimization problem, with an adaptively determined updating step-size. Besides, to deal with partially labeled training data, we propose a new objective constraint modeling both the labeled and unlabeled parts in the partially labeled training data for the parameter learning of CRF models. The superior learning efficiency of the proposed method is verified by the experiment results on two public datasets. We also demonstrate the powerfulness of our method for handling partially labeled training data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751494",
        "reference_list": [
            {
                "year": "2011",
                "id": 9
            },
            {
                "year": "2009",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Adaptation models",
                "Labeling",
                "Training data",
                "Data models",
                "Optimization",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "gradient methods",
                "image processing",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image parsing",
                "learning CRF",
                "adaptive subgradient descent method",
                "parameter learning",
                "convex optimization problem",
                "objective constraint modeling"
            ],
            "Author Keywords": [
                "Image Parsing",
                "Adaptive Subgradient Descent",
                "Conditional Random Field"
            ]
        },
        "id": 384,
        "cited_by": []
    },
    {
        "title": "Robust Trajectory Clustering for Motion Segmentation",
        "authors": [
            "Feng Shi",
            "Zhong Zhou",
            "Jiangjian Xiao",
            "Wei Wu"
        ],
        "abstract": "Due to occlusions and objects' non-rigid deformation in the scene, the obtained motion trajectories from common trackers may contain a number of missing or mis-associated entries. To cluster such corrupted point based trajectories into multiple motions is still a hard problem. In this paper, we present an approach that exploits temporal and spatial characteristics from tracked points to facilitate segmentation of incomplete and corrupted trajectories, thereby obtain highly robust results against severe data missing and noises. Our method first uses the Discrete Cosine Transform (DCT) bases as a temporal smoothness constraint on trajectory projection to ensure the validity of resulting components to repair pathological trajectories. Then, based on an observation that the trajectories of foreground and background in a scene may have different spatial distributions, we propose a two-stage clustering strategy that first performs foreground-background separation then segments remaining foreground trajectories. We show that, with this new clustering strategy, sequences with complex motions can be accurately segmented by even using a simple translational model. Finally, a series of experiments on Hopkins 155 dataset and Berkeley motion segmentation dataset show the advantage of our method over other state-of-the-art motion segmentation algorithms in terms of both effectiveness and robustness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751495",
        "reference_list": [
            {
                "year": "2009",
                "id": 110
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Motion segmentation",
                "Discrete cosine transforms",
                "Clustering algorithms",
                "Computer vision",
                "Tracking",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "discrete cosine transforms",
                "image motion analysis",
                "image segmentation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust motion trajectory clustering",
                "motion segmentation algorithm",
                "object nonrigid deformation",
                "occlusions",
                "corrupted point based trajectory",
                "spatial characteristics",
                "temporal characteristics",
                "discrete cosine transform",
                "DCT",
                "temporal smoothness constraint",
                "trajectory projection",
                "pathological trajectory",
                "spatial distributions",
                "two-stage clustering strategy",
                "foreground-background separation",
                "foreground trajectories",
                "Berkeley motion segmentation dataset",
                "Hopkins 155 dataset"
            ]
        },
        "id": 385,
        "cited_by": [
            {
                "year": "2017",
                "id": 445
            },
            {
                "year": "2015",
                "id": 365
            },
            {
                "year": "2015",
                "id": 483
            }
        ]
    },
    {
        "title": "Robust Subspace Clustering via Half-Quadratic Minimization",
        "authors": [
            "Yingya Zhang",
            "Zhenan Sun",
            "Ran He",
            "Tieniu Tan"
        ],
        "abstract": "Subspace clustering has important and wide applications in computer vision and pattern recognition. It is a challenging task to learn low-dimensional subspace structures due to the possible errors (e.g., noise and corruptions) existing in high-dimensional data. Recent subspace clustering methods usually assume a sparse representation of corrupted errors and correct the errors iteratively. However large corruptions in real-world applications can not be well addressed by these methods. A novel optimization model for robust subspace clustering is proposed in this paper. The objective function of our model mainly includes two parts. The first part aims to achieve a sparse representation of each high-dimensional data point with other data points. The second part aims to maximize the correntropy between a given data point and its low-dimensional representation with other points. Correntropy is a robust measure so that the influence of large corruptions on subspace clustering can be greatly suppressed. An extension of our method with explicit introduction of representation error terms into the model is also proposed. Half-quadratic minimization is provided as an efficient solution to the proposed robust subspace clustering formulations. Experimental results on Hopkins 155 dataset and Extended Yale Database B demonstrate that our method outperforms state-of-the-art subspace clustering methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751496",
        "reference_list": [
            {
                "year": "2011",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 8,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Clustering algorithms",
                "Vectors",
                "Noise",
                "Optimization",
                "Motion segmentation",
                "Face"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "iterative methods",
                "minimisation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust subspace clustering",
                "half-quadratic minimization",
                "computer vision",
                "pattern recognition",
                "low-dimensional subspace structures",
                "subspace clustering method",
                "sparse representation",
                "corrupted errors",
                "iterative error correction",
                "optimization model",
                "high-dimensional data point",
                "correntropy maximization",
                "low-dimensional representation",
                "representation error",
                "Hopkins 155 dataset",
                "extended Yale database B"
            ]
        },
        "id": 386,
        "cited_by": []
    },
    {
        "title": "Structured Learning of Sum-of-Submodular Higher Order Energy Functions",
        "authors": [
            "Alexander Fix",
            "Thorsten Joachims",
            "Sung Min Park",
            "Ramin Zabih"
        ],
        "abstract": "Sub modular functions can be exactly minimized in polynomial time, and the special case that graph cuts solve with max flow [19] has had significant impact in computer vision [5, 21, 28]. In this paper we address the important class of sum-of-sub modular (SoS) functions [2, 18], which can be efficiently minimized via a variant of max flow called sub modular flow [6]. SoS functions can naturally express higher order priors involving, e.g., local image patches, however, it is difficult to fully exploit their expressive power because they have so many parameters. Rather than trying to formulate existing higher order priors as an SoS function, we take a discriminative learning approach, effectively searching the space of SoS functions for a higher order prior that performs well on our training set. We adopt a structural SVM approach [15, 34] and formulate the training problem in terms of quadratic programming, as a result we can efficiently search the space of SoS priors via an extended cutting-plane algorithm. We also show how the state-of-the-art max flow method for vision problems [11] can be modified to efficiently solve the sub modular flow problem. Experimental comparisons are made against the OpenCV implementation of the Grab Cut interactive segmentation technique [28], which uses hand-tuned parameters instead of machine learning. On a standard dataset [12] our method learns higher order priors with hundreds of parameter values, and produces significantly better segmentations. While our focus is on binary labeling problems, we show that our techniques can be naturally generalized to handle more than two labels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751497",
        "reference_list": [
            {
                "year": "2011",
                "id": 129
            },
            {
                "year": "2009",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Vectors",
                "Support vector machines",
                "Minimization",
                "Optimization",
                "Computer vision",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "learning (artificial intelligence)",
                "quadratic programming",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "binary labeling problems",
                "image segmentations",
                "machine learning",
                "hand-tuned parameters",
                "OpenCV",
                "GrabCut interactive segmentation technique",
                "max flow method",
                "extended cutting-plane algorithm",
                "quadratic programming",
                "structural SVM approach",
                "discriminative learning approach",
                "local image patches",
                "submodular flow",
                "SoS functions",
                "computer vision",
                "polynomial time",
                "sum-of-submodular higher order energy functions",
                "structured learning"
            ],
            "Author Keywords": [
                "Max flow",
                "Graph cuts",
                "Structured prediction"
            ]
        },
        "id": 387,
        "cited_by": [
            {
                "year": "2015",
                "id": 196
            }
        ]
    },
    {
        "title": "Joint Learning of Discriminative Prototypes and Large Margin Nearest Neighbor Classifiers",
        "authors": [
            "Martin K\u00f6stinger",
            "Paul Wohlhart",
            "Peter M. Roth",
            "Horst Bischof"
        ],
        "abstract": "In this paper, we raise important issues concerning the evaluation complexity of existing Mahalanobis metric learning methods. The complexity scales linearly with the size of the dataset. This is especially cumbersome on large scale or for real-time applications with limited time budget. To alleviate this problem we propose to represent the dataset by a fixed number of discriminative prototypes. In particular, we introduce a new method that jointly chooses the positioning of prototypes and also optimizes the Mahalanobis distance metric with respect to these. We show that choosing the positioning of the prototypes and learning the metric in parallel leads to a drastically reduced evaluation effort while maintaining the discriminative essence of the original dataset. Moreover, for most problems our method performing k-nearest prototype (k-NP) classification on the condensed dataset leads to even better generalization compared to k-NN classification using all data. Results on a variety of challenging benchmarks demonstrate the power of our method. These include standard machine learning datasets as well as the challenging Public Figures Face Database. On the competitive machine learning benchmarks we are comparable to the state-of-the-art while being more efficient. On the face benchmark we clearly outperform the state-of-the-art in Mahalanobis metric learning with drastically reduced evaluation effort.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751498",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2009",
                "id": 46
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Prototypes",
                "Measurement",
                "Complexity theory",
                "Benchmark testing",
                "Training",
                "Databases",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "competitive machine learning benchmarks",
                "public figures face database",
                "standard machine learning datasets",
                "k-NN classification",
                "k-NP classification",
                "performing k-nearest prototype",
                "Mahalanobis distance metric",
                "complexity scales",
                "Mahalanobis metric learning methods",
                "evaluation complexity",
                "large margin nearest neighbor classifiers",
                "discriminative prototypes"
            ],
            "Author Keywords": [
                "Mahalanobis Metric Learning",
                "Prototype Learning",
                "k-NN Classification",
                "k-Nearest Prototype Classification",
                "Speedup",
                "Evaluation Time"
            ]
        },
        "id": 388,
        "cited_by": []
    },
    {
        "title": "Dictionary Learning and Sparse Coding on Grassmann Manifolds: An Extrinsic Solution",
        "authors": [
            "Mehrtash Harandi",
            "Conrad Sanderson",
            "Chunhua Shen",
            "Brian Lovell"
        ],
        "abstract": "Recent advances in computer vision and machine learning suggest that a wide range of problems can be addressed more appropriately by considering non-Euclidean geometry. In this paper we explore sparse dictionary learning over the space of linear subspaces, which form Riemannian structures known as Grassmann manifolds. To this end, we propose to embed Grassmann manifolds into the space of symmetric matrices by an isometric mapping, which enables us to devise a closed-form solution for updating a Grassmann dictionary, atom by atom. Furthermore, to handle non-linearity in data, we propose a kernelised version of the dictionary learning algorithm. Experiments on several classification tasks (face recognition, action recognition, dynamic texture classification) show that the proposed approach achieves considerable improvements in discrimination accuracy, in comparison to state-of-the-art methods such as kernelised Affine Hull Method and graph-embedding Grassmann discriminant analysis.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751499",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 17,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Dictionaries",
                "Encoding",
                "Symmetric matrices",
                "Kernel",
                "Vectors",
                "Sparse matrices"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image coding",
                "learning (artificial intelligence)",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dictionary learning algorithm",
                "sparse coding",
                "Grassmann manifolds",
                "computer vision",
                "machine learning",
                "nonEuclidean geometry",
                "Riemannian structures",
                "symmetric matrices",
                "isometric mapping",
                "closed-form solution",
                "kernelised affine hull method",
                "graphembedding Grassmann discriminant analysis"
            ],
            "Author Keywords": [
                "Grassmann manifolds",
                "sparse coding",
                "dictionary learning",
                "image-set",
                "dynamic texture classification",
                "action recognition"
            ]
        },
        "id": 389,
        "cited_by": [
            {
                "year": "2017",
                "id": 323
            },
            {
                "year": "2015",
                "id": 8
            }
        ]
    },
    {
        "title": "Latent Multitask Learning for View-Invariant Action Recognition",
        "authors": [
            "Behrooz Mahasseni",
            "Sinisa Todorovic"
        ],
        "abstract": "This paper presents an approach to view-invariant action recognition, where human poses and motions exhibit large variations across different camera viewpoints. When each viewpoint of a given set of action classes is specified as a learning task then multitask learning appears suitable for achieving view invariance in recognition. We extend the standard multitask learning to allow identifying: (1) latent groupings of action views (i.e., tasks), and (2) discriminative action parts, along with joint learning of all tasks. This is because it seems reasonable to expect that certain distinct views are more correlated than some others, and thus identifying correlated views could improve recognition. Also, part-based modeling is expected to improve robustness against self-occlusion when actors are imaged from different views. Results on the benchmark datasets show that we outperform standard multitask learning by 21.9%, and the state-of-the-art alternatives by 4.5-6%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751500",
        "reference_list": [
            {
                "year": "2009",
                "id": 121
            },
            {
                "year": "2011",
                "id": 72
            },
            {
                "year": "2007",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 3,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Standards",
                "Cameras",
                "Accuracy",
                "Optimization",
                "Feature extraction",
                "Training",
                "Closed-form solutions"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image recognition",
                "learning (artificial intelligence)",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "latent multitask learning",
                "view-invariant action recognition",
                "camera viewpoints",
                "discriminative action parts",
                "part-based modeling",
                "self-occlusion",
                "action classes",
                "human poses",
                "human motions",
                "action estimation"
            ],
            "Author Keywords": [
                "View-Invariant",
                "Action Recognition",
                "Multitask Learning"
            ]
        },
        "id": 390,
        "cited_by": [
            {
                "year": "2017",
                "id": 223
            }
        ]
    },
    {
        "title": "Concurrent Action Detection with Structural Prediction",
        "authors": [
            "Ping Wei",
            "Nanning Zheng",
            "Yibiao Zhao",
            "Song-Chun Zhu"
        ],
        "abstract": "Action recognition has often been posed as a classification problem, which assumes that a video sequence only have one action class label and different actions are independent. However, a single human body can perform multiple concurrent actions at the same time, and different actions interact with each other. This paper proposes a concurrent action detection model where the action detection is formulated as a structural prediction problem. In this model, an interval in a video sequence can be described by multiple action labels. An detected action interval is determined both by the unary local detector and the relations with other actions. We use a wavelet feature to represent the action sequence, and design a composite temporal logic descriptor to describe the action relations. The model parameters are trained by structural SVM learning. Given a long video sequence, a sequential decision window search algorithm is designed to detect the actions. Experiments on our new collected concurrent action dataset demonstrate the strength of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751501",
        "reference_list": [
            {
                "year": "2011",
                "id": 61
            },
            {
                "year": "2013",
                "id": 408
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 11,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Video sequences",
                "Joints",
                "Vectors",
                "Detectors",
                "Three-dimensional displays",
                "Keyboards",
                "Wavelet transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image recognition",
                "image sequences",
                "learning (artificial intelligence)",
                "object detection",
                "search problems",
                "video signal processing",
                "wavelet transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structural prediction",
                "action recognition",
                "classification problem",
                "video sequence",
                "concurrent action detection model",
                "structural prediction problem",
                "action labels",
                "action interval determination",
                "unary local detector",
                "wavelet feature",
                "action sequence representation",
                "composite temporal logic descriptor",
                "model parameter training",
                "structural SVM learning",
                "sequential decision window search algorithm",
                "concurrent action dataset"
            ]
        },
        "id": 391,
        "cited_by": [
            {
                "year": "2017",
                "id": 308
            }
        ]
    },
    {
        "title": "Recognising Human-Object Interaction via Exemplar Based Modelling",
        "authors": [
            "Jian-Fang Hu",
            "Wei-Shi Zheng",
            "Jianhuang Lai",
            "Shaogang Gong",
            "Tao Xiang"
        ],
        "abstract": "Human action can be recognised from a single still image by modelling Human-object interaction (HOI), which infers the mutual spatial structure information between human and object as well as their appearance. Existing approaches rely heavily on accurate detection of human and object, and estimation of human pose. They are thus sensitive to large variations of human poses, occlusion and unsatisfactory detection of small size objects. To overcome this limitation, a novel exemplar based approach is proposed in this work. Our approach learns a set of spatial pose-object interaction exemplars, which are density functions describing how a person is interacting with a manipulated object for different activities spatially in a probabilistic way. A representation based on our HOI exemplar thus has great potential for being robust to the errors in human/object detection and pose estimation. A new framework consists of a proposed exemplar based HOI descriptor and an activity specific matching model that learns the parameters is formulated for robust human activity recognition. Experiments on two benchmark activity datasets demonstrate that the proposed approach obtains state-of-the-art performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751502",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2011",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Torso",
                "Estimation",
                "Training",
                "Dictionaries",
                "Probes",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human-object interaction",
                "exemplar based modelling",
                "pose estimation",
                "spatial pose-object interaction exemplars",
                "object detection",
                "exemplar based HOI descriptor",
                "robust human activity recognition"
            ],
            "Author Keywords": [
                "Human-Object Interaction",
                "exemplar modelling",
                "action recognition"
            ]
        },
        "id": 392,
        "cited_by": []
    },
    {
        "title": "Human Re-identification by Matching Compositional Template with Cluster Sampling",
        "authors": [
            "Yuanlu Xu",
            "Liang Lin",
            "Wei-Shi Zheng",
            "Xiaobai Liu"
        ],
        "abstract": "This paper aims at a newly raising task in visual surveillance: re-identifying people at a distance by matching body information, given several reference examples. Most of existing works solve this task by matching a reference template with the target individual, but often suffer from large human appearance variability (e.g. different poses/views, illumination) and high false positives in matching caused by conjunctions, occlusions or surrounding clutters. Addressing these problems, we construct a simple yet expressive template from a few reference images of a certain individual, which represents the body as an articulated assembly of compositional and alternative parts, and propose an effective matching algorithm with cluster sampling. This algorithm is designed within a candidacy graph whose vertices are matching candidates (i.e. a pair of source and target body parts), and iterates in two steps for convergence. (i) It generates possible partial matches based on compatible and competitive relations among body parts. (ii) It confirms the partial matches to generate a new matching solution, which is accepted by the Markov Chain Monte Carlo (MCMC) mechanism. In the experiments, we demonstrate the superior performance of our approach on three public databases compared to existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751503",
        "reference_list": [
            {
                "year": "2007",
                "id": 179
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 14,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Kinematics",
                "Clustering algorithms",
                "Feature extraction",
                "Detectors",
                "Head",
                "Torso"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image matching",
                "image sampling",
                "Markov processes",
                "Monte Carlo methods",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human reidentification",
                "compositional template matching",
                "cluster sampling",
                "visual surveillance",
                "body information matching",
                "reference template matching",
                "reference images",
                "candidacy graph",
                "Markov Chain Monte Carlo mechanism",
                "public databases"
            ],
            "Author Keywords": [
                "Human re-identification",
                "Video Surveillance",
                "Object Recognition",
                "Computer Vision"
            ]
        },
        "id": 393,
        "cited_by": [
            {
                "year": "2017",
                "id": 117
            },
            {
                "year": "2017",
                "id": 339
            },
            {
                "year": "2017",
                "id": 540
            },
            {
                "year": "2015",
                "id": 357
            },
            {
                "year": "2015",
                "id": 425
            },
            {
                "year": "2015",
                "id": 522
            },
            {
                "year": "2013",
                "id": 315
            }
        ]
    },
    {
        "title": "Dynamic Scene Deblurring",
        "authors": [
            "Tae Hyun Kim",
            "Byeongjoo Ahn",
            "Kyoung Mu Lee"
        ],
        "abstract": "Most conventional single image deblurring methods assume that the underlying scene is static and the blur is caused by only camera shake. In this paper, in contrast to this restrictive assumption, we address the deblurring problem of general dynamic scenes which contain multiple moving objects as well as camera shake. In case of dynamic scenes, moving objects and background have different blur motions, so the segmentation of the motion blur is required for deblurring each distinct blur motion accurately. Thus, we propose a novel energy model designed with the weighted sum of multiple blur data models, which estimates different motion blurs and their associated pixel-wise weights, and resulting sharp image. In this framework, the local weights are determined adaptively and get high values when the corresponding data models have high data fidelity. And, the weight information is used for the segmentation of the motion blur. Non-local regularization of weights are also incorporated to produce more reliable segmentation results. A convex optimization-based method is used for the solution of the proposed energy model. Experimental results demonstrate that our method outperforms conventional approaches in deblurring both dynamic scenes and static scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751504",
        "reference_list": [
            {
                "year": "2011",
                "id": 58
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 10,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image restoration",
                "Motion segmentation",
                "Data models",
                "Cameras",
                "Vectors",
                "Dynamics"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "convex programming",
                "image motion analysis",
                "image restoration",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "static scenes",
                "convex optimization-based method",
                "weights nonlocal regularization",
                "data fidelity",
                "data models",
                "pixel-wise weights",
                "multiple blur data models",
                "energy model",
                "motion blur segmentation",
                "blur motions",
                "general dynamic scenes",
                "deblurring problem",
                "camera shake",
                "image deblurring methods",
                "dynamic scene deblurring"
            ]
        },
        "id": 394,
        "cited_by": [
            {
                "year": "2017",
                "id": 113
            },
            {
                "year": "2017",
                "id": 346
            }
        ]
    },
    {
        "title": "Directed Acyclic Graph Kernels for Action Recognition",
        "authors": [
            "Ling Wang",
            "Hichem Sahbi"
        ],
        "abstract": "One of the trends of action recognition consists in extracting and comparing mid-level features which encode visual and motion aspects of objects into scenes. However, when scenes contain high-level semantic actions with many interacting parts, these mid-level features are not sufficient to capture high level structures as well as high order causal relationships between moving objects resulting into a clear drop in performances. In this paper, we address this issue and we propose an alternative action recognition method based on a novel graph kernel. In the main contributions of this work, we first describe actions in videos using directed a cyclic graphs (DAGs), that naturally encode pair wise interactions between moving object parts, and then we compare these DAGs by analyzing the spectrum of their sub-patterns that capture complex higher order interactions. This extraction and comparison process is computationally tractable, resulting from the a cyclic property of DAGs, and it also defines a positive semi-definite kernel. When plugging the latter into support vector machines, we obtain an action recognition algorithm that overtakes related work, including graph-based methods, on a standard evaluation dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751505",
        "reference_list": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2011",
                "id": 254
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 8,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Trajectory",
                "Feature extraction",
                "Tensile stress",
                "Convergence",
                "Clustering algorithms",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "directed graphs",
                "feature extraction",
                "image coding",
                "image motion analysis",
                "spectral analysis",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "directed acyclic graph kernels",
                "action recognition method",
                "mid-level feature extraction",
                "visual encoding",
                "motion encoding",
                "high-level semantic actions",
                "high level structures",
                "DAGs",
                "spectrum analysis",
                "complex higher order interactions",
                "positive semidefinite kernel",
                "support vector machines",
                "graph-based methods",
                "standard evaluation dataset"
            ]
        },
        "id": 395,
        "cited_by": []
    },
    {
        "title": "Learning View-Invariant Sparse Representations for Cross-View Action Recognition",
        "authors": [
            "Jingjing Zheng",
            "Zhuolin Jiang"
        ],
        "abstract": "We present an approach to jointly learn a set of view-specific dictionaries and a common dictionary for cross-view action recognition. The set of view-specific dictionaries is learned for specific views while the common dictionary is shared across different views. Our approach represents videos in each view using both the corresponding view-specific dictionary and the common dictionary. More importantly, it encourages the set of videos taken from different views of the same action to have similar sparse representations. In this way, we can align view-specific features in the sparse feature spaces spanned by the view-specific dictionary set and transfer the view-shared features in the sparse feature space spanned by the common dictionary. Meanwhile, the incoherence between the common dictionary and the view-specific dictionary set enables us to exploit the discrimination information encoded in view-specific features and view-shared features separately. In addition, the learned common dictionary not only has the capability to represent actions from unseen views, but also makes our approach effective in a semi-supervised setting where no correspondence videos exist and only a few labels exist in the target view. Extensive experiments using the multi-view IXMAS dataset demonstrate that our approach outperforms many recent approaches for cross-view action recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751506",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2009",
                "id": 121
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2009",
                "id": 56
            },
            {
                "year": "2007",
                "id": 10
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 21,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Videos",
                "Feature extraction",
                "Shape",
                "Optimization",
                "Joints",
                "Correlation"
            ],
            "INSPEC: Controlled Indexing": [
                "dictionaries",
                "gesture recognition",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "view-invariant sparse representations",
                "cross-view action recognition",
                "view-specific dictionaries",
                "common dictionary",
                "view-specific features",
                "sparse feature spaces",
                "discrimination information",
                "view-shared features",
                "semisupervised setting",
                "multiview IXMAS dataset",
                "novel dictionary learning framework"
            ],
            "Author Keywords": [
                "dictionary learning",
                "action recognition"
            ]
        },
        "id": 396,
        "cited_by": [
            {
                "year": "2017",
                "id": 223
            }
        ]
    },
    {
        "title": "Event Detection in Complex Scenes Using Interval Temporal Constraints",
        "authors": [
            "Yifan Zhang",
            "Qiang Ji",
            "Hanqing Lu"
        ],
        "abstract": "In complex scenes with multiple atomic events happening sequentially or in parallel, detecting each individual event separately may not always obtain robust and reliable result. It is essential to detect them in a holistic way which incorporates the causality and temporal dependency among them to compensate the limitation of current computer vision techniques. In this paper, we propose an interval temporal constrained dynamic Bayesian network to extend Allen's interval algebra network (IAN) [2] from a deterministic static model to a probabilistic dynamic system, which can not only capture the complex interval temporal relationships, but also model the evolution dynamics and handle the uncertainty from the noisy visual observation. In the model, the topology of the IAN on each time slice and the interlinks between the time slices are discovered by an advanced structure learning method. The duration of the event and the unsynchronized time lags between two correlated event intervals are captured by a duration model, so that we can better determine the temporal boundary of the event. Empirical results on two real world datasets show the power of the proposed interval temporal constrained model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751507",
        "reference_list": [
            {
                "year": "2003",
                "id": 190
            },
            {
                "year": "2009",
                "id": 149
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Integrated circuit modeling",
                "Probabilistic logic",
                "Noise measurement",
                "Visualization",
                "Topology",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "computer vision",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "event detection",
                "interval temporal constraint",
                "temporal dependency",
                "computer vision technique",
                "interval temporal constrained dynamic Bayesian network",
                "interval algebra network",
                "deterministic static model",
                "probabilistic dynamic system",
                "evolution dynamics",
                "noisy visual observation",
                "IAN",
                "structure learning method"
            ]
        },
        "id": 397,
        "cited_by": []
    },
    {
        "title": "Towards Understanding Action Recognition",
        "authors": [
            "Hueihan Jhuang",
            "Juergen Gall",
            "Silvia Zuffi",
            "Cordelia Schmid",
            "Michael J. Black"
        ],
        "abstract": "Although action recognition in videos is widely studied, current methods often fail on real-world datasets. Many recent approaches improve accuracy and robustness to cope with challenging video sequences, but it is often unclear what affects the results most. This paper attempts to provide insights based on a systematic performance evaluation using thoroughly-annotated data of human actions. We annotate human Joints for the HMDB dataset (J-HMDB). This annotation can be used to derive ground truth optical flow and segmentation. We evaluate current methods using this dataset and systematically replace the output of various algorithms with ground truth. This enables us to discover what is important - for example, should we work on improving flow algorithms, estimating human bounding boxes, or enabling pose estimation? In summary, we find that high-level pose features greatly outperform low/mid level features, in particular, pose over time is critical, but current pose estimation algorithms are not yet reliable enough to provide this information. We also find that the accuracy of a top-performing action recognition framework can be greatly increased by refining the underlying low/mid level features, this suggests it is important to improve optical flow and human detection algorithms. Our analysis and J-HMDB dataset should facilitate a deeper understanding of action recognition algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751508",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2011",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 113,
            "other": 67,
            "total": 180
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Joints",
                "Trajectory",
                "Estimation",
                "Accuracy",
                "Motion pictures",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "gesture recognition",
                "image segmentation",
                "image sequences",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action recognition",
                "real-world datasets",
                "video sequences",
                "systematic performance evaluation",
                "thoroughly-annotated data",
                "human actions",
                "joints-for-the HMDB dataset",
                "optical flow",
                "optical segmentation",
                "flow algorithms",
                "human bounding boxes",
                "pose estimation algorithms",
                "feature extraction",
                "J-HMDB",
                "human detection algorithms",
                "computer vision algorithms"
            ],
            "Author Keywords": [
                "action recognition",
                "dataset",
                "JHMDB",
                "annotation",
                "optical flow estimation",
                "pose estimation"
            ]
        },
        "id": 398,
        "cited_by": [
            {
                "year": "2017",
                "id": 29
            },
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 227
            },
            {
                "year": "2017",
                "id": 306
            },
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2017",
                "id": 448
            },
            {
                "year": "2017",
                "id": 462
            },
            {
                "year": "2017",
                "id": 466
            },
            {
                "year": "2017",
                "id": 610
            },
            {
                "year": "2015",
                "id": 353
            },
            {
                "year": "2015",
                "id": 359
            },
            {
                "year": "2015",
                "id": 366
            },
            {
                "year": "2015",
                "id": 368
            }
        ]
    },
    {
        "title": "Modifying the Memorability of Face Photographs",
        "authors": [
            "Aditya Khosla",
            "Wilma A. Bainbridge",
            "Antonio Torralba",
            "Aude Oliva"
        ],
        "abstract": "Contemporary life bombards us with many new images of faces every day, which poses non-trivial constraints on human memory. The vast majority of face photographs are intended to be remembered, either because of personal relevance, commercial interests or because the pictures were deliberately designed to be memorable. Can we make a portrait more memorable or more forgettable automatically? Here, we provide a method to modify the memorability of individual face photographs, while keeping the identity and other facial traits (e.g. age, attractiveness, and emotional magnitude) of the individual fixed. We show that face photographs manipulated to be more memorable (or more forgettable) are indeed more often remembered (or forgotten) in a crowd-sourcing experiment with an accuracy of 74%. Quantifying and modifying the 'memorability' of a face lends itself to many useful applications in computer vision and graphics, such as mnemonic aids for learning, photo editing applications for social networks and tools for designing memorable advertisements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751509",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 21,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Shape",
                "Measurement",
                "Support vector machines",
                "Principal component analysis",
                "Cost function",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "photography"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "memorability modification",
                "face photograph",
                "nontrivial constraint",
                "human memory",
                "facial traits",
                "crowd-sourcing experiment",
                "computer vision",
                "computer graphics",
                "photo editing application",
                "social networks",
                "mnemonic aids"
            ]
        },
        "id": 399,
        "cited_by": [
            {
                "year": "2017",
                "id": 584
            },
            {
                "year": "2015",
                "id": 121
            },
            {
                "year": "2015",
                "id": 266
            }
        ]
    },
    {
        "title": "A Practical Transfer Learning Algorithm for Face Verification",
        "authors": [
            "Xudong Cao",
            "David Wipf",
            "Fang Wen",
            "Genquan Duan",
            "Jian Sun"
        ],
        "abstract": "Face verification involves determining whether a pair of facial images belongs to the same or different subjects. This problem can prove to be quite challenging in many important applications where labeled training data is scarce, e.g., family album photo organization software. Herein we propose a principled transfer learning approach for merging plentiful source-domain data with limited samples from some target domain of interest to create a classifier that ideally performs nearly as well as if rich target-domain data were present. Based upon a surprisingly simple generative Bayesian model, our approach combines a KL-divergence based regularizer/prior with a robust likelihood function leading to a scalable implementation via the EM algorithm. As justification for our design choices, we later use principles from convex analysis to recast our algorithm as an equivalent structured rank minimization problem leading to a number of interesting insights related to solution structure and feature-transform invariance. These insights help to both explain the effectiveness of our algorithm as well as elucidate a wide variety of related Bayesian approaches. Experimental testing with challenging datasets validate the utility of the proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751510",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2007",
                "id": 212
            }
        ],
        "citation": {
            "ieee": 58,
            "other": 25,
            "total": 83
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayes methods",
                "Face",
                "Joints",
                "Algorithm design and analysis",
                "Computational modeling",
                "Testing",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "convex programming",
                "expectation-maximisation algorithm",
                "face recognition",
                "image classification",
                "learning (artificial intelligence)",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face verification",
                "facial images",
                "labeled training data",
                "family album photo organization software",
                "principled transfer learning approach",
                "source-domain data",
                "target-domain data",
                "generative Bayesian model",
                "KL-divergence-based regularizer-prior",
                "robust likelihood function",
                "EM algorithm",
                "convex analysis",
                "equivalent structured rank minimization problem",
                "feature-transform invariance",
                "Kullback-Leibler divergence-prior"
            ],
            "Author Keywords": [
                "transfer learning",
                "face verification"
            ]
        },
        "id": 400,
        "cited_by": [
            {
                "year": "2015",
                "id": 415
            },
            {
                "year": "2015",
                "id": 431
            }
        ]
    },
    {
        "title": "Learning to Predict Gaze in Egocentric Video",
        "authors": [
            "Yin Li",
            "Alireza Fathi",
            "James M. Rehg"
        ],
        "abstract": "We present a model for gaze prediction in egocentric video by leveraging the implicit cues that exist in camera wearer's behaviors. Specifically, we compute the camera wearer's head motion and hand location from the video and combine them to estimate where the eyes look. We further model the dynamic behavior of the gaze, in particular fixations, as latent variables to improve the gaze prediction. Our gaze prediction results outperform the state-of-the-art algorithms by a large margin on publicly available egocentric vision datasets. In addition, we demonstrate that we get a significant performance boost in recognizing daily actions and segmenting foreground objects by plugging in our gaze predictions into state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751511",
        "reference_list": [
            {
                "year": "2011",
                "id": 51
            },
            {
                "year": "2009",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 55,
            "other": 30,
            "total": 85
        },
        "keywords": {
            "IEEE Keywords": [
                "Head",
                "Hidden Markov models",
                "Feature extraction",
                "Predictive models",
                "Vectors",
                "Computational modeling",
                "Graphical models"
            ],
            "INSPEC: Controlled Indexing": [
                "gaze tracking",
                "gesture recognition",
                "image segmentation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gaze prediction",
                "egocentric video",
                "implicit cues",
                "camera wearer behaviors",
                "camera wearer head motion",
                "camera wearer hand location",
                "gaze dynamic behavior",
                "fixations",
                "latent variables",
                "egocentric vision datasets",
                "daily action recognition",
                "foreground object segmentation"
            ],
            "Author Keywords": [
                "Egocentric Vision",
                "Gaze Prediction",
                "Action Recognition",
                "Object Segmentation"
            ]
        },
        "id": 401,
        "cited_by": [
            {
                "year": "2015",
                "id": 157
            },
            {
                "year": "2015",
                "id": 502
            }
        ]
    },
    {
        "title": "Real-Time Articulated Hand Pose Estimation Using Semi-supervised Transductive Regression Forests",
        "authors": [
            "Danhang Tang",
            "Tsz-Ho Yu",
            "Tae-Kyun Kim"
        ],
        "abstract": "This paper presents the first semi-supervised transductive algorithm for real-time articulated hand pose estimation. Noisy data and occlusions are the major challenges of articulated hand pose estimation. In addition, the discrepancies among realistic and synthetic pose data undermine the performances of existing approaches that use synthetic data extensively in training. We therefore propose the Semi-supervised Transductive Regression (STR) forest which learns the relationship between a small, sparsely labelled realistic dataset and a large synthetic dataset. We also design a novel data-driven, pseudo-kinematic technique to refine noisy or occluded joints. Our contributions include: (i) capturing the benefits of both realistic and synthetic data via transductive learning, (ii) showing accuracies can be improved by considering unlabelled data, and (iii) introducing a pseudo-kinematic technique to refine articulations efficiently. Experimental results show not only the promising performance of our method with respect to noise and occlusions, but also its superiority over state-of-the-arts in accuracy, robustness and speed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751512",
        "reference_list": [
            {
                "year": "2011",
                "id": 138
            },
            {
                "year": "2011",
                "id": 52
            },
            {
                "year": "2009",
                "id": 189
            },
            {
                "year": "2009",
                "id": 64
            },
            {
                "year": "2007",
                "id": 135
            },
            {
                "year": "2011",
                "id": 265
            },
            {
                "year": "2011",
                "id": 157
            }
        ],
        "citation": {
            "ieee": 69,
            "other": 38,
            "total": 107
        },
        "keywords": {
            "IEEE Keywords": [
                "Joints",
                "Estimation",
                "Training",
                "Kinematics",
                "Noise measurement",
                "Training data",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time articulated hand pose estimation",
                "semisupervised transductive regression forests",
                "noisy data",
                "occlusions",
                "data-driven pseudokinematic technique",
                "transductive learning"
            ]
        },
        "id": 402,
        "cited_by": [
            {
                "year": "2017",
                "id": 327
            },
            {
                "year": "2017",
                "id": 329
            },
            {
                "year": "2015",
                "id": 91
            },
            {
                "year": "2015",
                "id": 208
            },
            {
                "year": "2015",
                "id": 370
            },
            {
                "year": "2015",
                "id": 371
            }
        ]
    },
    {
        "title": "Semantically-Based Human Scanpath Estimation with HMMs",
        "authors": [
            "Huiying Liu",
            "Dong Xu",
            "Qingming Huang",
            "Wen Li",
            "Min Xu",
            "Stephen Lin"
        ],
        "abstract": "We present a method for estimating human scan paths, which are sequences of gaze shifts that follow visual attention over an image. In this work, scan paths are modeled based on three principal factors that influence human attention, namely low-level feature saliency, spatial position, and semantic content. Low-level feature saliency is formulated as transition probabilities between different image regions based on feature differences. The effect of spatial position on gaze shifts is modeled as a Levy flight with the shifts following a 2D Cauchy distribution. To account for semantic content, we propose to use a Hidden Markov Model (HMM) with a Bag-of-Visual-Words descriptor of image regions. An HMM is well-suited for this purpose in that 1) the hidden states, obtained by unsupervised learning, can represent latent semantic concepts, 2) the prior distribution of the hidden states describes visual attraction to the semantic concepts, and 3) the transition probabilities represent human gaze shift patterns. The proposed method is applied to task-driven viewing processes. Experiments and analysis performed on human eye gaze data verify the effectiveness of this method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751513",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Semantics",
                "Visualization",
                "Probability",
                "Training",
                "Estimation",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "gaze tracking",
                "hidden Markov models",
                "image representation",
                "image sequences",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human eye gaze data",
                "task-driven viewing process",
                "human gaze shift pattern representation",
                "visual attraction",
                "latent semantic concept",
                "unsupervised learning",
                "hidden states",
                "bag-of-visual-word descriptor",
                "hidden Markov model",
                "2D Cauchy distribution",
                "Levy flight",
                "spatial position effect",
                "feature difference",
                "image region",
                "transition probability",
                "semantic content",
                "spatial position",
                "low-level feature saliency",
                "human attention",
                "visual attention",
                "gaze shift sequences",
                "HMM",
                "semantically-based human scanpath estimation"
            ],
            "Author Keywords": [
                "Attention",
                "Saliency",
                "Hidden Markov Model",
                "Gaze shift",
                "Levy flight"
            ]
        },
        "id": 403,
        "cited_by": []
    },
    {
        "title": "Enhanced Continuous Tabu Search for Parameter Estimation in Multiview Geometry",
        "authors": [
            "Guoqing Zhou",
            "Qing Wang"
        ],
        "abstract": "Optimization using the L_infty norm has been becoming an effective way to solve parameter estimation problems in multiview geometry. But the computational cost increases rapidly with the size of measurement data. Although some strategies have been presented to improve the efficiency of L_infty optimization, it is still an open issue. In the paper, we propose a novel approach under the framework of enhanced continuous tabu search (ECTS) for generic parameter estimation in multiview geometry. ECTS is an optimization method in the domain of artificial intelligence, which has an interesting ability of covering a wide solution space by promoting the search far away from current solution and consecutively decreasing the possibility of trapping in the local minima. Taking the triangulation as an example, we propose the corresponding ways in the key steps of ECTS, diversification and intensification. We also present theoretical proof to guarantee the global convergence of search with probability one. Experimental results have validated that the ECTS based approach can obtain global optimum efficiently, especially for large scale dimension of parameter. Potentially, the novel ECTS based algorithm can be applied in many applications of multiview geometry.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751514",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Optimization",
                "Parameter estimation",
                "Cameras",
                "Convergence",
                "Estimation",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "parameter estimation",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "L\u221e norm",
                "parameter estimation problems",
                "multiview geometry",
                "computational cost",
                "L\u221e optimization",
                "ECTS",
                "enhanced continuous tabu search",
                "generic parameter estimation",
                "artificial intelligence",
                "local minima"
            ]
        },
        "id": 404,
        "cited_by": []
    },
    {
        "title": "Global Fusion of Relative Motions for Robust, Accurate and Scalable Structure from Motion",
        "authors": [
            "Pierre Moulon",
            "Pascal Monasse",
            "Renaud Marlet"
        ],
        "abstract": "Multi-view structure from motion (SfM) estimates the position and orientation of pictures in a common 3D coordinate frame. When views are treated incrementally, this external calibration can be subject to drift, contrary to global methods that distribute residual errors evenly. We propose a new global calibration approach based on the fusion of relative motions between image pairs. We improve an existing method for robustly computing global rotations. We present an efficient a contrario trifocal tensor estimation method, from which stable and precise translation directions can be extracted. We also define an efficient translation registration method that recovers accurate camera positions. These components are combined into an original SfM pipeline. Our experiments show that, on most datasets, it outperforms in accuracy other existing incremental and global pipelines. It also achieves strikingly good running times: it is about 20 times faster than the other global method we could compare to, and as fast as the best incremental method. More importantly, it features better scalability properties.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751515",
        "reference_list": [],
        "citation": {
            "ieee": 52,
            "other": 41,
            "total": 93
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Robustness",
                "Tensile stress",
                "Estimation",
                "Pipelines",
                "Three-dimensional displays",
                "Accuracy"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "feature extraction",
                "image fusion",
                "image registration",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relative motions",
                "global fusion",
                "scalable structure from motion",
                "multiview structure from motion",
                "position estimation",
                "picture orientation estimation",
                "3D coordinate frame",
                "external calibration",
                "residual errors",
                "global calibration approach",
                "contrario trifocal tensor estimation method",
                "translation registration method",
                "camera positions",
                "SfM pipeline"
            ],
            "Author Keywords": [
                "Calibration",
                "Structure-from-Motion",
                "robust estimation"
            ]
        },
        "id": 405,
        "cited_by": []
    },
    {
        "title": "Internet Based Morphable Model",
        "authors": [
            "Ira Kemelmacher-Shlizerman"
        ],
        "abstract": "In this paper we present a new concept of building a morphable model directly from photos on the Internet. Morphable models have shown very impressive results more than a decade ago, and could potentially have a huge impact on all aspects of face modeling and recognition. One of the challenges, however, is to capture and register 3D laser scans of large number of people and facial expressions. Nowadays, there are enormous amounts of face photos on the Internet, large portion of which has semantic labels. We propose a framework to build a morph able model directly from photos, the framework includes dense registration of Internet photos, as well as, new single view shape reconstruction and modification algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751516",
        "reference_list": [
            {
                "year": "2011",
                "id": 221
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 13,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Lighting",
                "Image reconstruction",
                "Three-dimensional displays",
                "Solid modeling",
                "Internet"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image registration",
                "Internet",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Internet-based morphable model",
                "face modeling",
                "face recognition",
                "3D laser scan registration",
                "facial expressions",
                "face photos",
                "semantic labels",
                "single view shape reconstruction algorithm",
                "single view shape modification algorithm"
            ],
            "Author Keywords": [
                "morphable model",
                "faces",
                "3D reconstruction",
                "photometric stereo",
                "single view",
                "optical flow"
            ]
        },
        "id": 406,
        "cited_by": []
    },
    {
        "title": "Large-Scale Multi-resolution Surface Reconstruction from RGB-D Sequences",
        "authors": [
            "Frank Steinbrucker",
            "Christian Kerl",
            "Daniel Cremers",
            "Jurgen Sturm"
        ],
        "abstract": "We propose a method to generate highly detailed, textured 3D models of large environments from RGB-D sequences. Our system runs in real-time on a standard desktop PC with a state-of-the-art graphics card. To reduce the memory consumption, we fuse the acquired depth maps and colors in a multi-scale octree representation of a signed distance function. To estimate the camera poses, we construct a pose graph and use dense image alignment to determine the relative pose between pairs of frames. We add edges between nodes when we detect loop-closures and optimize the pose graph to correct for long-term drift. Our implementation is highly parallelized on graphics hardware to achieve real-time performance. More specifically, we can reconstruct, store, and continuously update a colored 3D model of an entire corridor of nine rooms at high levels of detail in real-time on a single GPU with 2.5GB.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751517",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2011",
                "id": 295
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 32,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Octrees",
                "Arrays",
                "Surface reconstruction",
                "Three-dimensional displays",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image reconstruction",
                "image sequences",
                "octrees"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large scale multiresolution surface reconstruction",
                "RGB-D sequences",
                "multiscale octree representation",
                "signed distance function",
                "camera poses",
                "pose graph",
                "dense image alignment",
                "loop-closures",
                "3D model",
                "GPU"
            ]
        },
        "id": 407,
        "cited_by": [
            {
                "year": "2017",
                "id": 17
            },
            {
                "year": "2017",
                "id": 328
            },
            {
                "year": "2017",
                "id": 486
            },
            {
                "year": "2015",
                "id": 245
            }
        ]
    },
    {
        "title": "Modeling 4D Human-Object Interactions for Event and Object Recognition",
        "authors": [
            "Ping Wei",
            "Yibiao Zhao",
            "Nanning Zheng",
            "Song-Chun Zhu"
        ],
        "abstract": "Recognizing the events and objects in the video sequence are two challenging tasks due to the complex temporal structures and the large appearance variations. In this paper, we propose a 4D human-object interaction model, where the two tasks jointly boost each other. Our human-object interaction is defined in 4D space: i) the co occurrence and geometric constraints of human pose and object in 3D space, ii) the sub-events transition and objects coherence in 1D temporal dimension. We represent the structure of events, sub-events and objects in a hierarchical graph. For an input RGB-depth video, we design a dynamic programming beam search algorithm to: i) segment the video, ii) recognize the events, and iii) detect the objects simultaneously. For evaluation, we built a large-scale multiview 3D event dataset which contains 3815 video sequences and 383,036 RGBD frames captured by the Kinect cameras. The experiment results on this dataset show the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751518",
        "reference_list": [
            {
                "year": "2011",
                "id": 61
            },
            {
                "year": "2007",
                "id": 26
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 20,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Hidden Markov models",
                "Feature extraction",
                "Video sequences",
                "Vectors",
                "Solid modeling",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "geometry",
                "graph theory",
                "image segmentation",
                "image sequences",
                "object recognition",
                "search problems",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "event recognition",
                "video sequence",
                "complex temporal structures",
                "4D human-object interaction model",
                "geometric constraints",
                "co occurrence constraints",
                "human pose",
                "sub-events transition",
                "1D temporal dimension",
                "hierarchical graph",
                "RGB-depth video",
                "dynamic programming beam search algorithm",
                "large-scale multiview 3D event dataset",
                "Kinect cameras",
                "object detection"
            ]
        },
        "id": 408,
        "cited_by": [
            {
                "year": "2013",
                "id": 391
            }
        ]
    },
    {
        "title": "Modeling the Calibration Pipeline of the Lytro Camera for High Quality Light-Field Image Reconstruction",
        "authors": [
            "Donghyeon Cho",
            "Minhaeng Lee",
            "Sunyeong Kim",
            "Yu-Wing Tai"
        ],
        "abstract": "Light-field imaging systems have got much attention recently as the next generation camera model. A light-field imaging system consists of three parts: data acquisition, manipulation, and application. Given an acquisition system, it is important to understand how a light-field camera converts from its raw image to its resulting refocused image. In this paper, using the Lytro camera as an example, we describe step-by-step procedures to calibrate a raw light-field image. In particular, we are interested in knowing the spatial and angular coordinates of the micro lens array and the resampling process for image reconstruction. Since Lytro uses a hexagonal arrangement of a micro lens image, additional treatments in calibration are required. After calibration, we analyze and compare the performances of several resampling methods for image reconstruction with and without calibration. Finally, a learning based interpolation method is proposed which demonstrates a higher quality image reconstruction than previous interpolation methods including a method used in Lytro software.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751519",
        "reference_list": [
            {
                "year": "2011",
                "id": 36
            }
        ],
        "citation": {
            "ieee": 49,
            "other": 26,
            "total": 75
        },
        "keywords": {
            "IEEE Keywords": [
                "Lenses",
                "Image resolution",
                "Image reconstruction",
                "Interpolation",
                "Calibration",
                "Cameras",
                "Arrays"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Lytro camera",
                "high quality light-field image reconstruction",
                "next generation camera model",
                "angular coordinates",
                "spatial coordinates",
                "micro lens image"
            ],
            "Author Keywords": [
                "Lightfield Camera Calibration",
                "Lightfield Image Reconstruction"
            ]
        },
        "id": 409,
        "cited_by": []
    },
    {
        "title": "Face Recognition Using Face Patch Networks",
        "authors": [
            "Chaochao Lu",
            "Deli Zhao",
            "Xiaoou Tang"
        ],
        "abstract": "When face images are taken in the wild, the large variations in facial pose, illumination, and expression make face recognition challenging. The most fundamental problem for face recognition is to measure the similarity between faces. The traditional measurements such as various mathematical norms, Hausdorff distance, and approximate geodesic distance cannot accurately capture the structural information between faces in such complex circumstances. To address this issue, we develop a novel face patch network, based on which we define a new similarity measure called the random path (RP) measure. The RP measure is derived from the collective similarity of paths by performing random walks in the network. It can globally characterize the contextual and curved structures of the face space. To apply the RP measure, we construct two kinds of networks: the in-face network and the out-face network. The in-face network is drawn from any two face images and captures the local structural information. The out-face network is constructed from all the training face patches, thereby modeling the global structures of face space. The two face networks are structurally complementary and can be combined together to improve the recognition performance. Experiments on the Multi-PIE and LFW benchmarks show that the RP measure outperforms most of the state-of-art algorithms for face recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751520",
        "reference_list": [
            {
                "year": "2009",
                "id": 268
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Face recognition",
                "Level measurement",
                "Current measurement",
                "Vectors",
                "Manifolds",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "novel face patch network",
                "similarity measure",
                "random path measure",
                "RP measure",
                "face recognition",
                "random walks",
                "in-face network",
                "out-face network",
                "local structural information",
                "LFW benchmarks",
                "multi PIE benchmarks"
            ]
        },
        "id": 410,
        "cited_by": []
    },
    {
        "title": "Coupling Alignments with Recognition for Still-to-Video Face Recognition",
        "authors": [
            "Zhiwu Huang",
            "Xiaowei Zhao",
            "Shiguang Shan",
            "Ruiping Wang",
            "Xilin Chen"
        ],
        "abstract": "The Still-to-Video (S2V) face recognition systems typically need to match faces in low-quality videos captured under unconstrained conditions against high quality still face images, which is very challenging because of noise, image blur, low face resolutions, varying head pose, complex lighting, and alignment difficulty. To address the problem, one solution is to select the frames of `best quality' from videos (hereinafter called quality alignment in this paper). Meanwhile, the faces in the selected frames should also be geometrically aligned to the still faces offline well-aligned in the gallery. In this paper, we discover that the interactions among the three tasks-quality alignment, geometric alignment and face recognition-can benefit from each other, thus should be performed jointly. With this in mind, we propose a Coupling Alignments with Recognition (CAR) method to tightly couple these tasks via low-rank regularized sparse representation in a unified framework. Our method makes the three tasks promote mutually by a joint optimization in an Augmented Lagrange Multiplier routine. Extensive experiments on two challenging S2V datasets demonstrate that our method outperforms the state-of-the-art methods impressively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751521",
        "reference_list": [
            {
                "year": "2011",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 8,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Videos",
                "Face recognition",
                "Video sequences",
                "Probes",
                "Silicon",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image matching",
                "image representation",
                "image restoration",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "still-to-video face recognition",
                "S2V face recognition system",
                "image blurring",
                "low face resolution",
                "head pose variation",
                "videos quality alignment",
                "coupling alignments with recognition method",
                "CAR method",
                "low-rank regularized sparse representation",
                "optimization",
                "augmented Lagrange multiplier routine"
            ],
            "Author Keywords": [
                "still-to-video face recognition",
                "coupling alignments with recognition"
            ]
        },
        "id": 411,
        "cited_by": []
    },
    {
        "title": "Capturing Global Semantic Relationships for Facial Action Unit Recognition",
        "authors": [
            "Ziheng Wang",
            "Yongqiang Li",
            "Shangfei Wang",
            "Qiang Ji"
        ],
        "abstract": "In this paper we tackle the problem of facial action unit (AU) recognition by exploiting the complex semantic relationships among AUs, which carry crucial top-down information yet have not been thoroughly exploited. Towards this goal, we build a hierarchical model that combines the bottom-level image features and the top-level AU relationships to jointly recognize AUs in a principled manner. The proposed model has two major advantages over existing methods. 1) Unlike methods that can only capture local pair-wise AU dependencies, our model is developed upon the restricted Boltzmann machine and therefore can exploit the global relationships among AUs. 2) Although AU relationships are influenced by many related factors such as facial expressions, these factors are generally ignored by the current methods. Our model, however, can successfully capture them to more accurately characterize the AU relationships. Efficient learning and inference algorithms of the proposed model are also developed. Experimental results on benchmark databases demonstrate the effectiveness of the proposed approach in modelling complex AU relationships as well as its superior AU recognition performance over existing approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751522",
        "reference_list": [],
        "citation": {
            "ieee": 38,
            "other": 8,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Gold",
                "Mathematical model",
                "Equations",
                "Semantics",
                "Face recognition",
                "Marine vehicles",
                "Inference algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "Boltzmann machines",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial expressions",
                "restricted Boltzmann machine",
                "hierarchical model",
                "facial action unit recognition",
                "global semantic relationships"
            ],
            "Author Keywords": [
                "Spatiotemporal relationship",
                "AU recognition",
                "RBM"
            ]
        },
        "id": 412,
        "cited_by": [
            {
                "year": "2017",
                "id": 416
            },
            {
                "year": "2015",
                "id": 423
            },
            {
                "year": "2015",
                "id": 475
            }
        ]
    },
    {
        "title": "Estimating Human Pose with Flowing Puppets",
        "authors": [
            "Silvia Zuffi",
            "Javier Romero",
            "Cordelia Schmid",
            "Michael J. Black"
        ],
        "abstract": "We address the problem of upper-body human pose estimation in uncontrolled monocular video sequences, without manual initialization. Most current methods focus on isolated video frames and often fail to correctly localize arms and hands. Inferring pose over a video sequence is advantageous because poses of people in adjacent frames exhibit properties of smooth variation due to the nature of human and camera motion. To exploit this, previous methods have used prior knowledge about distinctive actions or generic temporal priors combined with static image likelihoods to track people in motion. Here we take a different approach based on a simple observation: Information about how a person moves from frame to frame is present in the optical flow field. We develop an approach for tracking articulated motions that \"links\" articulated shape models of people in adjacent frames through the dense optical flow. Key to this approach is a 2D shape model of the body that we use to compute how the body moves over time. The resulting \"flowing puppets\" provide a way of integrating image evidence across frames to improve pose inference. We apply our method on a challenging dataset of TV video sequences and show state-of-the-art performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751523",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 13,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Optical imaging",
                "Computational modeling",
                "Estimation",
                "Adaptive optics",
                "Image color analysis",
                "Joints"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image sequences",
                "object tracking",
                "pose estimation",
                "video cameras",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "upper-body human pose estimation",
                "uncontrolled monocular video sequences",
                "isolated video frames",
                "adjacent frames",
                "camera motion",
                "static image",
                "optical flow field",
                "articulated motion tracking",
                "articulated shape models",
                "2D shape model",
                "flowing puppets",
                "pose inference",
                "TV video sequences"
            ],
            "Author Keywords": [
                "human pose estimation",
                "optical flow",
                "video"
            ]
        },
        "id": 413,
        "cited_by": [
            {
                "year": "2015",
                "id": 224
            },
            {
                "year": "2015",
                "id": 486
            }
        ]
    },
    {
        "title": "Illuminant Chromaticity from Image Sequences",
        "authors": [
            "Veronique Prinet",
            "Dani Lischinski",
            "Michael Werman"
        ],
        "abstract": "We estimate illuminant chromaticity from temporal sequences, for scenes illuminated by either one or two dominant illuminants. While there are many methods for illuminant estimation from a single image, few works so far have focused on videos, and even fewer on multiple light sources. Our aim is to leverage information provided by the temporal acquisition, where either the objects or the camera or the light source are/is in motion in order to estimate illuminant color without the need for user interaction or using strong assumptions and heuristics. We introduce a simple physically-based formulation based on the assumption that the incident light chromaticity is constant over a short space-time domain. We show that a deterministic approach is not sufficient for accurate and robust estimation: however, a probabilistic formulation makes it possible to implicitly integrate away hidden factors that have been ignored by the physical model. Experimental results are reported on a dataset of natural video sequences and on the Gray Ball benchmark, indicating that we compare favorably with the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751524",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 0,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Estimation",
                "Videos",
                "Lighting",
                "Mathematical model",
                "Vectors",
                "Equations"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image sequences",
                "lighting",
                "natural scenes",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image sequences",
                "illuminant chromaticity estimation",
                "temporal sequences",
                "temporal acquisition",
                "light source",
                "camera",
                "illuminant color estimation",
                "physically-based formulation",
                "incident light chromaticity",
                "space-time domain",
                "deterministic approach",
                "robust estimation",
                "probabilistic formulation",
                "natural video sequences",
                "GrayBall benchmark"
            ],
            "Author Keywords": [
                "Low-level vision",
                "Image processing",
                "White balance",
                "Color constancy"
            ]
        },
        "id": 414,
        "cited_by": [
            {
                "year": "2017",
                "id": 572
            }
        ]
    },
    {
        "title": "Contextual Hypergraph Modeling for Salient Object Detection",
        "authors": [
            "Xi Li",
            "Yao Li",
            "Chunhua Shen",
            "Anthony Dick",
            "Anton Van Den Hengel"
        ],
        "abstract": "Salient object detection aims to locate objects that capture human attention within images. Previous approaches often pose this as a problem of image contrast analysis. In this work, we model an image as a hyper graph that utilizes a set of hyper edges to capture the contextual properties of image pixels or regions. As a result, the problem of salient object detection becomes one of finding salient vertices and hyper edges in the hyper graph. The main advantage of hyper graph modeling is that it takes into account each pixel's (or region's) affinity with its neighborhood as well as its separation from image background. Furthermore, we propose an alternative approach based on center-versus-surround contextual contrast analysis, which performs salient object detection by optimizing a cost-sensitive support vector machine (SVM) objective function. Experimental results on four challenging datasets demonstrate the effectiveness of the proposed approaches against the state-of-the-art approaches to salient object detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751525",
        "reference_list": [
            {
                "year": "2011",
                "id": 130
            },
            {
                "year": "2011",
                "id": 281
            },
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2011",
                "id": 191
            },
            {
                "year": "2011",
                "id": 13
            },
            {
                "year": "2011",
                "id": 29
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2011",
                "id": 173
            }
        ],
        "citation": {
            "ieee": 48,
            "other": 41,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Object detection",
                "Context",
                "Vectors",
                "Image edge detection",
                "Visualization",
                "Context modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image resolution",
                "object detection",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contextual hypergraph modeling",
                "salient object detection",
                "object location",
                "human attention capture",
                "image contrast analysis",
                "contextual image pixel properties",
                "salient vertices",
                "salient hyperedges",
                "pixel affinity",
                "region affinity",
                "center-versus-surround contextual contrast analysis",
                "cost-sensitive support vector machine objective function",
                "cost-sensitive SVM objective function"
            ],
            "Author Keywords": [
                "Saliency detection",
                "Salient Object Detection"
            ]
        },
        "id": 415,
        "cited_by": [
            {
                "year": "2017",
                "id": 109
            },
            {
                "year": "2017",
                "id": 426
            },
            {
                "year": "2015",
                "id": 45
            }
        ]
    },
    {
        "title": "Super-resolution via Transform-Invariant Group-Sparse Regularization",
        "authors": [
            "Carlos Fernandez-Granda",
            "Emmanuel J. Cand\u00e8s"
        ],
        "abstract": "We present a framework to super-resolve planar regions found in urban scenes and other man-made environments by taking into account their 3D geometry. Such regions have highly structured straight edges, but this prior is challenging to exploit due to deformations induced by the projection onto the imaging plane. Our method factors out such deformations by using recently developed tools based on convex optimization to learn a transform that maps the image to a domain where its gradient has a simple group-sparse structure. This allows to obtain a novel convex regularizer that enforces global consistency constraints between the edges of the image. Computational experiments with real images show that this data-driven approach to the design of regularizers promoting transform-invariant group sparsity is very effective at high super-resolution factors. We view our approach as complementary to most recent super-resolution methods, which tend to focus on hallucinating high-frequency textures.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751526",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 3,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Image edge detection",
                "Three-dimensional displays",
                "Digital TV",
                "Transforms",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "edge detection",
                "image resolution",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "superresolution factor",
                "transform-invariant group-sparse regularization",
                "planar region",
                "urban scene",
                "man-made environment",
                "3D geometry",
                "highly-structured straight edges",
                "imaging plane",
                "convex optimization",
                "image mapping",
                "group-sparse structure",
                "convex regularizer",
                "global consistency constraints",
                "image edges",
                "regularizer design",
                "superresolution method",
                "high-frequency textures"
            ],
            "Author Keywords": [
                "Super-resolution",
                "group sparsity",
                "camera projection",
                "low-rank textures",
                "convex optimization",
                "transform invariance",
                "deblurring"
            ]
        },
        "id": 416,
        "cited_by": [
            {
                "year": "2017",
                "id": 181
            },
            {
                "year": "2015",
                "id": 50
            }
        ]
    },
    {
        "title": "Optical Flow via Locally Adaptive Fusion of Complementary Data Costs",
        "authors": [
            "Tae Hyun Kim",
            "Hee Seok Lee",
            "Kyoung Mu Lee"
        ],
        "abstract": "Many state-of-the-art optical flow estimation algorithms optimize the data and regularization terms to solve ill-posed problems. In this paper, in contrast to the conventional optical flow framework that uses a single or fixed data model, we study a novel framework that employs locally varying data term that adaptively combines different multiple types of data models. The locally adaptive data term greatly reduces the matching ambiguity due to the complementary nature of the multiple data models. The optimal number of complementary data models is learnt by minimizing the redundancy among them under the minimum description length constraint (MDL). From these chosen data models, a new optical flow estimation energy model is designed with the weighted sum of the multiple data models, and a convex optimization-based highly effective and practical solution that finds the optical flow, as well as the weights is proposed. Comparative experimental results on the Middlebury optical flow benchmark show that the proposed method using the complementary data models outperforms the state-of-the art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751527",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 9,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Optical imaging",
                "Estimation",
                "Brightness",
                "Adaptation models",
                "Adaptive optics",
                "Data integration"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "data models",
                "image fusion",
                "image matching",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "locally adaptive fusion",
                "complementary data costs",
                "optical flow estimation algorithms",
                "regularization terms",
                "single data model",
                "fixed data model",
                "data term",
                "matching ambiguity",
                "redundancy minimization",
                "minimum description length constraint",
                "MDL",
                "optical flow estimation energy model",
                "weighted sum",
                "convex optimization",
                "Middlebury optical flow benchmark"
            ]
        },
        "id": 417,
        "cited_by": []
    },
    {
        "title": "Optimal Orthogonal Basis and Image Assimilation: Motion Modeling",
        "authors": [
            "Etienne Huot",
            "Giuseppe Papari",
            "Isabelle Herlin"
        ],
        "abstract": "This paper describes modeling and numerical computation of orthogonal bases, which are used to describe images and motion fields. Motion estimation from image data is then studied on subspaces spanned by these bases. A reduced model is obtained as the Galerkin projection on these subspaces of a physical model, based on Euler and optical flow equations. A data assimilation method is studied, which assimilates coefficients of image data in the reduced model in order to estimate motion coefficients. The approach is first quantified on synthetic data: it demonstrates the interest of model reduction as a compromise between results quality and computational cost. Results obtained on real data are then displayed so as to illustrate the method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751528",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Vectors",
                "Mathematical model",
                "Equations",
                "Motion estimation",
                "Numerical models",
                "Data assimilation",
                "Boundary conditions"
            ],
            "INSPEC: Controlled Indexing": [
                "data assimilation",
                "Galerkin method",
                "image sequences",
                "motion estimation",
                "numerical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optimal orthogonal basis",
                "image assimilation",
                "numerical computation",
                "motion estimation",
                "image data",
                "Galerkin projection",
                "physical model",
                "Euler equations",
                "optical flow equations",
                "data assimilation method",
                "synthetic data",
                "interest of model reduction",
                "computational cost"
            ],
            "Author Keywords": [
                "data assimilation",
                "galerkin projection",
                "motion estimation",
                "reduced model",
                "satellite image"
            ]
        },
        "id": 418,
        "cited_by": []
    },
    {
        "title": "A Generic Deformation Model for Dense Non-rigid Surface Registration: A Higher-Order MRF-Based Approach",
        "authors": [
            "Yun Zeng",
            "Chaohui Wang",
            "Xianfeng Gu",
            "Dimitris Samaras",
            "Nikos Paragios"
        ],
        "abstract": "We propose a novel approach for dense non-rigid 3D surface registration, which brings together Riemannian geometry and graphical models. To this end, we first introduce a generic deformation model, called Canonical Distortion Coefficients (CDCs), by characterizing the deformation of every point on a surface using the distortions along its two principle directions. This model subsumes the deformation groups commonly used in surface registration such as isometry and conformality, and is able to handle more complex deformations. We also derive its discrete counterpart which can be computed very efficiently in a closed form. Based on these, we introduce a higher-order Markov Random Field (MRF) model which seamlessly integrates our deformation model and a geometry/texture similarity metric. Then we jointly establish the optimal correspondences for all the points via maximum a posteriori (MAP) inference. Moreover, we develop a parallel optimization algorithm to efficiently perform the inference for the proposed higher-order MRF model. The resulting registration algorithm outperforms state-of-the-art methods in both dense non-rigid 3D surface registration and tracking.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751529",
        "reference_list": [
            {
                "year": "2007",
                "id": 100
            },
            {
                "year": "2005",
                "id": 50
            },
            {
                "year": "2011",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Jacobian matrices",
                "Deformable models",
                "Three-dimensional displays",
                "Measurement",
                "Computational modeling",
                "Mathematical model",
                "Inference algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "deformation",
                "distortion",
                "higher order statistics",
                "image registration",
                "image texture",
                "Markov processes",
                "maximum likelihood estimation",
                "optimisation",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "generic deformation model",
                "higher-order MRF-based approach",
                "dense nonrigid 3D surface registration",
                "Riemannian geometry",
                "graphical models",
                "canonical distortion coefficients",
                "CDCs",
                "conformality",
                "isometry",
                "higher-order Markov random field model",
                "geometry-texture similarity metric",
                "optimal correspondences",
                "maximum a posteriori inference",
                "MAP",
                "3D surface tracking",
                "parallel optimization algorithm"
            ]
        },
        "id": 419,
        "cited_by": []
    },
    {
        "title": "Bayesian 3D Tracking from Monocular Video",
        "authors": [
            "Ernesto Brau",
            "Jinyan Guan",
            "Kyle Simek",
            "Luca Del Pero",
            "Colin Reimer Dawson",
            "Kobus Barnard"
        ],
        "abstract": "We develop a Bayesian modeling approach for tracking people in 3D from monocular video with unknown cameras. Modeling in 3D provides natural explanations for occlusions and smoothness discontinuities that result from projection, and allows priors on velocity and smoothness to be grounded in physical quantities: meters and seconds vs. pixels and frames. We pose the problem in the context of data association, in which observations are assigned to tracks. A correct application of Bayesian inference to multi-target tracking must address the fact that the model's dimension changes as tracks are added or removed, and thus, posterior densities of different hypotheses are not comparable. We address this by marginalizing out the trajectory parameters so the resulting posterior over data associations has constant dimension. This is made tractable by using (a) Gaussian process priors for smooth trajectories and (b) approximately Gaussian likelihood functions. Our approach provides a principled method for incorporating multiple sources of evidence, we present results using both optical flow and object detector outputs. Results are comparable to recent work on 3D tracking and, unlike others, our method requires no pre-calibrated cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751530",
        "reference_list": [
            {
                "year": "2001",
                "id": 108
            },
            {
                "year": "2011",
                "id": 196
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Trajectory",
                "Computational modeling",
                "Solid modeling",
                "Target tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "Gaussian processes",
                "image sequences",
                "sensor fusion",
                "target tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian 3D tracking",
                "monocular video",
                "Bayesian modeling approach",
                "people tracking",
                "occlusions",
                "smoothness discontinuities",
                "data association context",
                "Bayesian inference",
                "multitarget tracking",
                "model dimension",
                "posterior hypothesis densities",
                "trajectory parameters",
                "Gaussian process priors",
                "smooth trajectories",
                "Gaussian likelihood functions",
                "principled method",
                "multiple-evidence sources",
                "optical flow output",
                "object detector output",
                "3D modeling"
            ],
            "Author Keywords": [
                "multi-object tracking",
                "3D scene modeling",
                "Bayesian inference",
                "MCMCDA"
            ]
        },
        "id": 420,
        "cited_by": [
            {
                "year": "2015",
                "id": 487
            }
        ]
    },
    {
        "title": "GOSUS: Grassmannian Online Subspace Updates with Structured-Sparsity",
        "authors": [
            "Jia Xu",
            "Vamsi K. Ithapu",
            "Lopamudra Mukherjee",
            "James M. Rehg",
            "Vikas Singh"
        ],
        "abstract": "We study the problem of online subspace learning in the context of sequential observations involving structured perturbations. In online subspace learning, the observations are an unknown mixture of two components presented to the model sequentially - the main effect which pertains to the subspace and a residual/error term. If no additional requirement is imposed on the residual, it often corresponds to noise terms in the signal which were unaccounted for by the main effect. To remedy this, one may impose \"structural\" contiguity, which has the intended effect of leveraging the secondary terms as a covariate that helps the estimation of the subspace itself, instead of merely serving as a noise residual. We show that the corresponding online estimation procedure can be written as an approximate optimization process on a Grassmannian. We propose an efficient numerical solution, GOSUS, Grassmannian Online Subspace Updates with Structured-sparsity, for this problem. GOSUS is expressive enough in modeling both homogeneous perturbations of the subspace and structural contiguities of outliers, and after certain manipulations, solvable via an alternating direction method of multipliers (ADMM). We evaluate the empirical performance of this algorithm on two problems of interest: online background subtraction and online multiple face tracking, and demonstrate that it achieves competitive performance with the state-of-the-art in near real time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751531",
        "reference_list": [
            {
                "year": "2007",
                "id": 16
            },
            {
                "year": "2009",
                "id": 8
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 18,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Estimation",
                "Vectors",
                "Optimization",
                "Principal component analysis",
                "Robustness",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "GOSUS",
                "Grassmannian online subspace updates",
                "online subspace learning",
                "sequential observations",
                "online estimation procedure",
                "approximate optimization process",
                "alternating direction method of multipliers",
                "ADMM",
                "online background subtraction",
                "online multiple face tracking"
            ],
            "Author Keywords": [
                "Online subspace learning",
                "Structured sparsity",
                "Manifold optimization",
                "online background subtraction",
                "online multiple face tracking"
            ]
        },
        "id": 421,
        "cited_by": [
            {
                "year": "2017",
                "id": 538
            }
        ]
    },
    {
        "title": "Fast Sparsity-Based Orthogonal Dictionary Learning for Image Restoration",
        "authors": [
            "Chenglong Bao",
            "Jian-Feng Cai",
            "Hui Ji"
        ],
        "abstract": "In recent years, how to learn a dictionary from input images for sparse modelling has been one very active topic in image processing and recognition. Most existing dictionary learning methods consider an over-complete dictionary, e.g. the K-SVD method. Often they require solving some minimization problem that is very challenging in terms of computational feasibility and efficiency. However, if the correlations among dictionary atoms are not well constrained, the redundancy of the dictionary does not necessarily improve the performance of sparse coding. This paper proposed a fast orthogonal dictionary learning method for sparse image representation. With comparable performance on several image restoration tasks, the proposed method is much more computationally efficient than the over-complete dictionary based learning methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751532",
        "reference_list": [
            {
                "year": "2009",
                "id": 292
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 7,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Encoding",
                "Image restoration",
                "Minimization",
                "Computational modeling",
                "Sparse matrices",
                "Approximation algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "dictionaries",
                "image representation",
                "image restoration",
                "minimisation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast sparsity based orthogonal dictionary learning",
                "image restoration",
                "sparse modelling",
                "image processing",
                "image recognition",
                "dictionary learning methods",
                "K-SVD method",
                "minimization problem",
                "dictionary atoms",
                "sparse coding",
                "image representation"
            ],
            "Author Keywords": [
                "sparse representation",
                "image restoration",
                "dictionary learning"
            ]
        },
        "id": 422,
        "cited_by": [
            {
                "year": "2015",
                "id": 8
            }
        ]
    },
    {
        "title": "Data-Driven 3D Primitives for Single Image Understanding",
        "authors": [
            "David F. Fouhey",
            "Abhinav Gupta",
            "Martial Hebert"
        ],
        "abstract": "What primitives should we use to infer the rich 3D world behind an image? We argue that these primitives should be both visually discriminative and geometrically informative and we present a technique for discovering such primitives. We demonstrate the utility of our primitives by using them to infer 3D surface normals given a single image. Our technique substantially outperforms the state-of-the-art and shows improved cross-dataset performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751533",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2009",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 17,
            "total": 57
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Detectors",
                "Training",
                "Geometry",
                "Visualization",
                "Context",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "data-driven 3D primitives",
                "single image understanding",
                "3D surface",
                "cross-dataset performance",
                "scene recognition"
            ]
        },
        "id": 423,
        "cited_by": [
            {
                "year": "2017",
                "id": 139
            },
            {
                "year": "2015",
                "id": 87
            },
            {
                "year": "2015",
                "id": 117
            },
            {
                "year": "2015",
                "id": 147
            },
            {
                "year": "2015",
                "id": 245
            },
            {
                "year": "2015",
                "id": 295
            },
            {
                "year": "2015",
                "id": 311
            },
            {
                "year": "2013",
                "id": 217
            }
        ]
    },
    {
        "title": "Learning Discriminative Part Detectors for Image Classification and Cosegmentation",
        "authors": [
            "Jian Sun",
            "Jean Ponce"
        ],
        "abstract": "In this paper, we address the problem of learning discriminative part detectors from image sets with category labels. We propose a novel latent SVM model regularized by group sparsity to learn these part detectors. Starting from a large set of initial parts, the group sparsity regularizer forces the model to jointly select and optimize a set of discriminative part detectors in a max-margin framework. We propose a stochastic version of a proximal algorithm to solve the corresponding optimization problem. We apply the proposed method to image classification and co segmentation, and quantitative experiments with standard benchmarks show that it matches or improves upon the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751534",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 337
            },
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2011",
                "id": 21
            },
            {
                "year": "2007",
                "id": 33
            },
            {
                "year": "2011",
                "id": 316
            },
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2011",
                "id": 39
            },
            {
                "year": "2011",
                "id": 168
            }
        ],
        "citation": {
            "ieee": 54,
            "other": 17,
            "total": 71
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Training",
                "Support vector machines",
                "Image segmentation",
                "Cost function",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image segmentation",
                "learning (artificial intelligence)",
                "object detection",
                "optimisation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative part detector learning",
                "image classification",
                "image cosegmentation",
                "category labels",
                "latent SVM model",
                "group sparsity regularizer",
                "max-margin framework",
                "proximal algorithm",
                "optimization problem"
            ]
        },
        "id": 424,
        "cited_by": [
            {
                "year": "2017",
                "id": 60
            },
            {
                "year": "2017",
                "id": 69
            },
            {
                "year": "2015",
                "id": 110
            },
            {
                "year": "2015",
                "id": 158
            },
            {
                "year": "2015",
                "id": 215
            }
        ]
    },
    {
        "title": "A Deformable Mixture Parsing Model with Parselets",
        "authors": [
            "Jian Dong",
            "Qiang Chen",
            "Wei Xia",
            "Zhongyang Huang",
            "Shuicheng Yan"
        ],
        "abstract": "In this work, we address the problem of human parsing, namely partitioning the human body into semantic regions, by using the novel Parselet representation. Previous works often consider solving the problem of human pose estimation as the prerequisite of human parsing. We argue that these approaches cannot obtain optimal pixel level parsing due to the inconsistent targets between these tasks. In this paper, we propose to use Parselets as the building blocks of our parsing model. Parselets are a group of parsable segments which can generally be obtained by low-level over-segmentation algorithms and bear strong semantic meaning. We then build a Deformable Mixture Parsing Model (DMPM) for human parsing to simultaneously handle the deformation and multi-modalities of Parselets. The proposed model has two unique characteristics: (1) the possible numerous modalities of Parse let ensembles are exhibited as the ``And-Or\" structure of sub-trees, (2) to further solve the practical problem of Parselet occlusion or absence, we directly model the visibility property at some leaf nodes. The DMPM thus directly solves the problem of human parsing by searching for the best graph configuration from a pool of Parse let hypotheses without intermediate tasks. Comprehensive evaluations demonstrate the encouraging performance of the proposed approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751535",
        "reference_list": [
            {
                "year": "2011",
                "id": 91
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 13,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Semantics",
                "Deformable models",
                "Estimation",
                "Feature extraction",
                "Labeling",
                "Hair"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image representation",
                "image segmentation",
                "pose estimation",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable mixture parsing model",
                "human parsing",
                "human body partitioning",
                "semantic regions",
                "parselet representation",
                "human pose estimation",
                "optimal pixel level parsing",
                "low-level over-segmentation algorithm",
                "DMPM",
                "parselet multimodality",
                "parselet ensembles",
                "and-or subtree structure",
                "parselet occlusion",
                "parselet absence",
                "leaf nodes",
                "graph configuration searching",
                "parselet hypothesis"
            ]
        },
        "id": 425,
        "cited_by": [
            {
                "year": "2015",
                "id": 154
            },
            {
                "year": "2015",
                "id": 373
            }
        ]
    },
    {
        "title": "Joint Inverted Indexing",
        "authors": [
            "Yan Xia",
            "Kaiming He",
            "Fang Wen",
            "Jian Sun"
        ],
        "abstract": "Inverted indexing is a popular non-exhaustive solution to large scale search. An inverted file is built by a quantizer such as k-means or a tree structure. It has been found that multiple inverted files, obtained by multiple independent random quantizers, are able to achieve practically good recall and speed. Instead of computing the multiple quantizers independently, we present a method that creates them jointly. Our method jointly optimizes all code words in all quantizers. Then it assigns these code words to the quantizers. In experiments this method shows significant improvement over various existing methods that use multiple independent quantizers. On the one-billion set of SIFT vectors, our method is faster and more accurate than a recent state-of-the-art inverted indexing method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751536",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 11,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Joints",
                "Indexing",
                "Vectors",
                "Quantization (signal)",
                "Optimization",
                "Lattices",
                "Search engines"
            ],
            "INSPEC: Controlled Indexing": [
                "data structures",
                "database indexing",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple hash tables",
                "SIFT vectors",
                "codewords",
                "multiple independent random quantizers",
                "tree structure",
                "k-means",
                "inverted file",
                "large scale search",
                "joint inverted indexing"
            ]
        },
        "id": 426,
        "cited_by": [
            {
                "year": "2015",
                "id": 216
            }
        ]
    },
    {
        "title": "Improving Graph Matching via Density Maximization",
        "authors": [
            "Chao Wang",
            "Lei Wang",
            "Lingqiao Liu"
        ],
        "abstract": "Graph matching has been widely used in various applications in computer vision due to its powerful performance. However, it poses three challenges to image sparse feature matching: (1) The combinatorial nature limits the size of the possible matches, (2) It is sensitive to outliers because the objective function prefers more matches, (3) It works poorly when handling many-to-many object correspondences, due to its assumption of one single cluster for each graph. In this paper, we address these problems with a unified framework-Density Maximization. We propose a graph density local estimator (DLE) to measure the quality of matches. Density Maximization aims to maximize the DLE values both locally and globally. The local maximization of DLE finds the clusters of nodes as well as eliminates the outliers. The global maximization of DLE efficiently refines the matches by exploring a much larger matching space. Our Density Maximization is orthogonal to specific graph matching algorithms. Experimental evaluation demonstrates that it significantly boosts the true matches and enables graph matching to handle both outliers and many-to-many object correspondences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751537",
        "reference_list": [
            {
                "year": "2009",
                "id": 164
            },
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2007",
                "id": 137
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 2,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Linear programming",
                "Clustering algorithms",
                "Clutter",
                "Density measurement",
                "Educational institutions",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "graph theory",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph matching",
                "density maximization",
                "computer vision",
                "image sparse feature matching",
                "combinatorial nature",
                "outliers",
                "graph density local estimator",
                "DLE",
                "many-to-many object correspondences"
            ]
        },
        "id": 427,
        "cited_by": []
    },
    {
        "title": "Attribute Adaptation for Personalized Image Search",
        "authors": [
            "Adriana Kovashka",
            "Kristen Grauman"
        ],
        "abstract": "Current methods learn monolithic attribute predictors, with the assumption that a single model is sufficient to reflect human understanding of a visual attribute. However, in reality, humans vary in how they perceive the association between a named property and image content. For example, two people may have slightly different internal models for what makes a shoe look \"formal\", or they may disagree on which of two scenes looks \"more cluttered\". Rather than discount these differences as noise, we propose to learn user-specific attribute models. We adapt a generic model trained with annotations from multiple users, tailoring it to satisfy user-specific labels. Furthermore, we propose novel techniques to infer user-specific labels based on transitivity and contradictions in the user's search history. We demonstrate that adapted attributes improve accuracy over both existing monolithic models as well as models that learn from scratch with user-specific data alone. In addition, we show how adapted attributes are useful to personalize image search, whether with binary or relative attributes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751538",
        "reference_list": [
            {
                "year": "2011",
                "id": 286
            },
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2013",
                "id": 37
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2009",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 15,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Training",
                "Visualization",
                "Support vector machines",
                "Data models",
                "Footwear",
                "History"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "personalized image search",
                "monolithic attribute predictors",
                "user-specific attribute model",
                "binary attributes",
                "generic model",
                "user-specific labels",
                "image content"
            ],
            "Author Keywords": [
                "attributes",
                "personalization",
                "domain adaptation",
                "image retrieval"
            ]
        },
        "id": 428,
        "cited_by": [
            {
                "year": "2017",
                "id": 141
            },
            {
                "year": "2017",
                "id": 374
            },
            {
                "year": "2015",
                "id": 119
            },
            {
                "year": "2015",
                "id": 162
            },
            {
                "year": "2015",
                "id": 269
            },
            {
                "year": "2013",
                "id": 37
            }
        ]
    },
    {
        "title": "Feature Weighting via Optimal Thresholding for Video Analysis",
        "authors": [
            "Zhongwen Xu",
            "Yi Yang",
            "Ivor Tsang",
            "Nicu Sebe",
            "Alexander G. Hauptmann"
        ],
        "abstract": "Fusion of multiple features can boost the performance of large-scale visual classification and detection tasks like TRECVID Multimedia Event Detection (MED) competition [1]. In this paper, we propose a novel feature fusion approach, namely Feature Weighting via Optimal Thresholding (FWOT) to effectively fuse various features. FWOT learns the weights, thresholding and smoothing parameters in a joint framework to combine the decision values obtained from all the individual features and the early fusion. To the best of our knowledge, this is the first work to consider the weight and threshold factors of fusion problem simultaneously. Compared to state-of-the-art fusion algorithms, our approach achieves promising improvements on HMDB [8] action recognition dataset and CCV [5] video classification dataset. In addition, experiments on two TRECVID MED 2011 collections show that our approach outperforms the state-of-the-art fusion methods for complex event detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751539",
        "reference_list": [
            {
                "year": "2011",
                "id": 325
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 16,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Mel frequency cepstral coefficient",
                "Kernel",
                "Event detection",
                "Trajectory",
                "Vectors",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image fusion",
                "object detection",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video analysis",
                "multiple feature fusion approach",
                "large-scale visual classification",
                "large-scale visual detection tasks",
                "TRECVID multimedia event detection competition",
                "MED",
                "feature weighting via optimal thresholding",
                "FWOT",
                "image smoothing parameters",
                "threshold factors",
                "weight factors",
                "HMDB action recognition dataset",
                "CCV video classification dataset",
                "complex event detection"
            ]
        },
        "id": 429,
        "cited_by": [
            {
                "year": "2017",
                "id": 76
            },
            {
                "year": "2013",
                "id": 262
            }
        ]
    },
    {
        "title": "Volumetric Semantic Segmentation Using Pyramid Context Features",
        "authors": [
            "Jonathan T. Barron",
            "Mark D. Biggin",
            "Pablo Arbel\u00e1ez",
            "David W. Knowles",
            "Soile V.E. Keranen",
            "Jitendra Malik"
        ],
        "abstract": "We present an algorithm for the per-voxel semantic segmentation of a three-dimensional volume. At the core of our algorithm is a novel \"pyramid context\" feature, a descriptive representation designed such that exact per-voxel linear classification can be made extremely efficient. This feature not only allows for efficient semantic segmentation but enables other aspects of our algorithm, such as novel learned features and a stacked architecture that can reason about self-consistency. We demonstrate our technique on 3D fluorescence microscopy data of Drosophila embryos for which we are able to produce extremely accurate semantic segmentations in a matter of minutes, and for which other algorithms fail due to the size and high-dimensionality of the data, or due to the difficulty of the task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751540",
        "reference_list": [
            {
                "year": "2011",
                "id": 22
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 1,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Context",
                "Semantics",
                "Feature extraction",
                "Vectors",
                "Embryo",
                "Algorithm design and analysis",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "biology computing",
                "fluorescence",
                "image segmentation",
                "optical microscopy"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "volumetric semantic segmentation algorithm",
                "pyramid context features",
                "per-voxel semantic segmentation",
                "three-dimensional volume",
                "stacked architecture",
                "3D fluorescence microscopy data",
                "Drosophila embryos",
                "novel learned features"
            ]
        },
        "id": 430,
        "cited_by": [
            {
                "year": "2015",
                "id": 42
            }
        ]
    },
    {
        "title": "Efficient Hand Pose Estimation from a Single Depth Image",
        "authors": [
            "Chi Xu",
            "Li Cheng"
        ],
        "abstract": "We tackle the practical problem of hand pose estimation from a single noisy depth image. A dedicated three-step pipeline is proposed: Initial estimation step provides an initial estimation of the hand in-plane orientation and 3D location, Candidate generation step produces a set of 3D pose candidate from the Hough voting space with the help of the rotational invariant depth features, Verification step delivers the final 3D hand pose as the solution to an optimization problem. We analyze the depth noises, and suggest tips to minimize their negative impacts on the overall performance. Our approach is able to work with Kinect-type noisy depth images, and reliably produces pose estimations of general motions efficiently (12 frames per second). Extensive experiments are conducted to qualitatively and quantitatively evaluate the performance with respect to the state-of-the-art methods that have access to additional RGB images. Our approach is shown to deliver on par or even better results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751541",
        "reference_list": [
            {
                "year": "2011",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 58,
            "other": 31,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Estimation",
                "Noise",
                "Joints",
                "Solid modeling",
                "Kinematics",
                "Vegetation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Hough transforms",
                "image colour analysis",
                "motion estimation",
                "pipeline processing",
                "pose estimation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "RGB images",
                "quantitative performance evaluation",
                "qualitative performance evaluation",
                "motion estimation",
                "Kinect-type noisy depth images",
                "optimization problem",
                "rotational invariant depth features",
                "Hough voting space",
                "3D hand pose candidate",
                "candidate generation step",
                "3D location",
                "initial hand in-plane orientation estimation",
                "three-step pipeline",
                "noisy depth image",
                "hand pose estimation"
            ],
            "Author Keywords": [
                "realtime",
                "hand pose estimation",
                "random forest"
            ]
        },
        "id": 431,
        "cited_by": [
            {
                "year": "2017",
                "id": 121
            },
            {
                "year": "2017",
                "id": 327
            },
            {
                "year": "2017",
                "id": 329
            },
            {
                "year": "2015",
                "id": 91
            },
            {
                "year": "2015",
                "id": 208
            },
            {
                "year": "2015",
                "id": 260
            },
            {
                "year": "2015",
                "id": 370
            }
        ]
    },
    {
        "title": "Synergistic Clustering of Image and Segment Descriptors for Unsupervised Scene Understanding",
        "authors": [
            "Daniel M. Steinberg",
            "Oscar Pizarro",
            "Stefan B. Williams"
        ],
        "abstract": "With the advent of cheap, high fidelity, digital imaging systems, the quantity and rate of generation of visual data can dramatically outpace a humans ability to label or annotate it. In these situations there is scope for the use of unsupervised approaches that can model these datasets and automatically summarise their content. To this end, we present a totally unsupervised, and annotation-less, model for scene understanding. This model can simultaneously cluster whole-image and segment descriptors, thereby forming an unsupervised model of scenes and objects. We show that this model outperforms other unsupervised models that can only cluster one source of information (image or segment) at once. We are able to compare unsupervised and supervised techniques using standard measures derived from confusion matrices and contingency tables. This shows that our unsupervised model is competitive with current supervised and weakly-supervised models for scene understanding on standard datasets. We also demonstrate our model operating on a dataset with more than 100,000 images collected by an autonomous underwater vehicle.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751542",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Visualization",
                "Clustering algorithms",
                "Layout",
                "Computational modeling",
                "Standards",
                "Underwater vehicles"
            ],
            "INSPEC: Controlled Indexing": [
                "autonomous underwater vehicles",
                "image representation",
                "image segmentation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "synergistic clustering",
                "image descriptor",
                "segment descriptor",
                "unsupervised scene understanding",
                "digital imaging system",
                "visual data",
                "annotation-less model",
                "image segment",
                "contingency table",
                "weakly-supervised model",
                "autonomous underwater vehicle"
            ],
            "Author Keywords": [
                "Scene understanding",
                "unsupervised learning",
                "clustering",
                "hierarchical Bayesian models",
                "topic models",
                "variational Bayes"
            ]
        },
        "id": 432,
        "cited_by": []
    },
    {
        "title": "Random Grids: Fast Approximate Nearest Neighbors and Range Searching for Image Search",
        "authors": [
            "Dror Aiger",
            "Efi Kokiopoulou",
            "Ehud Rivlin"
        ],
        "abstract": "We propose two solutions for both nearest neighbors and range search problems. For the nearest neighbors problem, we propose a c-approximate solution for the restricted version of the decision problem with bounded radius which is then reduced to the nearest neighbors by a known reduction. For range searching we propose a scheme that learns the parameters in a learning stage adopting them to the case of a set of points with low intrinsic dimension that are embedded in high dimensional space (common scenario for image point descriptors). We compare our algorithms to the best known methods for these problems, i.e. LSH, ANN and FLANN. We show analytically and experimentally that we can do better for moderate approximation factor. Our algorithms are trivial to parallelize. In the experiments conducted, running on couple of million images, our algorithms show meaningful speed-ups when compared with the above mentioned methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751543",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 0,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Data structures",
                "Indexes",
                "Approximation algorithms",
                "Training",
                "Approximation methods",
                "Search problems",
                "Artificial neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "decision theory",
                "image matching",
                "learning (artificial intelligence)",
                "random processes",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "random grids",
                "fast approximate nearest neighbors",
                "image search",
                "range search problems",
                "bounded radius",
                "decision problem",
                "c-approximate solution",
                "learning stage",
                "low intrinsic dimension",
                "high dimensional space",
                "ANN",
                "LSH",
                "FLANN",
                "approximation factor",
                "image matching"
            ],
            "Author Keywords": [
                "ANN",
                "Image Search",
                "Range Search",
                "Nearest Neighbors"
            ]
        },
        "id": 433,
        "cited_by": []
    },
    {
        "title": "Discovering Details and Scene Structure with Hierarchical Iconoid Shift",
        "authors": [
            "Tobias Weyand",
            "Bastian Leibe"
        ],
        "abstract": "Current landmark recognition engines are typically aimed at recognizing building-scale landmarks, but miss interesting details like portals, statues or windows. This is because they use a flat clustering that summarizes all photos of a building facade in one cluster. We propose Hierarchical Iconoid Shift, a novel landmark clustering algorithm capable of discovering such details. Instead of just a collection of clusters, the output of HIS is a set of dendrograms describing the detail hierarchy of a landmark. HIS is based on the novel Hierarchical Medoid Shift clustering algorithm that performs a continuous mode search over the complete scale space. HMS is completely parameter-free, has the same complexity as Medoid Shift and is easy to parallelize. We evaluate HIS on 800k images of 34 landmarks and show that it can extract an often surprising amount of detail and structure that can be applied, e.g., to provide a mobile user with more detailed information on a landmark or even to extend the landmark's Wikipedia article.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751544",
        "reference_list": [
            {
                "year": "2001",
                "id": 58
            },
            {
                "year": "2009",
                "id": 78
            },
            {
                "year": "2009",
                "id": 269
            },
            {
                "year": "2007",
                "id": 24
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2011",
                "id": 143
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Internet",
                "Bandwidth",
                "Corona",
                "Encyclopedias",
                "Electronic publishing"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "pattern clustering",
                "structural engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical medoid shift clustering algorithm",
                "dendrograms",
                "HIS",
                "landmark clustering algorithm",
                "landmark recognition engines",
                "scene structure",
                "hierarchical iconoid shift"
            ],
            "Author Keywords": [
                "image clustering",
                "hierarchical clustering",
                "scale space",
                "medoid shift",
                "semantic labelling"
            ]
        },
        "id": 434,
        "cited_by": []
    },
    {
        "title": "Strong Appearance and Expressive Spatial Models for Human Pose Estimation",
        "authors": [
            "Leonid Pishchulin",
            "Mykhaylo Andriluka",
            "Peter Gehler",
            "Bernt Schiele"
        ],
        "abstract": "Typical approaches to articulated pose estimation combine spatial modelling of the human body with appearance modelling of body parts. This paper aims to push the state-of-the-art in articulated pose estimation in two ways. First we explore various types of appearance representations aiming to substantially improve the body part hypotheses. And second, we draw on and combine several recently proposed powerful ideas such as more flexible spatial models as well as image-conditioned spatial models. In a series of experiments we draw several important conclusions: (1) we show that the proposed appearance representations are complementary, (2) we demonstrate that even a basic tree-structure spatial human body model achieves state-of-the-art performance when augmented with the proper appearance representation, and (3) we show that the combination of the best performing appearance model with a flexible image-conditioned spatial model achieves the best result, significantly improving over the state of the art, on the ``Leeds Sports Poses'' and ``Parse'' benchmarks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751545",
        "reference_list": [],
        "citation": {
            "ieee": 65,
            "other": 29,
            "total": 94
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Torso",
                "Joints",
                "Head",
                "Estimation",
                "Training",
                "Biological system modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "strong appearance spatial models",
                "expressive spatial models",
                "human pose estimation",
                "articulated pose estimation",
                "spatial modelling",
                "appearance modelling",
                "appearance representations",
                "body part hypotheses",
                "flexible spatial models",
                "image-conditioned spatial models",
                "tree-structure spatial human body model",
                "flexible image-conditioned spatial model",
                "leeds sports poses",
                "parse"
            ],
            "Author Keywords": [
                "human pose estimation"
            ]
        },
        "id": 435,
        "cited_by": [
            {
                "year": "2017",
                "id": 127
            },
            {
                "year": "2017",
                "id": 134
            },
            {
                "year": "2017",
                "id": 246
            },
            {
                "year": "2017",
                "id": 274
            },
            {
                "year": "2017",
                "id": 319
            },
            {
                "year": "2017",
                "id": 363
            },
            {
                "year": "2017",
                "id": 365
            },
            {
                "year": "2017",
                "id": 458
            },
            {
                "year": "2015",
                "id": 220
            },
            {
                "year": "2015",
                "id": 264
            },
            {
                "year": "2015",
                "id": 374
            }
        ]
    },
    {
        "title": "3D Sub-query Expansion for Improving Sketch-Based Multi-view Image Retrieval",
        "authors": [
            "Yen-Liang Lin",
            "Cheng-Yu Huang",
            "Hao-Jeng Wang",
            "Winston Hsu"
        ],
        "abstract": "We propose a 3D sub-query expansion approach for boosting sketch-based multi-view image retrieval. The core idea of our method is to automatically convert two (guided) 2D sketches into an approximated 3D sketch model, and then generate multi-view sketches as expanded sub-queries to improve the retrieval performance. To learn the weights among synthesized views (sub-queries), we present a new multi-query feature to model the similarity between sub-queries and dataset images, and formulate it into a convex optimization problem. Our approach shows superior performance compared with the state-of-the-art approach on a public multi-view image dataset. Moreover, we also conduct sensitivity tests to analyze the parameters of our approach based on the gathered user sketches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751546",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2007",
                "id": 146
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Image reconstruction",
                "Visualization",
                "Image retrieval",
                "Image edge detection",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "public multiview image dataset",
                "sensitivity tests",
                "subqueries",
                "dataset images",
                "convex optimization problem",
                "retrieval performance",
                "3D sketch model",
                "2D sketches",
                "3D subquery expansion approach",
                "sketch-based multiview image retrieval"
            ]
        },
        "id": 436,
        "cited_by": [
            {
                "year": "2017",
                "id": 582
            }
        ]
    },
    {
        "title": "Predicting Primary Gaze Behavior Using Social Saliency Fields",
        "authors": [
            "Hyun Soo Park",
            "Eakta Jain",
            "Yaser Sheikh"
        ],
        "abstract": "We present a method to predict primary gaze behavior in a social scene. Inspired by the study of electric fields, we posit \"social charges\"-latent quantities that drive the primary gaze behavior of members of a social group. These charges induce a gradient field that defines the relationship between the social charges and the primary gaze direction of members in the scene. This field model is used to predict primary gaze behavior at any location or time in the scene. We present an algorithm to estimate the time-varying behavior of these charges from the primary gaze behavior of measured observers in the scene. We validate the model by evaluating its predictive precision via cross-validation in a variety of social scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751547",
        "reference_list": [
            {
                "year": "2009",
                "id": 33
            },
            {
                "year": "2009",
                "id": 204
            },
            {
                "year": "2009",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 5,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Equations",
                "Three-dimensional displays",
                "Force",
                "Predictive models",
                "Computational modeling",
                "Feature extraction",
                "Hidden Markov models"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "social sciences computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "primary gaze behavior prediction",
                "social saliency fields",
                "gradient field",
                "time-varying behavior",
                "motion estimation"
            ],
            "Author Keywords": [
                "Social scene understanding",
                "Gaze prediction"
            ]
        },
        "id": 437,
        "cited_by": [
            {
                "year": "2017",
                "id": 150
            },
            {
                "year": "2015",
                "id": 372
            }
        ]
    },
    {
        "title": "Efficient Higher-Order Clustering on the Grassmann Manifold",
        "authors": [
            "Suraj Jain",
            "Venu Madhav Govindu"
        ],
        "abstract": "The higher-order clustering problem arises when data is drawn from multiple subspaces or when observations fit a higher-order parametric model. Most solutions to this problem either decompose higher-order similarity measures for use in spectral clustering or explicitly use low-rank matrix representations. In this paper we present our approach of Sparse Grassmann Clustering (SGC) that combines attributes of both categories. While we decompose the higher order similarity tensor, we cluster data by directly finding a low dimensional representation without explicitly building a similarity matrix. By exploiting recent advances in online estimation on the Grassmann manifold (GROUSE) we develop an efficient and accurate algorithm that works with individual columns of similarities or partial observations thereof. Since it avoids the storage and decomposition of large similarity matrices, our method is efficient, scalable and has low memory requirements even for large-scale data. We demonstrate the performance of our SGC method on a variety of segmentation problems including planar segmentation of Kinect depth maps and motion segmentation of the Hopkins 155 dataset for which we achieve performance comparable to the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751548",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 12,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Clustering algorithms",
                "Tensile stress",
                "Vectors",
                "Estimation",
                "Indexes",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "image segmentation",
                "pattern clustering",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "higher-order clustering problem",
                "higher-order parametric model",
                "sparse Grassmann clustering",
                "higher-order similarity tensor decomposition",
                "data low dimensional representation",
                "online estimation",
                "Grassmann manifold",
                "large-scale data",
                "SGC method",
                "segmentation problems",
                "planar segmentation",
                "Kinect depth maps",
                "motion segmentation",
                "Hopkins 155 dataset",
                "computer vision"
            ],
            "Author Keywords": [
                "higher-order grouping",
                "tensor decomposition",
                "subspace estimation",
                "grassmann manifold",
                "motion segmentation"
            ]
        },
        "id": 438,
        "cited_by": [
            {
                "year": "2015",
                "id": 323
            }
        ]
    },
    {
        "title": "Paper Doll Parsing: Retrieving Similar Styles to Parse Clothing Items",
        "authors": [
            "Kota Yamaguchi",
            "M. Hadi Kiapour",
            "Tamara L. Berg"
        ],
        "abstract": "Clothing recognition is an extremely challenging problem due to wide variation in clothing item appearance, layering, and style. In this paper, we tackle the clothing parsing problem using a retrieval based approach. For a query image, we find similar styles from a large database of tagged fashion images and use these examples to parse the query. Our approach combines parsing from: pre-trained global clothing models, local clothing models learned on the fly from retrieved examples, and transferred parse masks (paper doll item transfer) from retrieved examples. Experimental evaluation shows that our approach significantly outperforms state of the art in parsing accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751549",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2011",
                "id": 137
            },
            {
                "year": "2011",
                "id": 194
            }
        ],
        "citation": {
            "ieee": 64,
            "other": 46,
            "total": 110
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Training",
                "Computational modeling",
                "Smoothing methods",
                "Footwear",
                "Skin"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "paper doll parsing",
                "clothing recognition",
                "retrieval based approach",
                "fashion images",
                "pre-trained global clothing models",
                "local clothing models",
                "transferred parse masks"
            ],
            "Author Keywords": [
                "clothing parsing",
                "clothing recognition",
                "segmentation"
            ]
        },
        "id": 439,
        "cited_by": [
            {
                "year": "2017",
                "id": 40
            },
            {
                "year": "2017",
                "id": 441
            },
            {
                "year": "2015",
                "id": 154
            },
            {
                "year": "2015",
                "id": 373
            }
        ]
    },
    {
        "title": "A Unified Video Segmentation Benchmark: Annotation, Metrics and Analysis",
        "authors": [
            "Fabio Galasso",
            "Naveen Shankar Nagaraja",
            "Tatiana Jim\u00e9nez C\u00e1rdenas",
            "Thomas Brox",
            "Bernt Schiele"
        ],
        "abstract": "Video segmentation research is currently limited by the lack of a benchmark dataset that covers the large variety of sub problems appearing in video segmentation and that is large enough to avoid over fitting. Consequently, there is little analysis of video segmentation which generalizes across subtasks, and it is not yet clear which and how video segmentation should leverage the information from the still-frames, as previously studied in image segmentation, alongside video specific information, such as temporal volume, motion and occlusion. In this work we provide such an analysis based on annotations of a large video dataset, where each video is manually segmented by multiple persons. Moreover, we introduce a new volume-based metric that includes the important aspect of temporal consistency, that can deal with segmentation hierarchies, and that reflects the tradeoff between over-segmentation and segmentation accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751550",
        "reference_list": [
            {
                "year": "2009",
                "id": 106
            },
            {
                "year": "2011",
                "id": 220
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2007",
                "id": 3
            }
        ],
        "citation": {
            "ieee": 48,
            "other": 27,
            "total": 75
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Image segmentation",
                "Motion segmentation",
                "Business process re-engineering",
                "Benchmark testing",
                "Clustering algorithms",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unified video segmentation benchmark",
                "video segmentation research",
                "video segmentation analysis",
                "video specific information",
                "temporal volume",
                "occlusion",
                "annotations",
                "video dataset annotations",
                "volume-based metric",
                "segmentation hierarchies"
            ],
            "Author Keywords": [
                "Video segmentation",
                "Benchmark",
                "Video analysis",
                "Dataset",
                "Metrics",
                "Motion segmentation",
                "Non-rigid motion",
                "Camera motion",
                "Hierarchical segmentation",
                "Image segmentation"
            ]
        },
        "id": 440,
        "cited_by": []
    },
    {
        "title": "What is the Most EfficientWay to Select Nearest Neighbor Candidates for Fast Approximate Nearest Neighbor Search?",
        "authors": [
            "Masakazu Iwamura",
            "Tomokazu Sato",
            "Koichi Kise"
        ],
        "abstract": "Approximate nearest neighbor search (ANNS) is a basic and important technique used in many tasks such as object recognition. It involves two processes: selecting nearest neighbor candidates and performing a brute-force search of these candidates. Only the former though has scope for improvement. In most existing methods, it approximates the space by quantization. It then calculates all the distances between the query and all the quantized values (e.g., clusters or bit sequences), and selects a fixed number of candidates close to the query. The performance of the method is evaluated based on accuracy as a function of the number of candidates. This evaluation seems rational but poses a serious problem; it ignores the computational cost of the process of selection. In this paper, we propose a new ANNS method that takes into account costs in the selection process. Whereas existing methods employ computationally expensive techniques such as comparative sort and heap, the proposed method does not. This realizes a significantly more efficient search. We have succeeded in reducing computation times by one-third compared with the state-of-theart on an experiment using 100 million SIFT features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751551",
        "reference_list": [
            {
                "year": "2009",
                "id": 303
            },
            {
                "year": "2011",
                "id": 206
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 6,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial neural networks",
                "Quantization (signal)",
                "Clustering algorithms",
                "Upper bound",
                "Accuracy",
                "Approximation methods",
                "Indexes"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern recognition",
                "search problems",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nearest neighbor candidates",
                "fast approximate nearest neighbor search",
                "ANNS",
                "object recognition",
                "brute-force search",
                "quantized values",
                "selection process",
                "SIFT features"
            ]
        },
        "id": 441,
        "cited_by": [
            {
                "year": "2015",
                "id": 216
            }
        ]
    },
    {
        "title": "Distributed Low-Rank Subspace Segmentation",
        "authors": [
            "Ameet Talwalkar",
            "Lester Mackey",
            "Yadong Mu",
            "Shih-Fu Chang",
            "Michael I. Jordan"
        ],
        "abstract": "Vision problems ranging from image clustering to motion segmentation to semi-supervised learning can naturally be framed as subspace segmentation problems, in which one aims to recover multiple low-dimensional subspaces from noisy and corrupted input data. Low-Rank Representation (LRR), a convex formulation of the subspace segmentation problem, is provably and empirically accurate on small problems but does not scale to the massive sizes of modern vision datasets. Moreover, past work aimed at scaling up low-rank matrix factorization is not applicable to LRR given its non-decomposable constraints. In this work, we propose a novel divide-and-conquer algorithm for large-scale subspace segmentation that can cope with LRR's non-decomposable constraints and maintains LRR's strong recovery guarantees. This has immediate implications for the scalability of subspace segmentation, which we demonstrate on a benchmark face recognition dataset and in simulations. We then introduce novel applications of LRR-based subspace segmentation to large-scale semi-supervised learning for multimedia event detection, concept detection, and image tagging. In each case, we obtain state-of-the-art results and order-of-magnitude speed ups.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751552",
        "reference_list": [
            {
                "year": "2011",
                "id": 310
            },
            {
                "year": "2011",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 5,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Accuracy",
                "Image segmentation",
                "Face",
                "Timing",
                "Semisupervised learning",
                "Matrix decomposition",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "divide and conquer methods",
                "face recognition",
                "image motion analysis",
                "image segmentation",
                "learning (artificial intelligence)",
                "matrix decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "order-of-magnitude speed ups",
                "image tagging",
                "concept detection",
                "multimedia event detection",
                "LRR-based subspace segmentation",
                "benchmark face recognition dataset",
                "LRR nondecomposable constraints",
                "large-scale subspace segmentation",
                "divide-and-conquer algorithm",
                "low-rank matrix factorization",
                "vision datasets",
                "convex formulation",
                "low-rank representation",
                "noisy input data",
                "corrupted input data",
                "multiple low-dimensional subspaces",
                "subspace segmentation problems",
                "semisupervised learning",
                "motion segmentation",
                "image clustering",
                "vision problems",
                "distributed low-rank subspace segmentation"
            ],
            "Author Keywords": [
                "Low-rank methods",
                "Subspace Segmentation",
                "Divide-and-conquer",
                "Distributed",
                "Scalable"
            ]
        },
        "id": 442,
        "cited_by": [
            {
                "year": "2015",
                "id": 497
            }
        ]
    },
    {
        "title": "Action Recognition with Improved Trajectories",
        "authors": [
            "Heng Wang",
            "Cordelia Schmid"
        ],
        "abstract": "Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751553",
        "reference_list": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2011",
                "id": 179
            },
            {
                "year": "2009",
                "id": 62
            }
        ],
        "citation": {
            "ieee": 668,
            "other": 411,
            "total": 1079
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Cameras",
                "Optical imaging",
                "Adaptive optics",
                "Vectors",
                "Detectors",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image matching",
                "image representation",
                "image sequences",
                "motion estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "MBH",
                "HOF",
                "motion-based descriptors",
                "human detector",
                "human motion",
                "RANSAC",
                "dense optical flow",
                "SURF descriptors",
                "feature point matching",
                "camera motion estimation",
                "action recognition",
                "video representation",
                "dense trajectory"
            ]
        },
        "id": 443,
        "cited_by": [
            {
                "year": "2017",
                "id": 10
            },
            {
                "year": "2017",
                "id": 29
            },
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 74
            },
            {
                "year": "2017",
                "id": 76
            },
            {
                "year": "2017",
                "id": 139
            },
            {
                "year": "2017",
                "id": 175
            },
            {
                "year": "2017",
                "id": 216
            },
            {
                "year": "2017",
                "id": 224
            },
            {
                "year": "2017",
                "id": 225
            },
            {
                "year": "2017",
                "id": 226
            },
            {
                "year": "2017",
                "id": 304
            },
            {
                "year": "2017",
                "id": 306
            },
            {
                "year": "2017",
                "id": 307
            },
            {
                "year": "2017",
                "id": 308
            },
            {
                "year": "2017",
                "id": 371
            },
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2017",
                "id": 396
            },
            {
                "year": "2017",
                "id": 466
            },
            {
                "year": "2017",
                "id": 552
            },
            {
                "year": "2017",
                "id": 580
            },
            {
                "year": "2017",
                "id": 590
            },
            {
                "year": "2017",
                "id": 600
            },
            {
                "year": "2017",
                "id": 607
            },
            {
                "year": "2015",
                "id": 120
            },
            {
                "year": "2015",
                "id": 139
            },
            {
                "year": "2015",
                "id": 184
            },
            {
                "year": "2015",
                "id": 272
            },
            {
                "year": "2015",
                "id": 273
            },
            {
                "year": "2015",
                "id": 311
            },
            {
                "year": "2015",
                "id": 356
            },
            {
                "year": "2015",
                "id": 359
            },
            {
                "year": "2015",
                "id": 362
            },
            {
                "year": "2015",
                "id": 366
            },
            {
                "year": "2015",
                "id": 368
            },
            {
                "year": "2015",
                "id": 468
            },
            {
                "year": "2015",
                "id": 483
            },
            {
                "year": "2015",
                "id": 494
            },
            {
                "year": "2015",
                "id": 498
            },
            {
                "year": "2015",
                "id": 501
            },
            {
                "year": "2015",
                "id": 506
            },
            {
                "year": "2015",
                "id": 508
            },
            {
                "year": "2015",
                "id": 509
            },
            {
                "year": "2015",
                "id": 512
            },
            {
                "year": "2015",
                "id": 513
            },
            {
                "year": "2013",
                "id": 226
            }
        ]
    },
    {
        "title": "Action Recognition with Actons",
        "authors": [
            "Jun Zhu",
            "Baoyuan Wang",
            "Xiaokang Yang",
            "Wenjun Zhang",
            "Zhuowen Tu"
        ],
        "abstract": "With the improved accessibility to an exploding amount of video data and growing demands in a wide range of video analysis applications, video-based action recognition/classification becomes an increasingly important task in computer vision. In this paper, we propose a two-layer structure for action recognition to automatically exploit a mid-level ``acton'' representation. The actons are learned via a new max-margin multi-channel multiple instance learning framework. The learned actons (with no requirement for detailed manual annotations) thus observe a property of being compact, informative, discriminative, and easy to scale. This is different from the standard unsupervised (e.g. k-means) or supervised (e.g. random forests) coding strategies in action recognition. Applying the learned actons in our two-layer structure yields the state-of-the-art classification performance on Youtube and HMDB51 datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751554",
        "reference_list": [
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2011",
                "id": 316
            },
            {
                "year": "2009",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 20,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Feature extraction",
                "Vectors",
                "Training",
                "Visualization",
                "Encoding",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video analysis applications",
                "video-based action recognition",
                "video-based action classification",
                "computer vision",
                "two-layer structure",
                "midlevel acton representation",
                "max margin multichannel multiple instance learning framework",
                "supervised coding strategies",
                "unsupervised coding strategies"
            ]
        },
        "id": 444,
        "cited_by": [
            {
                "year": "2015",
                "id": 362
            }
        ]
    },
    {
        "title": "Domain Transfer Support Vector Ranking for Person Re-identification without Target Camera Label Information",
        "authors": [
            "Andy J. Ma",
            "Pong C. Yuen",
            "Jiawei Li"
        ],
        "abstract": "This paper addresses a new person re-identification problem without the label information of persons under non-overlapping target cameras. Given the matched (positive) and unmatched (negative) image pairs from source domain cameras, as well as unmatched (negative) image pairs which can be easily generated from target domain cameras, we propose a Domain Transfer Ranked Support Vector Machines (DTRSVM) method for re-identification under target domain cameras. To overcome the problems introduced due to the absence of matched (positive) image pairs in target domain, we relax the discriminative constraint to a necessary condition only relying on the positive mean in target domain. By estimating the target positive mean using source and target domain data, a new discriminative model with high confidence in target positive mean and low confidence in target negative image pairs is developed. Since the necessary condition may not truly preserve the discriminability, multi-task support vector ranking is proposed to incorporate the training data from source domain with label information. Experimental results show that the proposed DTRSVM outperforms existing methods without using label information in target cameras. And the top 30 rank accuracy can be improved by the proposed method upto 9.40% on publicly available person re-identification datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751555",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2003",
                "id": 125
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 12,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Vectors",
                "Mathematical model",
                "Equations",
                "Support vector machines",
                "Optimization",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image matching",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "domain transfer support vector ranking",
                "person re-identification problem",
                "nonoverlapping target cameras",
                "pairs",
                "unmatched image pairs",
                "domain transfer ranked support vector machines",
                "DTRSVM method",
                "discriminative constraint",
                "discriminative model",
                "discriminability",
                "multitask support vector ranking",
                "label information",
                "reidentification datasets"
            ],
            "Author Keywords": [
                "Person Re-Identification",
                "Domain Adaptation"
            ]
        },
        "id": 445,
        "cited_by": [
            {
                "year": "2017",
                "id": 417
            },
            {
                "year": "2017",
                "id": 540
            },
            {
                "year": "2015",
                "id": 417
            },
            {
                "year": "2015",
                "id": 420
            }
        ]
    },
    {
        "title": "Finding Causal Interactions in Video Sequences",
        "authors": [
            "Mustafa Ayazoglu",
            "Burak Yilmaz",
            "Mario Sznaier",
            "Octavia Camps"
        ],
        "abstract": "This paper considers the problem of detecting causal interactions in video clips. Specifically, the goal is to detect whether the actions of a given target can be explained in terms of the past actions of a collection of other agents. We propose to solve this problem by recasting it into a directed graph topology identification, where each node corresponds to the observed motion of a given target, and each link indicates the presence of a causal correlation. As shown in the paper, this leads to a block-sparsification problem that can be efficiently solved using a modified Group-Lasso type approach, capable of handling missing data and outliers (due for instance to occlusion and mis-identified correspondences). Moreover, this approach also identifies time instants where the interactions between agents change, thus providing event detection capabilities. These results are illustrated with several examples involving non-trivial interactions amongst several human subjects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751556",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Vectors",
                "Time series analysis",
                "Noise measurement",
                "Noise",
                "Equations",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "directed graphs",
                "image motion analysis",
                "image sequences",
                "object detection",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "causal interactions detection",
                "video sequences",
                "video clips",
                "agents collection",
                "directed graph topology identification",
                "graph node",
                "target motion",
                "causal correlation",
                "block-sparsification problem",
                "modified Group-Lasso type approach",
                "time instants",
                "event detection capabilities"
            ],
            "Author Keywords": [
                "Granger Causality",
                "Sparse Graph Identification",
                "Block Sparsification"
            ]
        },
        "id": 446,
        "cited_by": []
    },
    {
        "title": "A New Adaptive Segmental Matching Measure for Human Activity Recognition",
        "authors": [
            "Shahriar Shariat",
            "Vladimir Pavlovic"
        ],
        "abstract": "The problem of human activity recognition is a central problem in many real-world applications. In this paper we propose a fast and effective segmental alignment-based method that is able to classify activities and interactions in complex environments. We empirically show that such model is able to recover the alignment that leads to improved similarity measures within sequence classes and hence, raises the classification performance. We also apply a bounding technique on the histogram distances to reduce the computation of the otherwise exhaustive search.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751557",
        "reference_list": [
            {
                "year": "2009",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Computational modeling",
                "Xenon",
                "Feature extraction",
                "Adaptation models",
                "Noise",
                "Hidden Markov models"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "image classification",
                "image matching",
                "image segmentation",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive segmental matching measure",
                "human activity recognition",
                "fast segmental alignment-based method",
                "effective segmental alignment-based method",
                "activity classification",
                "interaction classification",
                "alignment recovery",
                "improved similarity measures",
                "sequence class",
                "bounding technique",
                "histogram distances",
                "exhaustive search",
                "computation reduction"
            ],
            "Author Keywords": [
                "Activity Recognition",
                "Time-series alignment",
                "Segmentation and Matching"
            ]
        },
        "id": 447,
        "cited_by": []
    },
    {
        "title": "Saliency Detection in Large Point Sets",
        "authors": [
            "Elizabeth Shtrom",
            "George Leifman",
            "Ayellet Tal"
        ],
        "abstract": "While saliency in images has been extensively studied in recent years, there is very little work on saliency of point sets. This is despite the fact that point sets and range data are becoming ever more widespread and have myriad applications. In this paper we present an algorithm for detecting the salient points in unorganized 3D point sets. Our algorithm is designed to cope with extremely large sets, which may contain tens of millions of points. Such data is typical of urban scenes, which have recently become commonly available on the web. No previous work has handled such data. For general data sets, we show that our results are competitive with those of saliency detection of surfaces, although we do not have any connectivity information. We demonstrate the utility of our algorithm in two applications: producing a set of the most informative viewpoints and suggesting an informative city tour given a city scan.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751558",
        "reference_list": [
            {
                "year": "2007",
                "id": 244
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 13,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Cities and towns",
                "Three-dimensional displays",
                "Feature extraction",
                "Buildings",
                "Noise measurement",
                "Noise",
                "Poles and towers"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "edge detection",
                "set theory",
                "town and country planning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "saliency detection",
                "unorganized 3D point sets",
                "urban scenes",
                "connectivity information",
                "informative city tour",
                "city scan"
            ],
            "Author Keywords": [
                "Point sets",
                "Saliency",
                "Visual saliency"
            ]
        },
        "id": 448,
        "cited_by": [
            {
                "year": "2015",
                "id": 18
            },
            {
                "year": "2015",
                "id": 150
            }
        ]
    },
    {
        "title": "Motion-Aware KNN Laplacian for Video Matting",
        "authors": [
            "Dingzeyu Li",
            "Qifeng Chen",
            "Chi-Keung Tang"
        ],
        "abstract": "This paper demonstrates how the nonlocal principle benefits video matting via the KNN Laplacian, which comes with a straightforward implementation using motion-aware K nearest neighbors. In hindsight, the fundamental problem to solve in video matting is to produce spatio-temporally coherent clusters of moving foreground pixels. When used as described, the motion-aware KNN Laplacian is effective in addressing this fundamental problem, as demonstrated by sparse user markups typically on only one frame in a variety of challenging examples featuring ambiguous foreground and background colors, changing topologies with disocclusion, significant illumination changes, fast motion, and motion blur. When working with existing Laplacian-based systems, we expect our Laplacian can benefit them immediately with an improved clustering of moving foreground pixels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751559",
        "reference_list": [
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2009",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 6,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Laplace equations",
                "Vectors",
                "Optical imaging",
                "Image color analysis",
                "Integrated optics",
                "Optical scattering",
                "Noise reduction"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern clustering",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion aware KNN Laplacian",
                "video matting",
                "nonlocal principle",
                "motion aware K nearest neighbor",
                "spatio temporally coherent cluster",
                "moving foreground pixels",
                "sparse user markup",
                "foreground color",
                "background color",
                "changing topologies",
                "illumination change",
                "fast motion",
                "motion blur"
            ]
        },
        "id": 449,
        "cited_by": [
            {
                "year": "2015",
                "id": 174
            }
        ]
    },
    {
        "title": "Viewing Real-World Faces in 3D",
        "authors": [
            "Tal Hassner"
        ],
        "abstract": "We present a data-driven method for estimating the 3D shapes of faces viewed in single, unconstrained photos (aka \"in-the-wild\"). Our method was designed with an emphasis on robustness and efficiency - with the explicit goal of deployment in real-world applications which reconstruct and display faces in 3D. Our key observation is that for many practical applications, warping the shape of a reference face to match the appearance of a query, is enough to produce realistic impressions of the query's 3D shape. Doing so, however, requires matching visual features between the (possibly very different) query and reference images, while ensuring that a plausible face shape is produced. To this end, we describe an optimization process which seeks to maximize the similarity of appearances and depths, jointly, to those of a reference model. We describe our system for monocular face shape reconstruction and present both qualitative and quantitative experiments, comparing our method against alternative systems, and demonstrating its capabilities. Finally, as a testament to its suitability for real-world applications, we offer an open, on-line implementation of our system, providing unique means of instant 3D viewing of faces appearing in web photos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751560",
        "reference_list": [
            {
                "year": "2011",
                "id": 221
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 25,
            "total": 68
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three-dimensional displays",
                "Optimization",
                "Estimation",
                "Image reconstruction",
                "Solid modeling",
                "Vectors"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image matching",
                "image reconstruction",
                "optimisation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-world face view",
                "data-driven method",
                "3D shape",
                "unconstrained photos",
                "visual features matching",
                "optimization process",
                "monocular face shape reconstruction",
                "Web photos"
            ],
            "Author Keywords": [
                "Single-view 3D reconstruction",
                "Monocular 3D",
                "Faces"
            ]
        },
        "id": 450,
        "cited_by": [
            {
                "year": "2017",
                "id": 98
            },
            {
                "year": "2017",
                "id": 165
            },
            {
                "year": "2015",
                "id": 418
            }
        ]
    },
    {
        "title": "Accurate and Robust 3D Facial Capture Using a Single RGBD Camera",
        "authors": [
            "Yen-Lin Chen",
            "Hsiang-Tao Wu",
            "Fuhao Shi",
            "Xin Tong",
            "Jinxiang Chai"
        ],
        "abstract": "This paper presents an automatic and robust approach that accurately captures high-quality 3D facial performances using a single RGBD camera. The key of our approach is to combine the power of automatic facial feature detection and image-based 3D nonrigid registration techniques for 3D facial reconstruction. In particular, we develop a robust and accurate image-based nonrigid registration algorithm that incrementally deforms a 3D template mesh model to best match observed depth image data and important facial features detected from single RGBD images. The whole process is fully automatic and robust because it is based on single frame facial registration framework. The system is flexible because it does not require any strong 3D facial priors such as blend shape models. We demonstrate the power of our approach by capturing a wide range of 3D facial expressions using a single RGBD camera and achieve state-of-the-art accuracy by comparing against alternative methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751561",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 14,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Deformable models",
                "Facial features",
                "Face",
                "Data models",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "face recognition",
                "feature extraction",
                "image matching",
                "image reconstruction",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust 3D facial capture",
                "single-RGBD camera",
                "automatic approach",
                "high-quality 3D facial performance",
                "automatic facial feature detection technique",
                "image-based 3D nonrigid registration technique",
                "3D facial reconstruction",
                "image-based nonrigid registration algorithm accuracy",
                "robust image-based nonrigid registration algorithm",
                "3D template mesh model",
                "depth image data matching",
                "single-frame facial registration framework",
                "3D facial expressions"
            ],
            "Author Keywords": [
                "facial capture",
                "facial feature detection",
                "3D facial modeling",
                "kinect",
                "nonrigid registration"
            ]
        },
        "id": 451,
        "cited_by": []
    },
    {
        "title": "Visual Semantic Complex Network for Web Images",
        "authors": [
            "Shi Qiu",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "This paper proposes modeling the complex web image collections with an automatically generated graph structure called visual semantic complex network (VSCN). The nodes on this complex network are clusters of images with both visual and semantic consistency, called semantic concepts. These nodes are connected based on the visual and semantic correlations. Our VSCN with 33,240 concepts is generated from a collection of 10 million web images. A great deal of valuable information on the structures of the web image collections can be revealed by exploring the VSCN, such as the small-world behavior, concept community, in-degree distribution, hubs, and isolated concepts. It not only helps us better understand the web image collections at a macroscopic level, but also has many important practical applications. This paper presents two application examples: content-based image retrieval and image browsing. Experimental results show that the VSCN leads to significant improvement on both the precision of image retrieval (over 200%) and user experience for image browsing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751562",
        "reference_list": [
            {
                "year": "2011",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Visualization",
                "Correlation",
                "Communities",
                "Complex networks",
                "Vectors",
                "Automotive electronics"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image retrieval",
                "Internet",
                "semantic Web"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual semantic complex network",
                "VSCN",
                "complex Web image collections",
                "automatic generated graph structure",
                "semantic concepts",
                "small-world behavior",
                "concept community",
                "in-degree distribution",
                "hubs",
                "isolated concepts",
                "macroscopic level",
                "content-based image retrieval",
                "image browsing"
            ]
        },
        "id": 452,
        "cited_by": []
    },
    {
        "title": "What Do You Do? Occupation Recognition in a Photo via Social Context",
        "authors": [
            "Ming Shao",
            "Liangyue Li",
            "Yun Fu"
        ],
        "abstract": "In this paper, we investigate the problem of recognizing occupations of multiple people with arbitrary poses in a photo. Previous work utilizing single person's nearly frontal clothing information and fore/background context preliminarily proves that occupation recognition is computationally feasible in computer vision. However, in practice, multiple people with arbitrary poses are common in a photo, and recognizing their occupations is even more challenging. We argue that with appropriately built visual attributes, co-occurrence, and spatial configuration model that is learned through structure SVM, we can recognize multiple people's occupations in a photo simultaneously. To evaluate our method's performance, we conduct extensive experiments on a new well-labeled occupation database with 14 representative occupations and over 7K images. Results on this database validate our method's effectiveness and show that occupation recognition is solvable in a more general case.",
        "ieee_link": "https://ieeexplore.ieee.org/document/6751563",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2009",
                "id": 29
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2005",
                "id": 168
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 137
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 2,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Clothing",
                "Visualization",
                "Zinc",
                "Vectors",
                "Databases",
                "Support vector machines",
                "Standards"
            ],
            "INSPEC: Controlled Indexing": [
                "clothing",
                "computer vision",
                "pose estimation",
                "support vector machines",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "occupation recognition",
                "social context",
                "photo",
                "frontal clothing information",
                "fore-background context",
                "computer vision",
                "spatial configuration model",
                "SVM structure"
            ],
            "Author Keywords": [
                "Occupation Recognition",
                "Social Context",
                "Visual Attributes"
            ]
        },
        "id": 453,
        "cited_by": [
            {
                "year": "2017",
                "id": 388
            }
        ]
    }
]