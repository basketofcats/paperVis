[
    {
        "title": "Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images",
        "authors": [
            "Mateusz Malinowski",
            "Marcus Rohrbach",
            "Mario Fritz"
        ],
        "abstract": "We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410366",
        "reference_list": [
            {
                "year": "2015",
                "id": 506
            },
            {
                "year": "2013",
                "id": 209
            }
        ],
        "citation": {
            "ieee": 104,
            "other": 48,
            "total": 152
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Knowledge discovery",
                "Recurrent neural networks",
                "Natural languages",
                "Computer architecture",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "natural language processing",
                "recurrent neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "neurons",
                "neural-based approach",
                "question answering task",
                "real-world images",
                "visual Turing test",
                "image representation",
                "natural language processing",
                "neural-image-QA",
                "multimodal problem",
                "language output",
                "DAQUAR dataset",
                "DAQUAR-Consensus",
                "recurrent neural network"
            ]
        },
        "id": 0,
        "cited_by": [
            {
                "year": "2017",
                "id": 61
            },
            {
                "year": "2017",
                "id": 64
            },
            {
                "year": "2017",
                "id": 133
            },
            {
                "year": "2017",
                "id": 192
            },
            {
                "year": "2017",
                "id": 207
            },
            {
                "year": "2017",
                "id": 262
            },
            {
                "year": "2017",
                "id": 311
            },
            {
                "year": "2017",
                "id": 315
            },
            {
                "year": "2017",
                "id": 360
            },
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2017",
                "id": 431
            },
            {
                "year": "2017",
                "id": 432
            },
            {
                "year": "2015",
                "id": 270
            }
        ]
    },
    {
        "title": "Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing",
        "authors": [
            "Hamid Izadinia",
            "Fereshteh Sadeghi",
            "Santosh K. Divvala",
            "Hannaneh Hajishirzi",
            "Yejin Choi",
            "Ali Farhadi"
        ],
        "abstract": "We introduce Segment-Phrase Table (SPT), a large collection of bijective associations between textual phrases and their corresponding segmentations. Leveraging recent progress in object recognition and natural language semantics, we show how we can successfully build a high-quality segment-phrase table using minimal human supervision. More importantly, we demonstrate the unique value unleashed by this rich bimodal resource, for both vision as well as natural language understanding. First, we show that fine-grained textual labels facilitate contextual reasoning that helps in satisfying semantic constraints across image segments. This feature enables us to achieve state-of-the-art segmentation results on benchmark datasets. Next, we show that the association of high-quality segmentations to textual phrases aids in richer semantic understanding and reasoning of these textual phrases. Leveraging this feature, we motivate the problem of visual entailment and visual paraphrasing, and demonstrate its utility on a large dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410367",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2007",
                "id": 145
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Semantics",
                "Visualization",
                "Cognition",
                "Pragmatics",
                "Buildings",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "image segmentation",
                "inference mechanisms",
                "natural language processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "segment-phrase table",
                "semantic segmentation",
                "visual entailment",
                "visual paraphrasing",
                "bijective association",
                "textual phrase",
                "object recognition",
                "natural language semantics",
                "fine-grained textual label",
                "contextual reasoning",
                "semantic constraint"
            ]
        },
        "id": 1,
        "cited_by": []
    },
    {
        "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books",
        "authors": [
            "Yukun Zhu",
            "Ryan Kiros",
            "Rich Zemel",
            "Ruslan Salakhutdinov",
            "Raquel Urtasun",
            "Antonio Torralba",
            "Sanja Fidler"
        ],
        "abstract": "Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410368",
        "reference_list": [
            {
                "year": "2013",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 27,
            "total": 58
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion pictures",
                "Visualization",
                "Videos",
                "Semantics",
                "Grounding",
                "Voltage control",
                "Roads"
            ],
            "INSPEC: Controlled Indexing": [
                "entertainment",
                "neural nets",
                "text analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "story-like visual explanations",
                "fine-grained information",
                "high-level semantics",
                "movie releases",
                "descriptive explanations",
                "visual content",
                "neural sentence embedding",
                "large corpus",
                "video-text neural embedding",
                "similarity computing",
                "movie clips",
                "context-aware CNN",
                "movie-book alignment"
            ]
        },
        "id": 2,
        "cited_by": []
    },
    {
        "title": "Learning Query and Image Similarities with Ranking Canonical Correlation Analysis",
        "authors": [
            "Ting Yao",
            "Tao Mei",
            "Chong-Wah Ngo"
        ],
        "abstract": "One of the fundamental problems in image search is to learn the ranking functions, i.e., similarity between the query and image. The research on this topic has evolved through two paradigms: feature-based vector model and image ranker learning. The former relies on the image surrounding texts, while the latter learns a ranker based on human labeled query-image pairs. Each of the paradigms has its own limitation. The vector model is sensitive to the quality of text descriptions, and the learning paradigm is difficult to be scaled up as human labeling is always too expensive to obtain. We demonstrate in this paper that the above two limitations can be well mitigated by jointly exploring subspace learning and the use of click-through data. Specifically, we propose a novel Ranking Canonical Correlation Analysis (RCCA) for learning query and image similarities. RCCA initially finds a common subspace between query and image views by maximizing their correlations, and further simultaneously learns a bilinear query-image similarity function and adjusts the subspace to preserve the preference relations implicit in the click-through data. Once the subspace is finalized, query-image similarity can be computed by the bilinear similarity function on their mappings in this subspace. On a large-scale click-based image dataset with 11.7 million queries and one million images, RCCA is shown to be powerful for image search with superior performance over several state-of-the-art methods on both keyword-based and query-by-example tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410369",
        "reference_list": [
            {
                "year": "2013",
                "id": 265
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 13,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Visualization",
                "Feature extraction",
                "Bipartite graph",
                "Learning systems",
                "Computational modeling",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "learning (artificial intelligence)",
                "matrix algebra",
                "statistical analysis",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "query learning",
                "bilinear query-image similarity function",
                "ranking canonical correlation analysis",
                "image search",
                "ranking function learning",
                "feature-based vector model",
                "image ranker learning",
                "human labeled query-image pairs",
                "text descriptions",
                "RCCA",
                "click-through data",
                "large-scale click-based image dataset",
                "keyword-based task",
                "query-by-example task"
            ]
        },
        "id": 3,
        "cited_by": []
    },
    {
        "title": "Learning to See by Moving",
        "authors": [
            "Pulkit Agrawal",
            "Jo\u00e3o Carreira",
            "Jitendra Malik"
        ],
        "abstract": "The current dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it also possible to learn features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigated if the awareness of egomotion(i.e. self motion) can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We found that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410370",
        "reference_list": [],
        "citation": {
            "ieee": 70,
            "other": 54,
            "total": 124
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Cameras",
                "Streaming media",
                "Object recognition",
                "Feature extraction",
                "Training",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image motion analysis",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature learning",
                "computer vision",
                "training neural networks",
                "object recognition",
                "hand labelled images",
                "visual tasks",
                "biology",
                "living organisms",
                "visual perception",
                "egomotion",
                "self motion",
                "supervisory signal",
                "class labels",
                "mobile agents"
            ]
        },
        "id": 4,
        "cited_by": [
            {
                "year": "2017",
                "id": 63
            },
            {
                "year": "2017",
                "id": 69
            },
            {
                "year": "2017",
                "id": 216
            },
            {
                "year": "2017",
                "id": 233
            },
            {
                "year": "2017",
                "id": 338
            },
            {
                "year": "2017",
                "id": 451
            },
            {
                "year": "2017",
                "id": 618
            }
        ]
    },
    {
        "title": "Object Detection Using Generalization and Efficiency Balanced Co-Occurrence Features",
        "authors": [
            "Haoyu Ren",
            "Ze-Nian Li"
        ],
        "abstract": "In this paper, we propose a high-accuracy object detector based on co-occurrence features. Firstly, we introduce three kinds of local co-occurrence features constructed by the traditional Haar, LBP, and HOG respectively. Then the boosted detectors are learned, where each weak classifier corresponds to a local image region with a co-occurrence feature. In addition, we propose a Generalization and Efficiency Balanced (GEB) framework for boosting training. In the feature selection procedure, the discrimination ability, the generalization power, and the computation cost of the candidate features are all evaluated for decision. As a result, the boosted detector achieves both high accuracy and good efficiency. It also shows performance competitive with the state-of-the-art methods for pedestrian detection and general object detection tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410371",
        "reference_list": [
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2013",
                "id": 187
            },
            {
                "year": "2009",
                "id": 4
            },
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2011",
                "id": 185
            },
            {
                "year": "2005",
                "id": 211
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Object detection",
                "Detectors",
                "Histograms",
                "Training",
                "Boosting",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image classification",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Haar",
                "LBP",
                "HOG",
                "boosted detectors",
                "weak classifier",
                "local image region",
                "cooccurrence feature",
                "generalization and efficiency balanced framework",
                "GEB",
                "discrimination ability",
                "pedestrian general object detection tasks"
            ]
        },
        "id": 5,
        "cited_by": []
    },
    {
        "title": "Mining And-Or Graphs for Graph Matching and Object Discovery",
        "authors": [
            "Quanshi Zhang",
            "Ying Nian Wu",
            "Song-Chun Zhu"
        ],
        "abstract": "This paper reformulates the theory of graph mining on the technical basis of graph matching, and extends its scope of applications to computer vision. Given a set of attributed relational graphs (ARGs), we propose to use a hierarchical And-Or Graph (AoG) to model the pattern of maximal-size common subgraphs embedded in the ARGs, and we develop a general method to mine the AoG model from the unlabeled ARGs. This method provides a general solution to the problem of mining hierarchical models from unannotated visual data without exhaustive search of objects. We apply our method to RGB/RGB-D images and videos to demonstrate its generality and the wide range of applicability. The code will be available at https://sites.google.com/site/quanshizhang/mining-and-or-graphs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410372",
        "reference_list": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2013",
                "id": 3
            },
            {
                "year": "2011",
                "id": 130
            },
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2011",
                "id": 165
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Data mining",
                "Videos",
                "Data models",
                "Feature extraction",
                "Computer vision",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "data mining",
                "graph theory",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "graph mining",
                "graph matching",
                "object discovery",
                "computer vision",
                "attributed relational graph",
                "ARG",
                "hierarchical and-or graph",
                "AoG"
            ]
        },
        "id": 6,
        "cited_by": [
            {
                "year": "2017",
                "id": 293
            }
        ]
    },
    {
        "title": "Pose Induction for Novel Object Categories",
        "authors": [
            "Shubham Tulsiani",
            "Jo\u00e3o Carreira",
            "Jitendra Malik"
        ],
        "abstract": "We address the task of predicting pose for objects of unannotated object categories from a small seed set of annotated object classes. We present a generalized classifier that can reliably induce pose given a single instance of a novel category. In case of availability of a large collection of novel instances, our approach then jointly reasons over all instances to improve the initial estimates. We empirically validate the various components of our algorithm and quantitatively show that our method produces reliable pose estimates. We also show qualitative results on a diverse set of classes and further demonstrate the applicability of our system for learning shape models of novel object classes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410373",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Visualization",
                "Animals",
                "Three-dimensional displays",
                "Training",
                "Azimuth"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pose induction",
                "novel object category",
                "unannotated object category",
                "generalized classifier",
                "pose estimates"
            ]
        },
        "id": 7,
        "cited_by": []
    },
    {
        "title": "Dynamic Texture Recognition via Orthogonal Tensor Dictionary Learning",
        "authors": [
            "Yuhui Quan",
            "Yan Huang",
            "Hui Ji"
        ],
        "abstract": "Dynamic textures (DTs) are video sequences with stationary properties, which exhibit repetitive patterns over space and time. This paper aims at investigating the sparse coding based approach to characterizing local DT patterns for recognition. Owing to the high dimensionality of DT sequences, existing dictionary learning algorithms are not suitable for our purpose due to their high computational costs as well as poor scalability. To overcome these obstacles, we proposed a structured tensor dictionary learning method for sparse coding, which learns a dictionary structured with orthogonality and separability. The proposed method is very fast and more scalable to high-dimensional data than the existing ones. In addition, based on the proposed dictionary learning method, a DT descriptor is developed, which has better adaptivity, discriminability and scalability than the existing approaches. These advantages are demonstrated by the experiments on multiple datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410374",
        "reference_list": [
            {
                "year": "2013",
                "id": 422
            },
            {
                "year": "2003",
                "id": 161
            },
            {
                "year": "2007",
                "id": 254
            },
            {
                "year": "2013",
                "id": 389
            },
            {
                "year": "2005",
                "id": 6
            },
            {
                "year": "2011",
                "id": 154
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 8,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Tensile stress",
                "Encoding",
                "Learning systems",
                "Scalability",
                "Feature extraction",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "image recognition",
                "image sequences",
                "image texture",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "DT descriptor",
                "separability",
                "orthogonality",
                "structured tensor dictionary learning method",
                "local DT patterns",
                "sparse coding based approach",
                "stationary properties",
                "video sequences",
                "orthogonal tensor dictionary learning algorithm",
                "dynamic texture recognition"
            ]
        },
        "id": 8,
        "cited_by": [
            {
                "year": "2017",
                "id": 323
            }
        ]
    },
    {
        "title": "Convolutional Channel Features",
        "authors": [
            "Bin Yang",
            "Junjie Yan",
            "Zhen Lei",
            "Stan Z. Li"
        ],
        "abstract": "Deep learning methods are powerful tools but often suffer from expensive computation and limited flexibility. An alternative is to combine light-weight models with deep representations. As successful cases exist in several visual problems, a unified framework is absent. In this paper, we revisit two widely used approaches in computer vision, namely filtered channel features and Convolutional Neural Networks (CNN), and absorb merits from both by proposing an integrated method called Convolutional Channel Features (CCF). CCF transfers low-level features from pre-trained CNN models to feed the boosting forest model. With the combination of CNN features and boosting forest, CCF benefits from the richer capacity in feature representation compared with channel features, as well as lower cost in computation and storage compared with end-to-end CNN methods. We show that CCF serves as a good way of tailoring pre-trained CNN models to diverse tasks without fine-tuning the whole network to each task by achieving state-of-the-art performances in pedestrian detection, face detection, edge detection and object proposal generation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410375",
        "reference_list": [
            {
                "year": "2013",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 81,
            "other": 57,
            "total": 138
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Boosting",
                "Computational modeling",
                "Image edge detection",
                "Object detection",
                "Support vector machines",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "neural nets",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pedestrian detection",
                "face detection",
                "edge detection",
                "object proposal generation",
                "boosting forest model",
                "CCF",
                "CNN",
                "convolutional neural networks",
                "filtered channel features",
                "computer vision",
                "deep learning methods",
                "convolutional channel features"
            ]
        },
        "id": 9,
        "cited_by": [
            {
                "year": "2017",
                "id": 20
            },
            {
                "year": "2017",
                "id": 51
            },
            {
                "year": "2017",
                "id": 59
            },
            {
                "year": "2017",
                "id": 334
            },
            {
                "year": "2017",
                "id": 367
            },
            {
                "year": "2017",
                "id": 512
            }
        ]
    },
    {
        "title": "Local Convolutional Features with Unsupervised Training for Image Retrieval",
        "authors": [
            "Mattis Paulin",
            "Matthijs Douze",
            "Zaid Harchaoui",
            "Julien Mairal",
            "Florent Perronin",
            "Cordelia Schmid"
        ],
        "abstract": "Patch-level descriptors underlie several important computer vision tasks, such as stereo-matching or content-based image retrieval. We introduce a deep convolutional architecture that yields patch-level descriptors, as an alternative to the popular SIFT descriptor for image retrieval. The proposed family of descriptors, called Patch-CKN, adapt the recently introduced Convolutional Kernel Network (CKN), an unsupervised framework to learn convolutional architectures. We present a comparison framework to benchmark current deep convolutional approaches along with Patch-CKN for both patch and image retrieval, including our novel \"RomePatches\" dataset. Patch-CKN descriptors yield competitive results compared to supervised CNN alternatives on patch and image retrieval.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410376",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2011",
                "id": 76
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 27,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Image retrieval",
                "Kernel",
                "Detectors",
                "Computer architecture",
                "Lighting",
                "Three-dimensional displays",
                "Pipelines"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "content-based retrieval",
                "convolution",
                "image retrieval",
                "stereo image processing",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local convolutional features",
                "unsupervised training",
                "patch-level descriptors",
                "computer vision tasks",
                "stereo-matching",
                "content-based image retrieval",
                "deep convolutional architecture",
                "Patch-CKN descriptors",
                "convolutional kernel network",
                "patch retrieval",
                "RomePatches dataset"
            ]
        },
        "id": 10,
        "cited_by": [
            {
                "year": "2017",
                "id": 80
            }
        ]
    },
    {
        "title": "RIDE: Reversal Invariant Descriptor Enhancement",
        "authors": [
            "Lingxi Xie",
            "Jingdong Wang",
            "Weiyao Lin",
            "Bo Zhang",
            "Qi Tian"
        ],
        "abstract": "In many fine-grained object recognition datasets, image orientation (left/right) might vary from sample to sample. Since handcrafted descriptors such as SIFT are not reversal invariant, the stability of image representation based on them is consequently limited. A popular solution is to augment the datasets by adding a left-right reversed copy for each original image. This strategy improves recognition accuracy to some extent, but also brings the price of almost doubled time and memory consumptions. In this paper, we present RIDE (Reversal Invariant Descriptor Enhancement) for fine-grained object recognition. RIDE is a generalized algorithm which cancels out the impact of image reversal by estimating the orientation of local descriptors, and guarantees to produce the identical representation for an image and its left-right reversed copy. Experimental results reveal the consistent accuracy gain of RIDE with various types of descriptors. We also provide insightful discussions on the working mechanism of RIDE and its generalization to other applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410377",
        "reference_list": [
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2011",
                "id": 326
            },
            {
                "year": "2011",
                "id": 76
            },
            {
                "year": "2013",
                "id": 204
            },
            {
                "year": "2013",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Object recognition",
                "Image representation",
                "Birds",
                "Histograms",
                "Indexes",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "image enhancement",
                "image representation",
                "object recognition",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "recognition accuracy",
                "memory consumption",
                "generalized algorithm",
                "image reversal",
                "local descriptor",
                "identical representation",
                "consistent accuracy gain",
                "working mechanism",
                "left-right reversed copy",
                "image representation",
                "stability",
                "SIFT",
                "handcrafted descriptor",
                "image orientation",
                "fine-grained object recognition dataset",
                "reversal invariant descriptor enhancement",
                "RIDE"
            ]
        },
        "id": 11,
        "cited_by": [
            {
                "year": "2017",
                "id": 590
            }
        ]
    },
    {
        "title": "Discrete Tabu Search for Graph Matching",
        "authors": [
            "Kamil Adamczewski",
            "Yumin Suh",
            "Kyoung Mu Lee"
        ],
        "abstract": "Graph matching is a fundamental problem in computer vision. In this paper, we propose a novel graph matching algorithm based on tabu search [13]. The proposed method solves graph matching problem by casting it into an equivalent weighted maximum clique problem of the corresponding association graph, which we further penalize through introducing negative weights. Subsequent tabu search optimization allows for overcoming the convention of using positive weights. The method's distinct feature is that it utilizes the history of search to make more strategic decisions while looking for the optimal solution, thus effectively escaping local optima and in practice achieving superior results. The proposed method, unlike the existing algorithms, enables direct optimization in the original discrete space while encouraging rather than artificially enforcing hard one-to-one constraint, thus resulting in better solution. The experiments demonstrate the robustness of the algorithm in a variety of settings, presenting the state-of-the-art results. The code is available at http://cv.snu.ac.kr/research/~DTSGM/.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410378",
        "reference_list": [
            {
                "year": "2009",
                "id": 169
            },
            {
                "year": "2013",
                "id": 3
            },
            {
                "year": "2009",
                "id": 164
            },
            {
                "year": "2005",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 8,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Search problems",
                "Computer vision",
                "Space exploration",
                "Image edge detection",
                "Monte Carlo methods",
                "Programming"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "optimisation",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discrete tabu search",
                "graph matching algorithm",
                "computer vision",
                "equivalent weighted maximum clique problem",
                "association graph",
                "subsequent tabu search optimization"
            ]
        },
        "id": 12,
        "cited_by": []
    },
    {
        "title": "Discriminative Learning of Deep Convolutional Feature Point Descriptors",
        "authors": [
            "Edgar Simo-Serra",
            "Eduard Trulls",
            "Luis Ferraz",
            "Iasonas Kokkinos",
            "Pascal Fua",
            "Francesc Moreno-Noguer"
        ],
        "abstract": "Deep learning has revolutionalized image-level tasks such as classification, but patch-level tasks, such as correspondence, still rely on hand-crafted features, e.g. SIFT. In this paper we use Convolutional Neural Networks (CNNs) to learn discriminant patch representations and in particular train a Siamese network with pairs of (non-)corresponding patches. We deal with the large number of potential pairs with the combination of a stochastic sampling of the training set and an aggressive mining strategy biased towards patches that are hard to classify. By using the L2 distance during both training and testing we develop 128-D descriptors whose euclidean distances reflect patch similarity, and which can be used as a drop-in replacement for any task involving SIFT. We demonstrate consistent performance gains over the state of the art, and generalize well against scaling and rotation, perspective transformation, non-rigid deformation, and illumination changes. Our descriptors are efficient to compute and amenable to modern GPUs, and are publicly available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410379",
        "reference_list": [
            {
                "year": "2009",
                "id": 276
            },
            {
                "year": "2011",
                "id": 326
            },
            {
                "year": "2011",
                "id": 76
            }
        ],
        "citation": {
            "ieee": 110,
            "other": 67,
            "total": 177
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Three-dimensional displays",
                "Measurement",
                "Computer architecture",
                "Computer vision",
                "Computational modeling",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "image classification",
                "image sampling",
                "learning (artificial intelligence)",
                "neural nets",
                "stochastic processes",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative learning",
                "deep convolutional feature point descriptors",
                "convolutional neural networks",
                "CNN",
                "discriminant patch representations",
                "Siamese network training",
                "stochastic sampling",
                "mining strategy",
                "Euclidean distance",
                "SIFT",
                "L2 distance",
                "image classification"
            ]
        },
        "id": 13,
        "cited_by": [
            {
                "year": "2017",
                "id": 16
            },
            {
                "year": "2017",
                "id": 84
            },
            {
                "year": "2017",
                "id": 108
            },
            {
                "year": "2017",
                "id": 140
            },
            {
                "year": "2017",
                "id": 166
            },
            {
                "year": "2017",
                "id": 193
            },
            {
                "year": "2017",
                "id": 297
            },
            {
                "year": "2017",
                "id": 299
            },
            {
                "year": "2017",
                "id": 349
            },
            {
                "year": "2017",
                "id": 529
            }
        ]
    },
    {
        "title": "Amodal Completion and Size Constancy in Natural Scenes",
        "authors": [
            "Abhishek Kar",
            "Shubham Tulsiani",
            "Jo\u00e3o Carreira",
            "Jitendra Malik"
        ],
        "abstract": "We consider the problem of enriching current object detection systems with veridical object sizes and relative depth estimates from a single image. There are several technical challenges to this, such as occlusions, lack of calibration data and the scale ambiguity between object size and distance. These have not been addressed in full generality in previous work. Here we propose to tackle these issues by building upon advances in object recognition and using recently created large-scale datasets. We first introduce the task of amodal bounding box completion, which aims to infer the the full extent of the object instances in the image. We then propose a probabilistic framework for learning category-specific object size distributions from available annotations and leverage these in conjunction with amodal completions to infer veridical sizes of objects in novel images. Finally, we introduce a focal length prediction approach that exploits scene recognition to overcome inherent scale ambiguities and demonstrate qualitative results on challenging real-world scenes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410380",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 1,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Cameras",
                "Computer vision",
                "Object recognition",
                "Buildings",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "natural scenes",
                "object detection",
                "object recognition",
                "probability",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "amodal completion",
                "size constancy",
                "natural scene",
                "object detection system",
                "veridical object size",
                "relative depth estimate",
                "object recognition",
                "large-scale dataset",
                "amodal bounding box completion",
                "object instance",
                "probabilistic framework",
                "learning category-specific object size distribution",
                "veridical size",
                "focal length prediction approach",
                "scene recognition",
                "scale ambiguity",
                "real-world scene"
            ]
        },
        "id": 14,
        "cited_by": [
            {
                "year": "2015",
                "id": 137
            }
        ]
    },
    {
        "title": "Learning Where to Position Parts in 3D",
        "authors": [
            "Marco Pedersoli",
            "Tinne Tuytelaars"
        ],
        "abstract": "A common issue in deformable object detection is finding a good way to position the parts. This issue is even more outspoken when considering detection and pose estimation for 3D objects, where parts should be placed in a three-dimensional space. Some methods extract the 3D shape of the object from 3D CAD models. This limits their applicability to categories for which such models are available. Others represent the object with a predefined and simple shape (e.g. a cuboid). This extends the applicability of the model, but in many cases the pre-defined shape is too simple to properly represent the object in 3D. In this paper we propose a new method for the detection and pose estimation of 3D objects, that does not use any 3D CAD model or other 3D information. Starting from a simple and general 3D shape, we learn in a weakly supervised manner the 3D part locations that best fit the training data. As this method builds on a iterative estimation of the part locations, we introduce several speedups to make the method fast enough for practical experiments. We evaluate our model for the detection and pose estimation of faces and cars. Our method obtains results comparable with the state of the art, it is faster than most of the other approaches and does not need any additional 3D information.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410381",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Design automation",
                "Computational modeling",
                "Shape",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "iterative methods",
                "object detection",
                "pose estimation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable object detection",
                "parts position",
                "3D object detection",
                "pose estimation",
                "3D part locations",
                "iterative estimation",
                "face detection",
                "cars detection"
            ]
        },
        "id": 15,
        "cited_by": []
    },
    {
        "title": "Query Adaptive Similarity Measure for RGB-D Object Recognition",
        "authors": [
            "Yanhua Cheng",
            "Rui Cai",
            "Chi Zhang",
            "Zhiwei Li",
            "Xin Zhao",
            "Kaiqi Huang",
            "Yong Rui"
        ],
        "abstract": "This paper studies the problem of improving the top-1 accuracy of RGB-D object recognition. Despite of the impressive top-5 accuracies achieved by existing methods, their top-1 accuracies are not very satisfactory. The reasons are in two-fold: (1) existing similarity measures are sensitive to object pose and scale changes, as well as intra-class variations, and (2) effectively fusing RGB and depth cues is still an open problem. To address these problems, this paper first proposes a new similarity measure based on dense matching, through which objects in comparison are warped and aligned, to better tolerate variations. Towards RGB and depth fusion, we argue that a constant and golden weight doesn't exist. The two modalities have varying contributions when comparing objects from different categories. To capture such a dynamic characteristic, a group of matchers equipped with various fusion weights is constructed, to explore the responses of dense matching under different fusion configurations. All the response scores are finally merged following a learning-to-combination way, which provides quite good generalization ability in practice. The proposed approach win the best results on several public benchmarks, e.g., achieves 92.7% top-1 test accuracy on the Washington RGB-D object dataset, with a 5.1% improvement over the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410382",
        "reference_list": [
            {
                "year": "2011",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Image color analysis",
                "Shape",
                "Feature extraction",
                "Cameras",
                "Market research",
                "Art"
            ],
            "INSPEC: Controlled Indexing": [
                "image fusion",
                "image matching",
                "learning (artificial intelligence)",
                "object recognition",
                "query processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "query adaptive similarity measure",
                "RGB-D object recognition",
                "dense matching",
                "fusion weight",
                "learning-to-combination"
            ]
        },
        "id": 16,
        "cited_by": []
    },
    {
        "title": "Listening with Your Eyes: Towards a Practical Visual Speech Recognition System Using Deep Boltzmann Machines",
        "authors": [
            "Chao Sui",
            "Mohammed Bennamoun",
            "Roberto Togneri"
        ],
        "abstract": "This paper presents a novel feature learning method for visual speech recognition using Deep Boltzmann Machines (DBM). Unlike all existing visual feature extraction techniques which solely extracts features from video sequences, our method is able to explore both acoustic information and visual information to learn a better visual feature representation in the training stage. During the test stage, instead of using both audio and visual signals, only the videos are used for generating the missing audio feature, and both the given visual and given audio features are used to obtain a joint representation. We carried out our experiments on a large scale audio-visual data corpus, and experimental results show that our proposed techniques outperforms the performance of the hadncrafted features and features learned by other commonly used deep learning techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410383",
        "reference_list": [
            {
                "year": "2013",
                "id": 16
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Speech recognition",
                "Feature extraction",
                "Hidden Markov models",
                "Acoustics",
                "Noise measurement",
                "Speech"
            ],
            "INSPEC: Controlled Indexing": [
                "Boltzmann machines",
                "feature extraction",
                "image representation",
                "image sequences",
                "speech recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "practical visual speech recognition system",
                "deep Boltzmann machines",
                "feature learning method",
                "DBM",
                "visual feature extraction techniques",
                "video sequences",
                "acoustic information",
                "visual information",
                "visual feature representation",
                "audio signals",
                "visual signals",
                "joint representation",
                "audio visual data corpus",
                "deep learning techniques"
            ]
        },
        "id": 17,
        "cited_by": []
    },
    {
        "title": "Cluster-Based Point Set Saliency",
        "authors": [
            "Flora Ponjou Tasse",
            "Jiri Kosinka",
            "Neil Dodgson"
        ],
        "abstract": "We propose a cluster-based approach to point set saliency detection, a challenge since point sets lack topological information. A point set is first decomposed into small clusters, using fuzzy clustering. We evaluate cluster uniqueness and spatial distribution of each cluster and combine these values into a cluster saliency function. Finally, the probabilities of points belonging to each cluster are used to assign a saliency to each point. Our approach detects fine-scale salient features and uninteresting regions consistently have lower saliency values. We evaluate the proposed saliency model by testing our saliency-based keypoint detection against a 3D interest point detection benchmark. The evaluation shows that our method achieves a good balance between false positive and false negative error rates, without using any topological information.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410384",
        "reference_list": [
            {
                "year": "2013",
                "id": 190
            },
            {
                "year": "2007",
                "id": 244
            },
            {
                "year": "2013",
                "id": 448
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 7,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Graphical models",
                "Distribution functions",
                "Computational modeling",
                "Surface treatment",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "feature extraction",
                "fuzzy set theory",
                "pattern clustering",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "point set saliency detection",
                "cluster-based approach",
                "fuzzy clustering",
                "cluster uniqueness evaluation",
                "spatial distribution evaluation",
                "probabilities",
                "fine-scale salient feature detection",
                "uninteresting region detection",
                "saliency-based key-point detection",
                "3D interest point detection",
                "false positive error rates",
                "false negative error rates"
            ]
        },
        "id": 18,
        "cited_by": []
    },
    {
        "title": "A Comprehensive Multi-Illuminant Dataset for Benchmarking of the Intrinsic Image Algorithms",
        "authors": [
            "Shida Beigpour",
            "Andreas Kolb",
            "Sven Kunz"
        ],
        "abstract": "In this paper, we provide a new, real photo dataset with precise ground-truth for intrinsic image research. Prior ground-truth datasets have been restricted to rather simple illumination conditions and scene geometries, or have been enhanced using image synthesis methods. The dataset provided in this paper is based on complex multi-illuminant scenarios under multi-colored illumination conditions and challenging cast shadows. We provide full per-pixel intrinsic ground-truth data for these scenarios, i.e. reflectance, specularity, shading, and illumination for scenes as well as preliminary depth information. Furthermore, we evaluate 3 state-of-the-art intrinsic image recovery methods, using our dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410385",
        "reference_list": [
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2011",
                "id": 205
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Image color analysis",
                "Geometry",
                "Cameras",
                "Light sources",
                "Complexity theory",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image restoration",
                "lighting",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiilluminant dataset",
                "intrinsic image algorithm benchmarking",
                "real photo dataset",
                "scene geometries",
                "image synthesis methods",
                "multicolored illumination conditions",
                "full per-pixel intrinsic ground-truth data",
                "reflectance",
                "specularity",
                "shading",
                "preliminary depth information",
                "state-of-the-art intrinsic image recovery methods"
            ]
        },
        "id": 19,
        "cited_by": []
    },
    {
        "title": "PatchMatch-Based Automatic Lattice Detection for Near-Regular Textures",
        "authors": [
            "Siying Liu",
            "Tian-Tsong Ng",
            "Kalyan Sunkavalli",
            "Minh N. Do",
            "Eli Shechtman",
            "Nathan Carr"
        ],
        "abstract": "In this work, we investigate the problem of automatically inferring the lattice structure of near-regular textures (NRT) in real-world images. Our technique leverages the PatchMatch algorithm for finding k-nearest-neighbor (kNN) correspondences in an image. We use these kNNs to recover an initial estimate of the 2D wallpaper basis vectors, and seed vertices of the texture lattice. We iteratively expand this lattice by solving an MRF optimization problem. We show that we can discretize the space of good solutions for the MRF using the kNNs, allowing us to efficiently and accurately optimize the MRF energy function using the Particle Belief Propagation algorithm. We demonstrate our technique on a benchmark NRT dataset containing a wide range of images with geometric and photometric variations, and show that our method clearly outperforms the state of the art in terms of both texel detection rate and texel localization score.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410386",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Lattices",
                "Feature extraction",
                "Deformable models",
                "Distortion",
                "Proposals",
                "Belief propagation",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image texture",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "patchmatch-based automatic lattice detection",
                "near-regular texture",
                "real-world image",
                "k-nearest-neighbor",
                "kNN",
                "MRF optimization problem",
                "MRF energy function",
                "particle belief propagation algorithm",
                "photometric variation",
                "geometric variation",
                "texel detection rate",
                "texel localization score"
            ]
        },
        "id": 20,
        "cited_by": []
    },
    {
        "title": "A Data-Driven Metric for Comprehensive Evaluation of Saliency Models",
        "authors": [
            "Jia Li",
            "Changqun Xia",
            "Yafei Song",
            "Shu Fang",
            "Xiaowu Chen"
        ],
        "abstract": "In the past decades, hundreds of saliency models have been proposed for fixation prediction, along with dozens of evaluation metrics. However, existing metrics, which are often heuristically designed, may draw conflict conclusions in comparing saliency models. As a consequence, it becomes somehow confusing on the selection of metrics in comparing new models with state-of-the-arts. To address this problem, we propose a data-driven metric for comprehensive evaluation of saliency models. Instead of heuristically designing such a metric, we first conduct extensive subjective tests to find how saliency maps are assessed by the human-being. Based on the user data collected in the tests, nine representative evaluation metrics are directly compared by quantizing their performances in assessing saliency maps. Moreover, we propose to learn a data-driven metric by using Convolutional Neural Network. Compared with existing metrics, experimental results show that the data-driven metric performs the most consistently with the human-being in evaluating saliency maps as well as saliency models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410387",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2013",
                "id": 143
            },
            {
                "year": "2013",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 2,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Computational modeling",
                "GSM",
                "Solid modeling",
                "Predictive models",
                "Integrated circuit modeling",
                "Benchmark testing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convolutional neural network",
                "saliency maps",
                "fixation prediction",
                "saliency models",
                "comprehensive evaluation",
                "data-driven metric"
            ]
        },
        "id": 21,
        "cited_by": []
    },
    {
        "title": "A Matrix Decomposition Perspective to Multiple Graph Matching",
        "authors": [
            "Junchi Yan",
            "Hongteng Xu",
            "Hongyuan Zha",
            "Xiaokang Yang",
            "Huanxi Liu",
            "Stephen Chu"
        ],
        "abstract": "Graph matching has a wide spectrum of real-world applications and in general is known NP-hard. In many vision tasks, one realistic problem arises for finding the global node mappings across a batch of corrupted weighted graphs. This paper is an attempt to connect graph matching, especially multi-graph matching to the matrix decomposition model and its relevant on-the-shelf convex optimization algorithms. Our method aims to extract the common inliers and their synchronized permutations from disordered weighted graphs in the presence of deformation and outliers. Under the proposed framework, several variants can be derived in the hope of accommodating to other types of noises. Experimental results on both synthetic data and real images empirically show that the proposed paradigm exhibits several interesting behaviors and in many cases performs competitively with the state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410388",
        "reference_list": [
            {
                "year": "2013",
                "id": 3
            },
            {
                "year": "2011",
                "id": 227
            },
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2011",
                "id": 289
            },
            {
                "year": "2015",
                "id": 357
            },
            {
                "year": "2013",
                "id": 205
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Matrix decomposition",
                "Optimization",
                "Iterative methods",
                "Robustness",
                "Computer vision",
                "Image edge detection",
                "Encoding"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "graph theory",
                "image matching",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "matrix decomposition perspective",
                "multiple graph matching",
                "NP-hard problem",
                "global node mappings",
                "corrupted weighted graphs",
                "matrix decomposition model",
                "convex optimization algorithms",
                "synthetic data"
            ]
        },
        "id": 22,
        "cited_by": [
            {
                "year": "2015",
                "id": 357
            }
        ]
    },
    {
        "title": "Fast and Effective L0 Gradient Minimization by Region Fusion",
        "authors": [
            "Rang M. H. Nguyen",
            "Michael S. Brown"
        ],
        "abstract": "L 0 gradient minimization can be applied to an input signal to control the number of non-zero gradients. This is useful in reducing small gradients generally associated with signal noise, while preserving important signal features. In computer vision, L 0 gradient minimization has found applications in image denoising, 3D mesh denoising, and image enhancement. Minimizing the L 0 norm, however, is an NP-hard problem because of its non-convex property. As a result, existing methods rely on approximation strategies to perform the minimization. In this paper, we present a new method to perform L 0 gradient minimization that is fast and effective. Our method uses a descent approach based on region fusion that converges faster than other methods while providing a better approximation of the optimal L 0 norm. In addition, our method can be applied to both 2D images and 3D mesh topologies. The effectiveness of our approach is demonstrated on a number of examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410389",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 9,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Minimization",
                "Silicon",
                "Linear programming",
                "Three-dimensional displays",
                "Optimization",
                "Nickel",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image denoising",
                "image enhancement",
                "image fusion",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "gradient minimization",
                "region fusion",
                "computer vision",
                "image denoising",
                "3D mesh denoising",
                "image enhancement",
                "NP-hard problem",
                "2D images",
                "3D mesh topologies"
            ]
        },
        "id": 23,
        "cited_by": []
    },
    {
        "title": "Generic Promotion of Diffusion-Based Salient Object Detection",
        "authors": [
            "Peng Jiang",
            "Nuno Vasconcelos",
            "Jingliang Peng"
        ],
        "abstract": "In this work, we propose a generic scheme to promote any diffusion-based salient object detection algorithm by original ways to re-synthesize the diffusion matrix and construct the seed vector. We first make a novel analysis of the working mechanism of the diffusion matrix, which reveals the close relationship between saliency diffusion and spectral clustering. Following this analysis, we propose to re-synthesize the diffusion matrix from the most discriminative eigenvectors after adaptive re-weighting. Further, we propose to generate the seed vector based on the readily available diffusion maps, avoiding extra computation for color-based seed search. As a particular instance, we use inverse normalized Laplacian matrix as the original diffusion matrix and promote the corresponding salient object detection algorithm, which leads to superior performance as experimentally demonstrated.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410390",
        "reference_list": [
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2013",
                "id": 190
            },
            {
                "year": "2013",
                "id": 207
            },
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2013",
                "id": 371
            },
            {
                "year": "2011",
                "id": 29
            },
            {
                "year": "2013",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 7,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Eigenvalues and eigenfunctions",
                "Laplace equations",
                "Image color analysis",
                "Visualization",
                "Computer vision",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "eigenvalues and eigenfunctions",
                "image colour analysis",
                "matrix algebra",
                "object detection",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "diffusion-based salient object detection algorithm",
                "diffusion matrix",
                "saliency diffusion",
                "spectral clustering",
                "discriminative eigenvectors",
                "inverse normalized Laplacian matrix",
                "color-based seed search"
            ]
        },
        "id": 24,
        "cited_by": []
    },
    {
        "title": "Nighttime Haze Removal with Glow and Multiple Light Colors",
        "authors": [
            "Yu Li",
            "Robby T. Tan",
            "Michael S. Brown"
        ],
        "abstract": "This paper focuses on dehazing nighttime images. Most existing dehazing methods use models that are formulated to describe haze in daytime. Daytime models assume a single uniform light color attributed to a light source not directly visible in the scene. Nighttime scenes, however, commonly include visible lights sources with varying colors. These light sources also often introduce noticeable amounts of glow that is not present in daytime haze. To address these effects, we introduce a new nighttime haze model that accounts for the varying light sources and their glow. Our model is a linear combination of three terms: the direct transmission, airlight and glow. The glow term represents light from the light sources that is scattered around before reaching the camera. Based on the model, we propose a framework that first reduces the effect of the glow in the image, resulting in a nighttime image that consists of direct transmission and airlight only. We then compute a spatially varying atmospheric light map that encodes light colors locally. This atmospheric map is used to predict the transmission, which we use to obtain our nighttime scene reflection image. We demonstrate the effectiveness of our nighttime haze model and correction method on a number of examples and compare our results with existing daytime and nighttime dehazing methods' results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410391",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 23,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Atmospheric modeling",
                "Light sources",
                "Image color analysis",
                "Standards",
                "Cameras",
                "Computational modeling",
                "Scattering"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "image colour analysis",
                "light sources"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "light color nighttime scene reflection image",
                "atmospheric light map",
                "camera",
                "nighttime haze model",
                "daytime haze",
                "visible light source",
                "multiple light color",
                "nighttime haze removal"
            ]
        },
        "id": 25,
        "cited_by": []
    },
    {
        "title": "Conformal and Low-Rank Sparse Representation for Image Restoration",
        "authors": [
            "Jianwei Li",
            "Xiaowu Chen",
            "Dongqing Zou",
            "Bo Gao",
            "Wei Teng"
        ],
        "abstract": "Obtaining an appropriate dictionary is the key point when sparse representation is applied to computer vision or image processing problems such as image restoration. It is expected that preserving data structure during sparse coding and dictionary learning can enhance the recovery performance. However, many existing dictionary learning methods handle training samples individually, while missing relationships between samples, which result in dictionaries with redundant atoms but poor representation ability. In this paper, we propose a novel sparse representation approach called conformal and low-rank sparse representation (CLRSR) for image restoration problems. To achieve a more compact and representative dictionary, conformal property is introduced by preserving the angles of local geometry formed by neighboring samples in the feature space. Furthermore, imposing low-rank constraint on the coefficient matrix can lead more faithful subspaces and capture the global structure of data. We apply our CLRSR model to several image restoration tasks to demonstrate the effectiveness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410392",
        "reference_list": [
            {
                "year": "2013",
                "id": 245
            },
            {
                "year": "2011",
                "id": 159
            },
            {
                "year": "2011",
                "id": 89
            },
            {
                "year": "2011",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Image restoration",
                "Image coding",
                "Manifolds",
                "Image reconstruction",
                "Sparse matrices",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "image representation",
                "image restoration",
                "learning (artificial intelligence)",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "coefficient matrix",
                "feature space",
                "local geometry",
                "conformal property",
                "CLRSR",
                "dictionary learning method",
                "sparse coding",
                "data structure",
                "image processing",
                "computer vision",
                "image restoration",
                "conformal low-rank sparse representation"
            ]
        },
        "id": 26,
        "cited_by": []
    },
    {
        "title": "Patch Group Based Nonlocal Self-Similarity Prior Learning for Image Denoising",
        "authors": [
            "Jun Xu",
            "Lei Zhang",
            "Wangmeng Zuo",
            "David Zhang",
            "Xiangchu Feng"
        ],
        "abstract": "Patch based image modeling has achieved a great success in low level vision such as image denoising. In particular, the use of image nonlocal self-similarity (NSS) prior, which refers to the fact that a local patch often has many nonlocal similar patches to it across the image, has significantly enhanced the denoising performance. However, in most existing methods only the NSS of input degraded image is exploited, while how to utilize the NSS of clean natural images is still an open problem. In this paper, we propose a patch group (PG) based NSS prior learning scheme to learn explicit NSS models from natural images for high performance denoising. PGs are extracted from training images by putting nonlocal similar patches into groups, and a PG based Gaussian Mixture Model (PG-GMM) learning algorithm is developed to learn the NSS prior. We demonstrate that, owe to the learned PG-GMM, a simple weighted sparse coding model, which has a closed-form solution, can be used to perform image denoising effectively, resulting in high PSNR measure, fast speed, and particularly the best visual quality among all competing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410393",
        "reference_list": [
            {
                "year": "2009",
                "id": 292
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 44,
            "other": 31,
            "total": 75
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise reduction",
                "Image denoising",
                "Image restoration",
                "Training",
                "Noise measurement",
                "Dictionaries",
                "Wavelet transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image denoising",
                "image enhancement",
                "learning (artificial intelligence)",
                "mixture models"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image denoising",
                "image NSS prior",
                "image enhancement",
                "patch group based NSS prior learning scheme",
                "PG-based NSS prior learning scheme",
                "nonlocal similar patch",
                "PG-based Gaussian mixture model learning algorithm",
                "PG-GMM learning algorithm",
                "weighted sparse coding model",
                "PSNR",
                "nonlocal self-similarity prior learning"
            ]
        },
        "id": 27,
        "cited_by": [
            {
                "year": "2017",
                "id": 115
            },
            {
                "year": "2017",
                "id": 181
            },
            {
                "year": "2017",
                "id": 476
            }
        ]
    },
    {
        "title": "Automatic Thumbnail Generation Based on Visual Representativeness and Foreground Recognizability",
        "authors": [
            "Jingwei Huang",
            "Huarong Chen",
            "Bin Wang",
            "Stephen Lin"
        ],
        "abstract": "We present an automatic thumbnail generation technique based on two essential considerations: how well they visually represent the original photograph, and how well the foreground can be recognized after the cropping and downsizing steps of thumbnailing. These factors, while important for the image indexing purpose of thumbnails, have largely been ignored in previous methods, which instead are designed to highlight salient content while disregarding the effects of downsizing. We propose a set of image features for modeling these two considerations of thumbnails, and learn how to balance their relative effects on thumbnail generation through training on image pairs composed of photographs and their corresponding thumbnails created by an expert photographer. Experiments show the effectiveness of this approach on a variety of images, as well as its advantages over related techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410394",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Agriculture",
                "Visualization",
                "Image color analysis",
                "Training",
                "Feature extraction",
                "Image edge detection",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "foreground recognizability",
                "visual representativeness",
                "automatic thumbnail generation technique",
                "image indexing",
                "image recognition"
            ]
        },
        "id": 28,
        "cited_by": []
    },
    {
        "title": "SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks",
        "authors": [
            "Xun Huang",
            "Chengyao Shen",
            "Xavier Boix",
            "Qi Zhao"
        ],
        "abstract": "Saliency in Context (SALICON) is an ongoing effort that aims at understanding and predicting visual attention. Conventional saliency models typically rely on low-level image statistics to predict human fixations. While these models perform significantly better than chance, there is still a large gap between model prediction and human behavior. This gap is largely due to the limited capability of models in predicting eye fixations with strong semantic content, the so-called semantic gap. This paper presents a focused study to narrow the semantic gap with an architecture based on Deep Neural Network (DNN). It leverages the representational power of high-level semantics encoded in DNNs pretrained for object recognition. Two key components are fine-tuning the DNNs fully convolutionally with an objective function based on the saliency evaluation metrics, and integrating information at different image scales. We compare our method with 14 saliency models on 6 public eye tracking benchmark datasets. Results demonstrate that our DNNs can automatically learn features particularly for saliency prediction that surpass by a big margin the state-of-the-art. In addition, our model ranks top to date under all seven metrics on the MIT300 challenge set.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410395",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2013",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 77,
            "other": 47,
            "total": 124
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Computational modeling",
                "Measurement",
                "Predictive models",
                "Object recognition",
                "Neurons",
                "Spatial resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "gaze tracking",
                "learning (artificial intelligence)",
                "neural nets",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "saliency evaluation metrics",
                "image scales",
                "public eye tracking benchmark datasets",
                "feature learning",
                "MIT300 challenge set",
                "high-level semantics representational power",
                "DNN",
                "semantic content",
                "eye fixation prediction",
                "human fixation prediction",
                "low-level image statistics",
                "visual attention prediction",
                "saliency in context",
                "deep neural networks",
                "saliency prediction",
                "semantic gap reduction",
                "SALICON"
            ]
        },
        "id": 29,
        "cited_by": [
            {
                "year": "2017",
                "id": 344
            },
            {
                "year": "2017",
                "id": 503
            }
        ]
    },
    {
        "title": "A Novel Sparsity Measure for Tensor Recovery",
        "authors": [
            "Qian Zhao",
            "Deyu Meng",
            "Xu Kong",
            "Qi Xie",
            "Wenfei Cao",
            "Yao Wang",
            "Zongben Xu"
        ],
        "abstract": "In this paper, we propose a new sparsity regularizer for measuring the low-rank structure underneath a tensor. The proposed sparsity measure has a natural physical meaning which is intrinsically the size of the fundamental Kronecker basis to express the tensor. By embedding the sparsity measure into the tensor completion and tensor robust PCA frameworks, we formulate new models to enhance their capability in tensor recovery. Through introducing relaxation forms of the proposed sparsity measure, we also adopt the alternating direction method of multipliers (ADMM) for solving the proposed models. Experiments implemented on synthetic and multispectral image data sets substantiate the effectiveness of the proposed methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410396",
        "reference_list": [
            {
                "year": "2009",
                "id": 272
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 5,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Robustness",
                "Principal component analysis",
                "Computer vision",
                "Current measurement",
                "Computational modeling",
                "Brain modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image resolution",
                "principal component analysis",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multispectral image data set",
                "synthetic image data set",
                "alternating direction method of multipliers",
                "ADMM",
                "principal component analysis",
                "tensor robust PCA framework",
                "tensor completion",
                "Kronecker basis",
                "tensor recovery",
                "sparsity measure"
            ]
        },
        "id": 30,
        "cited_by": [
            {
                "year": "2017",
                "id": 181
            },
            {
                "year": "2017",
                "id": 348
            }
        ]
    },
    {
        "title": "Oriented Object Proposals",
        "authors": [
            "Shengfeng He",
            "Rynson W. H. Lau"
        ],
        "abstract": "In this paper, we propose a new approach to generate oriented object proposals (OOPs) to reduce the detection error caused by various orientations of the object. To this end, we propose to efficiently locate object regions according to pixelwise object probability, rather than measuring the objectness from a set of sampled windows. We formulate the proposal generation problem as a generative probabilistic model such that object proposals of different shapes (i.e., sizes and orientations) can be produced by locating the local maximum likelihoods. The new approach has three main advantages. First, it helps the object detector handle objects of different orientations. Second, as the shapes of the proposals may vary to fit the objects, the resulting proposals are tighter than the sampling windows with fixed sizes. Third, it avoids massive window sampling, and thereby reducing the number of proposals while maintaining a high recall. Experiments on the PASCAL VOC 2007 dataset show that the proposed OOP outperforms the state-of-the-art fast methods. Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios. Generating OOPs is very fast and takes only 0.5s per image.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410397",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2005",
                "id": 57
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2013",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Image color analysis",
                "Detectors",
                "Shape",
                "Object detection",
                "Histograms",
                "Search problems"
            ],
            "INSPEC: Controlled Indexing": [
                "image sampling",
                "maximum likelihood estimation",
                "object detection",
                "probability",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "oriented object proposals",
                "OOP",
                "detection error",
                "object orientations",
                "pixelwise object probability",
                "generative probabilistic model",
                "local maximum likelihoods",
                "window sampling",
                "PASCAL VOC 2007 dataset",
                "class-specific object detector",
                "object rotation scenarios"
            ]
        },
        "id": 31,
        "cited_by": []
    },
    {
        "title": "Learning Nonlinear Spectral Filters for Color Image Reconstruction",
        "authors": [
            "Michael Moeller",
            "Julia Diebold",
            "Guy Gilboa",
            "Daniel Cremers"
        ],
        "abstract": "This paper presents the idea of learning optimal filters for color image reconstruction based on a novel concept of nonlinear spectral image decompositions recently proposed by Guy Gilboa. We use a multiscale image decomposition approach based on total variation regularization and Bregman iterations to represent the input data as the sum of image layers containing features at different scales. Filtered images can be obtained by weighted linear combinations of the different frequency layers. We introduce the idea of learning optimal filters for the task of image denoising, and propose the idea of mixing high frequency components of different color channels. Our numerical experiments demonstrate that learning the optimal weights can significantly improve the results in comparison to the standard variational approach, and achieves state-of-the-art image denoising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410398",
        "reference_list": [
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "TV",
                "Image reconstruction",
                "Color",
                "Noise reduction",
                "Image denoising",
                "Correlation"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image denoising",
                "image filtering",
                "image reconstruction",
                "iterative methods",
                "learning (artificial intelligence)",
                "spectral analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning nonlinear spectral filter",
                "color image reconstruction",
                "learning optimal filter",
                "nonlinear spectral image decomposition",
                "multiscale image decomposition approach",
                "total variation regularization",
                "Bregman iteration",
                "image layer",
                "filtered image",
                "weighted linear combination",
                "frequency layer",
                "image denoising",
                "frequency component",
                "color channel",
                "optimal weight"
            ]
        },
        "id": 32,
        "cited_by": []
    },
    {
        "title": "Beyond White: Ground Truth Colors for Color Constancy Correction",
        "authors": [
            "Dongliang Cheng",
            "Brian Price",
            "Scott Cohen",
            "Michael S. Brown"
        ],
        "abstract": "A limitation in color constancy research is the inability to establish ground truth colors for evaluating corrected images. Many existing datasets contain images of scenes with a color chart included, however, only the chart's neutral colors (grayscale patches) are used to provide the ground truth for illumination estimation and correction. This is because the corrected neutral colors are known to lie along the achromatic line in the camera's color space (i.e. R=G=B), the correct RGB values of the other color patches are not known. As a result, most methods estimate a 3*3 diagonal matrix that ensures only the neutral colors are correct. In this paper, we describe how to overcome this limitation. Specifically, we show that under certain illuminations, a diagonal 3*3 matrix is capable of correcting not only neutral colors, but all the colors in a scene. This finding allows us to find the ground truth RGB values for the color chart in the camera's color space. We show how to use this information to correct all the images in existing datasets to have correct colors. Working from these new color corrected datasets, we describe how to modify existing color constancy algorithms to perform better image correction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410399",
        "reference_list": [
            {
                "year": "2007",
                "id": 262
            },
            {
                "year": "2013",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Lighting",
                "Cameras",
                "Transforms",
                "Sensitivity",
                "Estimation",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "lighting",
                "matrix algebra",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color constancy correction",
                "ground truth colors",
                "corrected image evaluation",
                "image datasets",
                "color chart scenes",
                "illumination estimation",
                "illumination correction",
                "corrected neutral colors",
                "color space",
                "color patch",
                "diagonal matrix estimation",
                "ground truth RGB values",
                "image correction"
            ]
        },
        "id": 33,
        "cited_by": []
    },
    {
        "title": "RGB-Guided Hyperspectral Image Upsampling",
        "authors": [
            "Hyeokhyen Kwon",
            "Yu-Wing Tai"
        ],
        "abstract": "Hyperspectral imaging usually lack of spatial resolution due to limitations of hardware design of imaging sensors. On the contrary, latest imaging sensors capture a RGB image with resolution of multiple times larger than a hyperspectral image. In this paper, we present an algorithm to enhance and upsample the resolution of hyperspectral images. Our algorithm consists of two stages: spatial upsampling stage and spectrum substitution stage. The spatial upsampling stage is guided by a high resolution RGB image of the same scene, and the spectrum substitution stage utilizes sparse coding to locally refine the upsampled hyperspectral image through dictionary substitution. Experiments show that our algorithm is highly effective and has outperformed state-of-the-art matrix factorization based approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410400",
        "reference_list": [
            {
                "year": "2013",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 3,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatial resolution",
                "Hyperspectral imaging",
                "Image reconstruction",
                "Dictionaries",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image enhancement",
                "image resolution",
                "image sampling",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hyperspectral image upsampling",
                "spatial resolution",
                "imaging sensor",
                "spatial upsampling stage",
                "spectrum substitution stage",
                "high resolution RGB image",
                "sparse coding",
                "dictionary substitution"
            ]
        },
        "id": 34,
        "cited_by": [
            {
                "year": "2017",
                "id": 347
            }
        ]
    },
    {
        "title": "Projection onto the Manifold of Elongated Structures for Accurate Extraction",
        "authors": [
            "Amos Sironi",
            "Vincent Lepetit",
            "Pascal Fua"
        ],
        "abstract": "Detection of elongated structures in 2D images and 3D image stacks is a critical prerequisite in many applications and Machine Learning-based approaches have recently been shown to deliver superior performance. However, these methods essentially classify individual locations and do not explicitly model the strong relationship that exists between neighboring ones. As a result, isolated erroneous responses, discontinuities, and topological errors are present in the resulting score maps. We solve this problem by projecting patches of the score map to their nearest neighbors in a set of ground truth training patches. Our algorithm induces global spatial consistency on the classifier score map and returns results that are provably geometrically consistent. We apply our algorithm to challenging datasets in four different domains and show that it compares favorably to state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410401",
        "reference_list": [
            {
                "year": "2007",
                "id": 70
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2009",
                "id": 292
            },
            {
                "year": "2013",
                "id": 270
            },
            {
                "year": "2013",
                "id": 193
            },
            {
                "year": "2011",
                "id": 22
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Training",
                "Manifolds",
                "Biomembranes",
                "Image segmentation",
                "Feature extraction",
                "Transforms"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)",
                "object detection",
                "structural engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "classifier score map",
                "global spatial consistency",
                "ground truth training patch",
                "patch projection",
                "score map",
                "topological error",
                "discontinuities",
                "isolated erroneous response",
                "individual location classification",
                "machine learning-based approach",
                "3D image stack",
                "2D image",
                "elongated structure detection",
                "extraction",
                "elongated structure manifold"
            ]
        },
        "id": 35,
        "cited_by": []
    },
    {
        "title": "Naive Bayes Super-Resolution Forest",
        "authors": [
            "Jordi Salvador",
            "Eduardo P\u00e9rez-Pellitero"
        ],
        "abstract": "This paper presents a fast, high-performance method for super resolution with external learning. The first contribution leading to the excellent performance is a bimodal tree for clustering, which successfully exploits the antipodal invariance of the coarse-to-high-res mapping of natural image patches and provides scalability to finer partitions of the underlying coarse patch space. During training an ensemble of such bimodal trees is computed, providing different linearizations of the mapping. The second and main contribution is a fast inference algorithm, which selects the most suitable mapping function within the tree ensemble for each patch by adopting a Local Naive Bayes formulation. The experimental validation shows promising scalability properties that reflect the suitability of the proposed model, which may also be generalized to other tasks. The resulting method is beyond one order of magnitude faster and performs objectively and subjectively better than the current state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410402",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2011",
                "id": 203
            },
            {
                "year": "2013",
                "id": 117
            },
            {
                "year": "2013",
                "id": 239
            },
            {
                "year": "2013",
                "id": 69
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 17,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Training",
                "Image reconstruction",
                "Manifolds",
                "Vegetation",
                "Dictionaries",
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "image resolution",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mapping function",
                "inference algorithm",
                "bimodal tree",
                "external learning",
                "naive Bayes superresolution forest"
            ]
        },
        "id": 36,
        "cited_by": []
    },
    {
        "title": "POP Image Fusion -- Derivative Domain Image Fusion without Reintegration",
        "authors": [
            "Graham D. Finlayson",
            "Alex E. Hayes"
        ],
        "abstract": "There are many applications where multiple images are fused to form a single summary greyscale or colour output, including computational photography (e.g. RGB-NIR), diffusion tensor imaging (medical), and remote sensing. Often, and intuitively, image fusion is carried out in the derivative domain. Here, a new composite fused derivative is found that best accounts for the detail across all images and then the resulting gradient field is reintegrated. However, the reintegration step generally hallucinates new detail (not appearing in any of the input image bands) including halo and bending artifacts. In this paper we avoid these hallucinated details by avoiding the reintegration step. Our work builds directly on the work of Socolinsky and Wolff who derive their equivalent gradient field from the per-pixel Di Zenzo structure tensor which is defined as the inner product of the image Jacobian. We show that the x-and y-derivatives of the projection of the original image onto the Principal characteristic vector of the Outer Product (POP) of the Jacobian generates the same equivalent gradient field. In so doing, we have derived a fused image that has the derivative structure we seek. Of course, this projection will be meaningful only where the Jacobian has non-zero derivatives, so we diffuse the projection directions using a bilateral filter before we calculate the fused image. The resulting POP fused image has maximal fused detail but avoids hallucinated artifacts. Experiments demonstrate our method delivers state of the art image fusion performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410403",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Image fusion",
                "Jacobian matrices",
                "Tensile stress",
                "Discrete wavelet transforms",
                "Matrix decomposition",
                "Eigenvalues and eigenfunctions",
                "Periodic structures"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image colour analysis",
                "image fusion",
                "Jacobian matrices",
                "principal component analysis",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "POP image fusion",
                "derivative domain image fusion",
                "colour output",
                "computational photography",
                "diffusion tensor imaging",
                "remote sensing",
                "composite fused derivatives",
                "gradient field",
                "hallucinated details",
                "equivalent gradient field",
                "per-pixel Di Zenzo structure tensor",
                "image Jacobian",
                "y-derivatives",
                "x-derivatives",
                "principal characteristic vector of the outer product",
                "POP",
                "nonzero derivatives",
                "bilateral filter"
            ]
        },
        "id": 37,
        "cited_by": [
            {
                "year": "2017",
                "id": 347
            }
        ]
    },
    {
        "title": "Adaptive Spatial-Spectral Dictionary Learning for Hyperspectral Image Denoising",
        "authors": [
            "Ying Fu",
            "Antony Lam",
            "Imari Sato",
            "Yoichi Sato"
        ],
        "abstract": "Hyperspectral imaging is beneficial in a diverse range of applications from diagnostic medicine, to agriculture, to surveillance to name a few. However, hyperspectral images often times suffer from degradation due to the limited light, which introduces noise into the imaging process. In this paper, we propose an effective model for hyperspectral image (HSI) denoising that considers underlying characteristics of HSIs: sparsity across the spatial-spectral domain, high correlation across spectra, and non-local self-similarity over space. We first exploit high correlation across spectra and non-local self-similarity over space in the noisy HSI to learn an adaptive spatial-spectral dictionary. Then, we employ the local and non-local sparsity of the HSI under the learned spatial-spectral dictionary to design an HSI denoising model, which can be effectively solved by an iterative numerical algorithm with parameters that are adaptively adjusted for different clusters and different noise levels. Experimental results on HSI denoising show that the proposed method can provide substantial improvements over the current state-of-the-art HSI denoising methods in terms of both objective metric and subjective visual quality.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410404",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2011",
                "id": 159
            },
            {
                "year": "2009",
                "id": 292
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Noise reduction",
                "Principal component analysis",
                "Adaptation models",
                "Spectral analysis",
                "Hyperspectral imaging",
                "Numerical models"
            ],
            "INSPEC: Controlled Indexing": [
                "geophysical image processing",
                "image denoising",
                "iterative methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive spatial-spectral dictionary learning",
                "hyperspectral image denoising",
                "diagnostic medicine",
                "HSI denoising model",
                "iterative numerical algorithm"
            ]
        },
        "id": 38,
        "cited_by": []
    },
    {
        "title": "Fully Connected Guided Image Filtering",
        "authors": [
            "Longquan Dai",
            "Mengke Yuan",
            "Feihu Zhang",
            "Xiaopeng Zhang"
        ],
        "abstract": "This paper presents a linear time fully connected guided filter by introducing the minimum spanning tree (MST) to the guided filter (GF). Since the intensity based filtering kernel of GF is apt to overly smooth edges and the fixed-shape local box support region adopted by GF is not geometric-adaptive, our filter introduces an extra spatial term, the tree similarity, to the filtering kernel of GF and substitutes the box window with the implicit support region by establishing all-pairs-connections among pixels in the image and assigning the spatial-intensity-aware similarity to these connections. The adaptive implicit support region composed by the pixels with large kernel weights in the entire image domain has a big advantage over the predefined local box window in presenting the structure of an image for the reason that: 1, MST can efficiently present the structure of an image, 2, the kernel weight of our filter considers the tree distance defined on the MST. Due to these reasons, our filter achieves better edge-preserving results. We demonstrate the strength of the proposed filter in several applications. Experimental results show that our method produces better results than state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410405",
        "reference_list": [
            {
                "year": "2011",
                "id": 205
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Kernel",
                "Linear programming",
                "Smoothing methods",
                "Optimization",
                "Estimation",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image filtering",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fully connected guided image filtering",
                "minimum spanning tree",
                "MST",
                "intensity based filtering kernel",
                "fixed-shape local box support region",
                "spatial-intensity-aware similarity",
                "edge preserving image filter"
            ]
        },
        "id": 39,
        "cited_by": [
            {
                "year": "2017",
                "id": 347
            }
        ]
    },
    {
        "title": "Segment Graph Based Image Filtering: Fast Structure-Preserving Smoothing",
        "authors": [
            "Feihu Zhang",
            "Longquan Dai",
            "Shiming Xiang",
            "Xiaopeng Zhang"
        ],
        "abstract": "In this paper, we design a new edge-aware structure, named segment graph, to represent the image and we further develop a novel double weighted average image filter (SGF) based on the segment graph. In our SGF, we use the tree distance on the segment graph to define the internal weight function of the filtering kernel, which enables the filter to smooth out high-contrast details and textures while preserving major image structures very well. While for the external weight function, we introduce a user specified smoothing window to balance the smoothing effects from each node of the segment graph. Moreover, we also set a threshold to adjust the edge-preserving performance. These advantages make the SGF more flexible in various applications and overcome the \"halo\" and \"leak\" problems appearing in most of the state-of-the-art approaches. Finally and importantly, we develop a linear algorithm for the implementation of our SGF, which has an O(N) time complexity for both gray-scale and high dimensional images, regardless of the kernel size and the intensity range. Typically, as one of the fastest edge-preserving filters, our CPU implementation achieves 0.15s per megapixel when performing filtering for 3-channel color images. The strength of the proposed filter is demonstrated by various applications, including stereo matching, optical flow, joint depth map upsampling, edge-preserving smoothing, edges detection, image abstraction and texture editing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410406",
        "reference_list": [
            {
                "year": "2013",
                "id": 123
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Smoothing methods",
                "Image segmentation",
                "Silicon",
                "Kernel",
                "Nonlinear optics",
                "Optical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "edge detection",
                "image colour analysis",
                "image filtering",
                "image representation",
                "image segmentation",
                "smoothing methods",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "segment graph based image filtering",
                "structure-preserving smoothing",
                "edge-aware structure",
                "named segment graph",
                "image representation",
                "double weighted average image filter",
                "SGF",
                "tree distance",
                "internal weight function",
                "filtering kernel",
                "high-contrast details",
                "image structures",
                "external weight function",
                "edge-preserving performance",
                "halo problem",
                "leak problem",
                "O(N) time complexity",
                "gray-scale images",
                "high dimensional images",
                "edge-preserving filters",
                "CPU implementation",
                "3-channel color images",
                "stereo matching",
                "optical flow",
                "texture editing",
                "image abstraction",
                "image denoising",
                "edge detection",
                "edge-preserving smoothing",
                "joint depth map upsampling"
            ]
        },
        "id": 40,
        "cited_by": []
    },
    {
        "title": "Deep Networks for Image Super-Resolution with Sparse Prior",
        "authors": [
            "Zhaowen Wang",
            "Ding Liu",
            "Jianchao Yang",
            "Wei Han",
            "Thomas Huang"
        ],
        "abstract": "Deep learning techniques have been successfully applied in many areas of computer vision, including low-level image restoration problems. For image super-resolution, several models based on deep neural networks have been recently proposed and attained superior performance that overshadows all previous handcrafted models. The question then arises whether large-capacity and data-driven models have become the dominant solution to the ill-posed super-resolution problem. In this paper, we argue that domain expertise represented by the conventional sparse coding model is still valuable, and it can be combined with the key ingredients of deep learning to achieve further improved results. We show that a sparse coding model particularly designed for super-resolution can be incarnated as a neural network, and trained in a cascaded structure from end to end. The interpretation of the network based on sparse coding leads to much more efficient and effective training, as well as a reduced model size. Our model is evaluated on a wide range of images, and shows clear advantage over existing state-of-the-art methods in terms of both restoration accuracy and human subjective quality.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410407",
        "reference_list": [
            {
                "year": "2011",
                "id": 159
            },
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2013",
                "id": 239
            }
        ],
        "citation": {
            "ieee": 135,
            "other": 76,
            "total": 211
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Training",
                "Image coding",
                "Machine learning",
                "Neural networks",
                "Dictionaries",
                "Neurons"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image resolution",
                "learning (artificial intelligence)",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image superresolution",
                "deep learning techniques",
                "computer vision",
                "low-level image restoration problems",
                "deep neural networks",
                "data-driven models",
                "sparse coding model",
                "restoration accuracy",
                "human subjective quality"
            ]
        },
        "id": 41,
        "cited_by": [
            {
                "year": "2017",
                "id": 26
            },
            {
                "year": "2017",
                "id": 264
            },
            {
                "year": "2017",
                "id": 476
            },
            {
                "year": "2017",
                "id": 484
            }
        ]
    },
    {
        "title": "Convolutional Color Constancy",
        "authors": [
            "Jonathan T. Barron"
        ],
        "abstract": "Color constancy is the problem of inferring the color of the light that illuminated a scene, usually so that the illumination color can be removed. Because this problem is underconstrained, it is often solved by modeling the statistical regularities of the colors of natural objects and illumination. In contrast, in this paper we reformulate the problem of color constancy as a 2D spatial localization task in a log-chrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the color constancy problem. By directly learning how to discriminate between correctly white-balanced images and poorly white-balanced images, our model is able to improve performance on standard benchmarks by nearly 40%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410408",
        "reference_list": [
            {
                "year": "2013",
                "id": 430
            },
            {
                "year": "2013",
                "id": 237
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 14,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Histograms",
                "Lighting",
                "Object detection",
                "Training",
                "Convolution",
                "Cognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "learning (artificial intelligence)",
                "object detection",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convolutional color constancy",
                "illumination color",
                "statistical regularity",
                "2D spatial localization task",
                "object detection",
                "learning",
                "log-chrominance space",
                "white-balanced image"
            ]
        },
        "id": 42,
        "cited_by": [
            {
                "year": "2017",
                "id": 572
            }
        ]
    },
    {
        "title": "Learning Ordinal Relationships for Mid-Level Vision",
        "authors": [
            "Daniel Zoran",
            "Phillip Isola",
            "Dilip Krishnan",
            "William T. Freeman"
        ],
        "abstract": "We propose a framework that infers mid-level visual properties of an image by learning about ordinal relationships. Instead of estimating metric quantities directly, the system proposes pairwise relationship estimates for points in the input image. These sparse probabilistic ordinal measurements are globalized to create a dense output map of continuous metric measurements. Estimating order relationships between pairs of points has several advantages over metric estimation: it solves a simpler problem than metric regression, humans are better at relative judgements, so data collection is easier, ordinal relationships are invariant to monotonic transformations of the data, thereby increasing the robustness of the system and providing qualitatively different information. We demonstrate that this frame-work works well on two important mid-level vision tasks: intrinsic image decomposition and depth from an RGB image. We train two systems with the same architecture on data from these two modalities. We provide an analysis of the resulting models, showing that they learn a number of simple rules to make ordinal decisions. We apply our algorithm to depth estimation, with good results, and intrinsic image decomposition, with state-of-the-art results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410409",
        "reference_list": [
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2009",
                "id": 1
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2015",
                "id": 387
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 12,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Estimation",
                "Image edge detection",
                "Image decomposition",
                "Context",
                "Image segmentation",
                "Marine vehicles"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "estimation theory",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "midlevel visual property",
                "sparse probabilistic ordinal measurement",
                "continuous metric measurement",
                "intrinsic image decomposition",
                "RGB image",
                "depth estimation",
                "midlevel vision",
                "ordinal relationship"
            ]
        },
        "id": 43,
        "cited_by": [
            {
                "year": "2017",
                "id": 163
            },
            {
                "year": "2015",
                "id": 387
            }
        ]
    },
    {
        "title": "Thin Structure Estimation with Curvature Regularization",
        "authors": [
            "Dmitrii Marin",
            "Yuchen Zhong",
            "Maria Drangova",
            "Yuri Boykov"
        ],
        "abstract": "Many applications in vision require estimation of thin structures such as boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most previous approaches, we simultaneously detect and delineate thin structures with sub-pixel localization and real-valued orientation estimation. This is an ill-posed problem that requires regularization. We propose an objective function combining detection likelihoods with a prior minimizing curvature of the center-lines or surfaces. Unlike simple block-coordinate descent, we develop a novel algorithm that is able to perform joint optimization of location and detection variables more effectively. Our lower bound optimization algorithm applies to quadratic or absolute curvature. The proposed early vision framework is sufficiently general and it can be used in many higher-level applications. We illustrate the advantage of our approach on a range of 2D and 3D examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410410",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2013",
                "id": 366
            },
            {
                "year": "2009",
                "id": 2
            },
            {
                "year": "2007",
                "id": 80
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Optimization",
                "Estimation",
                "Three-dimensional displays",
                "Context",
                "Surface treatment",
                "Surface reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "estimation theory",
                "object detection",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "thin structure estimation",
                "curvature regularization",
                "computer vision",
                "subpixel localization",
                "real-valued orientation estimation",
                "detection likelihood",
                "lower bound optimization algorithm",
                "quadratic curvature",
                "absolute curvature"
            ]
        },
        "id": 44,
        "cited_by": []
    },
    {
        "title": "HARF: Hierarchy-Associated Rich Features for Salient Object Detection",
        "authors": [
            "Wenbin Zou",
            "Nikos Komodakis"
        ],
        "abstract": "The state-of-the-art salient object detection models are able to perform well for relatively simple scenes, yet for more complex ones, they still have difficulties in highlighting salient objects completely from background, largely due to the lack of sufficiently robust features for saliency prediction. To address such an issue, this paper proposes a novel hierarchy-associated feature construction framework for salient object detection, which is based on integrating elementary features from multi-level regions in a hierarchy. Furthermore, multi-layered deep learning features are introduced and incorporated as elementary features into this framework through a compact integration scheme. This leads to a rich feature representation, which is able to represent the context of the whole object/background and is much more discriminative as well as robust for salient object detection. Extensive experiments on the most widely used and challenging benchmark datasets demonstrate that the proposed approach substantially outperforms the state-of-the-art on salient object detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410411",
        "reference_list": [
            {
                "year": "2011",
                "id": 115
            },
            {
                "year": "2013",
                "id": 190
            },
            {
                "year": "2013",
                "id": 219
            },
            {
                "year": "2013",
                "id": 207
            },
            {
                "year": "2013",
                "id": 415
            },
            {
                "year": "2013",
                "id": 371
            },
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Object detection",
                "Image segmentation",
                "Robustness",
                "Computational modeling",
                "Visualization",
                "Biological system modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "HARF",
                "hierarchy-associated rich features",
                "salient object detection",
                "saliency prediction",
                "hierarchy-associated feature construction framework",
                "elementary features",
                "multilevel region",
                "multilayered deep learning features",
                "compact integration scheme",
                "feature representation"
            ]
        },
        "id": 45,
        "cited_by": []
    },
    {
        "title": "Deep Colorization",
        "authors": [
            "Zezhou Cheng",
            "Qingxiong Yang",
            "Bin Sheng"
        ],
        "abstract": "This paper investigates into the colorization problem which converts a grayscale image to a colorful version. This is a very difficult problem and normally requires manual adjustment to achieve artifact-free quality. For instance, it normally requires human-labelled color scribbles on the grayscale target image or a careful selection of colorful reference images (e.g., capturing the same scene in the grayscale target image). Unlike the previous methods, this paper aims at a high-quality fully-automatic colorization method. With the assumption of a perfect patch matching technique, the use of an extremely large-scale reference database (that contains sufficient color images) is the most reliable solution to the colorization problem. However, patch matching noise will increase with respect to the size of the reference database in practice. Inspired by the recent success in deep learning techniques which provide amazing modeling of large-scale data, this paper re-formulates the colorization problem so that deep learning techniques can be directly employed. To ensure artifact-free quality, a joint bilateral filtering based post-processing step is proposed. Numerous experiments demonstrate that our method outperforms the state-of-art algorithms both in terms of quality and speed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410412",
        "reference_list": [
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2013",
                "id": 15
            }
        ],
        "citation": {
            "ieee": 47,
            "other": 44,
            "total": 91
        },
        "keywords": {
            "IEEE Keywords": [
                "Gray-scale",
                "Image color analysis",
                "Feature extraction",
                "Neural networks",
                "Neurons",
                "Machine learning",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image filtering",
                "image matching",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep colorization",
                "grayscale image",
                "colorful image",
                "human-labelled color",
                "colorful reference images",
                "high-quality fully-automatic colorization method",
                "patch matching technique",
                "extremely large-scale reference database",
                "patch matching noise",
                "deep learning techniques",
                "joint bilateral filtering based post-processing"
            ]
        },
        "id": 46,
        "cited_by": [
            {
                "year": "2017",
                "id": 345
            },
            {
                "year": "2017",
                "id": 478
            }
        ]
    },
    {
        "title": "Image Matting with KL-Divergence Based Sparse Sampling",
        "authors": [
            "Levent Karacan",
            "Aykut Erdem",
            "Erkut Erdem"
        ],
        "abstract": "Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410413",
        "reference_list": [
            {
                "year": "2005",
                "id": 122
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 8,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Feature extraction",
                "Atmospheric measurements",
                "Particle measurements",
                "Robustness",
                "Mathematical model",
                "Linear programming"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image sampling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "KL-divergence based sparse sampling",
                "sampling-based image matting methods",
                "sparse subset selection problem",
                "feature extraction"
            ]
        },
        "id": 47,
        "cited_by": []
    },
    {
        "title": "Intrinsic Decomposition of Image Sequences from Local Temporal Variations",
        "authors": [
            "Pierre-Yves Laffont",
            "Jean-Charles Bazin"
        ],
        "abstract": "We present a method for intrinsic image decomposition, which aims to decompose images into reflectance and shading layers. Our input is a sequence of images with varying illumination acquired by a static camera, e.g. an indoor scene with a moving light source or an outdoor timelapse. We leverage the local color variations observed over time to infer constraints on the reflectance and solve the ill-posed image decomposition problem. In particular, we derive an adaptive local energy from the observations of each local neighborhood over time, and integrate distant pairwise constraints to enforce coherent decomposition across all surfaces with consistent shading changes. Our method is solely based on multiple observations of a Lambertian scene under varying illumination and does not require user interaction, scene geometry, or an explicit lighting model. We compare our results with several intrinsic decomposition methods on a number of synthetic and captured datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410414",
        "reference_list": [
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Image color analysis",
                "Light sources",
                "Geometry",
                "Robustness",
                "Image decomposition",
                "Image sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "geometry",
                "image colour analysis",
                "image segmentation",
                "image sequences",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "intrinsic image sequence decomposition methods",
                "local temporal variations",
                "shading layers",
                "static camera",
                "indoor scene",
                "outdoor timelapse",
                "local color variations",
                "ill-posed image decomposition problem",
                "distant pairwise constraints",
                "coherent decomposition",
                "Lambertian scene",
                "user interaction",
                "scene geometry"
            ]
        },
        "id": 48,
        "cited_by": [
            {
                "year": "2017",
                "id": 330
            }
        ]
    },
    {
        "title": "Low-Rank Tensor Approximation with Laplacian Scale Mixture Modeling for Multiframe Image Denoising",
        "authors": [
            "Weisheng Dong",
            "Guangyu Li",
            "Guangming Shi",
            "Xin Li",
            "Yi Ma"
        ],
        "abstract": "Patch-based low-rank models have shown effective in exploiting spatial redundancy of natural images especially for the application of image denoising. However, two-dimensional low-rank model can not fully exploit the spatio-temporal correlation in larger data sets such as multispectral images and 3D MRIs. In this work, we propose a novel low-rank tensor approximation framework with Laplacian Scale Mixture (LSM) modeling for multi-frame image denoising. First, similar 3D patches are grouped to form a tensor of d-order and high-order Singular Value Decomposition (HOSVD) is applied to the grouped tensor. Then the task of multiframe image denoising is formulated as a Maximum A Posterior (MAP) estimation problem with the LSM prior for tensor coefficients. Both unknown sparse coefficients and hidden LSM parameters can be efficiently estimated by the method of alternating optimization. Specifically, we have derived closed-form solutions for both subproblems. Experimental results on spectral and dynamic MRI images show that the proposed algorithm can better preserve the sharpness of important image structures and outperform several existing state-of-the-art multiframe denoising methods (e.g., BM4D and tensor dictionary learning).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410415",
        "reference_list": [
            {
                "year": "2009",
                "id": 292
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 7,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Image denoising",
                "Three-dimensional displays",
                "Laplace equations",
                "Noise reduction",
                "Silicon",
                "Dictionaries"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image denoising",
                "Laplace equations",
                "mixture models",
                "optimisation",
                "redundancy",
                "singular value decomposition",
                "spectral analysis",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Laplacian scale mixture modeling",
                "multiframe image denoising",
                "patch-based low-rank model",
                "spatial redundancy",
                "natural image",
                "two-dimensional low-rank model",
                "spatio-temporal correlation",
                "multispectral image",
                "3D MRI",
                "low-rank tensor approximation framework",
                "LSM modeling",
                "high-order singular value decomposition",
                "HOSVD",
                "grouped tensor",
                "maximum a posterior estimation problem",
                "MAP estimation problem",
                "LSM prior",
                "tensor coefficient",
                "tensor dictionary learning",
                "BM4D",
                "multiframe denoising method",
                "image structure",
                "dynamic MRI image",
                "spectral MRI image",
                "closed-form solution",
                "alternating optimization",
                "hidden LSM parameter",
                "sparse coefficient"
            ]
        },
        "id": 49,
        "cited_by": [
            {
                "year": "2017",
                "id": 25
            },
            {
                "year": "2017",
                "id": 181
            }
        ]
    },
    {
        "title": "Learning Parametric Distributions for Image Super-Resolution: Where Patch Matching Meets Sparse Coding",
        "authors": [
            "Yongbo Li",
            "Weisheng Dong",
            "Guangming Shi",
            "Xuemei Xie"
        ],
        "abstract": "Existing approaches toward Image super-resolution (SR) is often either data-driven (e.g., based on internet-scale matching and web image retrieval) or model-based (e.g., formulated as an Maximizing a Posterior estimation problem). The former is conceptually simple yet heuristic, while the latter is constrained by the fundamental limit of frequency aliasing. In this paper, we propose to develop a hybrid approach toward SR by combining those two lines of ideas. More specifically, the parameters underlying sparse distributions of desirable HR image patches are learned from a pair of LR image and retrieved HR images. Our hybrid approach can be interpreted as the first attempt of reconciling the difference between parametric and nonparametric models for low-level vision tasks. Experimental results show that the proposed hybrid SR method performs much better than existing state-of-the-art methods in terms of both subjective and objective image qualities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410416",
        "reference_list": [
            {
                "year": "2011",
                "id": 159
            },
            {
                "year": "2013",
                "id": 416
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 3,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Image retrieval",
                "Image reconstruction",
                "Dictionaries",
                "Laplace equations",
                "Visualization",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "image resolution",
                "image retrieval",
                "Internet",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning parametric distribution",
                "image super-resolution",
                "patch matching",
                "sparse coding",
                "Internet-scale matching",
                "Web image retrieval",
                "frequency aliasing",
                "sparse distribution",
                "HR image patch",
                "LR image",
                "retrieved HR image",
                "nonparametric model",
                "low-level vision task",
                "subjective image quality",
                "objective image quality"
            ]
        },
        "id": 50,
        "cited_by": []
    },
    {
        "title": "Improving Image Restoration with Soft-Rounding",
        "authors": [
            "Xing Mei",
            "Honggang Qi",
            "Bao-Gang Hu",
            "Siwei Lyu"
        ],
        "abstract": "Several important classes of images such as text, barcode and pattern images have the property that pixels can only take a distinct subset of values. This knowledge can benefit the restoration of such images, but it has not been widely considered in current restoration methods. In this work, we describe an effective and efficient approach to incorporate the knowledge of distinct pixel values of the pristine images into the general regularized least squares restoration framework. We introduce a new regularizer that attains zero at the designated pixel values and becomes a quadratic penalty function in the intervals between them. When incorporated into the regularized least squares restoration framework, this regularizer leads to a simple and efficient step that resembles and extends the rounding operation, which we term as soft-rounding. We apply the soft-rounding enhanced solution to the restoration of binary text/barcode images and pattern images with multiple distinct pixel values. Experimental results show that soft-rounding enhanced restoration methods achieve significant improvement in both visual quality and quantitative measures (PSNR and SSIM). Furthermore, we show that this regularizer can also benefit the restoration of general natural images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410417",
        "reference_list": [
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Image restoration",
                "Histograms",
                "Kernel",
                "Visualization",
                "Noise measurement",
                "Transforms",
                "Convolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image enhancement",
                "image resolution",
                "image restoration",
                "least squares approximations",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "regularized least square restoration framework",
                "quadratic penalty function",
                "binary text image restoration",
                "binary barcode image restoration",
                "multiple distinct pixel value",
                "soft-rounding enhanced restoration method",
                "visual quality",
                "PSNR",
                "SSIM",
                "general natural image restoration"
            ]
        },
        "id": 51,
        "cited_by": []
    },
    {
        "title": "See the Difference: Direct Pre-Image Reconstruction and Pose Estimation by Differentiating HOG",
        "authors": [
            "Wei-Chen Chiu",
            "Mario Fritz"
        ],
        "abstract": "The Histogram of Oriented Gradient (HOG) descriptor has led to many advances in computer vision over the last decade and is still part of many state of the art approaches. We realize that the associated feature computation is piecewise differentiable and therefore many pipelines which build on HOG can be made differentiable. This lends to advanced introspection as well as opportunities for end-to-end optimization. We present our implementation of \u0394HOG based on the auto-differentiation toolbox Chumpy [18] and show applications to pre-image visualization and pose estimation which extends the existing differentiable renderer OpenDR [19] pipeline. Both applications improve on the respective state-of-the-art HOG approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410418",
        "reference_list": [
            {
                "year": "2013",
                "id": 373
            },
            {
                "year": "2005",
                "id": 198
            },
            {
                "year": "2011",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Computational modeling",
                "Design automation",
                "Pipelines",
                "Histograms",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "gradient methods",
                "image reconstruction",
                "optimisation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pre-image reconstruction",
                "pose estimation",
                "HOG differentiation",
                "histogram of oriented gradient",
                "computer vision",
                "end-to-end optimization"
            ]
        },
        "id": 52,
        "cited_by": [
            {
                "year": "2015",
                "id": 106
            }
        ]
    },
    {
        "title": "An Efficient Statistical Method for Image Noise Level Estimation",
        "authors": [
            "Guangyong Chen",
            "Fengyuan Zhu",
            "Pheng Ann Heng"
        ],
        "abstract": "In this paper, we address the problem of estimating noise level from a single image contaminated by additive zero-mean Gaussian noise. We first provide rigorous analysis on the statistical relationship between the noise variance and the eigenvalues of the covariance matrix of patches within an image, which shows that many state-of-the-art noise estimation methods underestimate the noise level of an image. To this end, we derive a new nonparametric algorithm for efficient noise level estimation based on the observation that patches decomposed from a clean image often lie around a low-dimensional subspace. The performance of our method has been guaranteed both theoretically and empirically. Specifically, our method outperforms existing state-of-the-art algorithms on estimating noise level with the least executing time in our experiments. We further demonstrate that the denoising algorithm BM3D algorithm achieves optimal performance using noise variance estimated by our algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410419",
        "reference_list": [],
        "citation": {
            "ieee": 24,
            "other": 15,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Eigenvalues and eigenfunctions",
                "Noise level",
                "Estimation",
                "Gaussian distribution",
                "Mathematical model",
                "Covariance matrices",
                "Random variables"
            ],
            "INSPEC: Controlled Indexing": [
                "covariance matrices",
                "eigenvalues and eigenfunctions",
                "image denoising",
                "nonparametric statistics",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "noise variance estimation",
                "optimal performance",
                "BM3D denoising algorithm",
                "low-dimensional subspace",
                "clean image patches",
                "nonparametric algorithm",
                "image noise level",
                "covariance matrix",
                "eigenvalues",
                "noise variance",
                "statistical method",
                "image noise level estimation",
                "additive zero-mean Gaussian noise"
            ]
        },
        "id": 53,
        "cited_by": [
            {
                "year": "2017",
                "id": 114
            },
            {
                "year": "2017",
                "id": 115
            }
        ]
    },
    {
        "title": "Contour Detection and Characterization for Asynchronous Event Sensors",
        "authors": [
            "Francisco Barranco",
            "Ching L. Teo",
            "Cornelia Ferm\u00fcller",
            "Yiannis Aloimonos"
        ],
        "abstract": "The bio-inspired, asynchronous event-based dynamic vision sensor records temporal changes in the luminance of the scene at high temporal resolution. Since events are only triggered at significant luminance changes, most events occur at the boundary of objects and their parts. The detection of these contours is an essential step for further interpretation of the scene. This paper presents an approach to learn the location of contours and their border ownership using Structured Random Forests on event-based features that encode motion, timing, texture, and spatial orientations. The classifier integrates elegantly information over time by utilizing the classification results previously computed. Finally, the contour detection and boundary assignment are demonstrated in a layer-segmentation of the scene. Experimental results demonstrate good performance in boundary detection and segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410420",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2013",
                "id": 221
            },
            {
                "year": "2009",
                "id": 71
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Motion segmentation",
                "Voltage control",
                "Computer vision",
                "Image segmentation",
                "Feature extraction",
                "Sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "brightness",
                "computer vision",
                "image classification",
                "image resolution",
                "image segmentation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "boundary segmentation",
                "boundary detection",
                "scene layer-segmentation",
                "boundary assignment",
                "structured random forest",
                "temporal resolution",
                "bio-inspired asynchronous event-based dynamic vision sensor record",
                "contour detection"
            ]
        },
        "id": 54,
        "cited_by": []
    },
    {
        "title": "Class-Specific Image Deblurring",
        "authors": [
            "Saeed Anwar",
            "Cong Phuoc Huynh",
            "Fatih Porikli"
        ],
        "abstract": "In image deblurring, a fundamental problem is that the blur kernel suppresses a number of spatial frequencies that are difficult to recover reliably. In this paper, we explore the potential of a class-specific image prior for recovering spatial frequencies attenuated by the blurring process. Specifically, we devise a prior based on the class-specific subspace of image intensity responses to band-pass filters. We learn that the aggregation of these subspaces across all frequency bands serves as a good class-specific prior for the restoration of frequencies that cannot be recovered with generic image priors. In an extensive validation, our method, equipped with the above prior, yields greater image quality than many state-of-the-art methods by up to 5 dB in terms of image PSNR, across various image categories including portraits, cars, cats, pedestrians and household objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410421",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image restoration",
                "Training",
                "Image edge detection",
                "Band-pass filters",
                "Frequency-domain analysis",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "band-pass filters",
                "image filtering",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image PSNR",
                "image quality",
                "frequency restoration",
                "class-specific subspace",
                "band-pass filters",
                "image intensity responses",
                "spatial frequency recovery",
                "class-specific image prior",
                "class-specific image deblurring"
            ]
        },
        "id": 55,
        "cited_by": [
            {
                "year": "2017",
                "id": 112
            }
        ]
    },
    {
        "title": "High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and Its Applications to High-Level Vision",
        "authors": [
            "Gedas Bertasius",
            "Jianbo Shi",
            "Lorenzo Torresani"
        ],
        "abstract": "Most of the current boundary detection systems rely exclusively on low-level features, such as color and texture. However, perception studies suggest that humans employ object-level reasoning when judging if a particular pixel is a boundary. Inspired by this observation, in this work we show how to predict boundaries by exploiting object-level features from a pretrained object-classification network. Our method can be viewed as a \"High-for-Low\" approach where high-level object features inform the low-level boundary detection process. Our model achieves state-of-the-art performance on an established boundary detection benchmark and it is efficient to run. Additionally, we show that due to the semantic nature of our boundaries we can use them to aid a number of high-level vision tasks. We demonstrate that using our boundaries we improve the performance of state-of-the-art methods on the problems of semantic boundary labeling, semantic segmentation and object proposal generation. We can view this process as a \"Low-for-High'\" scheme, where low-level boundaries aid high-level vision tasks. Thus, our contributions include a boundary detection system that is accurate, efficient, generalizes well to multiple datasets, and is also shown to improve existing state-of-the-art high-level vision methods on three distinct tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410422",
        "reference_list": [
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2011",
                "id": 11
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 18,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Semantics",
                "Convolutional codes",
                "Interpolation",
                "Training",
                "Image edge detection",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction",
                "image classification",
                "image colour analysis",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "low-level boundary detection process",
                "deep object features",
                "high-level vision tasks",
                "low-level color feature",
                "low-level texture feature",
                "boundary prediction",
                "object-level features",
                "pretrained object-classification network",
                "high-for-low approach",
                "semantic boundary labeling",
                "semantic segmentation",
                "object proposal generation"
            ]
        },
        "id": 56,
        "cited_by": []
    },
    {
        "title": "Variational Depth Superresolution Using Example-Based Edge Representations",
        "authors": [
            "David Ferstl",
            "Matthias R\u00fcther",
            "Horst Bischof"
        ],
        "abstract": "In this paper we propose a novel method for depth image superresolution which combines recent advances in example based upsampling with variational superresolution based on a known blur kernel. Most traditional depth superresolution approaches try to use additional high resolution intensity images as guidance for superresolution. In our method we learn a dictionary of edge priors from an external database of high and low resolution examples. In a novel variational sparse coding approach this dictionary is used to infer strong edge priors. Additionally to the traditional sparse coding constraints the difference in the overlap of neighboring edge patches is minimized in our optimization. These edge priors are used in a novel variational superresolution as anisotropic guidance of the higher order regularization. Both the sparse coding and the variational superresolution of the depth are solved based on a primal-dual formulation. In an exhaustive numerical and visual evaluation we show that our method clearly outperforms existing approaches on multiple real and synthetic datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410423",
        "reference_list": [
            {
                "year": "2013",
                "id": 123
            },
            {
                "year": "2013",
                "id": 117
            },
            {
                "year": "2011",
                "id": 205
            },
            {
                "year": "2013",
                "id": 239
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 14,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Spatial resolution",
                "Image edge detection",
                "Image reconstruction",
                "Encoding",
                "Energy resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image resolution",
                "image sampling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational depth superresolution",
                "example-based edge representation",
                "depth image superresolution",
                "example based upsampling",
                "variational superresolution",
                "blur kernel",
                "depth superresolution approach",
                "high resolution intensity image",
                "edge prior",
                "external database",
                "resolution example",
                "variational sparse coding approach",
                "sparse coding constraint",
                "neighboring edge patch",
                "anisotropic guidance",
                "higher order regularization",
                "primal-dual formulation",
                "numerical evaluation",
                "visual evaluation",
                "real dataset",
                "synthetic dataset"
            ]
        },
        "id": 57,
        "cited_by": []
    },
    {
        "title": "Conditioned Regression Models for Non-blind Single Image Super-Resolution",
        "authors": [
            "Gernot Riegler",
            "Samuel Schulter",
            "Matthias R\u00fcther",
            "Horst Bischof"
        ],
        "abstract": "Single image super-resolution is an important task in the field of computer vision and finds many practical applications. Current state-of-the-art methods typically rely on machine learning algorithms to infer a mapping from low-to high-resolution images. These methods use a single fixed blur kernel during training and, consequently, assume the exact same kernel underlying the image formation process for all test images. However, this setting is not realistic for practical applications, because the blur is typically different for each test image. In this paper, we loosen this restrictive constraint and propose conditioned regression models (including convolutional neural networks and random forests) that can effectively exploit the additional kernel information during both, training and inference. This allows for training a single model, while previous methods need to be re-trained for every blur kernel individually to achieve good results, which we demonstrate in our evaluations. We also empirically show that the proposed conditioned regression models (i) can effectively handle scenarios where the blur kernel is different for each image and (ii) outperform related approaches trained for only a single kernel.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410424",
        "reference_list": [
            {
                "year": "2013",
                "id": 353
            },
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2013",
                "id": 117
            },
            {
                "year": "2013",
                "id": 239
            },
            {
                "year": "2005",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 5,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image resolution",
                "Training",
                "Dictionaries",
                "Adaptation models",
                "Computer vision",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image resolution",
                "image restoration",
                "learning (artificial intelligence)",
                "neural nets",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "conditioned regression models",
                "nonblind single image superresolution",
                "computer vision",
                "machine learning algorithms",
                "high-resolution images",
                "low-resolution images",
                "fixed blur kernel",
                "image formation process",
                "convolutional neural networks",
                "random forests"
            ]
        },
        "id": 58,
        "cited_by": []
    },
    {
        "title": "Video Super-Resolution via Deep Draft-Ensemble Learning",
        "authors": [
            "Renjie Liao",
            "Xin Tao",
            "Ruiyu Li",
            "Ziyang Ma",
            "Jiaya Jia"
        ],
        "abstract": "We propose a new direction for fast video super-resolution (VideoSR) via a SR draft ensemble, which is defined as the set of high-resolution patch candidates before final image deconvolution. Our method contains two main components -- i.e., SR draft ensemble generation and its optimal reconstruction. The first component is to renovate traditional feedforward reconstruction pipeline and greatly enhance its ability to compute different super resolution results considering large motion variation and possible errors arising in this process. Then we combine SR drafts through the nonlinear process in a deep convolutional neural network (CNN). We analyze why this framework is proposed and explain its unique advantages compared to previous iterative methods to update different modules in passes. Promising experimental results are shown on natural video sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410425",
        "reference_list": [
            {
                "year": "2013",
                "id": 78
            }
        ],
        "citation": {
            "ieee": 37,
            "other": 11,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Image resolution",
                "Deconvolution",
                "Feedforward neural networks",
                "Motion estimation",
                "Optical imaging",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "deconvolution",
                "image motion analysis",
                "image reconstruction",
                "image resolution",
                "image sequences",
                "neural nets",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "natural video sequences",
                "VideoSR",
                "CNN",
                "deep convolutional neural network",
                "motion variation",
                "feedforward reconstruction pipeline",
                "SR draft ensemble generation",
                "image deconvolution",
                "high-resolution patch candidates",
                "deep draft-ensemble learning",
                "video super-resolution"
            ]
        },
        "id": 59,
        "cited_by": [
            {
                "year": "2017",
                "id": 263
            },
            {
                "year": "2017",
                "id": 469
            }
        ]
    },
    {
        "title": "Pan-Sharpening with a Hyper-Laplacian Penalty",
        "authors": [
            "Yiyong Jiang",
            "Xinghao Ding",
            "Delu Zeng",
            "Yue Huang",
            "John Paisley"
        ],
        "abstract": "Pan-sharpening is the task of fusing spectral information in low resolution multispectral images with spatial information in a corresponding high resolution panchromatic image. In such approaches, there is a trade-off between spectral and spatial quality, as well as computational efficiency. We present a method for pan-sharpening in which a sparsity-promoting objective function preserves both spatial and spectral content, and is efficient to optimize. Our objective incorporates the l 1/2 -norm in a way that can leverage recent computationally efficient methods, and l 1 for which the alternating direction method of multipliers can be used. Additionally, our objective penalizes image gradients to enforce high resolution fidelity, and exploits the Fourier domain forfurther computational efficiency. Visual quality metrics demonstrate that our proposed objective function can achieve higher spatial and spectral resolution than several previous well-known methods with competitive computational efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410426",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "TV",
                "Image reconstruction",
                "Linear programming",
                "Laplace equations",
                "Spatial resolution",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "Fourier analysis",
                "image fusion",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "objective function",
                "visual quality metrics",
                "computational efficiency",
                "Fourier domain",
                "high resolution fidelity",
                "image gradients",
                "multipliers alternating direction method",
                "l1/2-norm",
                "spectral content preservation",
                "spatial content preservation",
                "sparsity-promoting objective function",
                "high resolution panchromatic image",
                "low resolution multispectral images",
                "spectral information fusion",
                "hyper-Laplacian penalty",
                "pan-sharpening"
            ]
        },
        "id": 60,
        "cited_by": [
            {
                "year": "2017",
                "id": 183
            }
        ]
    },
    {
        "title": "Video Restoration Against Yin-Yang Phasing",
        "authors": [
            "Xiaolin Wu",
            "Zhenhao Li",
            "Xiaowei Deng"
        ],
        "abstract": "A common video degradation problem, which is largely untreated in literature, is what we call Yin-Yang Phasing (YYP). YYP is characterized by involuntary, dramatic flip-flop in the intensity and possibly chromaticity of an object as the video plays. Such temporal artifacts occur under ill illumination conditions and are triggered by object or/and camera motions, which mislead the settings of camera's auto-exposure and white point. In this paper, we investigate the problem and propose a video restoration technique to suppress YYP artifacts and retain temporal consistency of objects appearance via inter-frame, spatially-adaptive, optimal tone mapping. The video quality can be further improved by a novel image enhancer designed in Weber's perception principle and by exploiting the second-order statistics of the scene. Experimental results are encouraging, pointing to an effective, practical solution for a common but surprisingly understudied problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410427",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Cameras",
                "Histograms",
                "Image restoration",
                "Degradation",
                "Visualization",
                "Spatial resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image enhancement",
                "image motion analysis",
                "image restoration",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Yin-Yang phasing",
                "video degradation problem",
                "YYP",
                "illumination conditions",
                "camera motions",
                "camera auto-exposure",
                "video restoration technique",
                "temporal consistency",
                "optimal tone mapping",
                "video quality",
                "image enhancer",
                "Weber perception principle",
                "second-order statistics"
            ]
        },
        "id": 61,
        "cited_by": []
    },
    {
        "title": "Rolling Shutter Super-Resolution",
        "authors": [
            "Abhijith Punnappurath",
            "Vijay Rengarajan",
            "A. N. Rajagopalan"
        ],
        "abstract": "Classical multi-image super-resolution (SR) algorithms, designed for CCD cameras, assume that the motion among the images is global. But CMOS sensors that have increasingly started to replace their more expensive CCD counterparts in many applications do not respect this assumption if there is a motion of the camera relative to the scene during the exposure duration of an image because of the row-wise acquisition mechanism. In this paper, we study the hitherto unexplored topic of multi-image SR in CMOS cameras. We initially develop an SR observation model that accounts for the row-wise distortions called the \"rolling shutter\" (RS) effect observed in images captured using non-stationary CMOS cameras. We then propose a unified RS-SR framework to obtain an RS-free high-resolution image (and the row-wise motion) from distorted low-resolution images. We demonstrate the efficacy of the proposed scheme using synthetic data as well as real images captured using a hand-held CMOS camera. Quantitative and qualitative assessments reveal that our method significantly advances the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410428",
        "reference_list": [
            {
                "year": "2013",
                "id": 251
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image resolution",
                "CMOS integrated circuits",
                "Signal resolution",
                "Charge coupled devices",
                "Sensor arrays"
            ],
            "INSPEC: Controlled Indexing": [
                "CMOS image sensors",
                "image capture",
                "image motion analysis",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "rolling shutter superresolution",
                "RS-SR framework",
                "multiimage SR",
                "CMOS camera",
                "SR observation model",
                "image capture",
                "row-wise motion"
            ]
        },
        "id": 62,
        "cited_by": [
            {
                "year": "2017",
                "id": 422
            }
        ]
    },
    {
        "title": "Learning Large-Scale Automatic Image Colorization",
        "authors": [
            "Aditya Deshpande",
            "Jason Rock",
            "David Forsyth"
        ],
        "abstract": "We describe an automated method for image colorization that learns to colorize from examples. Our method exploits a LEARCH framework to train a quadratic objective function in the chromaticity maps, comparable to a Gaussian random field. The coefficients of the objective function are conditioned on image features, using a random forest. The objective function admits correlations on long spatial scales, and can control spatial error in the colorization of the image. Images are then colorized by minimizing this objective function. We demonstrate that our method strongly outperforms a natural baseline on large-scale experiments with images of real scenes using a demanding loss function. We demonstrate that learning a model that is conditioned on scene produces improved results. We show how to incorporate a desired color histogram into the objective function, and that doing so can lead to further improvements in results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410429",
        "reference_list": [
            {
                "year": "2009",
                "id": 300
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 17,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Linear programming",
                "Image color analysis",
                "Optimization",
                "Histograms",
                "Regression tree analysis",
                "Color",
                "Standards"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large-scale automatic image colorization learning",
                "LEARCH framework",
                "quadratic objective function",
                "chromaticity maps",
                "random forest",
                "spatial error control",
                "color histogram"
            ]
        },
        "id": 63,
        "cited_by": []
    },
    {
        "title": "Compression Artifacts Reduction by a Deep Convolutional Network",
        "authors": [
            "Chao Dong",
            "Yubin Deng",
            "Chen Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Lossy compression introduces complex compression artifacts, particularly the blocking artifacts, ringing effects and blurring. Existing algorithms either focus on removing blocking artifacts and produce blurred output, or restores sharpened images that are accompanied with ringing effects. Inspired by the deep convolutional networks (DCN) on super-resolution, we formulate a compact and efficient network for seamless attenuation of different compression artifacts. We also demonstrate that a deeper model can be effectively trained with the features learned in a shallow network. Following a similar \"easy to hard\" idea, we systematically investigate several practical transfer settings and show the effectiveness of transfer learning in low level vision problems. Our method shows superior performance than the state-of-the-arts both on the benchmark datasets and the real-world use cases (i.e. Twitter).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410430",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            }
        ],
        "citation": {
            "ieee": 112,
            "other": 40,
            "total": 152
        },
        "keywords": {
            "IEEE Keywords": [
                "Image coding",
                "Feature extraction",
                "Image restoration",
                "Image resolution",
                "Noise measurement",
                "Image reconstruction",
                "Transform coding"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "image resolution",
                "image restoration",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "compression artifacts reduction",
                "deep convolutional network",
                "lossy compression",
                "complex compression artifacts",
                "blocking artifacts",
                "ringing effect",
                "blurring",
                "image restoration",
                "image superresolution",
                "transfer learning",
                "low level vision problems"
            ]
        },
        "id": 64,
        "cited_by": [
            {
                "year": "2017",
                "id": 27
            },
            {
                "year": "2017",
                "id": 472
            },
            {
                "year": "2017",
                "id": 476
            },
            {
                "year": "2017",
                "id": 478
            },
            {
                "year": "2017",
                "id": 507
            }
        ]
    },
    {
        "title": "Multiple-Hypothesis Affine Region Estimation with Anisotropic LoG Filters",
        "authors": [
            "Takahiro Hasegawa",
            "Mitsuru Ambai",
            "Kohta Ishikawa",
            "Gou Koutaki",
            "Yuji Yamauchi",
            "Takayoshi Yamashita",
            "Hironobu Fujiyoshi"
        ],
        "abstract": "We propose a method for estimating multiple-hypothesis affine regions from a keypoint by using an anisotropic Laplacian-of-Gaussian (LoG) filter. Although conventional affine region detectors, such as Hessian/Harris-Affine, iterate to find an affine region that fits a given image patch, such iterative searching is adversely affected by an initial point. To avoid this problem, we allow multiple detections from a single keypoint. We demonstrate that the responses of all possible anisotropic LoG filters can be efficiently computed by factorizing them in a similar manner to spectral SIFT. A large number of LoG filters that are densely sampled in a parameter space are reconstructed by a weighted combination of a limited number of representative filters, called \"eigenfilters\", by using singular value decomposition. Also, the reconstructed filter responses of the sampled parameters can be interpolated to a continuous representation by using a series of proper functions. This results in efficient multiple extrema searching in a continuous space. Experiments revealed that our method has higher repeatability than the conventional methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410431",
        "reference_list": [
            {
                "year": "2011",
                "id": 12
            },
            {
                "year": "2011",
                "id": 324
            },
            {
                "year": "2001",
                "id": 69
            },
            {
                "year": "2011",
                "id": 326
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Eigenvalues and eigenfunctions",
                "Estimation",
                "Convolution",
                "Shape",
                "Image reconstruction",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image filtering",
                "image matching",
                "image representation",
                "singular value decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "continuous space",
                "multiple extrema searching",
                "continuous representation",
                "sampled parameters",
                "reconstructed filter responses",
                "singular value decomposition",
                "eigenfilters",
                "representative filters",
                "parameter space",
                "spectral SIFT",
                "iterative searching",
                "image patch",
                "Hessian/Harris-Affine",
                "affine region detectors",
                "anisotropic Laplacian-of-Gaussian",
                "multiple-hypothesis affine regions",
                "anisotropic LoG filters",
                "multiple-hypothesis affine region estimation"
            ]
        },
        "id": 65,
        "cited_by": [
            {
                "year": "2017",
                "id": 601
            }
        ]
    },
    {
        "title": "A Self-Paced Multiple-Instance Learning Framework for Co-Saliency Detection",
        "authors": [
            "Dingwen Zhang",
            "Deyu Meng",
            "Chao Li",
            "Lu Jiang",
            "Qian Zhao",
            "Junwei Han"
        ],
        "abstract": "As an interesting and emerging topic, co-saliency detection aims at simultaneously extracting common salient objects in a group of images. Traditional co-saliency detection approaches rely heavily on human knowledge for designing hand-crafted metrics to explore the intrinsic patterns underlying co-salient objects. Such strategies, however, always suffer from poor generalization capability to flexibly adapt various scenarios in real applications, especially due to their lack of insightful understanding of the biological mechanisms of human visual co-attention. To alleviate this problem, we propose a novel framework for this task, by naturally reformulating it as a multiple-instance learning (MIL) problem and further integrating it into a self-paced learning (SPL) regime. The proposed framework on one hand is capable of fitting insightful metric measurements and discovering common patterns under co-salient regions in a self-learning way by MIL, and on the other hand tends to promise the learning reliability and stability by simulating the human learning process through SPL. Experiments on benchmark datasets have demonstrated the effectiveness of the proposed framework as compared with the state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410432",
        "reference_list": [
            {
                "year": "2011",
                "id": 43
            },
            {
                "year": "2011",
                "id": 21
            },
            {
                "year": "2013",
                "id": 49
            },
            {
                "year": "2005",
                "id": 235
            },
            {
                "year": "2013",
                "id": 219
            }
        ],
        "citation": {
            "ieee": 24,
            "other": 10,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Visualization",
                "Reliability",
                "Training",
                "Detectors",
                "Search problems",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "object detection",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "self-paced multiple-instance learning framework",
                "co-saliency detection",
                "salient objects extraction",
                "MIL problem",
                "self-paced learning",
                "SPL",
                "metric measurements",
                "patterns discovery",
                "co-salient regions",
                "learning reliability",
                "learning stability",
                "human learning process"
            ]
        },
        "id": 66,
        "cited_by": [
            {
                "year": "2017",
                "id": 76
            },
            {
                "year": "2017",
                "id": 194
            }
        ]
    },
    {
        "title": "External Patch Prior Guided Internal Clustering for Image Denoising",
        "authors": [
            "Fei Chen",
            "Lei Zhang",
            "Huimin Yu"
        ],
        "abstract": "Natural image modeling plays a key role in many vision problems such as image denoising. Image priors are widely used to regularize the denoising process, which is an ill-posed inverse problem. One category of denoising methods exploit the priors (e.g., TV, sparsity) learned from external clean images to reconstruct the given noisy image, while another category of methods exploit the internal prior (e.g., self-similarity) to reconstruct the latent image. Though the internal prior based methods have achieved impressive denoising results, the improvement of visual quality will become very difficult with the increase of noise level. In this paper, we propose to exploit image external patch prior and internal self-similarity prior jointly, and develop an external patch prior guided internal clustering algorithm for image denoising. It is known that natural image patches form multiple subspaces. By utilizing Gaussian mixture models (GMMs) learning, image similar patches can be clustered and the subspaces can be learned. The learned GMMs from clean images are then used to guide the clustering of noisy-patches of the input noisy images, followed by a low-rank approximation process to estimate the latent subspace for image recovery. Numerical experiments show that the proposed method outperforms many state-of-the-art denoising algorithms such as BM3D and WNNM.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410433",
        "reference_list": [
            {
                "year": "2009",
                "id": 292
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 19,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Image denoising",
                "Noise reduction",
                "Covariance matrices",
                "Noise measurement",
                "Image reconstruction",
                "Minimization",
                "Euclidean distance"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image denoising",
                "image reconstruction",
                "learning (artificial intelligence)",
                "mixture models",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image denoising",
                "external patch prior guided internal clustering",
                "natural image modeling",
                "illposed inverse problem",
                "latent image reconstruction",
                "visual quality improvement",
                "natural image patch",
                "Gaussian mixture model learning",
                "GMM learning",
                "low-rank approximation process",
                "image recovery"
            ]
        },
        "id": 67,
        "cited_by": [
            {
                "year": "2017",
                "id": 476
            },
            {
                "year": "2017",
                "id": 506
            }
        ]
    },
    {
        "title": "Self-Calibration of Optical Lenses",
        "authors": [
            "Michael Hirsch",
            "Bernhard Sch\u00f6lkopf"
        ],
        "abstract": "Even high-quality lenses suffer from optical aberrations, especially when used at full aperture. Furthermore, there are significant lens-to-lens deviations due to manufacturing tolerances, often rendering current software solutions like DxO, Lightroom, and PTLens insufficient as they don't adapt and only include generic lens blur models. We propose a method that enables the self-calibration of lenses from a natural image, or a set of such images. To this end we develop a machine learning framework that is able to exploit several recorded images and distills the available information into an accurate model of the considered lens.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410434",
        "reference_list": [
            {
                "year": "2011",
                "id": 58
            },
            {
                "year": "2011",
                "id": 83
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Lenses",
                "Optical imaging",
                "Adaptive optics",
                "Deconvolution",
                "Estimation",
                "Optical variables measurement",
                "Apertures"
            ],
            "INSPEC: Controlled Indexing": [
                "aberrations",
                "calibration",
                "image processing",
                "learning (artificial intelligence)",
                "lenses",
                "optical information processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "self-calibration",
                "optical lenses",
                "high-quality lenses",
                "optical aberrations",
                "lens-to-lens deviations",
                "software solutions",
                "natural image",
                "machine learning"
            ]
        },
        "id": 68,
        "cited_by": []
    },
    {
        "title": "Illumination Robust Color Naming via Label Propagation",
        "authors": [
            "Yuanliu Liu",
            "Zejian Yuan",
            "Badong Chen",
            "Jianru Xue",
            "Nanning Zheng"
        ],
        "abstract": "Color composition is an important property for many computer vision tasks like image retrieval and object classification. In this paper we address the problem of inferring the color composition of the intrinsic reflectance of objects, where the shadows and highlights may change the observed color dramatically. We achieve this through color label propagation without recovering the intrinsic reflectance beforehand. Specifically, the color labels are propagated between regions sharing the same reflectance, and the direction of propagation is promoted to be from regions under full illumination and normal view angles to abnormal regions. We detect shadowed and highlighted regions as well as pairs of regions that have similar reflectance. A joint inference process is adopted to trim the inconsistent identities and connections. For evaluation we collect three datasets of images under noticeable highlights and shadows. Experimental results show that our model can effectively describe the color composition of real-world images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410435",
        "reference_list": [
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2009",
                "id": 58
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Image edge detection",
                "Lighting",
                "Robustness",
                "Histograms",
                "Computer vision",
                "Green products"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image colour analysis",
                "inference mechanisms",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color naming",
                "label propagation",
                "color composition",
                "computer vision",
                "object intrinsic reflectance",
                "inference process"
            ]
        },
        "id": 69,
        "cited_by": []
    },
    {
        "title": "Unsupervised Cross-Modal Synthesis of Subject-Specific Scans",
        "authors": [
            "Raviteja Vemulapalli",
            "Hien Van Nguyen",
            "Shaohua Kevin Zhou"
        ],
        "abstract": "Recently, cross-modal synthesis of subject-specific scans has been receiving significant attention from the medical imaging community. Though various synthesis approaches have been introduced in the recent past, most of them are either tailored to a specific application or proposed for the supervised setting, i.e., they assume the availability of training data from the same set of subjects in both source and target modalities. But, collecting multiple scans from each subject is undesirable. Hence, to address this issue, we propose a general unsupervised cross-modal medical image synthesis approach that works without paired training data. Given a source modality image of a subject, we first generate multiple target modality candidate values for each voxel independently using cross-modal nearest neighbor search. Then, we select the best candidate values jointly for all the voxels by simultaneously maximizing a global mutual information cost function and a local spatial consistency cost function. Finally, we use coupled sparse representation for further refinement of synthesized images. Our experiments on generating T1-MRI brain scans from T2-MRI and vice versa demonstrate that the synthesis capability of the proposed unsupervised approach is comparable to various state-of-the-art supervised approaches in the literature.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410436",
        "reference_list": [
            {
                "year": "2013",
                "id": 311
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Image generation",
                "Biomedical imaging",
                "Nearest neighbor searches",
                "Image resolution",
                "Training data",
                "Training",
                "Magnetic resonance imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "subject-specific scans",
                "medical imaging community",
                "general unsupervised cross-modal medical image synthesis approach",
                "global mutual information cost function",
                "local spatial consistency cost function",
                "coupled sparse representation",
                "T1-MRI brain scans",
                "T2-MRI"
            ]
        },
        "id": 70,
        "cited_by": []
    },
    {
        "title": "Learning to Boost Filamentary Structure Segmentation",
        "authors": [
            "Lin Gu",
            "Li Cheng"
        ],
        "abstract": "The challenging problem of filamentary structure segmentation has a broad range of applications in biological and medical fields. A critical yet challenging issue remains on how to detect and restore the small filamentary fragments from backgrounds: The small fragments are of diverse shapes and appearances, meanwhile the backgrounds could be cluttered and ambiguous. Focusing on this issue, this paper proposes an iterative two-step learning-based approach to boost the performance based on a base segmenter arbitrarily chosen from a number of existing segmenters: We start with an initial partial segmentation where the filamentary structure obtained is of high confidence based on this existing segmenter. We also define a scanning horizon as epsilon balls centred around the partial segmentation result. Step one of our approach centers on a data-driven latent classification tree model to detect the filamentary fragments. This model is learned via a training process, where a large number of distinct local figure/background separation scenarios are established and geometrically organized into a tree structure. Step two spatially restores the isolated fragments back to the current partial segmentation, which is accomplished by means of completion fields and matting. Both steps are then alternated with the growth of partial segmentation result, until the input image space is entirely explored. Our approach is rather generic and can be easily augmented to a wide range of existing supervised/unsupervised segmenters to produce an improved result. This has been empirically verified on specific filamentary structure segmentation tasks: retinal blood vessel segmentation as well as neuronal segmentations, where noticeable improvement has been shown over the original state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410437",
        "reference_list": [
            {
                "year": "2013",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 8,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Retina",
                "Vegetation",
                "Image restoration",
                "Biomedical imaging",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "blood vessels",
                "eye",
                "image classification",
                "image restoration",
                "image segmentation",
                "learning (artificial intelligence)",
                "medical image processing",
                "object detection",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "filamentary structure segmentation boosting",
                "filamentary fragment detection",
                "filamentary fragment restoration",
                "iterative two-step learning-based approach",
                "base segmenter",
                "initial partial segmentation",
                "scanning horizon",
                "epsilon balls",
                "data-driven latent classification tree model",
                "training process",
                "local figure/background separation scenarios",
                "image space",
                "supervised/unsupervised segmenter",
                "retinal blood vessel segmentation",
                "neuronal segmentation"
            ]
        },
        "id": 71,
        "cited_by": []
    },
    {
        "title": "Weakly-Supervised Structured Output Learning with Flexible and Latent Graphs Using High-Order Loss Functions",
        "authors": [
            "Gustavo Carneiro",
            "Tingying Peng",
            "Christine Bayer",
            "Nassir Navab"
        ],
        "abstract": "We introduce two new structured output models that use a latent graph, which is flexible in terms of the number of nodes and structure, where the training process minimises a high-order loss function using a weakly annotated training set. These models are developed in the context of microscopy imaging of malignant tumours, where the estimation of the number and proportion of classes of microcirculatory supply units (MCSU) is important in the assessment of the efficacy of common cancer treatments (an MCSU is a region of the tumour tissue supplied by a microvessel). The proposed methodologies take as input multimodal microscopy images of a tumour, and estimate the number and proportion of MCSU classes. This estimation is facilitated by the use of an underlying latent graph (not present in the manual annotations), where each MCSU is represented by a node in this graph, labelled with the MCSU class and image location. The training process uses the manual weak annotations available, consisting of the number of MCSU classes per training image, where the training objective is the minimisation of a high-order loss function based on the norm of the error between the manual and estimated annotations. One of the models proposed is based on a new flexible latent structure support vector machine (FLSSVM) and the other is based on a deep convolutional neural network (DCNN) model. Using a dataset of 89 weakly annotated pairs of multimodal images from eight tumours, we show that the quantitative results from DCNN are superior, but the qualitative results from FLSSVM are better and both display high correlation values regarding the number and proportion of MCSU classes compared to the manual annotations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410438",
        "reference_list": [
            {
                "year": "2011",
                "id": 282
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Tumors",
                "Training",
                "Manuals",
                "Cancer",
                "Support vector machines",
                "Microscopy",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "cancer",
                "graph theory",
                "medical image processing",
                "neural nets",
                "support vector machines",
                "tumours"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weakly-supervised structured output learning",
                "flexible graph",
                "latent graph",
                "high-order loss function",
                "microscopy imaging",
                "malignant tumour",
                "microcirculatory supply units",
                "cancer treatment",
                "flexible latent structure support vector machine",
                "FLSSVM",
                "deep convolutional neural network",
                "DCNN model"
            ]
        },
        "id": 72,
        "cited_by": []
    },
    {
        "title": "Efficient Classifier Training to Minimize False Merges in Electron Microscopy Segmentation",
        "authors": [
            "Toufiq Parag",
            "Dan C. Ciresan",
            "Alessandro Giusti"
        ],
        "abstract": "The prospect of neural reconstruction from Electron Microscopy (EM) images has been elucidated by the automatic segmentation algorithms. Although segmentation algorithms eliminate the necessity of tracing the neurons by hand, significant manual effort is still essential for correcting the mistakes they make. A considerable amount of human labor is also required for annotating groundtruth volumes for training the classifiers of a segmentation framework. It is critically important to diminish the dependence on human interaction in the overall reconstruction system. This study proposes a novel classifier training algorithm for EM segmentation aimed to reduce the amount of manual effort demanded by the groundtruth annotation and error refinement tasks. Instead of using an exhaustive pixel level groundtruth, an active learning algorithm is proposed for sparse labeling of pixel and boundaries of superpixels. Because over-segmentation errors are in general more tolerable and easier to correct than the under-segmentation errors, our algorithm is designed to prioritize minimization of false-merges over false-split mistakes. Our experiments on both 2D and 3D data suggest that the proposed method yields segmentation outputs that are more amenable to neural reconstruction than those of existing methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410439",
        "reference_list": [
            {
                "year": "2011",
                "id": 22
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 3,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Image segmentation",
                "Image reconstruction",
                "Three-dimensional displays",
                "Algorithm design and analysis",
                "Labeling",
                "Neurons"
            ],
            "INSPEC: Controlled Indexing": [
                "electron microscopy",
                "image reconstruction",
                "image segmentation",
                "medical image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "efficient classifier training",
                "electron microscopy segmentation",
                "neural reconstruction",
                "EM images",
                "automatic segmentation algorithm",
                "groundtruth volumes",
                "segmentation framework",
                "EM segmentation",
                "groundtruth annotation",
                "error refinement tasks",
                "pixel level groundtruth",
                "active learning algorithm",
                "pixel sparse labeling",
                "superpixel boundary",
                "over-segmentation errors",
                "under-segmentation errors",
                "false-merge minimization",
                "false-split mistakes"
            ]
        },
        "id": 73,
        "cited_by": []
    },
    {
        "title": "On Statistical Analysis of Neuroimages with Imperfect Registration",
        "authors": [
            "Won Hwa Kim",
            "Sathya N. Ravi",
            "Sterling C. Johnson",
            "Ozioma C. Okonkwo",
            "Vikas Singh"
        ],
        "abstract": "A variety of studies in neuroscience/neuroimaging seek to perform statistical inference on the acquired brain image scans for diagnosis as well as understanding the pathological manifestation of diseases. To do so, an important first step is to register (or co-register) all of the image data into a common coordinate system. This permits meaningful comparison of the intensities at each voxel across groups (e.g., diseased versus healthy) to evaluate the effects of the disease and/or use machine learning algorithms in a subsequent step. But errors in the underlying registration make this problematic, they either decrease the statistical power or make the follow-up inference tasks less effective/accurate. In this paper, we derive a novel algorithm which offers immunity to local errors in the underlying deformation field obtained from registration procedures. By deriving a deformation invariant representation of the image, the downstream analysis can be made more robust as if one had access to a (hypothetical) far superior registration procedure. Our algorithm is based on recent work on Scattering coefficients. Using this as a starting point, we show how results from harmonic analysis (especially, non-Euclidean wavelets) yields strategies for designing deformation and additive noise invariant representations of large 3-D brain image volumes. We present a set of results on synthetic and real brain images where we achieve robust statistical analysis even in the presence of substantial deformation errors, here, standard analysis procedures significantly under-perform and fail to identify the true signal.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410440",
        "reference_list": [
            {
                "year": "2005",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Wavelet transforms",
                "Scattering",
                "Diseases",
                "Brain",
                "Algorithm design and analysis",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "harmonic analysis",
                "image registration",
                "medical image processing",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "statistical analysis",
                "neuroimages",
                "imperfect registration",
                "statistical inference",
                "downstream analysis",
                "scattering coefficients",
                "harmonic analysis",
                "additive noise invariant representations",
                "3D brain image volumes",
                "deformation errors"
            ]
        },
        "id": 74,
        "cited_by": []
    },
    {
        "title": "Convex Optimization with Abstract Linear Operators",
        "authors": [
            "Steven Diamond",
            "Stephen Boyd"
        ],
        "abstract": "We introduce a convex optimization modeling framework that transforms a convex optimization problem expressed in a form natural and convenient for the user into an equivalent cone program in a way that preserves fast linear transforms in the original problem. By representing linear functions in the transformation process not as matrices, but as graphs that encode composition of abstract linear operators, we arrive at a matrix-free cone program, i.e., one whose data matrix is represented by an abstract linear operator and its adjoint. This cone program can then be solved by a matrix-free cone solver. By combining the matrix-free modeling framework and cone solver, we obtain a general method for efficiently solving convex optimization problems involving fast linear transforms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410441",
        "reference_list": [
            {
                "year": "2011",
                "id": 223
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 5,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Sparse matrices",
                "Convex functions",
                "Convolution",
                "Transforms",
                "Standards",
                "Signal processing algorithms",
                "Matrix converters"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "convex programming",
                "graph theory",
                "matrix algebra",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "abstract linear operators",
                "convex optimization modeling framework",
                "convex optimization problem",
                "equivalent cone program",
                "linear functions",
                "transformation process",
                "matrix-free cone program",
                "data matrix",
                "abstract linear operator",
                "matrix-free cone solver",
                "matrix-free modeling framework",
                "convex optimization problems",
                "fast linear transforms"
            ]
        },
        "id": 75,
        "cited_by": []
    },
    {
        "title": "Building Dynamic Cloud Maps from the Ground Up",
        "authors": [
            "Calvin Murdock",
            "Nathan Jacobs",
            "Robert Pless"
        ],
        "abstract": "Satellite imagery of cloud cover is extremely important for understanding and predicting weather. We demonstrate how this imagery can be constructed \"from the ground up\" without requiring expensive geo-stationary satellites. This is accomplished through a novel approach to approximate continental-scale cloud maps using only ground-level imagery from publicly-available webcams. We collected a year's worth of satellite data and simultaneously-captured, geo-located outdoor webcam images from 4388 sparsely distributed cameras across the continental USA. The satellite data is used to train a dynamic model of cloud motion alongside 4388 regression models (one for each camera) to relate ground-level webcam data to the satellite data at the camera's location. This novel application of large-scale computer vision to meteorology and remote sensing is enabled by a smoothed, hierarchically-regularized dynamic texture model whose system dynamics are driven to remain consistent with measurements from the geo-located webcams. We show that our hierarchical model is better able to incorporate sparse webcam measurements resulting in more accurate cloud maps in comparison to a standard dynamic textures implementation. Finally, we demonstrate that our model can be successfully applied to other natural image sequences from the DynTex database, suggesting a broader applicability of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410442",
        "reference_list": [
            {
                "year": "2001",
                "id": 89
            },
            {
                "year": "2007",
                "id": 155
            },
            {
                "year": "2001",
                "id": 163
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Clouds",
                "Webcams",
                "Satellites",
                "Data models",
                "Computational modeling",
                "Principal component analysis",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "geophysical image processing",
                "image sequences",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dynamic cloud maps",
                "satellite imagery",
                "continental-scale cloud maps",
                "cloud motion",
                "regression models",
                "ground-level webcam data",
                "large-scale computer vision",
                "geo-located webcams",
                "image sequences",
                "DynTex database"
            ]
        },
        "id": 76,
        "cited_by": [
            {
                "year": "2017",
                "id": 283
            }
        ]
    },
    {
        "title": "A Versatile Learning-Based 3D Temporal Tracker: Scalable, Robust, Online",
        "authors": [
            "David Joseph Tan",
            "Federico Tombari",
            "Slobodan Ilic",
            "Nassir Navab"
        ],
        "abstract": "This paper proposes a temporal tracking algorithm based on Random Forest that uses depth images to estimate and track the 3D pose of a rigid object in real-time. Compared to the state of the art aimed at the same goal, our algorithm holds important attributes such as high robustness against holes and occlusion, low computational cost of both learning and tracking stages, and low memory consumption. These are obtained (a) by a novel formulation of the learning strategy, based on a dense sampling of the camera viewpoints and learning independent trees from a single image for each camera view, as well as, (b) by an insightful occlusion handling strategy that enforces the forest to recognize the object's local and global structures. Due to these attributes, we report state-of-the-art tracking accuracy on benchmark datasets, and accomplish remarkable scalability with the number of targets, being able to simultaneously track the pose of over a hundred objects at 30~fps with an off-the-shelf CPU. In addition, the fast learning time enables us to extend our algorithm as a robust online tracker for model-free 3D objects under different viewpoints and appearance changes as demonstrated by the experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410443",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Three-dimensional displays",
                "Cameras",
                "Robustness",
                "Real-time systems",
                "Vegetation",
                "TV"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "learning (artificial intelligence)",
                "object tracking",
                "pose estimation",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporal tracking algorithm",
                "random forest",
                "depth images",
                "3D pose tracking",
                "computational cost",
                "memory consumption",
                "camera viewpoints dense sampling",
                "learning independent trees",
                "occlusion handling strategy",
                "object structure recognition",
                "target tracking",
                "model-free 3D objects",
                "object tracking"
            ]
        },
        "id": 77,
        "cited_by": []
    },
    {
        "title": "Realtime Edge-Based Visual Odometry for a Monocular Camera",
        "authors": [
            "Juan Jos\u00e9 Tarrio",
            "Sol Pedre"
        ],
        "abstract": "In this work we present a novel algorithm for realtime visual odometry for a monocular camera. The main idea is to develop an approach between classical feature-based visual odometry systems and modern direct dense/semi-dense methods, trying to benefit from the best attributes of both. Similar to feature-based systems, we extract information from the images, instead of working with raw image intensities as direct methods. In particular, the information extracted are the edges present in the image, while the rest of the algorithm is designed to take advantage of the structural information provided when pixels are treated as edges. Edge extraction is an efficient and higly parallelizable operation. The edge depth information extracted is dense enough to allow acceptable surface fitting, similar to modern semi-dense methods. This is a valuable attribute that feature-based odometry lacks. Experimental results show that the proposed method has similar drift than state of the art feature-based and direct methods, and is a simple algorithm that runs at realtime and can be parallelized. Finally, we have also developed an inertial aided version that successfully stabilizes an unmanned air vehicle in complex indoor environments using only a frontal camera, while running the complete solution in the embedded hardware on board the vehicle.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410444",
        "reference_list": [
            {
                "year": "2013",
                "id": 180
            },
            {
                "year": "2011",
                "id": 295
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 3,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Feature extraction",
                "Cameras",
                "Detectors",
                "Visualization",
                "Robustness",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "autonomous aerial vehicles",
                "cameras",
                "computer vision",
                "distance measurement",
                "edge detection",
                "real-time systems",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "embedded hardware",
                "frontal camera",
                "complex indoor environment",
                "unmanned air vehicle",
                "inertial aided version",
                "feature-based odometry",
                "surface fitting",
                "edge depth information extraction",
                "edge extraction",
                "structural information",
                "raw image intensity",
                "feature-based system",
                "direct dense/semi-dense method",
                "classical feature-based visual odometry system",
                "realtime visual odometry",
                "monocular camera",
                "realtime edge-based visual odometry"
            ]
        },
        "id": 78,
        "cited_by": [
            {
                "year": "2017",
                "id": 483
            }
        ]
    },
    {
        "title": "Fill and Transfer: A Simple Physics-Based Approach for Containability Reasoning",
        "authors": [
            "Lap-Fai Yu",
            "Noah Duncan",
            "Sai-Kit Yeung"
        ],
        "abstract": "The visual perception of object affordances has emerged as a useful ingredient for building powerful computer vision and robotic applications. In this paper we introduce a novel approach to reason about liquid containability - the affordance of containing liquid. Our approach analyzes container objects based on two simple physical processes: the Fill and Transfer of liquid. First, it reasons about whether a given 3D object is a liquid container and its best filling direction. Second, it proposes directions to transfer its contained liquid to the outside while avoiding spillage. We compare our simplified model with a common fluid dynamics simulation and demonstrate that our algorithm makes human-like choices about the best directions to fill containers and transfer liquid from them. We apply our approach to reason about the containability of several real-world objects acquired using a consumer-grade depth camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410445",
        "reference_list": [
            {
                "year": "2013",
                "id": 247
            },
            {
                "year": "2013",
                "id": 313
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 5,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Containers",
                "Liquids",
                "Three-dimensional displays",
                "Cognition",
                "Computers",
                "Physics",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "fluid dynamics",
                "inference mechanisms",
                "visual perception"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fill and transfer",
                "physics-based approach",
                "containability reasoning",
                "visual perception",
                "object affordance",
                "computer vision",
                "liquid container",
                "filling direction",
                "fluid dynamics simulation",
                "depth camera"
            ]
        },
        "id": 79,
        "cited_by": [
            {
                "year": "2017",
                "id": 197
            },
            {
                "year": "2017",
                "id": 309
            },
            {
                "year": "2017",
                "id": 309
            }
        ]
    },
    {
        "title": "On Linear Structure from Motion for Light Field Cameras",
        "authors": [
            "Ole Johannsen",
            "Antonin Sulc",
            "Bastian Goldluecke"
        ],
        "abstract": "We present a novel approach to relative pose estimation which is tailored to 4D light field cameras. From the relationships between scene geometry and light field structure and an analysis of the light field projection in terms of Pluecker ray coordinates, we deduce a set of linear constraints on ray space correspondences between a light field camera pair. These can be applied to infer relative pose of the light field cameras and thus obtain a point cloud reconstruction of the scene. While the proposed method has interesting relationships to pose estimation for generalized cameras based on ray-to-ray correspondence, our experiments demonstrate that our approach is both more accurate and computationally more efficient. It also compares favourably to direct linear pose estimation based on aligning the 3D point clouds obtained by reconstructing depth for each individual light field. To further validate the method, we employ the pose estimates to merge light fields captured with hand-held consumer light field cameras into refocusable panoramas.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410446",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 5,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Calibration",
                "Mathematical model",
                "Visualization",
                "Matrix decomposition"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image reconstruction",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear structure",
                "light field cameras",
                "pose estimation",
                "4D light field cameras",
                "scene geometry",
                "light field structure",
                "Pluecker ray coordinates",
                "point cloud reconstruction",
                "linear pose estimation",
                "3D point clouds"
            ]
        },
        "id": 80,
        "cited_by": [
            {
                "year": "2017",
                "id": 99
            },
            {
                "year": "2017",
                "id": 486
            }
        ]
    },
    {
        "title": "3D Object Reconstruction from Hand-Object Interactions",
        "authors": [
            "Dimitrios Tzionas",
            "Juergen Gall"
        ],
        "abstract": "Recent advances have enabled 3d object reconstruction approaches using a single off-the-shelf RGB-D camera. Although these approaches are successful for a wide range of object classes, they rely on stable and distinctive geometric or texture features. Many objects like mechanical parts, toys, household or decorative articles, however, are textureless and characterized by minimalistic shapes that are simple and symmetric. Existing in-hand scanning systems and 3d reconstruction techniques fail for such symmetric objects in the absence of highly distinctive features. In this work, we show that extracting 3d hand motion for in-hand scanning effectively facilitates the reconstruction of even featureless and highly symmetric objects and we present an approach that fuses the rich additional information of hands into a 3d reconstruction pipeline, significantly contributing to the state-of-the-art of in-hand scanning.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410447",
        "reference_list": [
            {
                "year": "2009",
                "id": 189
            },
            {
                "year": "2011",
                "id": 265
            },
            {
                "year": "2013",
                "id": 194
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 7,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Image reconstruction",
                "Cameras",
                "Visualization",
                "Feature extraction",
                "Real-time systems",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "feature extraction",
                "image motion analysis",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hand-object interactions",
                "3d object reconstruction",
                "RGB-D camera",
                "3d hand motion extraction",
                "in-hand scanning"
            ]
        },
        "id": 81,
        "cited_by": [
            {
                "year": "2017",
                "id": 121
            }
        ]
    },
    {
        "title": "Minimal Solvers for 3D Geometry from Satellite Imagery",
        "authors": [
            "Enliang Zheng",
            "Ke Wang",
            "Enrique Dunn",
            "Jan-Michael Frahm"
        ],
        "abstract": "We propose two novel minimal solvers which advance the state of the art in satellite imagery processing. Our methods are efficient and do not rely on the prior existence of complex inverse mapping functions to correlate 2D image coordinates and 3D terrain. Our first solver improves on the stereo correspondence problem for satellite imagery, in that we provide an exact image-to-object space mapping (where prior methods were inaccurate). Our second solver provides a novel mechanism for 3D point triangulation, which has improved robustness and accuracy over prior techniques. Given the usefulness and ubiquity of satellite imagery, our proposed methods allow for improved results in a variety of existing and future applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410448",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Satellites",
                "Computational modeling",
                "Three-dimensional displays",
                "Mathematical model",
                "Cameras",
                "Numerical models",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "geophysical image processing",
                "stereo image processing",
                "terrain mapping"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D point triangulation",
                "image-to-object space mapping",
                "stereo correspondence problem",
                "3D terrain",
                "2D image coordinates",
                "satellite imagery processing",
                "3D geometry",
                "minimal solvers"
            ]
        },
        "id": 82,
        "cited_by": []
    },
    {
        "title": "An Efficient Minimal Solution for Multi-camera Motion",
        "authors": [
            "Jonathan Ventura",
            "Clemens Arth",
            "Vincent Lepetit"
        ],
        "abstract": "Summary form only given. We propose an efficient method for estimating the motion of a multi-camera rig from a minimal set of feature correspondences. Existing methods for solving the multi-camera relative pose problem require extra correspondences, are slow to compute, and/or produce a multitude of solutions. Our solution uses a first-order approximation to relative pose in order to simplify the problem and produce an accurate estimate quickly. The solver is applicable to sequential multi-camera motion estimation and is fast enough for real-time implementation in a random sampling framework. Our experiments show that our approach is both stable and efficient on challenging test sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410449",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Computer vision",
                "Motion estimation",
                "Conferences",
                "Computer science",
                "Springs",
                "Real-time systems"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "cameras",
                "image sampling",
                "motion estimation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "minimal solution",
                "multicamera rig motion estimation",
                "feature correspondence",
                "multicamera relative pose problem",
                "first-order approximation",
                "random sampling framework"
            ]
        },
        "id": 83,
        "cited_by": []
    },
    {
        "title": "Learning Shape, Motion and Elastic Models in Force Space",
        "authors": [
            "Antonio Agudo",
            "Francesc Moreno-Noguer"
        ],
        "abstract": "In this paper, we address the problem of simultaneously recovering the 3D shape and pose of a deformable and potentially elastic object from 2D motion. This is a highly ambiguous problem typically tackled by using low-rank shape and trajectory constraints. We show that formulating the problem in terms of a low-rank force space that induces the deformation, allows for a better physical interpretation of the resulting priors and a more accurate representation of the actual object's behavior. However, this comes at the price of, besides force and pose, having to estimate the elastic model of the object. For this, we use an Expectation Maximization strategy, where each of these parameters are successively learned within partial M-steps, while robustly dealing with missing observations. We thoroughly validate the approach on both mocap and real sequences, showing more accurate 3D reconstructions than state-of-the-art, and additionally providing an estimate of the full elastic model with no a priori information.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410450",
        "reference_list": [
            {
                "year": "2013",
                "id": 247
            },
            {
                "year": "2009",
                "id": 307
            },
            {
                "year": "2011",
                "id": 101
            },
            {
                "year": "2011",
                "id": 262
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 1,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Force",
                "Trajectory",
                "Three-dimensional displays",
                "Solid modeling",
                "Deformable models",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "deformation",
                "elasticity",
                "expectation-maximisation algorithm",
                "image motion analysis",
                "image reconstruction",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model learning",
                "shape model",
                "motion model",
                "low-rank force space",
                "trajectory constraint",
                "deformation",
                "object elastic model estimation",
                "expectation maximization strategy",
                "3D reconstruction"
            ]
        },
        "id": 84,
        "cited_by": []
    },
    {
        "title": "A Versatile Scene Model with Differentiable Visibility Applied to Generative Pose Estimation",
        "authors": [
            "Helge Rhodin",
            "Nadia Robertini",
            "Christian Richardt",
            "Hans-Peter Seidel",
            "Christian Theobalt"
        ],
        "abstract": "Generative reconstruction methods compute the 3D configuration (such as pose and/or geometry) of a shape by optimizing the overlap of the projected 3D shape model with images. Proper handling of occlusions is a big challenge, since the visibility function that indicates if a surface point is seen from a camera can often not be formulated in closed form, and is in general discrete and non-differentiable at occlusion boundaries. We present a new scene representation that enables an analytically differentiable closed-form formulation of surface visibility. In contrast to previous methods, this yields smooth, analytically differentiable, and efficient to optimize pose similarity energies with rigorous occlusion handling, fewer local minima, and experimentally verified improved convergence of numerical optimization. The underlying idea is a new image formation model that represents opaque objects by a translucent medium with a smooth Gaussian density distribution which turns visibility into a smooth phenomenon. We demonstrate the advantages of our versatile scene model in several generative pose estimation problems, namely marker-less multi-object pose estimation, marker-less human motion capture with few cameras, and image-based 3D geometry estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410451",
        "reference_list": [
            {
                "year": "2007",
                "id": 163
            },
            {
                "year": "2011",
                "id": 120
            },
            {
                "year": "2009",
                "id": 219
            },
            {
                "year": "2011",
                "id": 140
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 8,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Solid modeling",
                "Computational modeling",
                "Numerical models",
                "Shape",
                "Solids"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian distribution",
                "geometry",
                "image motion analysis",
                "image reconstruction",
                "image representation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "versatile scene model",
                "differentiable visibility",
                "generative pose estimation",
                "reconstruction methods",
                "projected 3D shape model",
                "visibility function",
                "surface point",
                "occlusion boundaries",
                "scene representation",
                "analytically differentiable closed-form formulation",
                "surface visibility",
                "pose similarity energies",
                "occlusion handling",
                "local minima",
                "numerical optimization",
                "image formation model",
                "opaque objects representation",
                "translucent medium",
                "smooth Gaussian density distribution",
                "marker-less multiobject pose estimation",
                "marker-less human motion capture",
                "image-based 3D geometry estimation"
            ]
        },
        "id": 85,
        "cited_by": []
    },
    {
        "title": "Semantic Pose Using Deep Networks Trained on Synthetic RGB-D",
        "authors": [
            "Jeremie Papon",
            "Markus Schoeler"
        ],
        "abstract": "In this work we address the problem of indoor scene understanding from RGB-D images. Specifically, we propose to find instances of common furniture classes, their spatial extent, and their pose with respect to generalized class models. To accomplish this, we use a deep, wide, multi-output convolutional neural network (CNN) that predicts class, pose, and location of possible objects simultaneously. To overcome the lack of large annotated RGB-D training sets (especially those with pose), we use an on-the-fly rendering pipeline that generates realistic cluttered room scenes in parallel to training. We then perform transfer learning on the relatively small amount of publicly available annotated RGB-D data, and find that our model is able to successfully annotate even highly challenging real scenes. Importantly, our trained network is able to understand noisy and sparse observations of highly cluttered scenes with a remarkable degree of accuracy, inferring class and pose from a very limited set of cues. Additionally, our neural network is only moderately deep and computes class, pose and position in tandem, so the overall run-time is significantly faster than existing methods, estimating all output parameters simultaneously in parallel.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410452",
        "reference_list": [
            {
                "year": "2013",
                "id": 267
            },
            {
                "year": "2013",
                "id": 176
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 7,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Training",
                "Three-dimensional displays",
                "Rendering (computer graphics)",
                "Adaptation models",
                "Semantics",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "learning (artificial intelligence)",
                "neural nets",
                "pose estimation",
                "realistic images",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic pose",
                "deep network",
                "cluttered scene",
                "sparse observation",
                "noisy observation",
                "real scene",
                "transfer learning",
                "realistic cluttered room scene",
                "on-the-fly rendering pipeline",
                "RGB-D training set",
                "CNN",
                "multioutput convolutional neural network",
                "generalized class model",
                "furniture class",
                "RGB-D image",
                "indoor scene understanding",
                "synthetic RGB-D"
            ]
        },
        "id": 86,
        "cited_by": [
            {
                "year": "2017",
                "id": 50
            }
        ]
    },
    {
        "title": "Exploiting High Level Scene Cues in Stereo Reconstruction",
        "authors": [
            "Simon Hadfield",
            "Richard Bowden"
        ],
        "abstract": "We present a novel approach to 3D reconstruction which is inspired by the human visual system. This system unifies standard appearance matching and triangulation techniques with higher level reasoning and scene understanding, in order to resolve ambiguities between different interpretations of the scene. The types of reasoning integrated in the approach includes recognising common configurations of surface normals and semantic edges (e.g. convex, concave and occlusion boundaries). We also recognise the coplanar, collinear and symmetric structures which are especially common in man made environments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410453",
        "reference_list": [
            {
                "year": "2013",
                "id": 423
            },
            {
                "year": "2009",
                "id": 237
            },
            {
                "year": "2013",
                "id": 294
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Cognition",
                "Cost function",
                "Image matching",
                "Robustness",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "image matching",
                "image reconstruction",
                "inference mechanisms",
                "natural scenes",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high level scene cues",
                "stereo reconstruction",
                "3D reconstruction",
                "human visual system",
                "standard appearance matching",
                "triangulation techniques",
                "higher level reasoning",
                "scene understanding",
                "scene interpretations",
                "surface normals",
                "semantic edges",
                "common configuration recognition",
                "coplanar structure recognition",
                "collinear structure recognition",
                "symmetric structure recognition"
            ]
        },
        "id": 87,
        "cited_by": []
    },
    {
        "title": "Point Triangulation through Polyhedron Collapse Using the l\u221e Norm",
        "authors": [
            "Simon Donn\u00e9",
            "Bart Goossens",
            "Wilfried Philips"
        ],
        "abstract": "Multi-camera triangulation of feature points based on a minimisation of the overall \u2113 2 reprojection error can get stuck in suboptimal local minima or require slow global optimisation. For this reason, researchers have proposed optimising the \u2113 \u221e norm of the \u2113 2 single view reprojection errors, which avoids the problem of local minima entirely. In this paper we present a novel method for \u2113 \u221e triangulation that minimizes the \u2113 \u221e norm of the \u2113 \u221e reprojection errors: this apparently small difference leads to a much faster but equally accurate solution which is related to the MLE under the assumption of uniform noise. The proposed method adopts a new optimisation strategy based on solving simple quadratic equations. This stands in contrast with the fastest existing methods, which solve a sequence of more complex auxiliary Linear Programming or Second Order Cone Problems. The proposed algorithm performs well: for triangulation, it achieves the same accuracy as existing techniques while executing faster and being straightforward to implement.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410454",
        "reference_list": [
            {
                "year": "2007",
                "id": 247
            },
            {
                "year": "2005",
                "id": 88
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Maximum likelihood estimation",
                "Three-dimensional displays",
                "AWGN",
                "Cost function",
                "Indexing"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "geometry",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "polyhedron collapse",
                "\u2113\u221e norm minimization",
                "\u2113\u221e triangulation",
                "\u2113\u221e reprojection errors",
                "MLE",
                "uniform noise",
                "quadratic equations",
                "multicamera feature points triangulation"
            ]
        },
        "id": 88,
        "cited_by": [
            {
                "year": "2017",
                "id": 95
            }
        ]
    },
    {
        "title": "Optimizing the Viewing Graph for Structure-from-Motion",
        "authors": [
            "Chris Sweeney",
            "Torsten Sattler",
            "Tobias H\u00f6llerer",
            "Matthew Turk",
            "Marc Pollefeys"
        ],
        "abstract": "The viewing graph represents a set of views that are related by pairwise relative geometries. In the context of Structure-from-Motion (SfM), the viewing graph is the input to the incremental or global estimation pipeline. Much effort has been put towards developing robust algorithms to overcome potentially inaccurate relative geometries in the viewing graph during SfM. In this paper, we take a fundamentally different approach to SfM and instead focus on improving the quality of the viewing graph before applying SfM. Our main contribution is a novel optimization that improves the quality of the relative geometries in the viewing graph by enforcing loop consistency constraints with the epipolar point transfer. We show that this optimization greatly improves the accuracy of relative poses in the viewing graph and removes the need for filtering steps or robust algorithms typically used in global SfM methods. In addition, the optimized viewing graph can be used to efficiently calibrate cameras at scale. We combine our viewing graph optimization and focal length calibration into a global SfM pipeline that is more efficient than existing approaches. To our knowledge, ours is the first global SfM pipeline capable of handling uncalibrated image sets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410455",
        "reference_list": [
            {
                "year": "2009",
                "id": 231
            },
            {
                "year": "2013",
                "id": 64
            },
            {
                "year": "2013",
                "id": 59
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 14,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Image reconstruction",
                "Cameras",
                "Optimization",
                "Pipelines",
                "Image edge detection",
                "Matrix decomposition"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "computational geometry",
                "graph theory",
                "image filtering",
                "image motion analysis",
                "optimisation",
                "pose estimation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure-from-motion",
                "pairwise relative geometries",
                "incremental pipeline",
                "global estimation pipeline",
                "viewing graph quality improvement",
                "relative geometry quality improvement",
                "loop consistency constraints",
                "epipolar point transfer",
                "relative pose accuracy improvement",
                "filtering",
                "global SfM methods",
                "camera calibration",
                "focal length calibration",
                "viewing graph optimization",
                "uncalibrated image set handling"
            ]
        },
        "id": 89,
        "cited_by": []
    },
    {
        "title": "Intrinsic Scene Decomposition from RGB-D Images",
        "authors": [
            "Mohammed Hachama",
            "Bernard Ghanem",
            "Peter Wonka"
        ],
        "abstract": "In this paper, we address the problem of computing an intrinsic decomposition of the colors of a surface into an albedo and a shading term. The surface is reconstructed from a single or multiple RGB-D images of a static scene obtained from different views. We thereby extend and improve existing works in the area of intrinsic image decomposition. In a variational framework, we formulate the problem as a minimization of an energy composed of two terms: a data term and a regularity term. The first term is related to the image formation process and expresses the relation between the albedo, the surface normals, and the incident illumination. We use an affine shading model, a combination of a Lambertian model, and an ambient lighting term. This model is relevant for Lambertian surfaces. When available, multiple views can be used to handle view-dependent non-Lambertian reflections. The second term contains an efficient combination of l2 and l1-regularizers on the illumination vector field and albedo respectively. Unlike most previous approaches, especially Retinex-like techniques, these terms do not depend on the image gradient or texture, thus reducing the mixing shading/reflectance artifacts and leading to better results. The obtained non-linear optimization problem is efficiently solved using a cyclic block coordinate descent algorithm. Our method outperforms a range of state-of-the-art algorithms on a popular benchmark dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410456",
        "reference_list": [
            {
                "year": "2013",
                "id": 30
            },
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2001",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 4,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Image color analysis",
                "Three-dimensional displays",
                "Surface reconstruction",
                "Image reconstruction",
                "Robustness",
                "Coherence"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "nonlinear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "intrinsic scene decomposition",
                "albedo term",
                "shading term",
                "multiple RGB-D images",
                "single RGB-D images",
                "static scene",
                "variational framework",
                "data term",
                "regularity term",
                "image formation process",
                "surface normals",
                "incident illumination",
                "Lambertian reflections",
                "l1-regularizers",
                "l2-regularizers",
                "Retinex-like techniques",
                "nonlinear optimization problem",
                "cyclic block coordinate descent algorithm",
                "affine shading model",
                "ambient lighting term"
            ]
        },
        "id": 90,
        "cited_by": []
    },
    {
        "title": "3D Hand Pose Estimation Using Randomized Decision Forest with Segmentation Index Points",
        "authors": [
            "Peiyi Li",
            "Haibin Ling",
            "Xi Li",
            "Chunyuan Liao"
        ],
        "abstract": "In this paper, we propose a real-time 3D hand pose estimation algorithm using the randomized decision forest framework. Our algorithm takes a depth image as input and generates a set of skeletal joints as output. Previous decision forest-based methods often give labels to all points in a point cloud at a very early stage and vote for the joint locations. By contrast, our algorithm only tracks a set of more flexible virtual landmark points, named segmentation index points (SIPs), before reaching the final decision at a leaf node. Roughly speaking, a SIP represents the centroid of a subset of skeletal joints, which are to be located at the leaves of the branch expanded from the SIP. Inspired by recent latent regression forest-based hand pose estimation framework (Tang et al. 2014), we integrate SIP into the framework with several important improvements: First, we devise a new forest growing strategy, whose decision is made using a randomized feature guided by SIPs. Second, we speed-up the training procedure since only SIPs, not the skeletal joints, are estimated at non-leaf nodes. Third, the experimental results on public benchmark datasets show clearly the advantage of the proposed algorithm over previous state-of-the-art methods, and our algorithm runs at 55.5 fps on a normal CPU without parallelism.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410457",
        "reference_list": [
            {
                "year": "2011",
                "id": 52
            },
            {
                "year": "2009",
                "id": 189
            },
            {
                "year": "2011",
                "id": 265
            },
            {
                "year": "2013",
                "id": 306
            },
            {
                "year": "2013",
                "id": 402
            },
            {
                "year": "2013",
                "id": 431
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 13,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Vegetation",
                "Three-dimensional displays",
                "Joints",
                "Resource description framework",
                "Indexes"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "feature extraction",
                "image segmentation",
                "palmprint recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "skeleton detection",
                "SIP",
                "segmentation index point",
                "randomized decision forest",
                "3D hand pose estimation"
            ]
        },
        "id": 91,
        "cited_by": []
    },
    {
        "title": "Accurate Camera Calibration Robust to Defocus Using a Smartphone",
        "authors": [
            "Hyowon Ha",
            "Yunsu Bok",
            "Kyungdon Joo",
            "Jiyoung Jung",
            "In So Kweon"
        ],
        "abstract": "We propose a novel camera calibration method for defocused images using a smartphone under the assumption that the defocus blur is modeled as a convolution of a sharp image with a Gaussian point spread function (PSF). In contrast to existing calibration approaches which require well-focused images, the proposed method achieves accurate camera calibration with severely defocused images. This robustness to defocus is due to the proposed set of unidirectional binary patterns, which simplifies 2D Gaussian deconvolution to a 1D Gaussian deconvolution problem with multiple observations. By capturing the set of patterns consecutively displayed on a smartphone, we formulate the feature extraction as a deconvolution problem to estimate feature point locations in sub-pixel accuracy and the blur kernel in each location. We also compensate the error in camera parameters due to refraction of the glass panel of the display device. We evaluate the performance of the proposed method on synthetic and real data. Even under severe defocus, our method shows accurate camera calibration result.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410458",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Feature extraction",
                "Calibration",
                "Deconvolution",
                "Kernel",
                "Image edge detection",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "convolution",
                "deconvolution",
                "estimation theory",
                "feature extraction",
                "Gaussian processes",
                "optical transfer function",
                "smart phones"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera calibration method",
                "image defocus",
                "smart phone",
                "image convolution",
                "Gaussian point spread function",
                "PSF",
                "binary pattern",
                "feature extraction",
                "deconvolution problem",
                "feature point location estimation"
            ]
        },
        "id": 92,
        "cited_by": []
    },
    {
        "title": "High Quality Structure from Small Motion for Rolling Shutter Cameras",
        "authors": [
            "Sunghoon Im",
            "Hyowon Ha",
            "Gyeongmin Choe",
            "Hae-Gon Jeon",
            "Kyungdon Joo",
            "In So Kweon"
        ],
        "abstract": "We present a practical 3D reconstruction method to obtain a high-quality dense depth map from narrow-baseline image sequences captured by commercial digital cameras, such as DSLRs or mobile phones. Depth estimation from small motion has gained interest as a means of various photographic editing, but important limitations present themselves in the form of depth uncertainty due to a narrow baseline and rolling shutter. To address these problems, we introduce a novel 3D reconstruction method from narrow-baseline image sequences that effectively handles the effects of a rolling shutter that occur from most of commercial digital cameras. Additionally, we present a depth propagation method to fill in the holes associated with the unknown pixels based on our novel geometric guidance model. Both qualitative and quantitative experimental results show that our new algorithm consistently generates better 3D depth maps than those by the state-of-the-art method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410459",
        "reference_list": [
            {
                "year": "2011",
                "id": 205
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 2,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Feature extraction",
                "Image sequences",
                "Interpolation",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D depth maps",
                "quantitative experimental result",
                "qualitative experimental result",
                "geometric guidance model",
                "depth propagation method",
                "digital cameras",
                "depth uncertainty",
                "photographic editing",
                "depth estimation",
                "mobile phones",
                "DSLRs",
                "narrow-baseline image sequences",
                "dense depth map",
                "3D reconstruction method",
                "rolling shutter cameras",
                "high quality structure"
            ]
        },
        "id": 93,
        "cited_by": [
            {
                "year": "2017",
                "id": 98
            }
        ]
    },
    {
        "title": "Photogeometric Scene Flow for High-Detail Dynamic 3D Reconstruction",
        "authors": [
            "Paulo F. U. Gotardo",
            "Tomas Simon",
            "Yaser Sheikh",
            "Iain Matthews"
        ],
        "abstract": "Photometric stereo (PS) is an established technique for high-detail reconstruction of 3D geometry and appearance. To correct for surface integration errors, PS is often combined with multiview stereo (MVS). With dynamic objects, PS reconstruction also faces the problem of computing optical flow (OF) for image alignment under rapid changes in illumination. Current PS methods typically compute optical flow and MVS as independent stages, each one with its own limitations and errors introduced by early regularization. In contrast, scene flow methods estimate geometry and motion, but lack the fine detail from PS. This paper proposes photogeometric scene flow (PGSF) for high-quality dynamic 3D reconstruction. PGSF performs PS, OF, and MVS simultaneously. It is based on two key observations: (i) while image alignment improves PS, PS allows for surfaces to be relit to improve alignment, (ii) PS provides surface gradients that render the smoothness term in MVS unnecessary, leading to truly data-driven, continuous depth estimates. This synergy is demonstrated in the quality of the resulting RGB appearance, 3D geometry, and 3D motion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410460",
        "reference_list": [
            {
                "year": "2011",
                "id": 277
            },
            {
                "year": "2007",
                "id": 100
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 6,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Three-dimensional displays",
                "Geometry",
                "Image reconstruction",
                "Surface reconstruction",
                "Cameras",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image reconstruction",
                "image resolution",
                "image sequences",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photogeometric scene flow",
                "high-detail dynamic 3D reconstruction",
                "3D geometry",
                "surface integration error",
                "multiview stereo",
                "MVS",
                "photometric stereo reconstruction",
                "optical flow",
                "scene flow method",
                "geometry estimation",
                "high-quality dynamic 3D reconstruction",
                "rendering",
                "synergy",
                "RGB appearance"
            ]
        },
        "id": 94,
        "cited_by": []
    },
    {
        "title": "Blur-Aware Disparity Estimation from Defocus Stereo Images",
        "authors": [
            "Ching-Hui Chen",
            "Hui Zhou",
            "Timo Ahonen"
        ],
        "abstract": "Defocus blur usually causes performance degradation in establishing the visual correspondence between stereo images. We propose a blur-aware disparity estimation method that is robust to the mismatch of focus in stereo images. The relative blur resulting from the mismatch of focus between stereo images is approximated as the difference of the square diameters of the blur kernels. Based on the defocus and stereo model, we propose the relative blur versus disparity (RBD) model that characterizes the relative blur as a second-order polynomial function of disparity. Our method alternates between RBD model update and disparity update in each iteration. The RBD model in return refines the disparity estimation by updating the matching cost and aggregation weight to compensate the mismatch of focus. Experiments using both synthesized and real datasets demonstrate the effectiveness of our proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410461",
        "reference_list": [
            {
                "year": "2009",
                "id": 18
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Estimation",
                "Cameras",
                "Apertures",
                "Lenses",
                "Visualization",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image matching",
                "polynomials",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "blur-aware disparity estimation",
                "defocus stereo images",
                "defocus blur",
                "visual correspondence",
                "relative blur versus disparity model",
                "second-order polynomial function of disparity",
                "RBD model update",
                "disparity update",
                "focus mismatch"
            ]
        },
        "id": 95,
        "cited_by": []
    },
    {
        "title": "Global Structure-from-Motion by Similarity Averaging",
        "authors": [
            "Zhaopeng Cui",
            "Ping Tan"
        ],
        "abstract": "Global structure-from-motion (SfM) methods solve all cameras simultaneously from all available relative motions. It has better potential in both reconstruction accuracy and computation efficiency than incremental methods. However, global SfM is challenging, mainly because of two reasons. Firstly, translation averaging is difficult, since an essential matrix only tells the direction of relative translation. Secondly, it is also hard to filter out bad essential matrices due to feature matching failures. We propose to compute a sparse depth image at each camera to solve both problems. Depth images help to upgrade an essential matrix to a similarity transformation, which can determine the scale of relative translation. Thus, camera registration is formulated as a well-posed similarity averaging problem. Depth images also make the filtering of essential matrices simple and effective. In this way, translation averaging can be solved robustly in two convex L1 optimization problems, which reach the global optimum rapidly. We demonstrate this method in various examples including sequential data, Internet data, and ambiguous data with repetitive scene structures.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410462",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2013",
                "id": 64
            },
            {
                "year": "2013",
                "id": 59
            },
            {
                "year": "2013",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 8,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Sparse matrices",
                "Robustness",
                "Optimization",
                "Internet",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "convex programming",
                "feature extraction",
                "image matching",
                "image reconstruction",
                "image registration",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "global structure-from-motion",
                "similarity averaging",
                "SfM method",
                "reconstruction accuracy",
                "computation efficiency",
                "translation averaging",
                "feature matching failure",
                "sparse depth image",
                "essential matrix",
                "similarity transformation",
                "relative translation scale",
                "camera registration",
                "convex L1 optimization problem"
            ]
        },
        "id": 96,
        "cited_by": []
    },
    {
        "title": "Massively Parallel Multiview Stereopsis by Surface Normal Diffusion",
        "authors": [
            "Silvano Galliani",
            "Katrin Lasinger",
            "Konrad Schindler"
        ],
        "abstract": "We present a new, massively parallel method for high-quality multiview matching. Our work builds on the Patchmatch idea: starting from randomly generated 3D planes in scene space, the best-fitting planes are iteratively propagated and refined to obtain a 3D depth and normal field per view, such that a robust photo-consistency measure over all images is maximized. Our main novelties are on the one hand to formulate Patchmatch in scene space, which makes it possible to aggregate image similarity across multiple views and obtain more accurate depth maps. And on the other hand a modified, diffusion-like propagation scheme that can be massively parallelized and delivers dense multiview correspondence over ten 1.9-Megapixel images in 3 seconds, on a consumer-grade GPU. Our method uses a slanted support window and thus has no fronto-parallel bias, it is completely local and parallel, such that computation time scales linearly with image size, and inversely proportional to the number of parallel threads. Furthermore, it has low memory footprint (four values per pixel, independent of the depth range). It therefore scales exceptionally well and can handle multiple large images at high depth resolution. Experiments on the DTU and Middlebury multiview datasets as well as oblique aerial images show that our method achieves very competitive results with high accuracy and completeness, across a range of different scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410463",
        "reference_list": [
            {
                "year": "2013",
                "id": 294
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 21,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Image reconstruction",
                "Surface reconstruction",
                "Robustness",
                "Benchmark testing",
                "Standards",
                "Image resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image reconstruction",
                "image resolution",
                "stereo image processing",
                "visual perception"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "massively parallel multiview stereopsis",
                "surface normal diffusion",
                "high-quality multiview matching",
                "Patchmatch idea",
                "randomly generated 3D planes",
                "best-fitting planes",
                "robust photo-consistency measure",
                "image similarity",
                "depth maps",
                "modified diffusion-like propagation scheme",
                "1.9-megapixel images",
                "consumer-grade GPU",
                "high depth resolution",
                "DTU multiview datasets",
                "Middlebury multiview datasets",
                "oblique aerial images"
            ]
        },
        "id": 97,
        "cited_by": [
            {
                "year": "2017",
                "id": 166
            },
            {
                "year": "2017",
                "id": 243
            },
            {
                "year": "2017",
                "id": 326
            },
            {
                "year": "2017",
                "id": 491
            }
        ]
    },
    {
        "title": "Variational PatchMatch MultiView Reconstruction and Refinement",
        "authors": [
            "Philipp Heise",
            "Brian Jensen",
            "Sebastian Klose",
            "Alois Knoll"
        ],
        "abstract": "In this work we propose a novel approach to the problem of multi-view stereo reconstruction. Building upon the previously proposed PatchMatch stereo and PM-Huber algorithm we introduce an extension to the multi-view scenario that employs an iterative refinement scheme. Our proposed approach uses an extended and robustified volumetric truncated signed distance function representation, which is advantageous for the fusion of refined depth maps and also for raycasting the current reconstruction estimation together with estimated depth normals into arbitrary camera views. We formulate the combined multi-view stereo reconstruction and refinement as a variational optimization problem. The newly introduced plane based smoothing term in the energy formulation is guided by the current reconstruction confidence and the image contents. Further we propose an extension of the PatchMatch scheme with an additional KLT step to avoid unnecessary sampling iterations. Improper camera poses are corrected by a direct image aligment step that performs robust outlier compensation by means of a recently proposed kernel lifting framework. To speed up the optimization of the variational formulation an adapted scheme is used for faster convergence.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410464",
        "reference_list": [
            {
                "year": "2013",
                "id": 294
            },
            {
                "year": "2011",
                "id": 295
            },
            {
                "year": "2009",
                "id": 206
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Cameras",
                "Visualization",
                "Robustness",
                "Optimization",
                "Estimation",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "image fusion",
                "image reconstruction",
                "iterative methods",
                "optimisation",
                "ray tracing",
                "smoothing methods",
                "stereo image processing",
                "variational techniques"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview stereo refinement",
                "multiview stereo reconstruction",
                "iterative refinement scheme",
                "volumetric truncated signed distance function representation",
                "depth map fusion",
                "variational optimization problem",
                "plane based smoothing term",
                "energy formulation",
                "PatchMatch stereo",
                "KLT step",
                "direct image alignment",
                "outlier compensation",
                "kernel lifting framework",
                "convergence",
                "raycasting"
            ]
        },
        "id": 98,
        "cited_by": [
            {
                "year": "2017",
                "id": 402
            }
        ]
    },
    {
        "title": "As-Rigid-as-Possible Volumetric Shape-from-Template",
        "authors": [
            "Shaifali Parashar",
            "Daniel Pizarro",
            "Adrien Bartoli",
            "Toby Collins"
        ],
        "abstract": "The objective of Shape-from-Template (SfT) is to infer an object's shape from a single image and a 3D object template. Existing methods are called thin-shell SfT as they represent the object by its outer surface. This may be an open surface for thin objects such as a piece of paper or a closed surface for thicker objects such as a ball. We propose volumetric SfT, which specifically handles objects of the latter kind. Volumetric SfT uses the object's full volume to express the deformation constraints and reconstructs the object's surface and interior deformation. This is a challenging problem because for opaque objects, only a part of the outer surface is visible in the image. Inspired by mesh-editing techniques, we use an As-Rigid-As-Possible (ARAP) deformation model that softly imposes local rigidity. We formalise ARAP isometric SfT as a constrained variational optimisation problem which we solve using iterative optimisation. We present strategies to find an initial solution based on thin-shell SfT and volume propagation. Experiments with synthetic and real data show that our method has a typical maximum relative error of 5% in reconstructing the deformation of an entire object, including its back and interior for which no visual data is available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410465",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Three-dimensional displays",
                "Shape",
                "Deformable models",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image representation",
                "iterative methods",
                "optimisation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape-from-template",
                "3D object template",
                "thin-shell SfT",
                "object representation",
                "object reconstruction",
                "as-rigid-as-possible deformation model",
                "ARAP deformation model",
                "mesh-editing technique",
                "iterative optimisation"
            ]
        },
        "id": 99,
        "cited_by": [
            {
                "year": "2017",
                "id": 96
            }
        ]
    },
    {
        "title": "General Dynamic Scene Reconstruction from Multiple View Video",
        "authors": [
            "Armin Mustafa",
            "Hansung Kim",
            "Jean-Yves Guillemaut",
            "Adrian Hilton"
        ],
        "abstract": "This paper introduces a general approach to dynamic scene reconstruction from multiple moving cameras without prior knowledge or limiting constraints on the scene structure, appearance, or illumination. Existing techniques or dynamic scene reconstruction from multiple wide-baseline camera views primarily focus on accurate reconstruction in controlled environments, where the cameras are fixed and calibrated and background is known. These approaches are not robust for general dynamic scenes captured with sparse moving cameras. Previous approaches for outdoor dynamic scene reconstruction assume prior knowledge of the static background appearance and structure. The primary contributions of this paper are twofold: an automatic method for initial coarse dynamic scene segmentation and reconstruction without prior knowledge of background appearance or structure, and a general robust approach for joint segmentation refinement and dense reconstruction of dynamic scenes from multiple wide-baseline static or moving cameras. Evaluation is performed on a variety of indoor and outdoor scenes with cluttered backgrounds and multiple dynamic non-rigid objects such as people. Comparison with state-of-the-art approaches demonstrates improved accuracy in both multiple view segmentation and dense reconstruction. The proposed approach also eliminates the requirement for prior knowledge of scene structure and appearance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410466",
        "reference_list": [
            {
                "year": "2007",
                "id": 173
            },
            {
                "year": "2009",
                "id": 201
            },
            {
                "year": "2007",
                "id": 124
            },
            {
                "year": "2009",
                "id": 219
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Cameras",
                "Three-dimensional displays",
                "Robustness",
                "Optical sensors",
                "Image color analysis",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image reconstruction",
                "image segmentation",
                "image sensors",
                "lighting",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "general dynamic scene reconstruction",
                "multiple view video",
                "multiple moving cameras",
                "scene structure",
                "scene appearance",
                "scene illumination",
                "multiple wide-baseline camera views",
                "sparse moving cameras",
                "coarse dynamic scene segmentation",
                "coarse dynamic scene reconstruction",
                "wide-baseline static cameras",
                "dynamic nonrigid objects",
                "multiple view segmentation"
            ]
        },
        "id": 100,
        "cited_by": []
    },
    {
        "title": "The Joint Image Handbook",
        "authors": [
            "Matthew Trager",
            "Martial Hebert",
            "Jean Ponce"
        ],
        "abstract": "Given multiple perspective photographs, point correspondences form the \"joint image\", effectively a replica of three dimensional space distributed across its two-dimensional projections. This set can be characterized by multilinear equations over image coordinates, such as epipolar and trifocal constraints. We revisit in this paper the geometric and algebraic properties of the joint image, and address fundamental questions such as how many and which multilinearities are necessary and/or sufficient to determine camera geometry and/or image correspondences. The new theoretical results in this paper answer these questions in a very general setting and, in turn, are intended to serve as a \"handbook\" reference about multilinearities for practitioners.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410467",
        "reference_list": [
            {
                "year": "2005",
                "id": 88
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 3,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Geometry",
                "Manganese",
                "Tensile stress",
                "Conferences",
                "Computer vision",
                "Algebra"
            ],
            "INSPEC: Controlled Indexing": [
                "algebra",
                "cameras",
                "geometry",
                "image processing",
                "photography"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint image handbook",
                "perspective photographs",
                "point correspondences",
                "three dimensional space",
                "two-dimensional projections",
                "multilinear equations",
                "image coordinates",
                "epipolar constraints",
                "trifocal constraints",
                "geometric properties",
                "algebraic properties",
                "multilinearities",
                "camera geometry",
                "image correspondences"
            ]
        },
        "id": 101,
        "cited_by": []
    },
    {
        "title": "Direct, Dense, and Deformable: Template-Based Non-rigid 3D Reconstruction from RGB Video",
        "authors": [
            "Rui Yu",
            "Chris Russell",
            "Neill D. F. Campbell",
            "Lourdes Agapito"
        ],
        "abstract": "In this paper we tackle the problem of capturing the dense, detailed 3D geometry of generic, complex non-rigid meshes using a single RGB-only commodity video camera and a direct approach. While robust and even real-time solutions exist to this problem if the observed scene is static, for non-rigid dense shape capture current systems are typically restricted to the use of complex multi-camera rigs, take advantage of the additional depth channel available in RGB-D cameras, or deal with specific shapes such as faces or planar surfaces. In contrast, our method makes use of a single RGB video as input, it can capture the deformations of generic shapes, and the depth estimation is dense, per-pixel and direct. We first compute a dense 3D template of the shape of the object, using a short rigid sequence, and subsequently perform online reconstruction of the non-rigid mesh as it evolves over time. Our energy optimization approach minimizes a robust photometric cost that simultaneously estimates the temporal correspondences and 3D deformations with respect to the template mesh. In our experimental evaluation we show a range of qualitative results on novel datasets, we compare against an existing method that requires multi-frame optical flow, and perform a quantitative evaluation against other template-based approaches on a ground truth dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410468",
        "reference_list": [
            {
                "year": "2011",
                "id": 295
            },
            {
                "year": "2013",
                "id": 8
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 10,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three-dimensional displays",
                "Cameras",
                "Streaming media",
                "Image reconstruction",
                "Robustness",
                "Surface reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sequences",
                "real-time systems",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "template-based nonrigid 3D reconstruction",
                "3D geometry",
                "complex nonrigid meshes",
                "real-time solutions",
                "single RGB-only commodity video camera",
                "direct approach",
                "planar surfaces",
                "generic shapes",
                "depth estimation",
                "energy optimization approach",
                "multiframe optical flow",
                "quantitative evaluation",
                "template-based approaches",
                "ground truth dataset"
            ]
        },
        "id": 102,
        "cited_by": []
    },
    {
        "title": "Single Image Pop-Up from Discriminatively Learned Parts",
        "authors": [
            "Menglong Zhu",
            "Xiaowei Zhou",
            "Kostas Daniilidis"
        ],
        "abstract": "We introduce a new approach for estimating a fine grained 3D shape and continuous pose of an object from a single image. Given a training set of view exemplars, we learn and select appearance-based discriminative parts which are mapped onto the 3D model through a facility location optimization. The training set of 3D models is summarized into a set of basis shapes from which we can generalize by linear combination. Given a test image, we detect hypotheses for each part. The main challenge is to select from these hypotheses and compute the 3D pose and shape coefficients at the same time. To achieve this, we optimize a function that considers simultaneously the appearance matching of the parts as well as the geometric reprojection error. We apply the alternating direction method of multipliers (ADMM) to minimize the resulting convex function. Our main and novel contribution is the simultaneous solution for part localization and detailed 3D geometry estimation by maximizing both appearance and geometric compatibility with convex relaxation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410469",
        "reference_list": [
            {
                "year": "2013",
                "id": 376
            },
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2011",
                "id": 4
            },
            {
                "year": "2013",
                "id": 373
            },
            {
                "year": "2007",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Solid modeling",
                "Training",
                "Geometry",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "image matching",
                "optimisation",
                "pose estimation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single image pop-up",
                "fine grained 3D shape",
                "appearance-based discriminative parts",
                "3D model",
                "facility location optimization",
                "linear combination",
                "3D pose",
                "appearance matching",
                "geometric reprojection error",
                "alternating direction method of multipliers",
                "ADMM",
                "convex function",
                "3D geometry estimation",
                "geometric compatibility",
                "convex relaxation"
            ]
        },
        "id": 103,
        "cited_by": []
    },
    {
        "title": "Learning Informative Edge Maps for Indoor Scene Layout Prediction",
        "authors": [
            "Arun Mallya",
            "Svetlana Lazebnik"
        ],
        "abstract": "In this paper, we introduce new edge-based features for the task of recovering the 3D layout of an indoor scene from a single image. Indoor scenes have certain edges that are very informative about the spatial layout of the room, namely, the edges formed by the pairwise intersections of room faces (two walls, wall and ceiling, wall and floor). In contrast with previous approaches that rely on area-based features like geometric context and orientation maps, our method attempts to directly detect these informative edges. We learn to predict 'informative edge' probability maps using two recent methods that exploit local and global context, respectively: structured edge detection forests, and a fully convolutional network for pixelwise labeling. We show that the fully convolutional network is quite successful at predicting the informative edges even when they lack contrast or are occluded, and that the accuracy can be further improved by training the network to jointly predict the edges and the geometric context. Using features derived from the 'informative edge' maps, we learn a maximum margin structured classifier that achieves state-of-the-art performance on layout prediction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410470",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2013",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 11,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Layout",
                "Three-dimensional displays",
                "Context",
                "Clutter",
                "Training",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "edge detection",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "informative edge maps learning",
                "indoor scene layout prediction",
                "edge-based features",
                "3D layout",
                "area-based features",
                "geometric context",
                "orientation maps",
                "local context",
                "global context",
                "structured edge detection forests",
                "convolutional network",
                "pixelwise labeling",
                "maximum margin structured classifier"
            ]
        },
        "id": 104,
        "cited_by": [
            {
                "year": "2017",
                "id": 511
            }
        ]
    },
    {
        "title": "Multi-view Convolutional Neural Networks for 3D Shape Recognition",
        "authors": [
            "Hang Su",
            "Subhransu Maji",
            "Evangelos Kalogerakis",
            "Erik Learned-Miller"
        ],
        "abstract": "A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410471",
        "reference_list": [],
        "citation": {
            "ieee": 204,
            "other": 137,
            "total": 341
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Solid modeling",
                "Cameras",
                "Image recognition",
                "Computer architecture",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convolution",
                "image representation",
                "learning (artificial intelligence)",
                "mesh generation",
                "neural nets",
                "rendering (computer graphics)",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview convolutional neural networks",
                "3D shape recognition",
                "computer vision",
                "hand-drawn shape sketch recognition",
                "3D shape descriptors",
                "shape rendered views",
                "2D images",
                "learning",
                "view-based descriptors",
                "polygon mesh",
                "voxel grid",
                "3D shape representation"
            ]
        },
        "id": 105,
        "cited_by": [
            {
                "year": "2017",
                "id": 16
            },
            {
                "year": "2017",
                "id": 80
            },
            {
                "year": "2017",
                "id": 89
            },
            {
                "year": "2017",
                "id": 161
            },
            {
                "year": "2017",
                "id": 242
            }
        ]
    },
    {
        "title": "Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images",
        "authors": [
            "Alexander Krull",
            "Eric Brachmann",
            "Frank Michel",
            "Michael Ying Yang",
            "Stefan Gumhold",
            "Carsten Rother"
        ],
        "abstract": "Analysis-by-synthesis has been a successful approach for many tasks in computer vision, such as 6D pose estimation of an object in an RGB-D image which is the topic of this work. The idea is to compare the observation with the output of a forward process, such as a rendered image of the object of interest in a particular pose. Due to occlusion or complicated sensor noise, it can be difficult to perform this comparison in a meaningful way. We propose an approach that \"learns to compare\", while taking these difficulties into account. This is done by describing the posterior density of a particular object pose with a convolutional neural network (CNN) that compares observed and rendered images. The network is trained with the maximum likelihood paradigm. We observe empirically that the CNN does not specialize to the geometry or appearance of specific objects. It can be used with objects of vastly different shapes and appearances, and in different backgrounds. Compared to state-of-the-art, we demonstrate a significant improvement on two different datasets which include a total of eleven objects, cluttered background, and heavy occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410472",
        "reference_list": [
            {
                "year": "2013",
                "id": 380
            },
            {
                "year": "2015",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 19,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Probabilistic logic",
                "Computer vision",
                "Image segmentation",
                "Maximum likelihood estimation",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "geometry",
                "image colour analysis",
                "learning (artificial intelligence)",
                "maximum likelihood estimation",
                "neural nets",
                "pose estimation",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometry",
                "maximum likelihood paradigm",
                "rendered image",
                "CNN",
                "convolutional neural network",
                "computer vision",
                "analysis-by-synthesis",
                "RGB-D images",
                "6D pose estimation",
                "learning"
            ]
        },
        "id": 106,
        "cited_by": [
            {
                "year": "2017",
                "id": 403
            },
            {
                "year": "2017",
                "id": 406
            }
        ]
    },
    {
        "title": "3D Surface Profilometry Using Phase Shifting of De Bruijn Pattern",
        "authors": [
            "Matea \u00d0onlic",
            "Tomislav Petkovic",
            "Tomislav Pribanic"
        ],
        "abstract": "A novel structured light method for color 3D surface profilometry is proposed. The proposed method does not require color calibration of a camera-projector pair and may be used for reconstruction of both dynamic and static scenes. The method uses a structured light pattern that is a combination of a De Bruijn color sequence and of a sinusoidal fringe. For dynamic scenes a Hessian ridge detector and a Gaussian mixture model are combined to extract stripe centers and to identify color. Stripes are then uniquely identified using dynamic programming based on the Smith-Waterman algorithm and a De Bruijn window property. For static scenes phase-shifting and De Bruijn window property are combined to obtain a high accuracy reconstruction. We have tested the proposed method on multiple objects with challenging surfaces and different albedos that demonstrate usability and robustness of the method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410473",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Image reconstruction",
                "Three-dimensional displays",
                "Calibration",
                "Surface reconstruction",
                "Decoding",
                "Dynamic programming"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "dynamic programming",
                "Gaussian processes",
                "Hessian matrices",
                "image colour analysis",
                "image reconstruction",
                "image sequences",
                "mixture models"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D surface profilometry",
                "De Bruijn pattern",
                "structured light method",
                "color 3D surface profilometry",
                "color calibration",
                "camera-projector pair",
                "dynamic scenes",
                "De Bruijn color sequence",
                "sinusoidal fringe",
                "Hessian ridge detector",
                "Gaussian mixture model",
                "dynamic programming",
                "Smith-Waterman algorithm",
                "De Bruijn window property",
                "static scene phase-shifting"
            ]
        },
        "id": 107,
        "cited_by": []
    },
    {
        "title": "A Deep Visual Correspondence Embedding Model for Stereo Matching Costs",
        "authors": [
            "Zhuoyuan Chen",
            "Xun Sun",
            "Liang Wang",
            "Yinan Yu",
            "Chang Huang"
        ],
        "abstract": "This paper presents a data-driven matching cost for stereo matching. A novel deep visual correspondence embedding model is trained via Convolutional Neural Network on a large set of stereo images with ground truth disparities. This deep embedding model leverages appearance data to learn visual similarity relationships between corresponding image patches, and explicitly maps intensity values into an embedding feature space to measure pixel dissimilarities. Experimental results on KITTI and Middlebury data sets demonstrate the effectiveness of our model. First, we prove that the new measure of pixel dissimilarity outperforms traditional matching costs. Furthermore, when integrated with a global stereo framework, our method ranks top 3 among all two-frame algorithms on the KITTI benchmark. Finally, cross-validation results show that our model is able to make correct predictions for unseen data which are outside of its labeled training set.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410474",
        "reference_list": [
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 18,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Computational modeling",
                "Visualization",
                "Data models",
                "Training",
                "Neural networks",
                "Machine learning"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "neural nets",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep visual correspondence embedding model",
                "stereo matching costs",
                "data-driven matching cost",
                "convolutional neural network",
                "stereo images",
                "ground truth disparities",
                "visual similarity relationships",
                "image patches",
                "embedding feature space",
                "pixel dissimilarity measurement",
                "Middlebury data sets",
                "global stereo framework",
                "KITTI benchmark",
                "cross-validation results",
                "labeled training set"
            ]
        },
        "id": 108,
        "cited_by": [
            {
                "year": "2017",
                "id": 7
            },
            {
                "year": "2017",
                "id": 140
            },
            {
                "year": "2017",
                "id": 164
            },
            {
                "year": "2017",
                "id": 168
            },
            {
                "year": "2017",
                "id": 188
            },
            {
                "year": "2017",
                "id": 549
            }
        ]
    },
    {
        "title": "Learning Concept Embeddings with Combined Human-Machine Expertise",
        "authors": [
            "Michael J. Wilber",
            "Iljung S. Kwak",
            "David Kriegman",
            "Serge Belongie"
        ],
        "abstract": "This paper presents our work on \"SNaCK,\" a low-dimensional concept embedding algorithm that combines human expertise with automatic machine similarity kernels. Both parts are complimentary: human insight can capture relationships that are not apparent from the object's visual similarity and the machine can help relieve the human from having to exhaustively specify many constraints. We show that our SNaCK embeddings are useful in several tasks: distinguishing prime and nonprime numbers on MNIST, discovering labeling mistakes in the Caltech UCSD Birds (CUB) dataset with the help of deep-learned features, creating training datasets for bird classifiers, capturing subjective human taste on a new dataset of 10,000 foods, and qualitatively exploring an unstructured set of pictographic characters. Comparisons with the state-of-the-art in these tasks show that SNaCK produces better concept embeddings that require less human supervision than the leading methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410475",
        "reference_list": [
            {
                "year": "2011",
                "id": 20
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Visualization",
                "Labeling",
                "Birds",
                "Crowdsourcing",
                "Machine learning",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human-machine expertise",
                "low-dimensional concept embedding algorithm",
                "automatic machine similarity kernels",
                "human insight",
                "object visual similarity",
                "SNaCK embeddings",
                "nonprime numbers",
                "MNIST",
                "labeling mistakes discovery",
                "Caltech UCSD Birds",
                "CUB dataset",
                "deep-learned features",
                "bird classifiers",
                "pictographic characters",
                "supervised learning"
            ]
        },
        "id": 109,
        "cited_by": []
    },
    {
        "title": "Deep Multi-patch Aggregation Network for Image Style, Aesthetics, and Quality Estimation",
        "authors": [
            "Xin Lu",
            "Zhe Lin",
            "Xiaohui Shen",
            "Radom\u00edr Mech",
            "James Z. Wang"
        ],
        "abstract": "This paper investigates problems of image style, aesthetics, and quality estimation, which require fine-grained details from high-resolution images, utilizing deep neural network training approach. Existing deep convolutional neural networks mostly extracted one patch such as a down-sized crop from each image as a training example. However, one patch may not always well represent the entire image, which may cause ambiguity during training. We propose a deep multi-patch aggregation network training approach, which allows us to train models using multiple patches generated from one image. We achieve this by constructing multiple, shared columns in the neural network and feeding multiple patches to each of the columns. More importantly, we propose two novel network layers (statistics and sorting) to support aggregation of those patches. The proposed deep multi-patch aggregation network integrates shared feature learning and aggregation function learning into a unified framework. We demonstrate the effectiveness of the deep multi-patch aggregation network on the three problems, i.e., image style recognition, aesthetic quality categorization, and image quality estimation. Our models trained using the proposed networks significantly outperformed the state of the art in all three applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410476",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2013",
                "id": 424
            }
        ],
        "citation": {
            "ieee": 51,
            "other": 21,
            "total": 72
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Neural networks",
                "Feature extraction",
                "Estimation",
                "Image resolution",
                "Object detection",
                "Sorting"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image resolution",
                "learning (artificial intelligence)",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep multipatch aggregation network",
                "image aesthetics",
                "image quality estimation",
                "high-resolution images",
                "deep neural network training approach",
                "deep convolutional neural networks",
                "down-sized crop",
                "deep multipatch aggregation network training approach",
                "feature learning",
                "aggregation function learning",
                "aesthetic quality categorization",
                "image style recognition"
            ]
        },
        "id": 110,
        "cited_by": []
    },
    {
        "title": "Towards Computational Baby Learning: A Weakly-Supervised Approach for Object Detection",
        "authors": [
            "Xiaodan Liang",
            "Si Liu",
            "Yunchao Wei",
            "Luoqi Liu",
            "Liang Lin",
            "Shuicheng Yan"
        ],
        "abstract": "Intuitive observations show that a baby may inherently possess the capability of recognizing a new visual concept (e.g., chair, dog) by learning from only very few positive instances taught by parent(s) or others, and this recognition capability can be gradually further improved by exploring and/or interacting with the real instances in the physical world. Inspired by these observations, we propose a computational model for weakly-supervised object detection, based on prior knowledge modelling, exemplar learning and learning with video contexts. The prior knowledge is modeled with a pre-trained Convolutional Neural Network (CNN). When very few instances of a new concept are given, an initial concept detector is built by exemplar learning over the deep features the pre-trained CNN. The well-designed tracking solution is then used to discover more diverse instances from the massive online weakly labeled videos. Once a positive instance is detected/identified with high score in each video, more instances possibly from different view-angles and/or different distances are tracked and accumulated. Then the concept detector can be fine-tuned based on these new instances. This process can be repeated again and again till we obtain a very mature concept detector. Extensive experiments on Pascal VOC-07/10/12 object detection datasets [9] well demonstrate the effectiveness of our framework. It can beat the state-of-the-art full-training based performances by learning from very few samples for each object category, along with about 20,000 weakly labeled videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410477",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2013",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 10,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Pediatrics",
                "Computational modeling",
                "Context",
                "Object detection",
                "Training",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image recognition",
                "learning (artificial intelligence)",
                "neural nets",
                "object detection",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computational baby learning",
                "visual concept recognition",
                "recognition capability improvement",
                "weakly-supervised object detection",
                "prior knowledge modelling",
                "exemplar learning",
                "video contexts",
                "pre-trained convolutional neural network",
                "initial concept detector",
                "deep features",
                "pre-trained CNN",
                "online weakly labeled videos",
                "Pascal VOC-07/10/12 object detection datasets",
                "object category"
            ]
        },
        "id": 111,
        "cited_by": [
            {
                "year": "2017",
                "id": 190
            },
            {
                "year": "2017",
                "id": 354
            },
            {
                "year": "2015",
                "id": 154
            }
        ]
    },
    {
        "title": "Improving Image Classification with Location Context",
        "authors": [
            "Kevin Tang",
            "Manohar Paluri",
            "Li Fei-Fei",
            "Rob Fergus",
            "Lubomir Bourdev"
        ],
        "abstract": "With the widespread availability of cellphones and cameras that have GPS capabilities, it is common for images being uploaded to the Internet today to have GPS coordinates associated with them. In addition to research that tries to predict GPS coordinates from visual features, this also opens up the door to problems that are conditioned on the availability of GPS coordinates. In this work, we tackle the problem of performing image classification with location context, in which we are given the GPS coordinates for images in both the train and test phases. We explore different ways of encoding and extracting features from the GPS coordinates, and show how to naturally incorporate these features into a Convolutional Neural Network (CNN), the current state-of-the-art for most image classification and recognition problems. We also show how it is possible to simultaneously learn the optimal pooling radii for a subset of our features within the CNN framework. To evaluate our model and to help promote research in this area, we identify a set of location-sensitive concepts and annotate a subset of the Yahoo Flickr Creative Commons 100M dataset that has GPS coordinates with these concepts, which we make publicly available. By leveraging location context, we are able to achieve almost a 7% gain in mean average precision.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410478",
        "reference_list": [
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2007",
                "id": 155
            },
            {
                "year": "2013",
                "id": 231
            },
            {
                "year": "2013",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 9,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Global Positioning System",
                "Tagging",
                "Twitter",
                "Context",
                "Internet",
                "Image recognition",
                "Snow"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Global Positioning System",
                "image classification",
                "image coding",
                "mobile computing",
                "neural nets",
                "set theory",
                "smart phones"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Yahoo Flickr Creative Commons 100M dataset",
                "location-sensitive concepts",
                "CNN framework",
                "optimal pooling radii",
                "image recognition problems",
                "convolutional neural network",
                "feature encoding",
                "feature extraction",
                "GPS coordinates",
                "visual features",
                "Internet",
                "GPS capabilities",
                "cameras",
                "cellphones",
                "location context",
                "image classification"
            ]
        },
        "id": 112,
        "cited_by": [
            {
                "year": "2017",
                "id": 283
            }
        ]
    },
    {
        "title": "HICO: A Benchmark for Recognizing Human-Object Interactions in Images",
        "authors": [
            "Yu-Wei Chao",
            "Zhan Wang",
            "Yugeng He",
            "Jiaxuan Wang",
            "Jia Deng"
        ],
        "abstract": "We introduce a new benchmark \"Humans Interacting with Common Objects\" (HICO) for recognizing human-object interactions (HOI). We demonstrate the key features of HICO: a diverse set of interactions with common object categories, a list of well-defined, sense-based HOI categories, and an exhaustive labeling of co-occurring interactions with an object category in each image. We perform an in-depth analysis of representative current approaches and show that DNNs enjoy a significant edge. In addition, we show that semantic knowledge can significantly improve HOI recognition, especially for uncommon categories.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410479",
        "reference_list": [
            {
                "year": "2009",
                "id": 127
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2011",
                "id": 168
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 16,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Image recognition",
                "Bicycles",
                "Maintenance engineering",
                "Benchmark testing",
                "Semantics",
                "Pipelines"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "HICO",
                "Humans Interacting with Common Objects",
                "human-object interaction recognition",
                "images",
                "sense-based HOI categories",
                "cooccurring interaction exhaustive labeling",
                "DNNs",
                "HOI recognition"
            ]
        },
        "id": 113,
        "cited_by": [
            {
                "year": "2017",
                "id": 47
            },
            {
                "year": "2017",
                "id": 61
            },
            {
                "year": "2017",
                "id": 444
            }
        ]
    },
    {
        "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
        "authors": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1%, [26]) on this dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410480",
        "reference_list": [],
        "citation": {
            "ieee": 1265,
            "other": 931,
            "total": 2196
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Computational modeling",
                "Adaptation models",
                "Testing",
                "Gaussian distribution",
                "Biological neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human-level performance",
                "ILSVRC 2014 winner",
                "ImageNet 2012 classification dataset",
                "network architectures",
                "rectifier nonlinearities",
                "robust initialization method",
                "overfitting risk",
                "model fitting",
                "PReLU",
                "parametric rectified linear unit",
                "rectifier neural networks",
                "state-of-the-art neural networks",
                "rectified activation units",
                "ImageNet classification"
            ]
        },
        "id": 114,
        "cited_by": [
            {
                "year": "2017",
                "id": 21
            },
            {
                "year": "2017",
                "id": 31
            },
            {
                "year": "2017",
                "id": 44
            },
            {
                "year": "2017",
                "id": 48
            },
            {
                "year": "2017",
                "id": 52
            },
            {
                "year": "2017",
                "id": 68
            },
            {
                "year": "2017",
                "id": 134
            },
            {
                "year": "2017",
                "id": 218
            },
            {
                "year": "2017",
                "id": 242
            },
            {
                "year": "2017",
                "id": 278
            },
            {
                "year": "2017",
                "id": 282
            },
            {
                "year": "2017",
                "id": 288
            },
            {
                "year": "2017",
                "id": 295
            },
            {
                "year": "2017",
                "id": 298
            },
            {
                "year": "2017",
                "id": 331
            },
            {
                "year": "2017",
                "id": 334
            },
            {
                "year": "2017",
                "id": 341
            },
            {
                "year": "2017",
                "id": 398
            },
            {
                "year": "2017",
                "id": 414
            },
            {
                "year": "2017",
                "id": 459
            },
            {
                "year": "2017",
                "id": 476
            },
            {
                "year": "2017",
                "id": 504
            },
            {
                "year": "2017",
                "id": 511
            },
            {
                "year": "2017",
                "id": 545
            },
            {
                "year": "2017",
                "id": 546
            },
            {
                "year": "2017",
                "id": 568
            },
            {
                "year": "2015",
                "id": 296
            }
        ]
    },
    {
        "title": "Continuous Pose Estimation with a Spatial Ensemble of Fisher Regressors",
        "authors": [
            "Michele Fenzi",
            "Laura Leal-Taix\u00e9",
            "J\u00f6rn Ostermann",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this paper, we treat the problem of continuous pose estimation for object categories as a regression problem on the basis of only 2D training information. While regression is a natural framework for continuous problems, regression methods so far achieved inferior results with respect to 3D-based and 2D-based classification-and-refinement approaches. This may be attributed to their weakness to high intra-class variability as well as to noisy matching procedures and lack of geometrical constraints. We propose to apply regression to Fisher-encoded vectors computed from large cells by learning an array of Fisher regressors. Fisher encoding makes our algorithm flexible to variations in class appearance, while the array structure permits to indirectly introduce spatial context information in the approach. We formulate our problem as a MAP inference problem, where the likelihood function is composed of a generative term based on the prediction error generated by the ensemble of Fisher regressors as well as a discriminative term based on SVM classifiers. We test our algorithm on three publicly available datasets that envisage several difficulties, such as high intra-class variability, truncations, occlusions, and motion blur, obtaining state-of-the-art results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410481",
        "reference_list": [
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2011",
                "id": 188
            },
            {
                "year": "2011",
                "id": 331
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Training",
                "Feature extraction",
                "Encoding",
                "Three-dimensional displays",
                "Design automation"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image matching",
                "maximum likelihood estimation",
                "pose estimation",
                "prediction theory",
                "regression analysis",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatial ensemble",
                "continuous pose estimation",
                "object categories",
                "regression problem",
                "2D training information",
                "3D-based classification-and-refinement approaches",
                "2D-based classification-and-refinement approaches",
                "intra-class variability",
                "noisy matching procedures",
                "geometrical constraints",
                "Fisher-encoded vectors",
                "Fisher regressors",
                "Fisher encoding",
                "class appearance",
                "spatial context information",
                "MAP inference problem",
                "likelihood function",
                "prediction error",
                "SVM classifiers",
                "support vector machine"
            ]
        },
        "id": 115,
        "cited_by": []
    },
    {
        "title": "Adaptive Hashing for Fast Similarity Search",
        "authors": [
            "Fatih Cakir",
            "Stan Sclaroff"
        ],
        "abstract": "With the staggering growth in image and video datasets, algorithms that provide fast similarity search and compact storage are crucial. Hashing methods that map the data into Hamming space have shown promise, however, many of these methods employ a batch-learning strategy in which the computational cost and memory requirements may become intractable and infeasible with larger and larger datasets. To overcome these challenges, we propose an online learning algorithm based on stochastic gradient descent in which the hash functions are updated iteratively with streaming data. In experiments with three image retrieval benchmarks, our online algorithm attains retrieval accuracy that is comparable to competing state-of-the-art batch-learning solutions, while our formulation is orders of magnitude faster and being online it is adaptable to the variations of the data. Moreover, our formulation yields improved retrieval performance over a recently reported online hashing technique, Online Kernel Hashing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410482",
        "reference_list": [
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2013",
                "id": 318
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 6,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Binary codes",
                "Streaming media",
                "Benchmark testing",
                "Kernel",
                "Computer vision",
                "Computational efficiency",
                "Search problems"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "image retrieval",
                "learning (artificial intelligence)",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online kernel hashing",
                "online hashing technique",
                "retrieval performance",
                "batch-learning solution",
                "retrieval accuracy",
                "online algorithm",
                "image retrieval benchmark",
                "streaming data",
                "hash function",
                "stochastic gradient descent",
                "online learning algorithm",
                "memory requirement",
                "computational cost",
                "batch-learning strategy",
                "Hamming space",
                "hashing method",
                "compact storage",
                "video dataset",
                "image dataset",
                "staggering growth",
                "fast similarity search",
                "adaptive hashing"
            ]
        },
        "id": 116,
        "cited_by": [
            {
                "year": "2017",
                "id": 45
            }
        ]
    },
    {
        "title": "Single Image 3D without a Single 3D Image",
        "authors": [
            "David F. Fouhey",
            "Wajahat Hussain",
            "Abhinav Gupta",
            "Martial Hebert"
        ],
        "abstract": "Do we really need 3D labels in order to learn how to predict 3D? In this paper, we show that one can learn a mapping from appearance to 3D properties without ever seeing a single explicit 3D label. Rather than use explicit supervision, we use the regularity of indoor scenes to learn the mapping in a completely unsupervised manner. We demonstrate this on both a standard 3D scene understanding dataset as well as Internet images for which 3D is unavailable, precluding supervised learning. Despite never seeing a 3D label, our method produces competitive results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410483",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2013",
                "id": 423
            },
            {
                "year": "2013",
                "id": 267
            },
            {
                "year": "2009",
                "id": 237
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2013",
                "id": 231
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Detectors",
                "Face",
                "Training",
                "Dictionaries",
                "Standards"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single image 3D",
                "3D properties",
                "indoor scenes",
                "standard 3D scene",
                "Internet images",
                "supervised learning"
            ]
        },
        "id": 117,
        "cited_by": []
    },
    {
        "title": "Cross-Domain Image Retrieval with a Dual Attribute-Aware Ranking Network",
        "authors": [
            "Junshi Huang",
            "Rogerio Feris",
            "Qiang Chen",
            "Shuicheng Yan"
        ],
        "abstract": "We address the problem of cross-domain image retrieval, considering the following practical application: given a user photo depicting a clothing image, our goal is to retrieve the same or attribute-similar clothing items from online shopping stores. This is a challenging problem due to the large discrepancy between online shopping images, usually taken in ideal lighting/pose/background conditions, and user photos captured in uncontrolled conditions. To address this problem, we propose a Dual Attribute-aware Ranking Network (DARN) for retrieval feature learning. More specifically, DARN consists of two sub-networks, one for each domain, whose retrieval feature representations are driven by semantic attribute learning. We show that this attribute-guided learning is a key factor for retrieval accuracy improvement. In addition, to further align with the nature of the retrieval problem, we impose a triplet visual similarity constraint for learning to rank across the two subnetworks. Another contribution of our work is a large-scale dataset which makes the network learning feasible. We exploit customer review websites to crawl a large set of online shopping images and corresponding offline user photos with fine-grained clothing attributes, i.e., around 450,000 online shopping images and about 90,000 exact offline counterpart images of those online ones. All these images are collected from real-world consumer websites reflecting the diversity of the data modality, which makes this dataset unique and rare in the academic community. We extensively evaluate the retrieval performance of networks in different configurations. The top-20 retrieval accuracy is doubled when using the proposed DARN other than the current popular solution using pre-trained CNN features only (0.570 vs. 0.268).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410484",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2011",
                "id": 176
            },
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 137
            }
        ],
        "citation": {
            "ieee": 63,
            "other": 30,
            "total": 93
        },
        "keywords": {
            "IEEE Keywords": [
                "Clothing",
                "Visualization",
                "Image retrieval",
                "Semantics",
                "Shape",
                "Image color analysis",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "clothing",
                "consumer behaviour",
                "feature extraction",
                "image representation",
                "image retrieval",
                "learning (artificial intelligence)",
                "neural nets",
                "retail data processing",
                "Web sites"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cross-domain image retrieval",
                "dual attribute-aware ranking network",
                "clothing image",
                "attribute-similar clothing items",
                "online shopping stores",
                "online shopping images",
                "background conditions",
                "pose conditions",
                "lighting conditions",
                "DARN",
                "retrieval feature learning",
                "semantic attribute learning",
                "attribute-guided learning",
                "network learning",
                "customer review Web sites",
                "fine-grained clothing attributes",
                "real-world consumer Web sites",
                "data modality",
                "pretrained CNN features"
            ]
        },
        "id": 118,
        "cited_by": [
            {
                "year": "2017",
                "id": 40
            },
            {
                "year": "2017",
                "id": 58
            },
            {
                "year": "2017",
                "id": 153
            },
            {
                "year": "2017",
                "id": 195
            },
            {
                "year": "2017",
                "id": 276
            },
            {
                "year": "2017",
                "id": 441
            }
        ]
    },
    {
        "title": "Attribute-Graph: A Graph Based Approach to Image Ranking",
        "authors": [
            "Nikita Prabhu",
            "R. Venkatesh Babu"
        ],
        "abstract": "We propose a novel image representation, termed Attribute-Graph, to rank images by their semantic similarity to a given query image. An Attribute-Graph is an undirected fully connected graph, incorporating both local and global image characteristics. The graph nodes characterise objects as well as the overall scene context using mid-level semantic attributes, while the edges capture the object topology. We demonstrate the effectiveness of Attribute-Graphs by applying them to the problem of image ranking. We benchmark the performance of our algorithm on the 'rPascal' and 'rImageNet' datasets, which we have created in order to evaluate the ranking performance on complex queries containing multiple objects. Our experimental evaluation shows that modelling images as Attribute-Graphs results in improved ranking performance over existing techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410485",
        "reference_list": [
            {
                "year": "2013",
                "id": 428
            },
            {
                "year": "2013",
                "id": 37
            },
            {
                "year": "2013",
                "id": 92
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Semantics",
                "Feature extraction",
                "Context",
                "Image representation",
                "Image retrieval",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image representation",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "attribute-graph",
                "graph based approach",
                "image ranking",
                "image representation",
                "query image",
                "undirected fully connected graph"
            ]
        },
        "id": 119,
        "cited_by": []
    },
    {
        "title": "Contextual Action Recognition with R*CNN",
        "authors": [
            "Georgia Gkioxari",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "There are multiple cues in an image which reveal what action a person is performing. For example, a jogger has a pose that is characteristic for jogging, but the scene (e.g. road, trail) and the presence of other joggers can be an additional source of information. In this work, we exploit the simple observation that actions are accompanied by contextual cues to build a strong action recognition system. We adapt RCNN to use more than one region for classification while still maintaining the ability to localize the action. We call our system R*CNN. The action-specific models and the feature maps are trained jointly, allowing for action specific representations to emerge. R*CNN achieves 90.2% mean AP on the PASAL VOC Action dataset, outperforming all other approaches in the field by a significant margin. Last, we show that R*CNN is not limited to action recognition. In particular, R*CNN can also be used to tackle fine-grained tasks such as attribute classification. We validate this claim by reporting state-of-the-art performance on the Berkeley Attributes of People dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410486",
        "reference_list": [
            {
                "year": "2015",
                "id": 160
            },
            {
                "year": "2015",
                "id": 275
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 61,
            "other": 43,
            "total": 104
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Training",
                "Proposals",
                "Computer vision",
                "Computer architecture",
                "Object detection",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image motion analysis",
                "neural nets",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contextual action recognition",
                "R*CNN",
                "action-specific models",
                "feature maps",
                "action specific representations",
                "PASAL VOC action dataset",
                "attribute classification",
                "Berkeley attributes of people dataset"
            ]
        },
        "id": 120,
        "cited_by": [
            {
                "year": "2017",
                "id": 55
            },
            {
                "year": "2017",
                "id": 176
            },
            {
                "year": "2017",
                "id": 216
            },
            {
                "year": "2017",
                "id": 279
            },
            {
                "year": "2017",
                "id": 357
            },
            {
                "year": "2017",
                "id": 430
            }
        ]
    },
    {
        "title": "What Makes an Object Memorable?",
        "authors": [
            "Rachit Dubey",
            "Joshua Peterson",
            "Aditya Khosla",
            "Ming-Hsuan Yang",
            "Bernard Ghanem"
        ],
        "abstract": "Recent studies on image memorability have shed light on what distinguishes the memorability of different images and the intrinsic and extrinsic properties that make those images memorable. However, a clear understanding of the memorability of specific objects inside an image remains elusive. In this paper, we provide the first attempt to answer the question: what exactly is remembered about an image? We augment both the images and object segmentations from the PASCAL-S dataset with ground truth memorability scores and shed light on the various factors and properties that make an object memorable (or forgettable) to humans. We analyze various visual factors that may influence object memorability (e.g. color, visual saliency, and object categories). We also study the correlation between object and image memorability and find that image memorability is greatly affected by the memorability of its most memorable object. Lastly, we explore the effectiveness of deep learning and other computational approaches in predicting object memorability in images. Our efforts offer a deeper understanding of memorability in general thereby opening up avenues for a wide variety of applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410487",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2013",
                "id": 399
            },
            {
                "year": "2015",
                "id": 266
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 6,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Image segmentation",
                "Image color analysis",
                "Correlation",
                "Games",
                "Computer vision",
                "Presses"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image memorability",
                "object memorability",
                "object segmentation",
                "image segmentation",
                "PASCAL-S dataset",
                "ground truth memorability scores",
                "visual factors",
                "visual saliency",
                "object categories",
                "color"
            ]
        },
        "id": 121,
        "cited_by": [
            {
                "year": "2015",
                "id": 266
            }
        ]
    },
    {
        "title": "kNN Hashing with Factorized Neighborhood Representation",
        "authors": [
            "Kun Ding",
            "Chunlei Huo",
            "Bin Fan",
            "Chunhong Pan"
        ],
        "abstract": "Hashing is very effective for many tasks in reducing the processing time and in compressing massive databases. Although lots of approaches have been developed to learn data-dependent hash functions in recent years, how to learn hash functions to yield good performance with acceptable computational and memory cost is still a challenging problem. Based on the observation that retrieval precision is highly related to the kNN classification accuracy, this paper proposes a novel kNN-based supervised hashing method, which learns hash functions by directly maximizing the kNN accuracy of the Hamming-embedded training data. To make it scalable well to large problem, we propose a factorized neighborhood representation to parsimoniously model the neighborhood relationships inherent in training data. Considering that real-world data are often linearly inseparable, we further kernelize this basic model to improve its performance. As a result, the proposed method is able to learn accurate hashing functions with tolerable computation and storage cost. Experiments on four benchmarks demonstrate that our method outperforms the state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410488",
        "reference_list": [
            {
                "year": "2013",
                "id": 32
            },
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2013",
                "id": 318
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Binary codes",
                "Training",
                "Training data",
                "Computational modeling",
                "Data models",
                "Linear programming"
            ],
            "INSPEC: Controlled Indexing": [
                "embedded systems",
                "file organisation",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "factorized neighborhood representation",
                "data-dependent hash function",
                "kNN classification accuracy",
                "kNN-based supervised hashing method",
                "hamming-embedded training data",
                "storage cost"
            ]
        },
        "id": 122,
        "cited_by": []
    },
    {
        "title": "Multi-View Complementary Hash Tables for Nearest Neighbor Search",
        "authors": [
            "Xianglong Liu",
            "Lei Huang",
            "Cheng Deng",
            "Jiwen Lu",
            "Bo Lang"
        ],
        "abstract": "Recent years have witnessed the success of hashing techniques in fast nearest neighbor search. In practice many applications (eg., visual search, object detection, image matching, etc.) have enjoyed the benefits of complementary hash tables and information fusion over multiple views. However, most of prior research mainly focused on compact hash code cleaning, and rare work studies how to build multiple complementary hash tables, much less to adaptively integrate information stemming from multiple views. In this paper we first present a novel multi-view complementary hash table method that learns complementarity hash tables from the data with multiple views. For single multi-view table, using exemplar based feature fusion, we approximate the inherent data similarities with a low-rank matrix, and learn discriminative hash functions in an efficient way. To build complementary tables and meanwhile maintain scalable training and fast out-of-sample extension, an exemplar reweighting scheme is introduced to update the induced low-rank similarity in the sequential table construction framework, which indeed brings mutual benefits between tables by placing greater importance on exemplars shared by mis-separated neighbors. Extensive experiments on three large-scale image datasets demonstrate that the proposed method significantly outperforms various naive solutions and state-of-the-art multi-table methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410489",
        "reference_list": [
            {
                "year": "2015",
                "id": 463
            },
            {
                "year": "2013",
                "id": 378
            },
            {
                "year": "2011",
                "id": 206
            },
            {
                "year": "2013",
                "id": 283
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 6,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Binary codes",
                "Visualization",
                "Nearest neighbor searches",
                "Training",
                "Matrix decomposition",
                "Eigenvalues and eigenfunctions",
                "Image matching"
            ],
            "INSPEC: Controlled Indexing": [
                "Big Data",
                "feature extraction",
                "file organisation",
                "image fusion",
                "image retrieval",
                "matrix algebra",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nearest neighbor search",
                "hashing techniques",
                "visual search",
                "object detection",
                "image matching",
                "information fusion",
                "compact hash code cleaning",
                "information stemming",
                "multiview complementary hash table method",
                "exemplar based feature fusion",
                "data similarities",
                "low-rank matrix",
                "discriminative hash functions",
                "exemplar reweighting scheme",
                "large-scale image datasets"
            ]
        },
        "id": 123,
        "cited_by": [
            {
                "year": "2015",
                "id": 463
            }
        ]
    },
    {
        "title": "Scalable Person Re-identification: A Benchmark",
        "authors": [
            "Liang Zheng",
            "Liyue Shen",
            "Lu Tian",
            "Shengjin Wang",
            "Jingdong Wang",
            "Qi Tian"
        ],
        "abstract": "This paper contributes a new high quality dataset for person re-identification, named \"Market-1501\". Generally, current datasets: 1) are limited in scale, 2) consist of hand-drawn bboxes, which are unavailable under realistic settings, 3) have only one ground truth and one query image for each identity (close environment). To tackle these problems, the proposed Market-1501 dataset is featured in three aspects. First, it contains over 32,000 annotated bboxes, plus a distractor set of over 500K images, making it the largest person re-id dataset to date. Second, images in Market-1501 dataset are produced using the Deformable Part Model (DPM) as pedestrian detector. Third, our dataset is collected in an open system, where each identity has multiple images under each camera. As a minor contribution, inspired by recent advances in large-scale image search, this paper proposes an unsupervised Bag-of-Words descriptor. We view person re-identification as a special task of image search. In experiment, we show that the proposed descriptor yields competitive accuracy on VIPeR, CUHK03, and Market-1501 datasets, and is scalable on the large-scale 500k dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410490",
        "reference_list": [
            {
                "year": "2013",
                "id": 55
            },
            {
                "year": "2015",
                "id": 357
            },
            {
                "year": "2015",
                "id": 417
            },
            {
                "year": "2013",
                "id": 315
            }
        ],
        "citation": {
            "ieee": 286,
            "other": 181,
            "total": 467
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Detectors",
                "Visualization",
                "Benchmark testing",
                "Open systems",
                "Boosting",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "image retrieval",
                "open systems",
                "pedestrians"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scalable person reidentification",
                "hand-drawn bbox",
                "ground truth",
                "query im- age",
                "Market-1501 dataset",
                "deformable part model",
                "pedestrian detector",
                "DPM",
                "open system",
                "camera",
                "large-scale image search",
                "unsupervised bag-of-words descriptor"
            ]
        },
        "id": 124,
        "cited_by": [
            {
                "year": "2017",
                "id": 31
            },
            {
                "year": "2017",
                "id": 36
            },
            {
                "year": "2017",
                "id": 39
            },
            {
                "year": "2017",
                "id": 51
            },
            {
                "year": "2017",
                "id": 103
            },
            {
                "year": "2017",
                "id": 255
            },
            {
                "year": "2017",
                "id": 256
            },
            {
                "year": "2017",
                "id": 258
            },
            {
                "year": "2017",
                "id": 339
            },
            {
                "year": "2017",
                "id": 395
            },
            {
                "year": "2017",
                "id": 400
            },
            {
                "year": "2017",
                "id": 417
            },
            {
                "year": "2017",
                "id": 497
            },
            {
                "year": "2017",
                "id": 565
            },
            {
                "year": "2015",
                "id": 357
            },
            {
                "year": "2015",
                "id": 417
            }
        ]
    },
    {
        "title": "MMSS: Multi-modal Sharable and Specific Feature Learning for RGB-D Object Recognition",
        "authors": [
            "Anran Wang",
            "Jianfei Cai",
            "Jiwen Lu",
            "Tat-Jen Cham"
        ],
        "abstract": "Most of the feature-learning methods for RGB-D object recognition either learn features from color and depth modalities separately, or simply treat RGB-D as undifferentiated four-channel data, which cannot adequately exploit the relationship between different modalities. Motivated by the intuition that different modalities should contain not only some modal-specific patterns but also some shared common patterns, we propose a multi-modal feature learning framework for RGB-D object recognition. We first construct deep CNN layers for color and depth separately, and then connect them with our carefully designed multi-modal layers, which fuse color and depth information by enforcing a common part to be shared by features of different modalities. In this way, we obtain features reflecting shared properties as well as modal-specific properties in different modalities. The information of the multi-modal learning frameworks is back-propagated to the early CNN layers. Experimental results show that our proposed multi-modal feature learning method outperforms state-of-the-art approaches on two widely used RGB-D object benchmark datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410491",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 3,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Object recognition",
                "Feature extraction",
                "Labeling",
                "Sparse matrices",
                "Computer vision",
                "Learning systems"
            ],
            "INSPEC: Controlled Indexing": [
                "backpropagation",
                "image colour analysis",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "MMSS",
                "multimodal sharable and specific feature learning",
                "RGB-D object recognition",
                "color modalities",
                "depth modalities",
                "modal-specific patterns",
                "deep CNN layers",
                "modal-specific properties",
                "backpropagation",
                "convolutional neural network"
            ]
        },
        "id": 125,
        "cited_by": [
            {
                "year": "2017",
                "id": 569
            }
        ]
    },
    {
        "title": "Object Detection via a Multi-region and Semantic Segmentation-Aware CNN Model",
        "authors": [
            "Spyros Gidaris",
            "Nikos Komodakis"
        ],
        "abstract": "We propose an object detection system that relies on a multi-region deep convolutional neural network (CNN) that also encodes semantic segmentation-aware features. The resulting CNN-based representation aims at capturing a diverse set of discriminative appearance factors and exhibits localization sensitivity that is essential for accurate object localization. We exploit the above properties of our recognition module by integrating it on an iterative localization mechanism that alternates between scoring a box proposal and refining its location with a deep CNN regression model. Thanks to the efficient use of our modules, we detect objects with very high localization accuracy. On the detection challenges of PASCAL VOC2007 and PASCAL VOC2012 we achieve mAP of 78.2% and 73.9% correspondingly, surpassing any other published work by a significant margin.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410492",
        "reference_list": [
            {
                "year": "2011",
                "id": 238
            }
        ],
        "citation": {
            "ieee": 121,
            "other": 60,
            "total": 181
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Biological system modeling",
                "Semantics",
                "Feature extraction",
                "Computer architecture",
                "Context",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "neural nets",
                "object detection",
                "object recognition",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic segmentation-aware CNN model",
                "object detection system",
                "multiregion deep convolutional neural network",
                "semantic segmentation-aware features",
                "CNN-based representation",
                "discriminative appearance factors",
                "localization sensitivity",
                "accurate object localization",
                "recognition module",
                "iterative localization mechanism",
                "box proposal",
                "deep CNN regression model",
                "high localization accuracy",
                "PASCAL VOC2007",
                "PASCAL VOC2012"
            ]
        },
        "id": 126,
        "cited_by": [
            {
                "year": "2017",
                "id": 79
            },
            {
                "year": "2017",
                "id": 204
            },
            {
                "year": "2017",
                "id": 434
            },
            {
                "year": "2017",
                "id": 520
            },
            {
                "year": "2017",
                "id": 600
            },
            {
                "year": "2017",
                "id": 613
            }
        ]
    },
    {
        "title": "Neural Activation Constellations: Unsupervised Part Model Discovery with Convolutional Networks",
        "authors": [
            "Marcel Simon",
            "Erik Rodner"
        ],
        "abstract": "Part models of object categories are essential for challenging recognition tasks, where differences in categories are subtle and only reflected in appearances of small parts of the object. We present an approach that is able to learn part models in a completely unsupervised manner, without part annotations and even without given bounding boxes during learning. The key idea is to find constellations of neural activation patterns computed using convolutional neural networks. In our experiments, we outperform existing approaches for fine-grained recognition on the CUB200-2011, Oxford PETS, and Oxford Flowers dataset in case no part or bounding box annotations are available and achieve state-of-the-art performance for the Stanford Dog dataset. We also show the benefits of neural constellation models as a data augmentation technique for fine-tuning. Furthermore, our paper unites the areas of generic and fine-grained classification, since our approach is suitable for both scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410493",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2013",
                "id": 213
            },
            {
                "year": "2013",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 63,
            "other": 29,
            "total": 92
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Detectors",
                "Training",
                "Computational modeling",
                "Birds",
                "Feature extraction",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "neural nets",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "neural activation constellations",
                "unsupervised part model discovery",
                "convolutional networks",
                "neural activation patterns",
                "fine-grained recognition",
                "CUB200-2011",
                "Oxford PETS",
                "Oxford Flowers dataset",
                "Stanford Dog dataset",
                "data augmentation technique"
            ]
        },
        "id": 127,
        "cited_by": [
            {
                "year": "2017",
                "id": 53
            },
            {
                "year": "2017",
                "id": 521
            },
            {
                "year": "2017",
                "id": 547
            }
        ]
    },
    {
        "title": "Cascaded Sparse Spatial Bins for Efficient and Effective Generic Object Detection",
        "authors": [
            "David Novotny",
            "Jiri Matas"
        ],
        "abstract": "A novel efficient method for extraction of object proposals is introduced. Its \"objectness\" function exploits deep spatial pyramid features, a novel fast-to-compute HoG-based edge statistic and the EdgeBoxes score [42]. The efficiency is achieved by the use of spatial bins in a novel combination with sparsity-inducing group normalized SVM. State-of-the-art recall performance is achieved on Pascal VOC07, significantly outperforming methods with comparable speed. Interestingly, when only 100 proposals per image are considered the method attains 78 % recall on VOC07. The method improves mAP of the RCNN class-specific detector, increasing it by 10 points when only 50 proposals are used in each image. The system trained on twenty classes performs well on the two hundred class ILSVRC2013 set confirming generalization capability.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410494",
        "reference_list": [
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Feature extraction",
                "Support vector machines",
                "Detectors",
                "Image edge detection",
                "Image segmentation",
                "Pipelines"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction",
                "object detection",
                "recurrent neural nets",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cascaded sparse spatial bins",
                "generic object detection",
                "object proposal extraction",
                "objectness function",
                "deep spatial pyramid features",
                "fast-to-compute HoG-based edge statistics",
                "EdgeBoxes score",
                "sparsity-inducing group normalized SVM",
                "state-of-the-art recall performance",
                "Pascal VOC07",
                "mAP improvement",
                "RCNN state-of-the-art class-specific detector",
                "ILSVRC2013 set",
                "generalization capability"
            ]
        },
        "id": 128,
        "cited_by": []
    },
    {
        "title": "Probabilistic Label Relation Graphs with Ising Models",
        "authors": [
            "Nan Ding",
            "Jia Deng",
            "Kevin P. Murphy",
            "Hartmut Neven"
        ],
        "abstract": "We consider classification problems in which the label space has structure. A common example is hierarchical label spaces, corresponding to the case where one label subsumes another (e.g., animal subsumes dog). But labels can also be mutually exclusive (e.g., dog vs cat) or unrelated (e.g., furry, carnivore). To jointly model hierarchy and exclusion relations, the notion of a HEX (hierarchy and exclusion) graph was introduced in [8]. This combined a conditional random field (CRF) with a deep neural network (DNN), resulting in state of the art results when applied to visual object classification problems where the training labels were drawn from different levels of the ImageNet hierarchy (e.g., an image might be labeled with the basic level category \"dog\", rather than the more specific label \"husky\"). In this paper, we extend the HEX model to allow for soft or probabilistic relations between labels, which is useful when there is uncertainty about the relationship between two labels (e.g., an antelope is \"sort of\" furry, but not to the same degree as a grizzly bear). We call our new model pHEX, for probabilistic HEX. We show that the pHEX graph can be converted to an Ising model, which allows us to use existing off-the-shelf inference methods (in contrast to the HEX method, which needed specialized inference algorithms). Experimental results show significant improvements in a number of large-scale visual object classification tasks, outperforming the previous HEX model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410495",
        "reference_list": [
            {
                "year": "2011",
                "id": 105
            },
            {
                "year": "2013",
                "id": 345
            },
            {
                "year": "2007",
                "id": 224
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Probabilistic logic",
                "Mathematical model",
                "Animals",
                "Computational modeling",
                "Standards",
                "Law"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image classification",
                "inference mechanisms",
                "Ising model"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic label relation graphs",
                "hierarchical label spaces",
                "hierarchy and exclusion graph",
                "conditional random field",
                "deep neural network",
                "visual object classification problems",
                "ImageNet hierarchy",
                "HEX model",
                "probabilistic HEX",
                "pHEX graph",
                "Ising model",
                "inference methods"
            ]
        },
        "id": 129,
        "cited_by": []
    },
    {
        "title": "Predicting Good Features for Image Geo-Localization Using Per-Bundle VLAD",
        "authors": [
            "Hyo Jin Kim",
            "Enrique Dunn",
            "Jan-Michael Frahm"
        ],
        "abstract": "We address the problem of recognizing a place depicted in a query image by using a large database of geo-tagged images at a city-scale. In particular, we discover features that are useful for recognizing a place in a data-driven manner, and use this knowledge to predict useful features in a query image prior to the geo-localization process. This allows us to achieve better performance while reducing the number of features. Also, for both learning to predict features and retrieving geo-tagged images from the database, we propose per-bundle vector of locally aggregated descriptors (PBVLAD), where each maximally stable region is described by a vector of locally aggregated descriptors (VLAD) on multiple scale-invariant features detected within the region. Experimental results show the proposed approach achieves a significant improvement over other baseline methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410496",
        "reference_list": [
            {
                "year": "2013",
                "id": 212
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Training",
                "Feature extraction",
                "Support vector machines",
                "Robustness",
                "Image retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "geography",
                "image retrieval",
                "learning (artificial intelligence)",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "per-bundle VLAD",
                "place recognition",
                "query image",
                "geotagged image retrieval",
                "learning",
                "per-bundle vector of locally aggregated descriptors",
                "scale-invariant feature detection",
                "image geolocalization"
            ]
        },
        "id": 130,
        "cited_by": []
    },
    {
        "title": "Task-Driven Feature Pooling for Image Classification",
        "authors": [
            "Guo-Sen Xie",
            "Xu-Yao Zhang",
            "Xiangbo Shu",
            "Shuicheng Yan",
            "Cheng-Lin Liu"
        ],
        "abstract": "Feature pooling is an important strategy to achieve high performance in image classification. However, most pooling methods are unsupervised and heuristic. In this paper, we propose a novel task-driven pooling (TDP) model to directly learn the pooled representation from data in a discriminative manner. Different from the traditional methods (e.g., average and max pooling), TDP is an implicit pooling method which elegantly integrates the learning of representations into the given classification task. The optimization of TDP can equalize the similarities between the descriptors and the learned representation, and maximize the classification accuracy. TDP can be combined with the traditional BoW models (coding vectors) or the recent state-of-the-art CNN models (feature maps) to achieve a much better pooled representation. Furthermore, a self-training mechanism is used to generate the TDP representation for a new test image. A multi-task extension of TDP is also proposed to further improve the performance. Experiments on three databases (Flower-17, Indoor-67 and Caltech-101) well validate the effectiveness of our models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410497",
        "reference_list": [
            {
                "year": "2011",
                "id": 337
            },
            {
                "year": "2011",
                "id": 316
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 2,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Training",
                "Feature extraction",
                "Tensile stress",
                "Optimization",
                "Image coding",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image representation",
                "learning (artificial intelligence)",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "task-driven feature pooling",
                "image classification",
                "TDP model",
                "pooled representation learning",
                "implicit pooling method",
                "BoW model",
                "coding vector",
                "CNN model",
                "feature map",
                "self-training mechanism",
                "test image",
                "Flower-17 database",
                "Indoor-67 database",
                "Caltech-101 database"
            ]
        },
        "id": 131,
        "cited_by": []
    },
    {
        "title": "Cutting Edge: Soft Correspondences in Multimodal Scene Parsing",
        "authors": [
            "Sarah Taghavi Namin",
            "Mohammad Najafi",
            "Mathieu Salzmann",
            "Lars Petersson"
        ],
        "abstract": "Exploiting multiple modalities for semantic scene parsing has been shown to improve accuracy over the single modality scenario. Existing methods, however, assume that corresponding regions in two modalities have the same label. In this paper, we address the problem of data misalignment and label inconsistencies, e.g., due to moving objects, in semantic labeling, which violate the assumption of existing techniques. To this end, we formulate multimodal semantic labeling as inference in a CRF, and introduce latent nodes to explicitly model inconsistencies between two domains. These latent nodes allow us not only to leverage information from both domains to improve their labeling, but also to cut the edges between inconsistent regions. To eliminate the need for hand tuning the parameters of our model, we propose to learn intra-domain and inter-domain potential functions from training data. We demonstrate the benefits of our approach on two publicly available datasets containing 2D imagery and 3D point clouds. Thanks to our latent nodes and our learning strategy, our method outperforms the state-of-the-art in both cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410498",
        "reference_list": [
            {
                "year": "2013",
                "id": 176
            },
            {
                "year": "2009",
                "id": 87
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Labeling",
                "Semantics",
                "Feature extraction",
                "Image analysis",
                "Sensors",
                "Laser radar"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multimodal scene parsing",
                "conditional random field",
                "3D point clouds",
                "2D imagery",
                "intra-domain potential functions",
                "inter-domain potential functions",
                "latent nodes",
                "CRF",
                "inference",
                "multimodal semantic labeling",
                "label inconsistencies",
                "data misalignment"
            ]
        },
        "id": 132,
        "cited_by": []
    },
    {
        "title": "One Shot Learning via Compositions of Meaningful Patches",
        "authors": [
            "Alex Wong",
            "Alan Yuille"
        ],
        "abstract": "The task of discriminating one object from another is almost trivial for a human being. However, this task is computationally taxing for most modern machine learning methods, whereas, we perform this task at ease given very few examples for learning. It has been proposed that the quick grasp of concept may come from the shared knowledge between the new example and examples previously learned. We believe that the key to one-shot learning is the sharing of common parts as each part holds immense amounts of information on how a visual concept is constructed. We propose an unsupervised method for learning a compact dictionary of image patches representing meaningful components of an objects. Using those patches as features, we build a compositional model that outperforms a number of popular algorithms on a one-shot learning task. We demonstrate the effectiveness of this approach on hand-written digits and show that this model generalizes to multiple datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410499",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Dictionaries",
                "Visualization",
                "Training",
                "Feature extraction",
                "Skeleton",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "data analysis",
                "image processing",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "machine learning methods",
                "visual concept",
                "unsupervised method",
                "image patches",
                "compact dictionary",
                "one-shot learning task",
                "hand-written digits",
                "multiple datasets"
            ]
        },
        "id": 133,
        "cited_by": [
            {
                "year": "2017",
                "id": 318
            }
        ]
    },
    {
        "title": "FASText: Efficient Unconstrained Scene Text Detector",
        "authors": [
            "Michal Buta",
            "Luk\u00e1 Neumann",
            "Jir\u00ed Matas"
        ],
        "abstract": "We propose a novel easy-to-implement stroke detector based on an efficient pixel intensity comparison to surrounding pixels. Stroke-specific keypoints are efficiently detected and text fragments are subsequently extracted by local thresholding guided by keypoint properties. Classification based on effectively calculated features then eliminates non-text regions. The stroke-specific keypoints produce 2 times less region segmentations and still detect 25% more characters than the commonly exploited MSER detector and the process is 4 times faster. After a novel efficient classification step, the number of regions is reduced to 7 times less than the standard method and is still almost 3 times faster. All stages of the proposed pipeline are scale-and rotation-invariant and support a wide variety of scripts (Latin, Hebrew, Chinese, etc.) and fonts. When the proposed detector is plugged into a scene text localization and recognition pipeline, a state-of-the-art text localization accuracy is maintained whilst the processing time is significantly reduced.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410500",
        "reference_list": [
            {
                "year": "2013",
                "id": 97
            },
            {
                "year": "2005",
                "id": 196
            },
            {
                "year": "2011",
                "id": 326
            },
            {
                "year": "2011",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 21,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Lead",
                "Text recognition",
                "Standards",
                "Pipelines",
                "Robustness",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "character recognition",
                "feature extraction",
                "image classification",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "FASText",
                "unconstrained scene text detector",
                "stroke detector",
                "pixel intensity comparison",
                "stroke-specific keypoint detection",
                "text fragment extraction",
                "local thresholding",
                "classification",
                "scene text localization",
                "recognition pipeline",
                "text localization accuracy"
            ]
        },
        "id": 134,
        "cited_by": [
            {
                "year": "2017",
                "id": 321
            },
            {
                "year": "2017",
                "id": 525
            },
            {
                "year": "2017",
                "id": 550
            }
        ]
    },
    {
        "title": "Multi-scale Recognition with DAG-CNNs",
        "authors": [
            "Songfan Yang",
            "Deva Ramanan"
        ],
        "abstract": "We explore multi-scale convolutional neural nets (CNNs) for image classification. Contemporary approaches extract features from a single output layer. By extracting features from multiple layers, one can simultaneously reason about high, mid, and low-level features during classification. The resulting multi-scale architecture can itself be seen as a feed-forward model that is structured as a directed acyclic graph (DAG-CNNs). We use DAG-CNNs to learn a set of multi-scale features that can be effectively shared between coarse and fine-grained classification tasks. While fine-tuning such models helps performance, we show that even \"off-the-self\" multi-scale features perform quite well. We present extensive analysis and demonstrate state-of-the-art classification performance on three standard scene benchmarks (SUN397, MIT67, and Scene15). In terms of the heavily benchmarked MIT67 and Scene15 datasets, our results reduce the lowest previously-reported error by 23.9% and 9.5%, respectively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410501",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2013",
                "id": 0
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 25,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Computer architecture",
                "Computational modeling",
                "Benchmark testing",
                "Training",
                "Image recognition",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "directed graphs",
                "feature extraction",
                "image recognition",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale recognition",
                "DAG-CNN",
                "multiscale convolutional neural nets",
                "CNN",
                "image classification",
                "feature extraction",
                "multiscale architecture",
                "feed-forward model",
                "directed acyclic graph",
                "fine grained classification"
            ]
        },
        "id": 135,
        "cited_by": [
            {
                "year": "2017",
                "id": 432
            },
            {
                "year": "2017",
                "id": 582
            },
            {
                "year": "2017",
                "id": 591
            }
        ]
    },
    {
        "title": "Relaxed Multiple-Instance SVM with Application to Object Discovery",
        "authors": [
            "Xinggang Wang",
            "Zhuotun Zhu",
            "Cong Yao",
            "Xiang Bai"
        ],
        "abstract": "Multiple-instance learning (MIL) has served as an important tool for a wide range of vision applications, for instance, image classification, object detection, and visual tracking. In this paper, we propose a novel method to solve the classical MIL problem, named relaxed multiple-instance SVM (RMI-SVM). We treat the positiveness of instance as a continuous variable, use Noisy-OR model to enforce the MIL constraints, and optimize them jointly in a unified framework. The optimization problem can be efficiently solved using stochastic gradient decent. The extensive experiments demonstrate that RMI-SVM consistently achieves superior performance on various benchmarks for MIL. Moreover, we simply applied RMI-SVM to a challenging vision task, common object discovery. The state-of-the arts results of object discovery on PASCAL VOC datasets further confirm the advantages of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410502",
        "reference_list": [
            {
                "year": "2013",
                "id": 372
            },
            {
                "year": "2011",
                "id": 43
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 9,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Visualization",
                "Noise measurement",
                "Proposals",
                "Image edge detection",
                "Optimization",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "gradient methods",
                "learning (artificial intelligence)",
                "object detection",
                "optimisation",
                "stochastic processes",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relaxed multiple-instance SVM",
                "multiple-instance learning",
                "MIL",
                "vision applications",
                "RMI-SVM",
                "noisy-OR model",
                "optimization problem",
                "stochastic gradient decent",
                "common object discovery",
                "PASCAL VOC datasets"
            ]
        },
        "id": 136,
        "cited_by": []
    },
    {
        "title": "Im2Calories: Towards an Automated Mobile Vision Food Diary",
        "authors": [
            "Austin Myers",
            "Nick Johnston",
            "Vivek Rathod",
            "Anoop Korattikara",
            "Alex Gorban",
            "Nathan Silberman",
            "Sergio Guadarrama",
            "George Papandreou",
            "Jonathan Huang",
            "Kevin Murphy"
        ],
        "abstract": "We present a system which can recognize the contents of your meal from a single image, and then predict its nutritional contents, such as calories. The simplest version assumes that the user is eating at a restaurant for which we know the menu. In this case, we can collect images offline to train a multi-label classifier. At run time, we apply the classifier (running on your phone) to predict which foods are present in your meal, and we lookup the corresponding nutritional facts. We apply this method to a new dataset of images from 23 different restaurants, using a CNN-based classifier, significantly outperforming previous work. The more challenging setting works outside of restaurants. In this case, we need to estimate the size of the foods, as well as their labels. This requires solving segmentation and depth / volume estimation from a single image. We present CNN-based approaches to these problems, with promising preliminary results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410503",
        "reference_list": [
            {
                "year": "2015",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 41,
            "total": 80
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Mobile communication",
                "Visualization",
                "Image recognition",
                "Cameras",
                "Machine learning"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "image segmentation",
                "medical computing",
                "mobile computing",
                "neural nets",
                "personal computing",
                "prediction theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Im2Calories",
                "automated mobile vision food diary",
                "content recognition",
                "nutritional content prediction",
                "restaurant",
                "multilabel classifier",
                "food prediction",
                "nutritional facts",
                "CNN-based classifier",
                "food size estimation",
                "depth estimation",
                "volume estimation"
            ]
        },
        "id": 137,
        "cited_by": []
    },
    {
        "title": "LEWIS: Latent Embeddings for Word Images and Their Semantics",
        "authors": [
            "Albert Gordo",
            "Jon Almaz\u00e1n",
            "Naila Murray",
            "Florent Perronin"
        ],
        "abstract": "The goal of this work is to bring semantics into the tasks of text recognition and retrieval in natural images. Although text recognition and retrieval have received a lot of attention in recent years, previous works have focused on recognizing or retrieving exactly the same word used as a query, without taking the semantics into consideration. In this paper, we ask the following question: can we predict semantic concepts directly from a word image, without explicitly trying to transcribe the word image or its characters at any point? For this goal we propose a convolutional neural network (CNN) with a weighted ranking loss objective that ensures that the concepts relevant to the query image are ranked ahead of those that are not relevant. This can also be interpreted as learning a Euclidean space where word images and concepts are jointly embedded. This model is learned in an end-to-end manner, from image pixels to semantic concepts, using a dataset of synthetically generated word images and concepts mined from a lexical database (WordNet). Our results show that, despite the complexity of the task, word images and concepts can indeed be associated with a high degree of accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410504",
        "reference_list": [
            {
                "year": "2013",
                "id": 97
            },
            {
                "year": "2013",
                "id": 12
            },
            {
                "year": "2011",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Image recognition",
                "Text recognition",
                "Computer vision",
                "Image representation",
                "Neural networks",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image retrieval",
                "natural scenes",
                "neural nets",
                "text detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "LEWIS",
                "latent embeddings for word images and their semantics",
                "text recognition",
                "text retrieval",
                "natural image",
                "semantic concept",
                "convolutional neural network",
                "CNN",
                "weighted ranking loss objective",
                "query image",
                "Euclidean space",
                "image pixel",
                "lexical database",
                "WordNet",
                "task complexity"
            ]
        },
        "id": 138,
        "cited_by": []
    },
    {
        "title": "Per-Sample Kernel Adaptation for Visual Recognition and Grouping",
        "authors": [
            "Borislav Antic",
            "Bj\u00f6rn Ommer"
        ],
        "abstract": "Object, action, or scene representations that are corrupted by noise significantly impair the performance of visual recognition. Typically, partial occlusion, clutter, or excessive articulation affects only a subset of all feature dimensions and, most importantly, different dimensions are corrupted in different samples. Nevertheless, the common approach to this problem in feature selection and kernel methods is to down-weight or eliminate entire training samples or the same dimensions of all samples. Thus, valuable signal is lost, resulting in suboptimal classification. Our goal is, therefore, to adjust the contribution of individual feature dimensions when comparing any two samples and computing their similarity. Consequently, per-sample selection of informative dimensions is directly integrated into kernel computation. The interrelated problems of learning the parameters of a kernel classifier and determining the informative components of each sample are then addressed in a joint objective function. The approach can be integrated into the learning stage of any kernel-based visual recognition problem and it does not affect the computational performance in the retrieval phase. Experiments on diverse challenges of action recognition in videos and indoor scene classification show the general applicability of the approach and its ability to improve learning of visual representations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410505",
        "reference_list": [
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Noise measurement",
                "Visualization",
                "Training",
                "Support vector machines",
                "Reliability",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "feature selection",
                "gesture recognition",
                "image classification",
                "image representation",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "per-sample kernel adaptation",
                "visual grouping",
                "object representation",
                "action representation",
                "scene representation",
                "partial occlusion",
                "clutter",
                "feature selection",
                "kernel method",
                "suboptimal classification",
                "feature dimension",
                "per-sample selection",
                "informative dimension",
                "kernel computation",
                "kernel classifier",
                "joint objective function",
                "learning stage",
                "kernel-based visual recognition problem",
                "computational performance",
                "retrieval phase",
                "action recognition",
                "indoor scene classification",
                "visual representation"
            ]
        },
        "id": 139,
        "cited_by": [
            {
                "year": "2017",
                "id": 451
            },
            {
                "year": "2017",
                "id": 461
            }
        ]
    },
    {
        "title": "Fine-Grained Change Detection of Misaligned Scenes with Varied Illuminations",
        "authors": [
            "Wei Feng",
            "Fei-Peng Tian",
            "Qian Zhang",
            "Nan Zhang",
            "Liang Wan",
            "Jizhou Sun"
        ],
        "abstract": "Detecting fine-grained subtle changes among a scene is critically important in practice. Previous change detection methods, focusing on detecting large-scale significant changes, cannot do this well. This paper proposes a feasible end-to-end approach to this challenging problem. We start from active camera relocation that quickly relocates camera to nearly the same pose and position of the last time observation. To guarantee detection sensitivity and accuracy of minute changes, in an observation, we capture a group of images under multiple illuminations, which need only to be roughly aligned to the last time lighting conditions. Given two times observations, we formulate fine-grained change detection as a joint optimization problem of three related factors, i.e., normal-aware lighting difference, camera geometry correction flow, and real scene change mask. We solve the three factors in a coarse-to-fine manner and achieve reliable change decision by rank minimization. We build three real-world datasets to benchmark fine-grained change detection of misaligned scenes under varied multiple lighting conditions. Extensive experiments show the superior performance of our approach over state-of-the-art change detection methods and its ability to distinguish real scene changes from false ones caused by lighting variations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410506",
        "reference_list": [
            {
                "year": "2011",
                "id": 297
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 4,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Cameras",
                "Geometry",
                "Image color analysis",
                "DSL",
                "Reliability",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image sensors",
                "lighting",
                "object detection",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fine-grained subtle change detection",
                "misaligned scenes",
                "varied illuminations",
                "active camera relocation",
                "detection sensitivity",
                "varied multiple lighting conditions",
                "joint optimization problem",
                "normal-aware lighting difference",
                "camera geometry correction flow",
                "real scene change mask"
            ]
        },
        "id": 140,
        "cited_by": []
    },
    {
        "title": "Aggregating Local Deep Features for Image Retrieval",
        "authors": [
            "Artem Babenko Yandex",
            "Victor Lempitsky"
        ],
        "abstract": "Several recent works have shown that image descriptors produced by deep convolutional neural networks provide state-of-the-art performance for image classification and retrieval problems. It also has been shown that the activations from the convolutional layers can be interpreted as local features describing particular image regions. These local features can be aggregated using aggregating methods developed for local features (e.g. Fisher vectors), thus providing new powerful global descriptor. In this paper we investigate possible ways to aggregate local deep features to produce compact descriptors for image retrieval. First, we show that deep features and traditional hand-engineered features have quite different distributions of pairwise similarities, hence existing aggregation methods have to be carefully re-evaluated. Such re-evaluation reveals that in contrast to shallow features, the simple aggregation method based on sum pooling provides the best performance for deep convolutional features. This method is efficient, has few parameters, and bears little risk of overfitting when e.g. learning the PCA matrix. In addition, we suggest a simple yet efficient query expansion scheme suitable for the proposed aggregation method. Overall, the new compact global descriptor improves the state-of-the-art on four common benchmarks considerably.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410507",
        "reference_list": [
            {
                "year": "2013",
                "id": 174
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 12,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Image retrieval",
                "Principal component analysis",
                "Feature extraction",
                "Reliability",
                "Aggregates",
                "Buildings",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image retrieval",
                "matrix algebra",
                "neural nets",
                "principal component analysis",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "compact global descriptor",
                "query expansion scheme",
                "PCA matrix",
                "deep convolutional feature",
                "sum pooling",
                "shallow feature",
                "aggregation method",
                "pairwise similarity",
                "hand-engineered feature",
                "compact descriptor",
                "Fisher vector",
                "aggregating method",
                "image region",
                "local feature",
                "convolutional layer",
                "image retrieval problem",
                "image classification",
                "deep convolutional neural network",
                "image descriptor",
                "local deep feature"
            ]
        },
        "id": 141,
        "cited_by": [
            {
                "year": "2017",
                "id": 53
            },
            {
                "year": "2017",
                "id": 75
            },
            {
                "year": "2017",
                "id": 80
            },
            {
                "year": "2017",
                "id": 364
            }
        ]
    },
    {
        "title": "Learning Deep Object Detectors from 3D Models",
        "authors": [
            "Xingchao Peng",
            "Baochen Sun",
            "Karim Ali",
            "Kate Saenko"
        ],
        "abstract": "Crowdsourced 3D CAD models are easily accessible online, and can potentially generate an infinite number of training images for almost any object category. We show that augmenting the training data of contemporary Deep Convolutional Neural Net (DCNN) models with such synthetic data can be effective, especially when real training data is limited or not well matched to the target domain. Most freely available CAD models capture 3D shape but are often missing other low level cues, such as realistic object texture, pose, or background. In a detailed analysis, we use synthetic CAD images to probe the ability of DCNN to learn without these cues, with surprising findings. In particular, we show that when the DCNN is fine-tuned on the target detection task, it exhibits a large degree of invariance to missing low-level cues, but, when pretrained on generic ImageNet classification, it learns better when the low-level cues are simulated. We show that our synthetic DCNN training approach significantly outperforms previous methods on the benchmark PASCAL VOC2007 dataset when learning in the few-shot scenario and improves performance in a domain shift scenario on the Office benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410508",
        "reference_list": [],
        "citation": {
            "ieee": 48,
            "other": 34,
            "total": 82
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Three-dimensional displays",
                "Design automation",
                "Image color analysis",
                "Training",
                "Data models",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "CAD",
                "convolution",
                "learning (artificial intelligence)",
                "neural nets",
                "object detection",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep object detector learning",
                "3D CAD model",
                "deep convolutional neural net",
                "DCNN model",
                "CAD image",
                "target detection"
            ]
        },
        "id": 142,
        "cited_by": [
            {
                "year": "2017",
                "id": 136
            },
            {
                "year": "2017",
                "id": 282
            },
            {
                "year": "2017",
                "id": 584
            }
        ]
    },
    {
        "title": "Harvesting Discriminative Meta Objects with Deep CNN Features for Scene Classification",
        "authors": [
            "Ruobing Wu",
            "Baoyuan Wang",
            "Wenping Wang",
            "Yizhou Yu"
        ],
        "abstract": "Recent work on scene classification still makes use of generic CNN features in a rudimentary manner. In this paper, we present a novel pipeline built upon deep CNN features to harvest discriminative visual objects and parts for scene classification. We first use a region proposal technique to generate a set of high-quality patches potentially containing objects, and apply a pre-trained CNN to extract generic deep features from these patches. Then we perform both unsupervised and weakly supervised learning to screen these patches and discover discriminative ones representing category-specific objects and parts. We further apply discriminative clustering enhanced with local CNN fine-tuning to aggregate similar objects and parts into groups, called meta objects. A scene image representation is constructed by pooling the feature response maps of all the learned meta objects at multiple spatial scales. We have confirmed that the scene image representation obtained using this new pipeline is capable of delivering state-of-the-art performance on two popular scene benchmark datasets, MIT Indoor 67 [22] and Sun397 [31].",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410509",
        "reference_list": [
            {
                "year": "2011",
                "id": 316
            },
            {
                "year": "2011",
                "id": 165
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 11,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Pipelines",
                "Feature extraction",
                "Proposals",
                "Visualization",
                "Computers",
                "Supervised learning",
                "Aggregates"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "feature extraction",
                "image classification",
                "image representation",
                "neural nets",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature response map",
                "scene image representation",
                "unsupervised learning",
                "feature extraction",
                "scene classification",
                "deep CNN",
                "convolutional neural network",
                "metaobject learning"
            ]
        },
        "id": 143,
        "cited_by": [
            {
                "year": "2017",
                "id": 591
            },
            {
                "year": "2017",
                "id": 603
            }
        ]
    },
    {
        "title": "Scalable Nonlinear Embeddings for Semantic Category-Based Image Retrieval",
        "authors": [
            "Gaurav Sharma",
            "Bernt Schiele"
        ],
        "abstract": "We propose a novel algorithm for the task of supervised discriminative distance learning by nonlinearly embedding vectors into a low dimensional Euclidean space. We work in the challenging setting where supervision is with constraints on similar and dissimilar pairs while training. The proposed method is derived by an approximate kernelization of a linear Mahalanobis-like distance metric learning algorithm and can also be seen as a kernel neural network. The number of model parameters and test time evaluation complexity of the proposed method are O(dD) where D is the dimensionality of the input features and d is the dimension of the projection space -- this is in contrast to the usual kernelization methods as, unlike them, the complexity does not scale linearly with the number of training examples. We propose a stochastic gradient based learning algorithm which makes the method scalable (w.r.t. the number of training examples), while being nonlinear. We train the method with up to half a million training pairs of 4096 dimensional CNN features. We give empirical comparisons with relevant baselines on seven challenging datasets for the task of low dimensional semantic category based image retrieval.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410510",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Kernel",
                "Training",
                "Computer vision",
                "Complexity theory",
                "Approximation algorithms",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computational geometry",
                "computer vision",
                "distance learning",
                "gradient methods",
                "image retrieval",
                "learning (artificial intelligence)",
                "neural nets",
                "stochastic processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scalable nonlinear embeddings",
                "semantic category-based image retrieval",
                "supervised discriminative distance learning",
                "nonlinearly embedding vectors",
                "low dimensional Euclidean space",
                "linear Mahalanobis-like distance metric learning algorithm",
                "kernel neural network",
                "stochastic gradient based learning algorithm"
            ]
        },
        "id": 144,
        "cited_by": []
    },
    {
        "title": "Person Re-Identification Ranking Optimisation by Discriminant Context Information Analysis",
        "authors": [
            "Jorge Garc\u00eda",
            "Niki Martinel",
            "Christian Micheloni",
            "Alfredo Gardel"
        ],
        "abstract": "Person re-identification is an open and challenging problem in computer vision. Existing re-identification approaches focus on optimal methods for features matching (e.g., metric learning approaches) or study the inter-camera transformations of such features. These methods hardly ever pay attention to the problem of visual ambiguities shared between the first ranks. In this paper, we focus on such a problem and introduce an unsupervised ranking optimization approach based on discriminant context information analysis. The proposed approach refines a given initial ranking by removing the visual ambiguities common to first ranks. This is achieved by analyzing their content and context information. Extensive experiments on three publicly available benchmark datasets and different baseline methods have been conducted. Results demonstrate a remarkable improvement in the first positions of the ranking. Regardless of the selected dataset, state-of-the-art methods are strongly outperformed by our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410511",
        "reference_list": [
            {
                "year": "2013",
                "id": 55
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 12,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Probes",
                "Context",
                "Training",
                "Feature extraction",
                "Measurement",
                "Information analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "learning (artificial intelligence)",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "person reidentification ranking optimisation",
                "discriminant context information analysis",
                "computer vision",
                "unsupervised ranking optimization approach"
            ]
        },
        "id": 145,
        "cited_by": [
            {
                "year": "2017",
                "id": 339
            }
        ]
    },
    {
        "title": "Unsupervised Generation of a View Point Annotated Car Dataset from Videos",
        "authors": [
            "Nima Sedaghat",
            "Thomas Brox"
        ],
        "abstract": "Object recognition approaches have recently been extended to yield, aside of the object class output, also viewpoint or pose. Training such approaches typically requires additional viewpoint or keypoint annotation in the training data or, alternatively, synthetic CAD models. In this paper, we present an approach that creates a dataset of images annotated with bounding boxes and viewpoint labels in a fully automated manner from videos. We assume that the scene is static in order to reconstruct 3D surfaces via structure from motion. We automatically detect when the reconstruction fails and normalize for the viewpoint of the 3D models by aligning the reconstructed point clouds. Exemplarily for cars we show that we can expand a large dataset of annotated single images and obtain improved performance when training a viewpoint regressor on this joined dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410512",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Videos",
                "Histograms",
                "Image reconstruction",
                "Cameras",
                "Solid modeling",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "automobiles",
                "computer graphics",
                "image classification",
                "image motion analysis",
                "image reconstruction",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "viewpoint annotated car dataset unsupervised generation",
                "bounding boxes",
                "viewpoint labels",
                "3D surface reconstruction",
                "structure from motion",
                "point clouds",
                "annotated single images",
                "viewpoint regressor",
                "videos"
            ]
        },
        "id": 146,
        "cited_by": [
            {
                "year": "2017",
                "id": 136
            },
            {
                "year": "2017",
                "id": 548
            }
        ]
    },
    {
        "title": "Structured Indoor Modeling",
        "authors": [
            "Satoshi Ikehata",
            "Hang Yang",
            "Yasutaka Furukawa"
        ],
        "abstract": "This paper presents a novel 3D modeling framework that reconstructs an indoor scene as a structured model from panorama RGBD images. A scene geometry is represented as a graph, where nodes correspond to structural elements such as rooms, walls, and objects. The approach devises a structure grammar that defines how a scene graph can be manipulated. The grammar then drives a principled new reconstruction algorithm, where the grammar rules are sequentially applied to recover a structured model. The paper also proposes a new room segmentation algorithm and an offset-map reconstruction algorithm that are used in the framework and can enforce architectural shape priors far beyond existing state-of-the-art. The structured scene representation enables a variety of novel applications, ranging from indoor scene visualization, automated floorplan generation, Inverse-CAD, and more. We have tested our framework and algorithms on six synthetic and five real datasets with qualitative and quantitative evaluations. The source code and the data are available at the project website [15].",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410513",
        "reference_list": [
            {
                "year": "2013",
                "id": 423
            },
            {
                "year": "2009",
                "id": 10
            },
            {
                "year": "2013",
                "id": 267
            },
            {
                "year": "2009",
                "id": 237
            },
            {
                "year": "2009",
                "id": 241
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 21,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Grammar",
                "Image reconstruction",
                "Computational modeling",
                "Geometry",
                "Reconstruction algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "graph theory",
                "image colour analysis",
                "image reconstruction",
                "image segmentation",
                "modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "offset-map reconstruction algorithm",
                "room segmentation algorithm",
                "scene geometry",
                "RGB-D image",
                "indoor scene reconstruction",
                "3D modelling framework",
                "structured graph"
            ]
        },
        "id": 147,
        "cited_by": [
            {
                "year": "2017",
                "id": 231
            },
            {
                "year": "2017",
                "id": 402
            },
            {
                "year": "2017",
                "id": 486
            }
        ]
    },
    {
        "title": "3D Time-Lapse Reconstruction from Internet Photos",
        "authors": [
            "Ricardo Martin-Brualla",
            "David Gallup",
            "Steven M. Seitz"
        ],
        "abstract": "Given an Internet photo collection of a landmark, we compute a 3D time-lapse video sequence where a virtual camera moves continuously in time and space. While previous work assumed a static camera, the addition of camera motion during the time-lapse creates a very compelling impression of parallax. Achieving this goal, however, requires addressing multiple technical challenges, including solving for time-varying depth maps, regularizing 3D point color profiles over time, and reconstructing high quality, hole-free images at every frame from the projected profiles. Our results show photorealistic time-lapses of skylines and natural scenes over many years, with dramatic parallax effects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410514",
        "reference_list": [
            {
                "year": "2007",
                "id": 173
            },
            {
                "year": "2011",
                "id": 295
            },
            {
                "year": "2007",
                "id": 24
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 6,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Internet",
                "Image reconstruction",
                "Image color analysis",
                "Geometry",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D time-lapse reconstruction",
                "Internet photo collection",
                "video sequence",
                "static camera",
                "camera motion",
                "multiple technical challenges",
                "time-varying depth maps",
                "3D point color profiles",
                "photorealistic time-lapses",
                "skylines",
                "natural scenes",
                "parallax effects"
            ]
        },
        "id": 148,
        "cited_by": [
            {
                "year": "2017",
                "id": 412
            }
        ]
    },
    {
        "title": "Global, Dense Multiscale Reconstruction for a Billion Points",
        "authors": [
            "Benjamin Ummenhofer",
            "Thomas Brox"
        ],
        "abstract": "We present a variational approach for surface reconstruction from a set of oriented points with scale information. We focus particularly on scenarios with non-uniform point densities due to images taken from different distances. In contrast to previous methods, we integrate the scale information in the objective and globally optimize the signed distance function of the surface on a balanced octree grid. We use a finite element discretization on the dual structure of the octree minimizing the number of variables. The tetrahedral mesh is generated efficiently from the dual structure, and also memory efficiency is optimized, such that robust data terms can be used even on very large scenes. The surface normals are explicitly optimized and used for surface extraction to improve the reconstruction at edges and corners.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410515",
        "reference_list": [
            {
                "year": "2007",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 8,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Octrees",
                "Image reconstruction",
                "Robustness",
                "Surface reconstruction",
                "Cost function",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "finite element analysis",
                "image reconstruction",
                "octrees",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense multiscale reconstruction",
                "surface reconstruction",
                "scale information",
                "nonuniform point density",
                "signed distance function",
                "balanced octree grid",
                "finite element discretization",
                "dual structure",
                "tetrahedral mesh",
                "surface extraction",
                "global multiscale reconstruction"
            ]
        },
        "id": 149,
        "cited_by": []
    },
    {
        "title": "On the Visibility of Point Clouds",
        "authors": [
            "Sagi Katz",
            "Ayellet Tal"
        ],
        "abstract": "Is it possible to determine the visible subset of points directly from a given point cloud? Interestingly, it was shown that this is indeed the case - despite the fact that points cannot occlude each other, this task can be performed without surface reconstruction or normal estimation. The operator is very simple - it first transforms the points to a new domain and then constructs the convex hull in that domain. Points that lie on the convex hull of the transformed set of points are the images of the visible points. This operator found numerous applications in computer vision, including face reconstruction, keypoint detection, finding the best viewpoints, reduction of points, and many more. The current paper addresses a fundamental question: What properties should a transformation function satisfy, in order to be utilized in this operator? We show that three such properties are sufficient: the sign of the function, monotonicity, and a condition regarding the function's parameter. The correctness of an algorithm that satisfies these three properties is proved. Finally, we show an interesting application of the operator - assignment of visibility-confidence score. This feature is missing from previous approaches, where a binary yes/no visibility is determined. This score can be utilized in various applications, we illustrate its use in view-dependent curvature estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410516",
        "reference_list": [
            {
                "year": "2013",
                "id": 448
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 1,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Three-dimensional displays",
                "Image reconstruction",
                "Face",
                "Surface reconstruction",
                "Estimation",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convex programming",
                "estimation theory",
                "face recognition",
                "image reconstruction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "view-dependent curvature estimation",
                "visibility-confidence score",
                "transformation function",
                "keypoint detection",
                "computer vision",
                "convex hull",
                "surface reconstruction",
                "point clouds"
            ]
        },
        "id": 150,
        "cited_by": []
    },
    {
        "title": "Weakly Supervised Graph Based Semantic Segmentation by Learning Communities of Image-Parts",
        "authors": [
            "Niloufar Pourian",
            "S. Karthikeyan",
            "B. S. Manjunath"
        ],
        "abstract": "We present a weakly-supervised approach to semantic segmentation. The goal is to assign pixel-level labels given only partial information, for example, image-level labels. This is an important problem in many application scenarios where it is difficult to get accurate segmentation or not feasible to obtain detailed annotations. The proposed approach starts with an initial coarse segmentation, followed by a spectral clustering approach that groups related image parts into communities. A community-driven graph is then constructed that captures spatial and feature relationships between communities while a label graph captures correlations between image labels. Finally, mapping the image level labels to appropriate communities is formulated as a convex optimization problem. The proposed approach does not require location information for image level labels and can be trained using partially labeled datasets. Compared to the state-of-the-art weakly supervised approaches, we achieve a significant performance improvement of 9% on MSRC-21 dataset and 11% on LabelMe dataset, while being more than 300 times faster.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410517",
        "reference_list": [
            {
                "year": "2013",
                "id": 42
            },
            {
                "year": "2011",
                "id": 81
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Semantics",
                "Training",
                "Databases",
                "Visualization",
                "Partitioning algorithms",
                "Correlation"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "graph theory",
                "image segmentation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weakly supervised graph",
                "semantic segmentation",
                "image-parts",
                "pixel-level label",
                "image-level label",
                "spectral clustering",
                "community-driven graph",
                "label graph",
                "convex optimization problem"
            ]
        },
        "id": 151,
        "cited_by": [
            {
                "year": "2017",
                "id": 222
            }
        ]
    },
    {
        "title": "Piecewise Flat Embedding for Image Segmentation",
        "authors": [
            "Yizhou Yu",
            "Chaowei Fang",
            "Zicheng Liao"
        ],
        "abstract": "Image segmentation is a critical step in many computer vision tasks, including high-level visual recognition and scene understanding as well as low-level photo and video processing. In this paper, we propose a new nonlinear embedding, called piecewise flat embedding, for image segmentation. Based on the theory of sparse signal recovery, piecewise flat embedding attempts to identify segment boundaries while significantly suppressing variations within segments. We adopt an L1-regularized energy term in the formulation to promote sparse solutions. We further devise an effective two-stage numerical algorithm based on Bregman iterations to solve the proposed embedding. Piecewise flat embedding can be easily integrated into existing image segmentation frameworks, including segmentation based on spectral clustering and hierarchical segmentation based on contour detection. Experiments on BSDS500 indicate that segmentation algorithms incorporating this embedding can achieve significantly improved results in both frameworks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410518",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 2,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Linear programming",
                "Laplace equations",
                "Clustering algorithms",
                "Context",
                "Manifolds",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "edge detection",
                "image segmentation",
                "iterative methods",
                "optimisation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "piecewise flat embedding",
                "image segmentation frameworks",
                "computer vision tasks",
                "high-level visual recognition",
                "scene understanding",
                "low-level photo processing",
                "low-level video processing",
                "nonlinear embedding",
                "sparse signal recovery",
                "segment boundary identification",
                "L1-regularized energy term",
                "two-stage numerical algorithm",
                "Bregman iterations",
                "spectral clustering",
                "contour detection",
                "BSDS500"
            ]
        },
        "id": 152,
        "cited_by": []
    },
    {
        "title": "Semantic Image Segmentation via Deep Parsing Network",
        "authors": [
            "Ziwei Liu",
            "Xiaoxiao Li",
            "Ping Luo",
            "Chen-Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "This paper addresses semantic image segmentation by incorporating rich information into Markov Random Field (MRF), including high-order relations and mixture of label contexts. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN architecture to model unary terms and additional layers are carefully devised to approximate the mean field algorithm (MF) for pairwise terms. It has several appealing properties. First, different from the recent works that combined CNN and MRF, where many iterations of MF were required for each training image during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing works as its special cases. Third, DPN makes MF easier to be parallelized and speeded up in Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC 2012 dataset, where a single DPN model yields a new state-of-the-art segmentation accuracy of 77.5%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410519",
        "reference_list": [
            {
                "year": "2009",
                "id": 85
            },
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2013",
                "id": 330
            },
            {
                "year": "2003",
                "id": 1
            }
        ],
        "citation": {
            "ieee": 118,
            "other": 59,
            "total": 177
        },
        "keywords": {
            "IEEE Keywords": [
                "Context",
                "Semantics",
                "Computational modeling",
                "Graphics processing units",
                "Image segmentation",
                "Labeling",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "convolution",
                "graphics processing units",
                "image segmentation",
                "iterative methods",
                "learning (artificial intelligence)",
                "Markov processes",
                "neural nets",
                "semantic networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic image segmentation",
                "deep parsing network",
                "DPN",
                "Markov random field",
                "MRF",
                "high-order relation",
                "label context mixture",
                "iterative algorithm",
                "convolutional neural network",
                "CNN",
                "mean field algorithm",
                "MF approximation",
                "graphical processing unit",
                "GPU"
            ]
        },
        "id": 153,
        "cited_by": [
            {
                "year": "2017",
                "id": 137
            },
            {
                "year": "2017",
                "id": 214
            },
            {
                "year": "2017",
                "id": 277
            },
            {
                "year": "2017",
                "id": 286
            },
            {
                "year": "2017",
                "id": 290
            },
            {
                "year": "2017",
                "id": 343
            },
            {
                "year": "2017",
                "id": 418
            },
            {
                "year": "2017",
                "id": 523
            },
            {
                "year": "2017",
                "id": 524
            },
            {
                "year": "2017",
                "id": 546
            }
        ]
    },
    {
        "title": "Human Parsing with Contextualized Convolutional Neural Network",
        "authors": [
            "Xiaodan Liang",
            "Chunyan Xu",
            "Xiaohui Shen",
            "Jianchao Yang",
            "Si Liu",
            "Jinhui Tang",
            "Liang Lin",
            "Shuicheng Yan"
        ],
        "abstract": "In this work, we address the human parsing task with a novel Contextualized Convolutional Neural Network (Co-CNN) architecture, which well integrates the cross-layer context, global image-level context, within-super-pixel context and cross-super-pixel neighborhood context into a unified network. Given an input human image, Co-CNN produces the pixel-wise categorization in an end-to-end way. First, the cross-layer context is captured by our basic local-to-global-to-local structure, which hierarchically combines the global semantic structure and the local fine details within the cross-layers. Second, the global image-level label prediction is used as an auxiliary objective in the intermediate layer of the Co-CNN, and its outputs are further used for guiding the feature learning in subsequent convolutional layers to leverage the global image-level context. Finally, to further utilize the local super-pixel contexts, the within-super-pixel smoothing and cross-super-pixel neighbourhood voting are formulated as natural sub-components of the Co-CNN to achieve the local label consistency in both training and testing process. Comprehensive evaluations on two public datasets well demonstrate the significant superiority of our Co-CNN architecture over other state-of-the-arts for human parsing. In particular, the F-1 score on the large dataset [15] reaches 76.95% by Co-CNN, significantly higher than 62.81% and 64.38% by the state-of-the-art algorithms, M-CNN [21] and ATR [15], respectively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410520",
        "reference_list": [
            {
                "year": "2015",
                "id": 182
            },
            {
                "year": "2013",
                "id": 425
            },
            {
                "year": "2015",
                "id": 111
            },
            {
                "year": "2011",
                "id": 194
            },
            {
                "year": "2013",
                "id": 439
            }
        ],
        "citation": {
            "ieee": 41,
            "other": 29,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Context",
                "Semantics",
                "Spatial resolution",
                "Smoothing methods",
                "Labeling",
                "Neural networks",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "learning (artificial intelligence)",
                "neural nets",
                "smoothing methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local label consistency",
                "cross-super-pixel neighbourhood voting",
                "within-super-pixel smoothing",
                "local super-pixel context",
                "convolutional layers",
                "feature learning",
                "global image-level label prediction",
                "local fine details",
                "global semantic structure",
                "local-to-global-to-local structure",
                "pixel-wise categorization",
                "Co-CNN",
                "cross-super-pixel neighborhood context",
                "within-super-pixel context",
                "global image-level context",
                "cross-layer context",
                "contextualized convolutional neural network architecture",
                "human parsing"
            ]
        },
        "id": 154,
        "cited_by": [
            {
                "year": "2017",
                "id": 88
            },
            {
                "year": "2017",
                "id": 176
            },
            {
                "year": "2017",
                "id": 214
            }
        ]
    },
    {
        "title": "Holistically-Nested Edge Detection",
        "authors": [
            "Saining Xie",
            "Zhuowen Tu"
        ],
        "abstract": "We develop a new edge detection algorithm that addresses two critical issues in this long-standing vision problem: (1) holistic image training, and (2) multi-scale feature learning. Our proposed method, holistically-nested edge detection (HED), turns pixel-wise edge classification into image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are crucially important in order to approach the human ability to resolve the challenging ambiguity in edge and object boundary detection. We significantly advance the state-of-the-art on the BSD500 dataset (ODS F-score of 0.782) and the NYU Depth dataset (ODS F-score of 0.746), and do so with an improved speed (0.4 second per image) that is orders of magnitude faster than recent CNN-based edge detection algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410521",
        "reference_list": [
            {
                "year": "2007",
                "id": 144
            }
        ],
        "citation": {
            "ieee": 276,
            "other": 182,
            "total": 458
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Training",
                "Neural networks",
                "Detectors",
                "Feature extraction",
                "Machine learning",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "feature extraction",
                "feedforward neural nets",
                "image classification",
                "image representation",
                "image resolution",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "holistically-nested edge detection algorithm",
                "holistic image training",
                "multiscale feature learning",
                "pixel-wise edge classification",
                "image-to-image prediction",
                "deep learning model",
                "convolutional neural networks",
                "deeply-supervised nets",
                "hierarchical representations",
                "object boundary detection",
                "BSD500 dataset",
                "NYU Depth dataset"
            ]
        },
        "id": 155,
        "cited_by": [
            {
                "year": "2017",
                "id": 21
            },
            {
                "year": "2017",
                "id": 53
            },
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2017",
                "id": 110
            },
            {
                "year": "2017",
                "id": 142
            },
            {
                "year": "2017",
                "id": 143
            },
            {
                "year": "2017",
                "id": 202
            },
            {
                "year": "2017",
                "id": 212
            },
            {
                "year": "2017",
                "id": 234
            },
            {
                "year": "2017",
                "id": 243
            },
            {
                "year": "2017",
                "id": 252
            },
            {
                "year": "2017",
                "id": 285
            },
            {
                "year": "2017",
                "id": 340
            },
            {
                "year": "2017",
                "id": 341
            },
            {
                "year": "2017",
                "id": 362
            },
            {
                "year": "2017",
                "id": 368
            },
            {
                "year": "2017",
                "id": 380
            },
            {
                "year": "2017",
                "id": 432
            },
            {
                "year": "2017",
                "id": 582
            },
            {
                "year": "2017",
                "id": 587
            }
        ]
    },
    {
        "title": "Minimum Barrier Salient Object Detection at 80 FPS",
        "authors": [
            "Jianming Zhang",
            "Stan Sclaroff",
            "Zhe Lin",
            "Xiaohui Shen",
            "Brian Price",
            "Radom\u00edr Mech"
        ],
        "abstract": "We propose a highly efficient, yet powerful, salient object detection method based on the Minimum Barrier Distance (MBD) Transform. The MBD transform is robust to pixel-value fluctuation, and thus can be effectively applied on raw pixels without region abstraction. We present an approximate MBD transform algorithm with 100X speedup over the exact algorithm. An error bound analysis is also provided. Powered by this fast MBD transform algorithm, the proposed salient object detection method runs at 80 FPS, and significantly outperforms previous methods with similar speed on four large benchmark datasets, and achieves comparable or better performance than state-of-the-art methods. Furthermore, a technique based on color whitening is proposed to extend our method to leverage the appearance-based backgroundness cue. This extended version further improves the performance, while still being one order of magnitude faster than all the other leading methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410522",
        "reference_list": [
            {
                "year": "2013",
                "id": 207
            },
            {
                "year": "2013",
                "id": 371
            },
            {
                "year": "2013",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 105,
            "other": 50,
            "total": 155
        },
        "keywords": {
            "IEEE Keywords": [
                "Transforms",
                "Algorithm design and analysis",
                "Object detection",
                "Approximation algorithms",
                "Image color analysis",
                "Cost function",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "error analysis",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "minimum barrier salient object detection",
                "minimum barrier distance transform",
                "pixel-value fluctuation",
                "region abstraction",
                "MBD transform algorithm",
                "exact algorithm",
                "error bound analysis",
                "color whitening",
                "appearance-based backgroundness"
            ]
        },
        "id": 156,
        "cited_by": [
            {
                "year": "2017",
                "id": 109
            },
            {
                "year": "2017",
                "id": 110
            },
            {
                "year": "2017",
                "id": 148
            },
            {
                "year": "2017",
                "id": 426
            }
        ]
    },
    {
        "title": "Learning Image Representations Tied to Ego-Motion",
        "authors": [
            "Dinesh Jayaraman",
            "Kristen Grauman"
        ],
        "abstract": "Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance, i.e, they respond predictably to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410523",
        "reference_list": [
            {
                "year": "2013",
                "id": 401
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 27,
            "total": 65
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Image recognition",
                "Robot sensing systems",
                "Cameras",
                "Observers",
                "Training data",
                "Convolution"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "motion estimation",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning image representations",
                "ego-motion",
                "object images",
                "visual development",
                "visual learning methods",
                "motor signals",
                "egocentric video",
                "convolutional neural networks",
                "unsupervised feature learning approach",
                "visual recognition"
            ]
        },
        "id": 157,
        "cited_by": [
            {
                "year": "2017",
                "id": 69
            },
            {
                "year": "2017",
                "id": 139
            },
            {
                "year": "2017",
                "id": 216
            },
            {
                "year": "2017",
                "id": 233
            },
            {
                "year": "2017",
                "id": 618
            }
        ]
    },
    {
        "title": "Unsupervised Visual Representation Learning by Context Prediction",
        "authors": [
            "Carl Doersch",
            "Abhinav Gupta",
            "Alexei A. Efros"
        ],
        "abstract": "This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework [19] and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410524",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2013",
                "id": 424
            },
            {
                "year": "2015",
                "id": 311
            },
            {
                "year": "2013",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 92,
            "other": 68,
            "total": 160
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Context",
                "Training",
                "Semantics",
                "Data mining",
                "Predictive models",
                "Image representation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation",
                "object recognition",
                "prediction theory",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised visual representation learning",
                "context prediction",
                "spatial context",
                "supervisory signal",
                "unlabeled image collection",
                "image patches extraction",
                "convolutional neural net",
                "position prediction",
                "objects recognition",
                "feature representation",
                "within-image context",
                "visual similarity",
                "unsupervised visual discovery",
                "Pascal VOC 2011 detection dataset",
                "ConvNet",
                "R-CNN framework"
            ]
        },
        "id": 158,
        "cited_by": [
            {
                "year": "2017",
                "id": 69
            },
            {
                "year": "2017",
                "id": 87
            },
            {
                "year": "2017",
                "id": 139
            },
            {
                "year": "2017",
                "id": 164
            },
            {
                "year": "2017",
                "id": 216
            },
            {
                "year": "2017",
                "id": 338
            },
            {
                "year": "2017",
                "id": 424
            },
            {
                "year": "2017",
                "id": 451
            },
            {
                "year": "2017",
                "id": 461
            },
            {
                "year": "2017",
                "id": 618
            },
            {
                "year": "2015",
                "id": 311
            }
        ]
    },
    {
        "title": "Webly Supervised Learning of Convolutional Networks",
        "authors": [
            "Xinlei Chen",
            "Abhinav Gupta"
        ],
        "abstract": "We present an approach to utilize large amounts of web data for learning CNNs. Specifically inspired by curriculum learning, we present a two-step approach for CNN training. First, we use easy images to train an initial visual representation. We then use this initial CNN and adapt it to harder, more realistic images by leveraging the structure of data and categories. We demonstrate that our two-stage CNN outperforms a fine-tuned CNN trained on ImageNet on Pascal VOC 2012. We also demonstrate the strength of webly supervised learning by localizing objects in web images and training a R-CNN style [19] detector. It achieves the best performance on VOC 2007 where no VOC training data is used. Finally, we show our approach is quite robust to noise and performs comparably even when we use image search results from March 2013 (pre-CNN image search era).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410525",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2011",
                "id": 165
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 50,
            "other": 31,
            "total": 81
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Training",
                "Search engines",
                "Google",
                "Data models",
                "Noise measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "data structures",
                "image retrieval",
                "Internet",
                "learning (artificial intelligence)",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Webly supervised learning",
                "Web data",
                "CNN training",
                "initial visual representation",
                "data structure",
                "two-stage CNN",
                "ImageNet",
                "Pascal VOC 2012",
                "Web image",
                "R-CNN style detector",
                "VOC 2007",
                "VOC training data",
                "image search"
            ]
        },
        "id": 159,
        "cited_by": [
            {
                "year": "2017",
                "id": 87
            },
            {
                "year": "2017",
                "id": 201
            },
            {
                "year": "2017",
                "id": 418
            },
            {
                "year": "2017",
                "id": 439
            }
        ]
    },
    {
        "title": "Fast R-CNN",
        "authors": [
            "Ross Girshick"
        ],
        "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410526",
        "reference_list": [],
        "citation": {
            "ieee": 1820,
            "other": 1182,
            "total": 3002
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Proposals",
                "Feature extraction",
                "Object detection",
                "Pipelines",
                "Computer architecture",
                "Open source software"
            ],
            "INSPEC: Controlled Indexing": [
                "feedforward neural nets",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast R-CNN",
                "fast region-based convolutional network method",
                "object detection",
                "VGG16 network",
                "Python",
                "C++",
                "Caffe",
                "open-source MIT License"
            ]
        },
        "id": 160,
        "cited_by": [
            {
                "year": "2017",
                "id": 20
            },
            {
                "year": "2017",
                "id": 35
            },
            {
                "year": "2017",
                "id": 42
            },
            {
                "year": "2017",
                "id": 44
            },
            {
                "year": "2017",
                "id": 59
            },
            {
                "year": "2017",
                "id": 61
            },
            {
                "year": "2017",
                "id": 65
            },
            {
                "year": "2017",
                "id": 69
            },
            {
                "year": "2017",
                "id": 77
            },
            {
                "year": "2017",
                "id": 79
            },
            {
                "year": "2017",
                "id": 81
            },
            {
                "year": "2017",
                "id": 85
            },
            {
                "year": "2017",
                "id": 120
            },
            {
                "year": "2017",
                "id": 125
            },
            {
                "year": "2017",
                "id": 130
            },
            {
                "year": "2017",
                "id": 132
            },
            {
                "year": "2017",
                "id": 136
            },
            {
                "year": "2017",
                "id": 139
            },
            {
                "year": "2017",
                "id": 143
            },
            {
                "year": "2017",
                "id": 145
            },
            {
                "year": "2017",
                "id": 156
            },
            {
                "year": "2017",
                "id": 190
            },
            {
                "year": "2017",
                "id": 193
            },
            {
                "year": "2017",
                "id": 194
            },
            {
                "year": "2017",
                "id": 197
            },
            {
                "year": "2017",
                "id": 202
            },
            {
                "year": "2017",
                "id": 203
            },
            {
                "year": "2017",
                "id": 205
            },
            {
                "year": "2017",
                "id": 224
            },
            {
                "year": "2017",
                "id": 230
            },
            {
                "year": "2017",
                "id": 232
            },
            {
                "year": "2017",
                "id": 247
            },
            {
                "year": "2017",
                "id": 271
            },
            {
                "year": "2017",
                "id": 287
            },
            {
                "year": "2017",
                "id": 293
            },
            {
                "year": "2017",
                "id": 307
            },
            {
                "year": "2017",
                "id": 312
            },
            {
                "year": "2017",
                "id": 314
            },
            {
                "year": "2017",
                "id": 315
            },
            {
                "year": "2017",
                "id": 320
            },
            {
                "year": "2017",
                "id": 356
            },
            {
                "year": "2017",
                "id": 357
            },
            {
                "year": "2017",
                "id": 358
            },
            {
                "year": "2017",
                "id": 369
            },
            {
                "year": "2017",
                "id": 371
            },
            {
                "year": "2017",
                "id": 372
            },
            {
                "year": "2017",
                "id": 381
            },
            {
                "year": "2017",
                "id": 382
            },
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2017",
                "id": 431
            },
            {
                "year": "2017",
                "id": 434
            },
            {
                "year": "2017",
                "id": 436
            },
            {
                "year": "2017",
                "id": 437
            },
            {
                "year": "2017",
                "id": 444
            },
            {
                "year": "2017",
                "id": 446
            },
            {
                "year": "2017",
                "id": 463
            },
            {
                "year": "2017",
                "id": 465
            },
            {
                "year": "2017",
                "id": 478
            },
            {
                "year": "2017",
                "id": 485
            },
            {
                "year": "2017",
                "id": 508
            },
            {
                "year": "2017",
                "id": 509
            },
            {
                "year": "2017",
                "id": 512
            },
            {
                "year": "2017",
                "id": 512
            },
            {
                "year": "2017",
                "id": 517
            },
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2017",
                "id": 520
            },
            {
                "year": "2017",
                "id": 522
            },
            {
                "year": "2017",
                "id": 524
            },
            {
                "year": "2017",
                "id": 544
            },
            {
                "year": "2017",
                "id": 550
            },
            {
                "year": "2017",
                "id": 553
            },
            {
                "year": "2017",
                "id": 574
            },
            {
                "year": "2017",
                "id": 575
            },
            {
                "year": "2017",
                "id": 595
            },
            {
                "year": "2017",
                "id": 603
            },
            {
                "year": "2017",
                "id": 610
            },
            {
                "year": "2017",
                "id": 618
            },
            {
                "year": "2015",
                "id": 120
            },
            {
                "year": "2015",
                "id": 178
            },
            {
                "year": "2015",
                "id": 276
            },
            {
                "year": "2015",
                "id": 287
            }
        ]
    },
    {
        "title": "Bilinear CNN Models for Fine-Grained Visual Recognition",
        "authors": [
            "Tsung-Yu Lin",
            "Aruni RoyChowdhury",
            "Subhransu Maji"
        ],
        "abstract": "We propose bilinear models, a recognition architecture that consists of two feature extractors whose outputs are multiplied using outer product at each location of the image and pooled to obtain an image descriptor. This architecture can model local pairwise feature interactions in a translationally invariant manner which is particularly useful for fine-grained categorization. It also generalizes various orderless texture descriptors such as the Fisher vector, VLAD and O2P. We present experiments with bilinear models where the feature extractors are based on convolutional neural networks. The bilinear form simplifies gradient computation and allows end-to-end training of both networks using image labels only. Using networks initialized from the ImageNet dataset followed by domain specific fine-tuning we obtain 84.1% accuracy of the CUB-200-2011 dataset requiring only category labels at training time. We present experiments and visualizations that analyze the effects of fine-tuning and the choice two networks on the speed and accuracy of the models. Results show that the architecture compares favorably to the existing state of the art on a number of fine-grained datasets while being substantially simpler and easier to train. Moreover, our most accurate model is fairly efficient running at 8 frames/sec on a NVIDIA Tesla K40 GPU. The source code for the complete system will be made available at http://vis-www.cs.umass.edu/bcnn.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410527",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2011",
                "id": 20
            }
        ],
        "citation": {
            "ieee": 202,
            "other": 97,
            "total": 299
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Computational modeling",
                "Computer architecture",
                "Visualization",
                "Image recognition",
                "Atmospheric modeling",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "neural nets",
                "source code (software)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bilinear CNN model",
                "fine-grained visual recognition architecture",
                "feature extraction",
                "image descriptor",
                "local pairwise feature interaction",
                "orderless texture descriptor",
                "convolutional neural network",
                "image label",
                "ImageNet dataset",
                "NVIDIA Tesla K40 GPU",
                "source code"
            ]
        },
        "id": 161,
        "cited_by": [
            {
                "year": "2017",
                "id": 52
            },
            {
                "year": "2017",
                "id": 53
            },
            {
                "year": "2017",
                "id": 56
            },
            {
                "year": "2017",
                "id": 142
            },
            {
                "year": "2017",
                "id": 192
            },
            {
                "year": "2017",
                "id": 218
            },
            {
                "year": "2017",
                "id": 275
            },
            {
                "year": "2017",
                "id": 349
            },
            {
                "year": "2017",
                "id": 521
            },
            {
                "year": "2017",
                "id": 547
            },
            {
                "year": "2017",
                "id": 582
            },
            {
                "year": "2017",
                "id": 603
            }
        ]
    },
    {
        "title": "Discovering the Spatial Extent of Relative Attributes",
        "authors": [
            "Fanyi Xiao",
            "Yong Jae Lee"
        ],
        "abstract": "We present a weakly-supervised approach that discovers the spatial extent of relative attributes, given only pairs of ordered images. In contrast to traditional approaches that use global appearance features or rely on keypoint detectors, our goal is to automatically discover the image regions that are relevant to the attribute, even when the attribute's appearance changes drastically across its attribute spectrum. To accomplish this, we first develop a novel formulation that combines a detector with local smoothness to discover a set of coherent visual chains across the image collection. We then introduce an efficient way to generate additional chains anchored on the initial discovered ones. Finally, we automatically identify the most relevant visual chains, and create an ensemble image representation to model the attribute. Through extensive experiments, we demonstrate our method's promise relative to several baselines in modeling relative attributes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410528",
        "reference_list": [
            {
                "year": "2013",
                "id": 428
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2013",
                "id": 231
            },
            {
                "year": "2011",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 9,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Detectors",
                "Footwear",
                "Image representation",
                "Computer vision",
                "Computational modeling",
                "Scalability"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ensemble image representation",
                "image collection",
                "coherent visual chains",
                "keypoint detectors",
                "global appearance features",
                "weakly-supervised approach",
                "relative attributes"
            ]
        },
        "id": 162,
        "cited_by": [
            {
                "year": "2017",
                "id": 153
            },
            {
                "year": "2017",
                "id": 371
            },
            {
                "year": "2017",
                "id": 584
            }
        ]
    },
    {
        "title": "Deep Neural Decision Forests",
        "authors": [
            "Peter Kontschieder",
            "Madalina Fiterau",
            "Antonio Criminisi",
            "Samuel Rota Bul\u00f2"
        ],
        "abstract": "We present Deep Neural Decision Forests - a novel approach that unifies classification trees with the representation learning functionality known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find on-par or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410529",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 26,
            "total": 61
        },
        "keywords": {
            "IEEE Keywords": [
                "Vegetation",
                "Routing",
                "Decision trees",
                "Optimization",
                "Training",
                "Stochastic processes",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convolution",
                "decision trees",
                "image representation",
                "learning (artificial intelligence)",
                "neural nets",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep neural decision forest",
                "classification tree",
                "representation learning functionality",
                "deep convolutional network",
                "parameter optimization",
                "machine learning",
                "computer vision"
            ]
        },
        "id": 163,
        "cited_by": [
            {
                "year": "2017",
                "id": 221
            }
        ]
    },
    {
        "title": "Deep Fried Convnets",
        "authors": [
            "Zichao Yang",
            "Marcin Moczulski",
            "Misha Denil",
            "Nando de Freitas",
            "Alex Smola",
            "Le Song",
            "Ziyu Wang"
        ],
        "abstract": "The fully-connected layers of deep convolutional neural networks typically contain over 90% of the network parameters. Reducing the number of parameters while preserving predictive performance is critically important for training big models in distributed systems and for deployment in embedded devices. In this paper, we introduce a novel Adaptive Fastfood transform to reparameterize the matrix-vector multiplication of fully connected layers. Reparameterizing a fully connected layer with d inputs and n outputs with the Adaptive Fastfood transform reduces the storage and computational costs costs from O(nd) to O(n) and O(n log d) respectively. Using the Adaptive Fastfood transform in convolutional networks results in what we call a deep fried convnet. These convnets are end-to-end trainable, and enable us to attain substantial reductions in the number of parameters without affecting prediction accuracy on the MNIST and ImageNet datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410530",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 14,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Transforms",
                "Kernel",
                "Neural networks",
                "Sparse matrices",
                "Training",
                "Adaptive systems",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "matrix algebra",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Adaptive Fastfood transform",
                "network parameters",
                "deep convolutional neural networks"
            ]
        },
        "id": 164,
        "cited_by": [
            {
                "year": "2017",
                "id": 53
            },
            {
                "year": "2017",
                "id": 295
            }
        ]
    },
    {
        "title": "Semantic Component Analysis",
        "authors": [
            "Calvin Murdock",
            "Fernando De la Torre"
        ],
        "abstract": "Unsupervised and weakly-supervised visual learning in large image collections are critical in order to avoid the time-consuming and error-prone process of manual labeling. Standard approaches rely on methods like multiple-instance learning or graphical models, which can be computationally intensive and sensitive to initialization. On the other hand, simpler component analysis or clustering methods usually cannot achieve meaningful invariances or semantic interpretability. To address the issues of previous work, we present a simple but effective method called Semantic Component Analysis (SCA), which provides a decomposition of images into semantic components. Unsupervised SCA decomposes additive image representations into spatially-meaningful visual components that naturally correspond to object categories. Using an overcomplete representation that allows for rich instance-level constraints and spatial priors, SCA gives improved results and more interpretable components in comparison to traditional matrix factorization techniques. If weakly-supervised information is available in the form of image-level tags, SCA factorizes a set of images into semantic groups of superpixels. We also provide qualitative connections to traditional methods for component analysis (e.g. Grassmann averages, PCA, and NMF). The effectiveness of our approach is validated through synthetic data and on the MSRC2 and Sift Flow datasets, demonstrating competitive results in unsupervised and weakly-supervised semantic segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410531",
        "reference_list": [
            {
                "year": "2013",
                "id": 38
            },
            {
                "year": "2009",
                "id": 247
            },
            {
                "year": "2011",
                "id": 81
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Image segmentation",
                "Matrix decomposition",
                "Histograms",
                "Visualization",
                "Principal component analysis",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "principal component analysis",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic component analysis",
                "unsupervised visual learning",
                "weakly-supervised visual learning",
                "unsupervised SCA",
                "additive image representation",
                "spatially-meaningful visual component",
                "overcomplete representation",
                "instance-level constraint",
                "image-level tags"
            ]
        },
        "id": 165,
        "cited_by": []
    },
    {
        "title": "Low-Rank Matrix Factorization under General Mixture Noise Distributions",
        "authors": [
            "Xiangyong Cao",
            "Yang Chen",
            "Qian Zhao",
            "Deyu Meng",
            "Yao Wang",
            "Dong Wang",
            "Zongben Xu"
        ],
        "abstract": "Many computer vision problems can be posed as learning a low-dimensional subspace from high dimensional data. The low rank matrix factorization (LRMF) represents a commonly utilized subspace learning strategy. Most of the current LRMF techniques are constructed on the optimization problem using L_1 norm and L_2 norm, which mainly deal with Laplacian and Gaussian noise, respectively. To make LRMF capable of adapting more complex noise, this paper proposes a new LRMF model by assuming noise as Mixture of Exponential Power (MoEP) distributions and proposes a penalized MoEP model by combining the penalized likelihood method with MoEP distributions. Such setting facilitates the learned LRMF model capable of automatically fitting the real noise through MoEP distributions. Each component in this mixture is adapted from a series of preliminary super-or sub-Gaussian candidates. An Expectation Maximization (EM) algorithm is also designed to infer the parameters involved in the proposed PMoEP model. The advantage of our method is demonstrated by extensive experiments on synthetic data, face modeling and hyperspectral image restoration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410532",
        "reference_list": [
            {
                "year": "2013",
                "id": 166
            },
            {
                "year": "2013",
                "id": 222
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 2,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Data models",
                "Computer vision",
                "Gaussian noise",
                "Adaptation models",
                "Algorithm design and analysis",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "exponential distribution",
                "Gaussian processes",
                "image restoration",
                "matrix decomposition",
                "mixture models"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "low-rank matrix factorization",
                "general mixture noise distributions",
                "computer vision",
                "LRMF",
                "subspace learning strategy",
                "mixture of exponential power",
                "MoEP distributions",
                "penalized likelihood method",
                "expectation maximization algorithm",
                "EM algorithm",
                "hyperspectral image restoration"
            ]
        },
        "id": 166,
        "cited_by": [
            {
                "year": "2017",
                "id": 181
            }
        ]
    },
    {
        "title": "Web-Scale Image Clustering Revisited",
        "authors": [
            "Yannis Avrithis",
            "Yannis Kalantidis",
            "Evangelos Anagnostopoulos",
            "Ioannis Z. Emiris"
        ],
        "abstract": "Large scale duplicate detection, clustering and mining of documents or images has been conventionally treated with seed detection via hashing, followed by seed growing heuristics using fast search. Principled clustering methods, especially kernelized and spectral ones, have higher complexity and are difficult to scale above millions. Under the assumption of documents or images embedded in Euclidean space, we revisit recent advances in approximate k-means variants, and borrow their best ingredients to introduce a new one, inverted-quantized k-means (IQ-means). Key underlying concepts are quantization of data points and multi-index based inverted search from centroids to cells. Its quantization is a form of hashing and analogous to seed detection, while its updates are analogous to seed growing, yet principled in the sense of distortion minimization. We further design a dynamic variant that is able to determine the number of clusters k in a single run at nearly zero additional cost. Combined with powerful deep learned representations, we achieve clustering of a 100 million image collection on a single machine in less than one hour.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410533",
        "reference_list": [
            {
                "year": "2009",
                "id": 251
            },
            {
                "year": "2007",
                "id": 137
            },
            {
                "year": "2007",
                "id": 24
            },
            {
                "year": "2013",
                "id": 174
            },
            {
                "year": "2011",
                "id": 143
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 3,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Quantization (signal)",
                "Visualization",
                "Distortion",
                "Artificial neural networks",
                "Search problems",
                "Probabilistic logic",
                "Metadata"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "data mining",
                "document handling",
                "image processing",
                "minimisation",
                "pattern clustering",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "IQ-means",
                "data point quantization",
                "multiindex based inverted search",
                "distortion minimization",
                "dynamic variant design",
                "inverted-quantized k-means",
                "approximate k-means variants",
                "principled clustering methods",
                "seed growing heuristics",
                "hashing",
                "seed detection",
                "document mining",
                "document clustering",
                "duplicate detection",
                "Web-scale image clustering"
            ]
        },
        "id": 167,
        "cited_by": []
    },
    {
        "title": "Learning Discriminative Reconstructions for Unsupervised Outlier Removal",
        "authors": [
            "Yan Xia",
            "Xudong Cao",
            "Fang Wen",
            "Gang Hua",
            "Jian Sun"
        ],
        "abstract": "We study the problem of automatically removing outliers from noisy data, with application for removing outlier images from an image collection. We address this problem by utilizing the reconstruction errors of an autoencoder. We observe that when data are reconstructed from low-dimensional representations, the inliers and the outliers can be well separated according to their reconstruction errors. Based on this basic observation, we gradually inject discriminative information in the learning process of an autoencoder to make the inliers and the outliers more separable. Experiments on a variety of image datasets validate our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410534",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 7,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Noise measurement",
                "Training",
                "Training data",
                "Computer vision",
                "Principal component analysis",
                "Neurons"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "learning (artificial intelligence)",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative reconstructions learning",
                "unsupervised outlier removal",
                "image collection",
                "reconstruction errors",
                "autoencoder",
                "low-dimensional representations",
                "image datasets"
            ]
        },
        "id": 168,
        "cited_by": []
    },
    {
        "title": "Learning Deconvolution Network for Semantic Segmentation",
        "authors": [
            "Hyeonwoo Noh",
            "Seunghoon Hong",
            "Bohyung Han"
        ],
        "abstract": "We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410535",
        "reference_list": [
            {
                "year": "2015",
                "id": 182
            },
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2011",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 532,
            "other": 308,
            "total": 840
        },
        "keywords": {
            "IEEE Keywords": [
                "Deconvolution",
                "Semantics",
                "Image segmentation",
                "Visualization",
                "Feature extraction",
                "Shape",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "deconvolution",
                "image segmentation",
                "learning (artificial intelligence)",
                "neural nets",
                "prediction theory",
                "semantic networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deconvolution network learning",
                "semantic segmentation algorithm",
                "convolutional neural network",
                "CNN",
                "proposal-wise prediction"
            ]
        },
        "id": 169,
        "cited_by": [
            {
                "year": "2017",
                "id": 21
            },
            {
                "year": "2017",
                "id": 22
            },
            {
                "year": "2017",
                "id": 44
            },
            {
                "year": "2017",
                "id": 65
            },
            {
                "year": "2017",
                "id": 110
            },
            {
                "year": "2017",
                "id": 137
            },
            {
                "year": "2017",
                "id": 191
            },
            {
                "year": "2017",
                "id": 196
            },
            {
                "year": "2017",
                "id": 210
            },
            {
                "year": "2017",
                "id": 211
            },
            {
                "year": "2017",
                "id": 213
            },
            {
                "year": "2017",
                "id": 214
            },
            {
                "year": "2017",
                "id": 220
            },
            {
                "year": "2017",
                "id": 356
            },
            {
                "year": "2017",
                "id": 372
            },
            {
                "year": "2017",
                "id": 437
            },
            {
                "year": "2017",
                "id": 460
            },
            {
                "year": "2017",
                "id": 511
            },
            {
                "year": "2017",
                "id": 523
            },
            {
                "year": "2017",
                "id": 529
            },
            {
                "year": "2017",
                "id": 531
            },
            {
                "year": "2017",
                "id": 596
            }
        ]
    },
    {
        "title": "Conditional Random Fields as Recurrent Neural Networks",
        "authors": [
            "Shuai Zheng",
            "Sadeep Jayasumana",
            "Bernardino Romera-Paredes",
            "Vibhav Vineet",
            "Zhizhong Su",
            "Dalong Du",
            "Chang Huang",
            "Philip H. S. Torr"
        ],
        "abstract": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding. Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks. One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects. To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling. To this end, we formulate Conditional Random Fields with Gaussian pairwise potentials and mean-field approximate inference as Recurrent Neural Networks. This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs. Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation. We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410536",
        "reference_list": [
            {
                "year": "2015",
                "id": 182
            },
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2009",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 481,
            "other": 261,
            "total": 742
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Image segmentation",
                "Semantics",
                "Graphical models",
                "Machine learning",
                "Training",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "backpropagation",
                "Gaussian processes",
                "image segmentation",
                "neural nets",
                "probability",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "conditional random field",
                "recurrent neural network",
                "pixel-level labelling task",
                "semantic image segmentation",
                "image understanding",
                "deep learning technique",
                "convolutional neural network",
                "CNN",
                "CRF",
                "probabilistic graphical modelling",
                "Gaussian pairwise potential",
                "mean-field approximate inference",
                "back-propagation algorithm"
            ]
        },
        "id": 170,
        "cited_by": [
            {
                "year": "2017",
                "id": 28
            },
            {
                "year": "2017",
                "id": 81
            },
            {
                "year": "2017",
                "id": 135
            },
            {
                "year": "2017",
                "id": 137
            },
            {
                "year": "2017",
                "id": 143
            },
            {
                "year": "2017",
                "id": 277
            },
            {
                "year": "2017",
                "id": 286
            },
            {
                "year": "2017",
                "id": 343
            },
            {
                "year": "2017",
                "id": 375
            },
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2017",
                "id": 438
            },
            {
                "year": "2017",
                "id": 522
            },
            {
                "year": "2017",
                "id": 523
            },
            {
                "year": "2017",
                "id": 524
            },
            {
                "year": "2017",
                "id": 529
            },
            {
                "year": "2017",
                "id": 536
            },
            {
                "year": "2017",
                "id": 546
            },
            {
                "year": "2017",
                "id": 585
            },
            {
                "year": "2015",
                "id": 330
            }
        ]
    },
    {
        "title": "The One Triangle Three Parallelograms Sampling Strategy and Its Application in Shape Regression",
        "authors": [
            "Mikael Nilsson"
        ],
        "abstract": "The purpose of this paper is threefold. Firstly, the paper introduces the One Triangle Three Parallelograms (OTTP) sampling strategy, which can be viewed as a way to index pixels from a given shape and image. Secondly, a framework for cascaded shape regression, including the OTTP sampling, is presented. In short, this framework involves binary pixel tests for appearance features combined with shape features followed by a large linear system for each regression stage in the cascade. The proposed solution is found to produce state-of-the-art results on the task of facial landmark estimation. Thirdly, the dependence of accuracy of the landmark predictions and the placement of the mean shape within the detection box is discussed and a method to visualize it is presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410537",
        "reference_list": [
            {
                "year": "2013",
                "id": 188
            },
            {
                "year": "2001",
                "id": 214
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Feature extraction",
                "Training",
                "Indexes",
                "Estimation",
                "Computer vision",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "data visualisation",
                "feature extraction",
                "image sampling",
                "regression analysis",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "one triangle three parallelogram sampling strategy",
                "shape regression",
                "OTTP sampling strategy",
                "cascaded shape regression",
                "binary pixel tests",
                "shape features",
                "linear system",
                "regression stage",
                "facial landmark estimation",
                "landmark predictions",
                "detection box"
            ]
        },
        "id": 171,
        "cited_by": []
    },
    {
        "title": "Boosting Object Proposals: From Pascal to COCO",
        "authors": [
            "Jordi Pont-Tuset",
            "Luc Van Gool"
        ],
        "abstract": "Computer vision in general, and object proposals in particular, are nowadays strongly influenced by the databases on which researchers evaluate the performance of their algorithms. This paper studies the transition from the Pascal Visual Object Challenge dataset, which has been the benchmark of reference for the last years, to the updated, bigger, and more challenging Microsoft Common Objects in Context. We first review and deeply analyze the new challenges, and opportunities, that this database presents. We then survey the current state of the art in object proposals and evaluate it focusing on how it generalizes to the new dataset. In sight of these results, we propose various lines of research to take advantage of the new benchmark and improve the techniques. We explore one of these lines, which leads to an improvement over the state of the art of +5.2%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410538",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2013",
                "id": 316
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 5,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Databases",
                "Proposals",
                "Image segmentation",
                "Computer vision",
                "Visualization",
                "Object segmentation",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "object proposals",
                "Pascal Visual Object Challenge dataset",
                "Microsoft Common Objects in Context",
                "COCO",
                "Pascal",
                "databases"
            ]
        },
        "id": 172,
        "cited_by": []
    },
    {
        "title": "Secrets of GrabCut and Kernel K-Means",
        "authors": [
            "Meng Tang",
            "Ismail Ben Ayed",
            "Dmitrii Marin",
            "Yuri Boykov"
        ],
        "abstract": "The log-likelihood energy term in popular model-fitting segmentation methods, e.g. [39, 8, 28, 10], is presented as a generalized \"probabilistic K-means\" energy [16] for color space clustering. This interpretation reveals some limitations, e.g. over-fitting. We propose an alternative approach to color clustering using kernel K-means energy with well-known properties such as non-linear separation and scalability to higher-dimensional feature spaces. Our bound formulation for kernel K-means allows to combine general pair-wise feature clustering methods with image grid regularization using graph cuts, similarly to standard color model fitting techniques for segmentation. Unlike histogram or GMM fitting [39, 28], our approach is closely related to average association and normalized cut. But, in contrast to previous pairwise clustering algorithms, our approach can incorporate any standard geometric regularization in the image domain. We analyze extreme cases for kernel bandwidth (e.g. Gini bias) and demonstrate effectiveness of KNN-based adaptive bandwidth strategies. Our kernel K-means approach to segmentation benefits from higher-dimensional features where standard model fitting fails.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410539",
        "reference_list": [
            {
                "year": "2015",
                "id": 197
            },
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2009",
                "id": 35
            },
            {
                "year": "2009",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 4,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Image color analysis",
                "Histograms",
                "Standards",
                "Entropy",
                "Probabilistic logic",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "graph theory",
                "image colour analysis",
                "image segmentation",
                "pattern clustering",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "GrabCut",
                "log-likelihood energy term",
                "model-fitting segmentation methods",
                "generalized probabilistic K-means energy",
                "color space clustering",
                "kernel K-means energy",
                "higher-dimensional feature spaces",
                "bound formulation",
                "pair-wise feature clustering methods",
                "image grid regularization",
                "graph cuts",
                "normalized cut",
                "average association",
                "standard geometric regularization",
                "kernel bandwidth",
                "KNN-based adaptive bandwidth strategies"
            ]
        },
        "id": 173,
        "cited_by": []
    },
    {
        "title": "Video Matting via Sparse and Low-Rank Representation",
        "authors": [
            "Dongqing Zou",
            "Xiaowu Chen",
            "Guangying Cao",
            "Xiaogang Wang"
        ],
        "abstract": "We introduce a novel method of video matting via sparse and low-rank representation. Previous matting methods [10, 9] introduced a nonlocal prior to estimate the alpha matte and have achieved impressive results on some data. However, on one hand, searching inadequate or excessive samples may miss good samples or introduce noise, on the other hand, it is difficult to construct consistent nonlocal structures for pixels with similar features, yielding spatially and temporally inconsistent video mattes. In this paper, we proposed a novel video matting method to achieve spatially and temporally consistent matting result. Toward this end, a sparse and low-rank representation model is introduced to pursue consistent nonlocal structures for pixels with similar features. The sparse representation is used to adaptively select best samples and accurately construct the nonlocal structures for all pixels, while the low-rank representation is used to globally ensure consistent nonlocal structures for pixels with similar features. The two representations are combined to generate consistent video mattes. Experimental results show that our method has achieved high quality results in a variety of challenging examples featuring illumination changes, feature ambiguity, topology changes, transparency variation, dis-occlusion, fast motion and motion blur.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410540",
        "reference_list": [
            {
                "year": "2013",
                "id": 359
            },
            {
                "year": "2013",
                "id": 449
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Optical imaging",
                "Sparse matrices",
                "Topology",
                "Optical coupling",
                "Laplace equations",
                "Geometrical optics"
            ],
            "INSPEC: Controlled Indexing": [
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video matting",
                "sparse representation",
                "temporally consistent video mattes",
                "spatially consistent video mattes",
                "low-rank representation"
            ]
        },
        "id": 174,
        "cited_by": []
    },
    {
        "title": "Joint Object and Part Segmentation Using Deep Learned Potentials",
        "authors": [
            "Peng Wang",
            "Xiaohui Shen",
            "Zhe Lin",
            "Scott Cohen",
            "Brian Price",
            "Alan Yuille"
        ],
        "abstract": "Segmenting semantic objects from images and parsing them into their respective semantic parts are fundamental steps towards detailed object understanding in computer vision. In this paper, we propose a joint solution that tackles semantic object and part segmentation simultaneously, in which higher object-level context is provided to guide part segmentation, and more detailed part-level localization is utilized to refine object segmentation. Specifically, we first introduce the concept of semantic compositional parts (SCP) in which similar semantic parts are grouped and shared among different objects. A two-stream fully convolutional network (FCN) is then trained to provide the SCP and object potentials at each pixel. At the same time, a compact set of segments can also be obtained from the SCP predictions of the network. Given the potentials and the generated segments, in order to explore long-range context, we finally construct an efficient fully connected conditional random field (FCRF) to jointly predict the final object and part labels. Extensive evaluation on three different datasets shows that our approach can mutually enhance the performance of object and part segmentation, and outperforms the current state-of-the-art on both tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410541",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 272
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 6,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Image segmentation",
                "Training",
                "Context",
                "Object segmentation",
                "Legged locomotion",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "learning (artificial intelligence)",
                "prediction theory",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep learned potentials",
                "semantic object segmentation",
                "parsing",
                "object understanding",
                "computer vision",
                "part segmentation",
                "object-level context",
                "part-level localization",
                "semantic compositional parts",
                "two-stream fully convolutional network",
                "FCN",
                "object potentials",
                "SCP predictions",
                "fully connected conditional random field",
                "FCRF",
                "object labels",
                "part labels"
            ]
        },
        "id": 175,
        "cited_by": [
            {
                "year": "2017",
                "id": 60
            }
        ]
    },
    {
        "title": "Low-Rank Tensor Constrained Multiview Subspace Clustering",
        "authors": [
            "Changqing Zhang",
            "Huazhu Fu",
            "Si Liu",
            "Guangcan Liu",
            "Xiaochun Cao"
        ],
        "abstract": "In this paper, we explore the problem of multiview subspace clustering. We introduce a low-rank tensor constraint to explore the complementary information from multiple views and, accordingly, establish a novel method called Low-rank Tensor constrained Multiview Subspace Clustering (LT-MSC). Our method regards the subspace representation matrices of different views as a tensor, which captures dexterously the high order correlations underlying multiview data. Then the tensor is equipped with a low-rank constraint, which models elegantly the cross information among different views, reduces effectually the redundancy of the learned subspace representations, and improves the accuracy of clustering as well. The inference process of the affinity matrix for clustering is formulated as a tensor nuclear norm minimization problem, constrained with an additional L2,1-norm regularizer and some linear equalities. The minimization problem is convex and thus can be solved efficiently by an Augmented Lagrangian Alternating Direction Minimization (AL-ADM) method. Extensive experimental results on four benchmark datasets show the effectiveness of our proposed LT-MSC method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410542",
        "reference_list": [
            {
                "year": "2011",
                "id": 310
            },
            {
                "year": "2013",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 13,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Correlation",
                "Clustering methods",
                "Minimization",
                "Kernel",
                "Optimization",
                "Aerospace electronics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "pattern clustering",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "low rank tensor constrained multiview subspace clustering",
                "complementary information",
                "LT-MSC",
                "subspace representation matrices",
                "multiview data",
                "learned subspace representations",
                "inference process",
                "affinity matrix",
                "tensor nuclear norm minimization problem",
                "convex programming",
                "Augmented Lagrangian Alternating Direction Minimization",
                "AL-ADM",
                "machine learning",
                "computer vision"
            ]
        },
        "id": 176,
        "cited_by": []
    },
    {
        "title": "BodyPrint: Pose Invariant 3D Shape Matching of Human Bodies",
        "authors": [
            "Jiangping Wang",
            "Kai Ma",
            "Vivek Kumar Singh",
            "Thomas Huang",
            "Terrence Chen"
        ],
        "abstract": "3D human body shape matching has large potential on many real world applications, especially with the recent advances in the 3D range sensing technology. We address this problem by proposing a novel holistic human body shape descriptor called BodyPrint. To compute the bodyprint for a given body scan, we fit a deformable human body mesh and project the mesh parameters to a low-dimensional subspace which improves discriminability across different persons. Experiments are carried out on three real-world human body datasets to demonstrate that BodyPrint is robust to pose variation as well as missing information and sensor noise. It improves the matching accuracy significantly compared to conventional 3D shape matching techniques using local features. To facilitate practical applications where the shape database may grow over time, we also extend our learning framework to handle online updates.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410543",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2011",
                "id": 285
            },
            {
                "year": "2011",
                "id": 247
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three-dimensional displays",
                "Measurement",
                "Principal component analysis",
                "Robustness",
                "Training",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "learning (artificial intelligence)",
                "mesh generation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "BodyPrint",
                "pose invariant 3D human body shape matching",
                "3D range sensing technology",
                "holistic human body shape descriptor",
                "deformable human body mesh",
                "learning framework"
            ]
        },
        "id": 177,
        "cited_by": []
    },
    {
        "title": "The Middle Child Problem: Revisiting Parametric Min-Cut and Seeds for Object Proposals",
        "authors": [
            "Ahmad Humayun",
            "Fuxin Li",
            "James M. Rehg"
        ],
        "abstract": "Object proposals have recently fueled the progress in detection performance. These proposals aim to provide category-agnostic localizations for all objects in an image. One way to generate proposals is to perform parametric min-cuts over seed locations. This paper demonstrates that standard parametric-cut models are ineffective in obtaining medium-sized objects, which we refer to as the middle child problem. We propose a new energy minimization framework incorporating geodesic distances between segments which solves this problem. In addition, we introduce a new superpixel merging algorithm which can generate a small set of seeds that reliably cover a large number of objects of all sizes. We call our method POISE - \"Proposals for Objects from Improved Seeds and Energies.\" POISE enables parametric min-cuts to reach their full potential. On PASCAL VOC it generates ~2,640 segments with an average overlap of 0.81, whereas the closest competing methods require more than 4,200 proposals to reach the same accuracy. We show detailed quantitative comparisons against 5 state-of-the-art methods on PASCAL VOC and Microsoft COCO segmentation challenges.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410544",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2015",
                "id": 160
            },
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2013",
                "id": 106
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Image segmentation",
                "Measurement",
                "Minimization",
                "Merging",
                "Standards",
                "Reliability"
            ],
            "INSPEC: Controlled Indexing": [
                "category theory",
                "differential geometry",
                "minimisation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "POISE",
                "proposals for objects from improved seeds and energies",
                "geodesic distance",
                "energy minimization framework",
                "category-agnostic localization",
                "object detection",
                "object proposal",
                "seed location",
                "parametric min-cut model",
                "middle child problem"
            ]
        },
        "id": 178,
        "cited_by": []
    },
    {
        "title": "Contour Guided Hierarchical Model for Shape Matching",
        "authors": [
            "Yuanqi Su",
            "Yuehu Liu",
            "Bonan Cuan",
            "Nanning Zheng"
        ],
        "abstract": "For its simplicity and effectiveness, star model is popular in shape matching. However, it suffers from the loose geometric connections among parts. In the paper, we present a novel algorithm that reconsiders these connections and reduces the global matching to a set of interrelated local matching. For the purpose, we divide the shape template into overlapped parts and model the matching through a part-based layered structure that uses the latent variable to constrain parts' deformation. As for inference, each part is used for localizing candidates by the partial matching. Thanks to the contour fragments, the partial matching can be solved via modified dynamic programming. The overlapped regions among parts of the template are then explored to make the candidates of parts meet at their shared points. The process is fulfilled via a refined procedure based on iterative dynamic programming. Results on ETHZ shape and Inria Horse datasets demonstrate the benefits of the proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410545",
        "reference_list": [
            {
                "year": "2009",
                "id": 294
            },
            {
                "year": "2007",
                "id": 178
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 4,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Deformable models",
                "Image edge detection",
                "Transforms",
                "Shape measurement",
                "Periodic structures",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "dynamic programming",
                "image matching",
                "iterative methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "contour guided hierarchical model",
                "shape matching",
                "star model",
                "global matching",
                "interrelated local matching",
                "shape template",
                "overlapped parts",
                "part-based layered structure",
                "latent variable",
                "candidate localization",
                "partial matching",
                "contour fragments",
                "iterative dynamic programming",
                "ETHZ shape dataset",
                "Inria Horse datasets"
            ]
        },
        "id": 179,
        "cited_by": []
    },
    {
        "title": "Robust Image Segmentation Using Contour-Guided Color Palettes",
        "authors": [
            "Xiang Fu",
            "Chien-Yi Wang",
            "Chen Chen",
            "Changhu Wang",
            "C.-C. Jay Kuo"
        ],
        "abstract": "The contour-guided color palette (CCP) is proposed for robust image segmentation. It efficiently integrates contour and color cues of an image. To find representative colors of an image, color samples along long contours between regions, similar in spirit to machine learning methodology that focus on samples near decision boundaries, are collected followed by the mean-shift (MS) algorithm in the sampled color space to achieve an image-dependent color palette. This color palette provides a preliminary segmentation in the spatial domain, which is further fine-tuned by post-processing techniques such as leakage avoidance, fake boundary removal, and small region mergence. Segmentation performances of CCP and MS are compared and analyzed. While CCP offers an acceptable standalone segmentation result, it can be further integrated into the framework of layered spectral segmentation to produce a more robust segmentation. The superior performance of CCP-based segmentation algorithm is demonstrated by experiments on the Berkeley Segmentation Dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410546",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2009",
                "id": 104
            },
            {
                "year": "2001",
                "id": 160
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 5,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Image segmentation",
                "Robustness",
                "Bandwidth",
                "Quantization (signal)",
                "Spectral analysis",
                "Complexity theory"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image segmentation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust image segmentation",
                "contour-guided color palette",
                "color cues",
                "machine learning",
                "mean-shift algorithm",
                "image-dependent color palette",
                "leakage avoidance",
                "fake boundary removal",
                "small region mergence",
                "layered spectral segmentation"
            ]
        },
        "id": 180,
        "cited_by": []
    },
    {
        "title": "Joint Optimization of Segmentation and Color Clustering",
        "authors": [
            "Ekaterina Lobacheva",
            "Olga Veksler",
            "Yuri Boykov"
        ],
        "abstract": "Binary energy optimization is a popular approach for segmenting a color image into foreground/background regions. To model the appearance of the regions, color, a relatively high dimensional feature, should be handled effectively. A full color histogram is usually too sparse to be reliable. One approach is to explicitly reduce dimensionality by clustering or quantizing the color space. Another popular approach is to fit GMMs for soft implicit clustering of the color space. These approaches work well when the foreground/background are sufficiently distinct. In cases of more subtle difference in appearance, both approaches may reduce or even eliminate foreground/background distinction. This happens because either color clustering is performed completely independently from the segmentation process, as a preprocessing step (in clustering), or independently for the foreground and independently for the background (in GMM). We propose to make clustering an integral part of segmentation, by including a new clustering term in the energy function. Our energy function with a clustering term favours clusterings that make foreground/background appearance more distinct. Thus our energy function jointly optimizes over color clustering, foreground/background models, and segmentation. Exact optimization is not feasible, therefore we develop an approximate algorithm. We show the advantage of including the color clustering term into the energy function on camouflage images, as well as standard segmentation datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410547",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2001",
                "id": 160
            },
            {
                "year": "2013",
                "id": 220
            },
            {
                "year": "2009",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Image segmentation",
                "Histograms",
                "Computational modeling",
                "Optimization",
                "Clustering algorithms",
                "Standards"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image colour analysis",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color clustering",
                "binary energy optimization",
                "color image segmentation",
                "full color histogram",
                "approximate algorithm"
            ]
        },
        "id": 181,
        "cited_by": []
    },
    {
        "title": "BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation",
        "authors": [
            "Jifeng Dai",
            "Kaiming He",
            "Jian Sun"
        ],
        "abstract": "Recent leading approaches to semantic segmentation rely on deep convolutional networks trained with human-annotated, pixel-level segmentation masks. Such pixel-accurate supervision demands expensive labeling effort and limits the performance of deep networks that usually benefit from more training data. In this paper, we propose a method that achieves competitive accuracy but only requires easily obtained bounding box annotations. The basic idea is to iterate between automatically generating region proposals and training convolutional networks. These two steps gradually recover segmentation masks for improving the networks, and vise versa. Our method, called \"BoxSup\", produces competitive results (e.g., 62.0% mAP for validation) supervised by boxes only, on par with strong baselines (e.g., 63.8% mAP) fully supervised by masks under the same setting. By leveraging a large amount of bounding boxes, BoxSup further yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT [26].",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410548",
        "reference_list": [
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2013",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 127,
            "other": 53,
            "total": 180
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Image segmentation",
                "Semantics",
                "Proposals",
                "Labeling",
                "Linear programming",
                "Erbium"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "BoxSup",
                "bounding boxes",
                "convolutional networks",
                "semantic segmentation",
                "human-annotated mask",
                "pixel-level segmentation mask",
                "pixel-accurate supervision",
                "bounding box annotations",
                "convolutional network training",
                "PASCAL VOC 2012",
                "PASCAL-CONTEXT"
            ]
        },
        "id": 182,
        "cited_by": [
            {
                "year": "2017",
                "id": 81
            },
            {
                "year": "2017",
                "id": 277
            },
            {
                "year": "2017",
                "id": 286
            },
            {
                "year": "2017",
                "id": 519
            },
            {
                "year": "2017",
                "id": 524
            },
            {
                "year": "2017",
                "id": 546
            },
            {
                "year": "2017",
                "id": 596
            },
            {
                "year": "2015",
                "id": 154
            },
            {
                "year": "2015",
                "id": 169
            },
            {
                "year": "2015",
                "id": 170
            }
        ]
    },
    {
        "title": "Detection and Segmentation of 2D Curved Reflection Symmetric Structures",
        "authors": [
            "Ching L. Teo",
            "Cornelia Ferm\u00fcller",
            "Yiannis Aloimonos"
        ],
        "abstract": "Symmetry, as one of the key components of Gestalt theory, provides an important mid-level cue that serves as input to higher visual processes such as segmentation. In this work, we propose a complete approach that links the detection of curved reflection symmetries to produce symmetry-constrained segments of structures/regions in real images with clutter. For curved reflection symmetry detection, we leverage on patch-based symmetric features to train a Structured Random Forest classifier that detects multiscaled curved symmetries in 2D images. Next, using these curved symmetries, we modulate a novel symmetry-constrained foreground-background segmentation by their symmetry scores so that we enforce global symmetrical consistency in the final segmentation. This is achieved by imposing a pairwise symmetry prior that encourages symmetric pixels to have the same labels over a MRF-based representation of the input image edges, and the final segmentation is obtained via graph-cuts. Experimental results over four publicly available datasets containing annotated symmetric structures: 1) SYMMAX-300 [38], 2) BSD-Parts, 3) Weizmann Horse (both from [18]) and 4) NY-roads [35] demonstrate the approach's applicability to different environments with state-of-the-art performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410549",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2013",
                "id": 218
            },
            {
                "year": "2009",
                "id": 278
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Feature extraction",
                "Clutter",
                "Image color analysis",
                "Computer vision",
                "Visualization",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "2D curved reflection symmetric structures",
                "Gestalt theory",
                "visual processes",
                "real images",
                "patch-based symmetric features",
                "structured random forest classifier",
                "multiscaled curved symmetries",
                "2D images",
                "symmetry-constrained foreground-background segmentation",
                "global symmetrical consistency",
                "pairwise symmetry",
                "MRF-based representation",
                "graph-cuts",
                "annotated symmetric structures",
                "SYMMAX-300",
                "BSD-Parts",
                "Weizmann Horse",
                "NY-roads"
            ]
        },
        "id": 183,
        "cited_by": [
            {
                "year": "2017",
                "id": 82
            },
            {
                "year": "2017",
                "id": 285
            }
        ]
    },
    {
        "title": "Unsupervised Tube Extraction Using Transductive Learning and Dense Trajectories",
        "authors": [
            "Mihai Marian Puscas",
            "Enver Sangineto",
            "Dubravko Culibrk",
            "Nicu Sebe"
        ],
        "abstract": "We address the problem of automatic extraction of foreground objects from videos. The goal is to provide a method for unsupervised collection of samples which can be further used for object detection training without any human intervention. We use the well known Selective Search approach to produce an initial still-image based segmentation of the video frames. This initial set of proposals is pruned and temporally extended using optical flow and transductive learning. Specifically, we propose to use Dense Trajectories in order to robustly match and track candidate boxes over different frames. The obtained box tracks are used to collect samples for unsupervised training of track-specific detectors. Finally, the detectors are run on the videos to extract the final tubes. The combination of appearance-based static \"objectness\" (Selective Search), motion information (Dense Trajectories) and transductive learning (detectors are forced to \"overfit\" on the unsupervised data used for training) makes the proposed approach extremely robust. We outperform state-of-the-art systems by a large margin on common benchmarks used for tube proposal evaluation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410550",
        "reference_list": [
            {
                "year": "2011",
                "id": 253
            },
            {
                "year": "2013",
                "id": 221
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Electron tubes",
                "Videos",
                "Trajectory",
                "Detectors",
                "Optical imaging",
                "Training",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "object detection",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised tube extraction",
                "transductive learning",
                "dense trajectory",
                "object detection training",
                "still-image based segmentation",
                "video frame",
                "optical flow",
                "track-specific detector",
                "unsupervised data"
            ]
        },
        "id": 184,
        "cited_by": [
            {
                "year": "2017",
                "id": 152
            },
            {
                "year": "2017",
                "id": 462
            },
            {
                "year": "2017",
                "id": 609
            }
        ]
    },
    {
        "title": "Compositional Hierarchical Representation of Shape Manifolds for Classification of Non-manifold Shapes",
        "authors": [
            "Mete Ozay",
            "Umit Rusen Aktas",
            "Jeremy L. Wyatt",
            "Ale Leonardis"
        ],
        "abstract": "We address the problem of statistical learning of shape models which are invariant to translation, rotation and scale in compositional hierarchies when data spaces of measurements and shape spaces are not topological manifolds. In practice, this problem is observed while modeling shapes having multiple disconnected components, e.g. partially occluded shapes in cluttered scenes. We resolve the aforementioned problem by first reformulating the relationship between data and shape spaces considering the interaction between Receptive Fields (RFs) and Shape Manifolds (SMs) in a compositional hierarchical shape vocabulary. Then, we suggest a method to model the topological structure of the SMs for statistical learning of the geometric transformations of the shapes that are defined by group actions on the SMs. For this purpose, we design a disjoint union topology using an indexing mechanism for the formation of shape models on SMs in the vocabulary, recursively. We represent the topological relationship between shape components using graphs, which are aggregated to construct a hierarchical graph structure for the shape vocabulary. To this end, we introduce a framework to implement the indexing mechanisms for the employment of the vocabulary for structural shape classification. The proposed approach is used to construct invariant shape representations. Results on benchmark shape classification outperform state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410551",
        "reference_list": [
            {
                "year": "2013",
                "id": 155
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Manifolds",
                "Shape measurement",
                "Computational modeling",
                "Mathematical model",
                "Radio frequency",
                "Statistical learning"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "graph theory",
                "image classification",
                "image representation",
                "indexing",
                "learning (artificial intelligence)",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "compositional hierarchical representation",
                "nonmanifold shape classification",
                "statistical learning",
                "shape model",
                "receptive field",
                "RF",
                "shape manifold",
                "SM",
                "geometric transformation",
                "topological relationship",
                "graph theory",
                "indexing mechanism",
                "structural shape classification",
                "shape representation"
            ]
        },
        "id": 185,
        "cited_by": []
    },
    {
        "title": "Shell PCA: Statistical Shape Modelling in Shell Space",
        "authors": [
            "Chao Zhang",
            "Behrend Heeren",
            "Martin Rumpf",
            "William A. P. Smith"
        ],
        "abstract": "In this paper we describe how to perform Principal Components Analysis in \"shell space\". Thin shells are a physical model for surfaces with non-zero thickness whose deformation dissipates elastic energy. Thin shells, or their discrete counterparts, can be considered to reside in a shell space in which the notion of distance is given by the elastic energy required to deform one shape into another. It is in this setting that we show how to perform statistical analysis of a set of shapes (meshes in dense correspondence), providing a hybrid between physical and statistical shape modelling. The resulting models are better able to capture non-linear deformations, for example resulting from articulated motion, even when training data is very sparse compared to the dimensionality of the observation space.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410552",
        "reference_list": [
            {
                "year": "2007",
                "id": 372
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Manifolds",
                "Computational modeling",
                "Principal component analysis",
                "Data models",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "modelling",
                "principal component analysis",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "principal component analysis",
                "shell PCA",
                "statistical shape modelling",
                "elastic energy"
            ]
        },
        "id": 186,
        "cited_by": []
    },
    {
        "title": "Learning to Combine Mid-Level Cues for Object Proposal Generation",
        "authors": [
            "Tom Lee",
            "Sanja Fidler",
            "Sven Dickinson"
        ],
        "abstract": "In recent years, region proposals have replaced sliding windows in support of object recognition, offering more discriminating shape and appearance information through improved localization. One powerful approach for generating region proposals is based on minimizing parametric energy functions with parametric maxflow. In this paper, we introduce Parametric Min-Loss (PML), a novel structured learning framework for parametric energy functions. While PML is generally applicable to different domains, we use it in the context of region proposals to learn to combine a set of mid-level grouping cues to yield a small set of object region proposals with high recall. Our learning framework accounts for multiple diverse outputs, and is complemented by diversification seeds based on image location and color. This approach casts perceptual grouping and cue combination in a novel structured learning framework which yields baseline improvements on VOC 2012 and COCO 2014.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410553",
        "reference_list": [
            {
                "year": "2007",
                "id": 71
            },
            {
                "year": "2013",
                "id": 218
            },
            {
                "year": "2013",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Detectors",
                "Adaptation models",
                "Shape",
                "Graphical models",
                "Image color analysis",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "combine midlevel cues",
                "object proposal generation",
                "object recognition",
                "parametric energy functions",
                "parametric maxflow",
                "parametric min-loss",
                "PML",
                "object region proposals",
                "learning framework"
            ]
        },
        "id": 187,
        "cited_by": []
    },
    {
        "title": "Enhancing Road Maps by Parsing Aerial Images Around the World",
        "authors": [
            "Gell\u00e9rt M\u00e1ttyus",
            "Shenlong Wang",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "In recent years, contextual models that exploit maps have been shown to be very effective for many recognition and localization tasks. In this paper we propose to exploit aerial images in order to enhance freely available world maps. Towards this goal, we make use of OpenStreetMap and formulate the problem as the one of inference in a Markov random field parameterized in terms of the location of the road-segment centerlines as well as their width. This parameterization enables very efficient inference and returns only topologically correct roads. In particular, we can segment all OSM roads in the whole world in a single day using a small cluster of 10 computers. Importantly, our approach generalizes very well, it can be trained using only 1.5 km 2 aerial imagery and produce very accurate results in any location across the globe. We demonstrate the effectiveness of our approach outperforming the state-of-the-art in two new benchmarks that we collect. We then show how our enhanced maps are beneficial for semantic segmentation of ground images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410554",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2009",
                "id": 237
            },
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2009",
                "id": 32
            },
            {
                "year": "2013",
                "id": 94
            },
            {
                "year": "2013",
                "id": 44
            },
            {
                "year": "2005",
                "id": 235
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 4,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Roads",
                "Image segmentation",
                "Manuals",
                "Topology",
                "Robustness",
                "Computer vision",
                "Markov processes"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "Markov processes",
                "road traffic",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "enhancing road maps",
                "parsing aerial images",
                "OpenStreetMap",
                "Markov random field",
                "road segment centerlines",
                "aerial imagery",
                "semantic segmentation",
                "ground images"
            ]
        },
        "id": 188,
        "cited_by": [
            {
                "year": "2017",
                "id": 317
            },
            {
                "year": "2017",
                "id": 362
            }
        ]
    },
    {
        "title": "Probabilistic Appearance Models for Segmentation and Classification",
        "authors": [
            "Julia Kr\u00fcger",
            "Jan Ehrhardt",
            "Heinz Handels"
        ],
        "abstract": "Statistical shape and appearance models are often based on the accurate identification of one-to-one correspondences in a training data set. At the same time, the determination of these corresponding landmarks is the most challenging part of such methods. Hufnagel etal developed an alternative method using correspondence probabilities for a statistical shape model. We propose the use of probabilistic correspondences for statistical appearance models by incorporating appearance information into the framework. A point-based representation is employed representing the image by a set of vectors assembling position and appearances. Using probabilistic correspondences between these multi-dimensional feature vectors eliminates the need for extensive preprocessing to find corresponding landmarks and reduces the dependence of the generated model on the landmark positions. Then, a maximum a-posteriori approach is used to derive a single global optimization criterion with respect to model parameters and observation dependent parameters, that directly affects shape and appearance information of the considered structures. Model generation and fitting can be expressed by optimizing the same criterion. The developed framework describes the modeling process in a concise and flexible mathematical way and allows for additional constraints as topological regularity in the modeling process. Furthermore, it eliminates the demand for costly correspondence determination. We apply the model for segmentation and landmark identification in hand X-ray images, where segmentation information is modeled as further features in the vectorial image representation. The results demonstrate the feasibility of the model to reconstruct contours and landmarks for unseen test images. Furthermore, we apply the model for tissue classification, where a model is generated for healthy brain tissue using 2D MRI slices. Applying the model to images of stroke patients the probabilistic correspondences are used to...",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410555",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 3,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Shape",
                "Probabilistic logic",
                "Brain models",
                "Computational modeling",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "biomedical MRI",
                "feature extraction",
                "image classification",
                "image representation",
                "image segmentation",
                "maximum likelihood estimation",
                "medical image processing",
                "probability",
                "shape recognition",
                "statistical analysis",
                "topology"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic appearance models",
                "statistical shape models",
                "landmark determination",
                "probabilistic correspondences",
                "statistical appearance models",
                "point-based representation",
                "multidimensional feature vectors",
                "landmark positions",
                "maximum a-posteriori approach",
                "global optimization criterion",
                "model generation",
                "topological regularity",
                "X-ray images",
                "vectorial image representation",
                "tissue classification",
                "healthy brain tissue",
                "2D MRI slices"
            ]
        },
        "id": 189,
        "cited_by": []
    },
    {
        "title": "A Randomized Ensemble Approach to Industrial CT Segmentation",
        "authors": [
            "Hyojin Kim",
            "Jayaraman J. Thiagarajan",
            "Peer-Timo Bremer"
        ],
        "abstract": "Tuning the models and parameters of common segmentation approaches is challenging especially in the presence of noise and artifacts. Ensemble-based techniques attempt to compensate by randomly varying models and/or parameters to create a diverse set of hypotheses, which are subsequently ranked to arrive at the best solution. However, these methods have been restricted to cases where the underlying models are well-established, e.g. natural images. In practice, it is difficult to determine a suitable base-model and the amount of randomization required. Furthermore, for multi-object scenes no single hypothesis may perform well for all objects, reducing the overall quality of the results. This paper presents a new ensemble-based segmentation framework for industrial CT images demonstrating that comparatively simple models and randomization strategies can significantly improve the result over existing techniques. Furthermore, we introduce a per-object based ranking, followed by a consensus inference that can outperform even the best case scenario of existing hypothesis ranking approaches. We demonstrate the effectiveness of our approach using a set of noise and artifact rich CT images from baggage security and show that it significantly outperforms existing solutions in this area.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410556",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2011",
                "id": 24
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Computed tomography",
                "Semantics",
                "Object recognition",
                "Security",
                "Training data",
                "Metals"
            ],
            "INSPEC: Controlled Indexing": [
                "computerised tomography",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "baggage security",
                "consensus inference",
                "per-object based ranking",
                "industrial CT images",
                "ensemble-based segmentation framework",
                "base-model",
                "industrial CT segmentation",
                "randomized ensemble approach"
            ]
        },
        "id": 190,
        "cited_by": []
    },
    {
        "title": "Semi-Supervised Normalized Cuts for Image Segmentation",
        "authors": [
            "Selene E. Chew",
            "Nathan D. Cahill"
        ],
        "abstract": "Since its introduction as a powerful graph-based method for image segmentation, the Normalized Cuts (NCuts) algorithm has been generalized to incorporate expert knowledge about how certain pixels or regions should be grouped, or how the resulting segmentation should be biased to be correlated with priors. Previous approaches incorporate hard must-link constraints on how certain pixels should be grouped as well as hard cannot-link constraints on how other pixels should be separated into different groups. In this paper, we reformulate NCuts to allow both sets of constraints to be handled in a soft manner, enabling the user to tune the degree to which the constraints are satisfied. An approximate spectral solution to the reformulated problem exists without requiring explicit construction of a large, dense matrix, hence, computation time is comparable to that of unconstrained NCuts. Using synthetic data and real imagery, we show that soft handling of constraints yields better results than unconstrained NCuts and enables more robust clustering and segmentation than is possible when the constraints are strictly enforced.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410557",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Eigenvalues and eigenfunctions",
                "Partitioning algorithms",
                "Clustering algorithms",
                "Minimization",
                "Linear programming",
                "Chlorine"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "constraint handling",
                "expert systems",
                "graph theory",
                "image segmentation",
                "matrix algebra",
                "pattern clustering",
                "realistic images"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semi-supervised normalized cut",
                "image segmentation",
                "graph-based method",
                "NCuts algorithm",
                "expert knowledge",
                "must-link constraint",
                "approximate spectral solution",
                "dense matrix",
                "computation time",
                "unconstrained NCut",
                "synthetic data",
                "real imagery",
                "constraint handling",
                "robust clustering"
            ]
        },
        "id": 191,
        "cited_by": []
    },
    {
        "title": "StereoSnakes: Contour Based Consistent Object Extraction for Stereo Images",
        "authors": [
            "Ran Ju",
            "Tongwei Ren",
            "Gangshan Wu"
        ],
        "abstract": "Consistent object extraction plays an essential role for stereo image editing with the population of stereoscopic 3D media. Most previous methods perform segmentation on entire images for both views using dense stereo correspondence constraints. We find that for such kind of methods the computation is highly redundant since the two views are near-duplicate. Besides, the consistency may be violated due to the imperfectness of current stereo matching algorithms. In this paper, we propose a contour based method which searches for consistent object contours instead of regions. It integrates both stereo correspondence and object boundary constraints into an energy minimization framework. The proposed method has several advantages compared to previous works. First, the searching space is restricted in object boundaries thus the efficiency significantly improved. Second, the discriminative power of object contours results in a more consistent segmentation. Furthermore, the proposed method can effortlessly extend existing single-image segmentation methods to work in stereo scenarios. The experiment on the Adobe benchmark shows superior extraction accuracy and significant improvement of efficiency of our method to state-of-the-art. We also demonstrate in a few applications how our method can be used as a basic tool for stereo image editing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410558",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2011",
                "id": 145
            },
            {
                "year": "2009",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 5,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image color analysis",
                "Stereo image processing",
                "Three-dimensional displays",
                "Media",
                "Computational modeling",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image segmentation",
                "search problems",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "StereoSnakes",
                "contour based consistent object extraction",
                "stereo image editing",
                "stereoscopic 3D media",
                "dense stereo correspondence constraint",
                "stereo matching algorithm",
                "contour based method",
                "object contour",
                "object boundary constraint",
                "energy minimization framework",
                "searching space",
                "discriminative power",
                "single-image segmentation method",
                "stereo scenario",
                "Adobe benchmark",
                "extraction accuracy"
            ]
        },
        "id": 192,
        "cited_by": []
    },
    {
        "title": "Semantic Segmentation of RGBD Images with Mutex Constraints",
        "authors": [
            "Zhuo Deng",
            "Sinisa Todorovic",
            "Longin Jan Latecki"
        ],
        "abstract": "In this paper, we address the problem of semantic scene segmentation of RGB-D images of indoor scenes. We propose a novel image region labeling method which augments CRF formulation with hard mutual exclusion (mutex) constraints. This way our approach can make use of rich and accurate 3D geometric structure coming from Kinect in a principled manner. The final labeling result must satisfy all mutex constraints, which allows us to eliminate configurations that violate common sense physics laws like placing a floor above a night stand. Three classes of mutex constraints are proposed: global object co-occurrence constraint, relative height relationship constraint, and local support relationship constraint. We evaluate our approach on the NYU-Depth V2 dataset, which consists of 1449 cluttered indoor scenes, and also test generalization of our model trained on NYU-Depth V2 dataset directly on a recent SUN3D dataset without any new training. The experimental results show that we significantly outperform the state-of-the-art methods in scene labeling on both datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410559",
        "reference_list": [
            {
                "year": "2013",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 14,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Image segmentation",
                "Three-dimensional displays",
                "Labeling",
                "Visualization",
                "Feature extraction",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image segmentation",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic RGB-D image segmentation",
                "Mutex constraints",
                "semantic scene segmentation",
                "image region labeling method",
                "CRF formulation",
                "3D geometric structure",
                "Kinect",
                "mutex constraints",
                "global object cooccurrence constraint",
                "local support relationship constraint",
                "NYU-Depth V2 dataset",
                "cluttered indoor scenes",
                "test generalization",
                "SUN3D dataset"
            ]
        },
        "id": 193,
        "cited_by": [
            {
                "year": "2017",
                "id": 137
            },
            {
                "year": "2017",
                "id": 523
            }
        ]
    },
    {
        "title": "Weakly-and Semi-Supervised Learning of a Deep Convolutional Network for Semantic Image Segmentation",
        "authors": [
            "George Papandreou",
            "Liang-Chieh Chen",
            "Kevin P. Murphy",
            "Alan L. Yuille"
        ],
        "abstract": "Deep convolutional neural networks (DCNNs) trained on a large number of images with strong pixel-level annotations have recently significantly pushed the state-of-art in semantic image segmentation. We study the more challenging problem of learning DCNNs for semantic image segmentation from either (1) weakly annotated training data such as bounding boxes or image-level labels or (2) a combination of few strongly labeled and many weakly labeled images, sourced from one or multiple datasets. We develop Expectation-Maximization (EM) methods for semantic image segmentation model training under these weakly supervised and semi-supervised settings. Extensive experimental evaluation shows that the proposed techniques can learn models delivering competitive results on the challenging PASCAL VOC 2012 image segmentation benchmark, while requiring significantly less annotation effort. We share source code implementing the proposed system at https://bitbucket.org/deeplab/deeplab-public.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410560",
        "reference_list": [
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2011",
                "id": 228
            },
            {
                "year": "2009",
                "id": 35
            },
            {
                "year": "2013",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 74,
            "other": 77,
            "total": 151
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Training",
                "Semantics",
                "Benchmark testing",
                "Training data",
                "Convolutional codes",
                "Google"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "feedforward neural nets",
                "image segmentation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weakly-supervised learning",
                "semisupervised learning",
                "deep convolutional neural network",
                "semantic image segmentation model training",
                "DCNN",
                "pixel-level annotation",
                "weakly annotated training data",
                "strongly labeled image",
                "weakly labeled image",
                "expectation-maximization method",
                "EM method",
                "PASCAL VOC 2012 image segmentation benchmark"
            ]
        },
        "id": 194,
        "cited_by": [
            {
                "year": "2017",
                "id": 156
            },
            {
                "year": "2017",
                "id": 210
            },
            {
                "year": "2017",
                "id": 213
            },
            {
                "year": "2017",
                "id": 222
            },
            {
                "year": "2017",
                "id": 372
            },
            {
                "year": "2017",
                "id": 395
            },
            {
                "year": "2017",
                "id": 437
            },
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2017",
                "id": 536
            },
            {
                "year": "2017",
                "id": 551
            }
        ]
    },
    {
        "title": "Efficient Decomposition of Image and Mesh Graphs by Lifted Multicuts",
        "authors": [
            "M. Keuper",
            "E. Levinkov",
            "N. Bonneel",
            "G. Lavou\u00e9",
            "T. Brox",
            "B. Andres"
        ],
        "abstract": "Formulations of the Image Decomposition Problem [18] as a Multicut Problem (MP) w.r.t. a superpixel graph have received considerable attention. In contrast, instances of the MP w.r.t. a pixel grid graph have received little attention, firstly, because the MP is NP-hard and instances w.r.t. a pixel grid graph are hard to solve in practice, and, secondly, due to the lack of long-range terms in the objective function of the MP. We propose a generalization of the MP with long-range terms (LMP). We design and implement two efficient algorithms (primal feasible heuristics) for the MP and LMP which allow us to study instances of both problems w.r.t. the pixel grid graphs of the images in the BSDS-500 benchmark. The decompositions we obtain do not differ significantly from the state of the art, suggesting that the LMP is a competitive formulation of the Image Decomposition Problem. To demonstrate the generality of the LMP, we apply it also to the Mesh Decomposition Problem posed by the Princeton benchmark [16], obtaining state-of-the-art decompositions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410561",
        "reference_list": [
            {
                "year": "2011",
                "id": 332
            },
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2015",
                "id": 365
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 12,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Linear programming",
                "Image edge detection",
                "Image decomposition",
                "Algorithm design and analysis",
                "Benchmark testing",
                "Bayes methods",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "mesh generation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mesh graphs",
                "lifted multicut problem",
                "image decomposition problem",
                "multicut problem",
                "MP",
                "long range terms",
                "LMP",
                "pixel grid graphs",
                "BSDS-500 benchmark",
                "mesh decomposition problem",
                "image segmentation"
            ]
        },
        "id": 195,
        "cited_by": [
            {
                "year": "2017",
                "id": 445
            },
            {
                "year": "2017",
                "id": 493
            },
            {
                "year": "2015",
                "id": 365
            }
        ]
    },
    {
        "title": "Parsimonious Labeling",
        "authors": [
            "Puneet K. Dokania",
            "M. Pawan Kumar"
        ],
        "abstract": "We propose a new family of discrete energy minimization problems, which we call parsimonious labeling. Our energy function consists of unary potentials and high-order clique potentials. While the unary potentials are arbitrary, the clique potentials are proportional to the diversity of the set of unique labels assigned to the clique. Intuitively, our energy function encourages the labeling to be parsimonious, that is, use as few labels as possible. This in turn allows us to capture useful cues for important computer vision applications such as stereo correspondence and image denoising. Furthermore, we propose an efficient graph-cuts based algorithm for the parsimonious labeling problem that provides strong theoretical guarantees on the quality of the solution. Our algorithm consists of three steps. First, we approximate a given diversity using a mixture of a novel hierarchical Pn Potts model. Second, we use a divide-and-conquer approach for each mixture component, where each subproblem is solved using an efficient alpha-expansion algorithm. This provides us with a small number of putative labelings, one for each mixture component. Third, we choose the best putative labeling in terms of the energy value. Using both synthetic and standard real datasets, we show that our algorithm significantly outperforms other graph-cuts based approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410562",
        "reference_list": [
            {
                "year": "2013",
                "id": 387
            },
            {
                "year": "2011",
                "id": 153
            },
            {
                "year": "2009",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Random variables",
                "Measurement",
                "Approximation algorithms",
                "Computer vision",
                "Robustness",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "divide and conquer methods",
                "graph theory",
                "image denoising",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parsimonious labeling",
                "discrete energy minimization problems",
                "energy function",
                "unary potentials",
                "high-order clique potentials",
                "computer vision applications",
                "stereo correspondence",
                "image denoising",
                "graph-cuts based algorithm",
                "Potts model",
                "divide-and-conquer approach"
            ]
        },
        "id": 196,
        "cited_by": []
    },
    {
        "title": "Volumetric Bias in Segmentation and Reconstruction: Secrets and Solutions",
        "authors": [
            "Yuri Boykov",
            "Hossam Isack",
            "Carl Olsson",
            "Ismail Ben Ayed"
        ],
        "abstract": "Many standard optimization methods for segmentation and reconstruction compute ML model estimates for appearance or geometry of segments, e.g. Zhu-Yuille [23], Torr [20], Chan-Vese [6], GrabCut [18], Delong et al. [8]. We observe that the standard likelihood term in these formu-lations corresponds to a generalized probabilistic K-means energy. In learning it is well known that this energy has a strong bias to clusters of equal size [11], which we express as a penalty for KL divergence from a uniform distribution of cardinalities. However, this volumetric bias has been mostly ignored in computer vision. We demonstrate signif- icant artifacts in standard segmentation and reconstruction methods due to this bias. Moreover, we propose binary and multi-label optimization techniques that either (a) remove this bias or (b) replace it by a KL divergence term for any given target volume distribution. Our general ideas apply to continuous or discrete energy formulations in segmenta- tion, stereo, and other reconstruction problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410563",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2003",
                "id": 3
            },
            {
                "year": "2009",
                "id": 36
            },
            {
                "year": "2013",
                "id": 290
            },
            {
                "year": "2009",
                "id": 96
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Standards",
                "Entropy",
                "Optimization methods",
                "Computer vision",
                "Probabilistic logic",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "image segmentation",
                "maximum likelihood estimation",
                "optimisation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "volumetric bias",
                "segmentation method",
                "reconstruction method",
                "ML model estimates",
                "standard likelihood term",
                "probabilistic K-means energy",
                "computer vision",
                "binary optimization technique",
                "multilabel optimization technique",
                "KL divergence"
            ]
        },
        "id": 197,
        "cited_by": [
            {
                "year": "2015",
                "id": 173
            }
        ]
    },
    {
        "title": "Entropy Minimization for Convex Relaxation Approaches",
        "authors": [
            "Mohamed Souiai",
            "Martin R. Oswald",
            "Youngwook Kee",
            "Junmo Kim",
            "Marc Pollefeys",
            "Daniel Cremers"
        ],
        "abstract": "Despite their enormous success in solving hard combinatorial problems, convex relaxation approaches often suffer from the fact that the computed solutions are far from binary and that subsequent heuristic binarization may substantially degrade the quality of computed solutions. In this paper, we propose a novel relaxation technique which incorporates the entropy of the objective variable as a measure of relaxation tightness. We show both theoretically and experimentally that augmenting the objective function with an entropy term gives rise to more binary solutions and consequently solutions with a substantially tighter optimality gap. We use difference of convex function (DC) programming as an efficient and provably convergent solver for the arising convex-concave minimization problem. We evaluate this approach on three prominent non-convex computer vision challenges: multi-label inpainting, image segmentation and spatio-temporal multi-view reconstruction. These experiments show that our approach consistently yields better solutions with respect to the original integral optimization problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410564",
        "reference_list": [
            {
                "year": "2013",
                "id": 290
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Convex functions",
                "Image segmentation",
                "Programming",
                "Labeling",
                "Minimization",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "combinatorial mathematics",
                "computer vision",
                "concave programming",
                "convex programming",
                "image restoration",
                "image segmentation",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "entropy minimization",
                "convex relaxation",
                "hard combinatorial problem",
                "heuristic binarization",
                "convex function programming",
                "convex-concave minimization",
                "nonconvex computer vision",
                "multilabel inpainting",
                "image segmentation",
                "spatio-temporal multiview reconstruction"
            ]
        },
        "id": 198,
        "cited_by": []
    },
    {
        "title": "Adaptively Unified Semi-Supervised Dictionary Learning with Active Points",
        "authors": [
            "Xiaobo Wang",
            "Xiaojie Guo",
            "Stan Z. Li"
        ],
        "abstract": "Semi-supervised dictionary learning aims to construct a dictionary by utilizing both labeled and unlabeled data. To enhance the discriminative capability of the learned dictionary, numerous discriminative terms have been proposed by evaluating either the prediction loss or the class separation criterion on the coding vectors of labeled data, but with rare consideration of the power of the coding vectors corresponding to unlabeled data. In this paper, we present a novel semi-supervised dictionary learning method, which uses the informative coding vectors of both labeled and unlabeled data, and adaptively emphasizes the high confidence coding vectors of unlabeled data to enhance the dictionary discriminative capability simultaneously. By doing so, we integrate the discrimination of dictionary, the induction of classifier to new testing data and the transduction of labels to unlabeled data into a unified framework. To solve the proposed problem, an effective iterative algorithm is designed. Experimental results on a series of benchmark databases show that our method outperforms other state-of-the-art dictionary learning methods in most cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410565",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2013",
                "id": 258
            },
            {
                "year": "2011",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Dictionaries",
                "Solids",
                "Learning systems",
                "Data models",
                "Loss measurement",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "adaptive signal processing",
                "encoding",
                "iterative methods",
                "prediction theory",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptively unified semisupervised dictionary learning",
                "active points",
                "discriminative terms",
                "prediction loss",
                "class separation criterion",
                "informative coding vectors",
                "dictionary discriminative capability",
                "dictionary discrimination",
                "iterative algorithm"
            ]
        },
        "id": 199,
        "cited_by": []
    },
    {
        "title": "Constrained Convolutional Neural Networks for Weakly Supervised Segmentation",
        "authors": [
            "Deepak Pathak",
            "Philipp Kr\u00e4henb\u00fchl",
            "Trevor Darrell"
        ],
        "abstract": "We present an approach to learn a dense pixel-wise labeling from image-level tags. Each image-level tag imposes constraints on the output labeling of a Convolutional Neural Network (CNN) classifier. We propose Constrained CNN (CCNN), a method which uses a novel loss function to optimize for any set of linear constraints on the output space (i.e. predicted label distribution) of a CNN. Our loss formulation is easy to optimize and can be incorporated directly into standard stochastic gradient descent optimization. The key idea is to phrase the training objective as a biconvex optimization for linear models, which we then relax to nonlinear deep networks. Extensive experiments demonstrate the generality of our new learning framework. The constrained loss yields state-of-the-art results on weakly supervised semantic image segmentation. We further demonstrate that adding slightly more supervision can greatly improve the performance of the learning algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410566",
        "reference_list": [
            {
                "year": "2011",
                "id": 125
            }
        ],
        "citation": {
            "ieee": 81,
            "other": 37,
            "total": 118
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Image segmentation",
                "Labeling",
                "Neural networks",
                "Standards",
                "Semantics",
                "Convolutional codes"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "learning (artificial intelligence)",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "constrained convolutional neural networks",
                "dense pixel-wise labeling",
                "image-level tags",
                "CNN classifier",
                "loss formulation",
                "stochastic gradient descent optimization",
                "learning framework",
                "weakly supervised semantic image segmentation"
            ]
        },
        "id": 200,
        "cited_by": [
            {
                "year": "2017",
                "id": 41
            },
            {
                "year": "2017",
                "id": 206
            },
            {
                "year": "2017",
                "id": 210
            },
            {
                "year": "2017",
                "id": 213
            },
            {
                "year": "2017",
                "id": 222
            },
            {
                "year": "2017",
                "id": 286
            },
            {
                "year": "2017",
                "id": 371
            },
            {
                "year": "2017",
                "id": 372
            },
            {
                "year": "2017",
                "id": 385
            },
            {
                "year": "2017",
                "id": 385
            }
        ]
    },
    {
        "title": "A Multiscale Variable-Grouping Framework for MRF Energy Minimization",
        "authors": [
            "Omer Meir",
            "Meirav Galun",
            "Stav Yagev",
            "Ronen Basri",
            "Irad Yavneh"
        ],
        "abstract": "We present a multiscale approach for minimizing the energy associated with Markov Random Fields (MRFs) with energy functions that include arbitrary pairwise potentials. The MRF is represented on a hierarchy of successively coarser scales, where the problem on each scale is itself an MRF with suitably defined potentials. These representations are used to construct an efficient multiscale algorithm that seeks a minimal-energy solution to the original problem. The algorithm is iterative and features a bidirectional crosstalk between fine and coarse representations. We use consistency criteria to guarantee that the energy is nonincreasing throughout the iterative process. The algorithm is evaluated on real-world datasets, achieving competitive performance in relatively short run-times.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410567",
        "reference_list": [
            {
                "year": "2005",
                "id": 33
            },
            {
                "year": "2011",
                "id": 211
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Interpolation",
                "Inference algorithms",
                "Labeling",
                "Graphical models",
                "Topology",
                "Crosstalk",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale variable-grouping framework",
                "MRF energy minimization",
                "Markov random field",
                "MRF",
                "energy function",
                "pairwise potentials",
                "multiscale algorithm",
                "minimal-energy solution",
                "fine-coarse representation bidirectional crosstalk",
                "consistency criteria",
                "iterative process"
            ]
        },
        "id": 201,
        "cited_by": []
    },
    {
        "title": "Inferring M-Best Diverse Labelings in a Single One",
        "authors": [
            "Alexander Kirillov",
            "Bogdan Savchynskyy",
            "Dmitrij Schlesinger",
            "Dmitry Vetrov",
            "Carsten Rother"
        ],
        "abstract": "We consider the task of finding M-best diverse solutions in a graphical model. In a previous work by Batra et al. an algorithmic approach for finding such solutions was proposed, and its usefulness was shown in numerous applications. Contrary to previous work we propose a novel formulation of the problem in form of a single energy minimization problem in a specially constructed graphical model. We show that the method of Batra et al. can be considered as a greedy approximate algorithm for our model, whereas we introduce an efficient specialized optimization technique for it, based on alpha-expansion. We evaluate our method on two application scenarios, interactive and semantic image segmentation, with binary and multiple labels. In both cases we achieve considerably better error rates than state-of-the art diversity methods. Furthermore, we empirically discover that in the binary label case we were able to reach global optimality for all test instances.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410568",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Graphical models",
                "Diversity methods",
                "Minimization",
                "Computer vision",
                "Computational modeling",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "greedy algorithms",
                "image segmentation",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "M-best diverse labelings",
                "graphical model",
                "single energy minimization problem",
                "greedy approximate algorithm",
                "specialized optimization technique",
                "alpha-expansion",
                "application scenarios",
                "interactive image segmentation",
                "semantic image segmentation",
                "binary labels",
                "multiple labels",
                "global optimality"
            ]
        },
        "id": 202,
        "cited_by": []
    },
    {
        "title": "Convolutional Sparse Coding for Image Super-Resolution",
        "authors": [
            "Shuhang Gu",
            "Wangmeng Zuo",
            "Qi Xie",
            "Deyu Meng",
            "Xiangchu Feng",
            "Lei Zhang"
        ],
        "abstract": "Most of the previous sparse coding (SC) based super resolution (SR) methods partition the image into overlapped patches, and process each patch separately. These methods, however, ignore the consistency of pixels in overlapped patches, which is a strong constraint for image reconstruction. In this paper, we propose a convolutional sparse coding (CSC) based SR (CSC-SR) method to address the consistency issue. Our CSC-SR involves three groups of parameters to be learned: (i) a set of filters to decompose the low resolution (LR) image into LR sparse feature maps, (ii) a mapping function to predict the high resolution (HR) feature maps from the LR ones, and (iii) a set of filters to reconstruct the HR images from the predicted HR feature maps via simple convolution operations. By working directly on the whole image, the proposed CSC-SR algorithm does not need to divide the image into overlapped patches, and can exploit the image global correlation to produce more robust reconstruction of image local structures. Experimental results clearly validate the advantages of CSC over patch based SC in SR application. Compared with state-of-the-art SR methods, the proposed CSC-SR method achieves highly competitive PSNR results, while demonstrating better edge and texture preservation performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410569",
        "reference_list": [
            {
                "year": "2009",
                "id": 44
            },
            {
                "year": "2013",
                "id": 239
            },
            {
                "year": "2013",
                "id": 69
            },
            {
                "year": "2011",
                "id": 256
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 64,
            "other": 25,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Image coding",
                "Dictionaries",
                "Convolutional codes",
                "Image resolution",
                "Convolution",
                "Image reconstruction",
                "Encoding"
            ],
            "INSPEC: Controlled Indexing": [
                "filtering theory",
                "image coding",
                "image reconstruction",
                "image resolution",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "edge preservation performance",
                "texture preservation performance",
                "image local structure reconstruction",
                "image global correlation",
                "overlapped patches",
                "convolution operations",
                "HR feature maps",
                "high resolution feature maps",
                "mapping function",
                "LR sparse feature maps",
                "low resolution image",
                "CSC",
                "SR methods",
                "image super-resolution",
                "convolutional sparse coding"
            ]
        },
        "id": 203,
        "cited_by": [
            {
                "year": "2017",
                "id": 179
            },
            {
                "year": "2017",
                "id": 556
            }
        ]
    },
    {
        "title": "A Wavefront Marching Method for Solving the Eikonal Equation on Cartesian Grids",
        "authors": [
            "B. Cancela",
            "M. Ortega",
            "M. G. Penedo"
        ],
        "abstract": "This paper presents a new wavefront propagation method for dealing with the classic Eikonal equation. While classic Dijkstra-like graph-based techniques achieve the solution in O(M log M), they do not approximate the unique physically relevant solution very well. Fast Marching Methods (FMM) were created to efficiently solve the continuous problem. The proposed approximation tries to maintain the complexity, in order to make the algorithm useful in a wide range of contexts. The key idea behind our method is the creation of 'mini wave-fronts', which are combined to propagate the solution. Experimental results show the improvement in the accuracy with respect to the state of the art, while the average computational speed is maintained in O(M log M), similar to the FMM techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410570",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Mathematical model",
                "TV",
                "Computer vision",
                "Complexity theory",
                "Trajectory",
                "Computational modeling",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "graph theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "wavefront marching method",
                "Cartesian grids",
                "Eikonal equation",
                "new wavefront propagation method",
                "Dijkstra-like graph-based technique",
                "fast marching method",
                "FMM",
                "computer vision"
            ]
        },
        "id": 204,
        "cited_by": []
    },
    {
        "title": "A Projection Free Method for Generalized Eigenvalue Problem with a Nonsmooth Regularizer",
        "authors": [
            "Seong Jae Hwang",
            "Maxwell D. Collins",
            "Sathya N. Ravi",
            "Vamsi K. Ithapu",
            "Nagesh Adluru",
            "Sterling C. Johnson",
            "Vikas Singh"
        ],
        "abstract": "Eigenvalue problems are ubiquitous in computer vision, covering a very broad spectrum of applications ranging from estimation problems in multi-view geometry to image segmentation. Few other linear algebra problems have a more mature set of numerical routines available and many computer vision libraries leverage such tools extensively. However, the ability to call the underlying solver only as a \"black box\" can often become restrictive. Many 'human in the loop' settings in vision frequently exploit supervision from an expert, to the extent that the user can be considered a subroutine in the overall system. In other cases, there is additional domain knowledge, side or even partial information that one may want to incorporate within the formulation. In general, regularizing a (generalized) eigenvalue problem with such side information remains difficult. Motivated by these needs, this paper presents an optimization scheme to solve generalized eigenvalue problems (GEP) involving a (nonsmooth) regularizer. We start from an alternative formulation of GEP where the feasibility set of the model involves the Stiefel manifold. The core of this paper presents an end to end stochastic optimization scheme for the resultant problem. We show how this general algorithm enables improved statistical analysis of brain imaging data where the regularizer is derived from other 'views' of the disease pathology, involving clinical measurements and other image-derived representations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410571",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Eigenvalues and eigenfunctions",
                "Manifolds",
                "Optimization",
                "Computer vision",
                "Kernel",
                "Shape",
                "Computers"
            ],
            "INSPEC: Controlled Indexing": [
                "diseases",
                "eigenvalues and eigenfunctions",
                "medical image processing",
                "statistical analysis",
                "stochastic programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "projection free method",
                "generalized eigenvalue problem",
                "nonsmooth regularizer",
                "computer vision",
                "multiview geometry",
                "image segmentation",
                "linear algebra problems",
                "human in the loop settings",
                "domain knowledge",
                "partial information",
                "GEP",
                "Stiefel manifold",
                "stochastic optimization",
                "statistical analysis",
                "brain imaging data",
                "disease pathology",
                "clinical measurements",
                "image-derived representations"
            ]
        },
        "id": 205,
        "cited_by": []
    },
    {
        "title": "Optimizing Expected Intersection-Over-Union with Candidate-Constrained CRFs",
        "authors": [
            "Faruk Ahmed",
            "Dany Tarlow",
            "Dhruv Batra"
        ],
        "abstract": "We study the question of how to make loss-aware predictions in image segmentation settings where the evaluation function is the Intersection-over-Union (IoU) measure that is used widely in evaluating image segmentation systems. Currently, there are two dominant approaches: the first approximates the Expected-IoU (EIoU) score as Expected-Intersection-over-Expected-Union (EIoEU), and the second approach is to compute exact EIoU but only over a small set of high-quality candidate solutions. We begin by asking which approach we should favor for two typical image segmentation tasks. Studying this question leads to two new methods that draw ideas from both existing approaches. Our new methods use the EIoEU approximation paired with high quality candidate solutions. Experimentally we show that our new approaches lead to improved performance on both image segmentation tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410572",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 3,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Loss measurement",
                "Semantics",
                "Decision making",
                "Indexes",
                "Bayes methods"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image segmentation",
                "optimisation",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "expected intersection-over-union",
                "EIoU optimization",
                "EIoU approximation",
                "conditional random field",
                "CRF",
                "image segmentation"
            ]
        },
        "id": 206,
        "cited_by": [
            {
                "year": "2017",
                "id": 594
            }
        ]
    },
    {
        "title": "Higher-Order Inference for Multi-class Log-Supermodular Models",
        "authors": [
            "Jian Zhang",
            "Josip Djolonga",
            "Andreas Krause"
        ],
        "abstract": "Higher-order models have been shown to be very useful for a plethora of computer vision tasks. However, existing techniques have focused mainly on MAP inference. In this paper, we present the first efficient approach towards approximate Bayesian marginal inference in a general class of high-order, multi-label attractive models, where previous techniques slow down exponentially with the order (clique size). We formalize this task as performing inference in log-supermodular models under partition constraints, and present an efficient variational inference technique. The resulting optimization problems are convex and yield bounds on the partition function. We also obtain a fully factorized approximation to the posterior, which can be used in lieu of the true complicated distribution. We empirically demonstrate the performance of our approach by comparing it to traditional inference methods on a challenging high-fidelity multi-label image segmentation dataset. We obtain state-of-the-art classification accuracy for MAP inference, and substantially improved ROC curves using the approximate marginals.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410573",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Optimization",
                "Computer vision",
                "Uncertainty",
                "Probabilistic logic",
                "Image segmentation",
                "Inference algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "computer vision",
                "image segmentation",
                "inference mechanisms",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "higher-order inference",
                "multiclass log-supermodular model",
                "computer vision",
                "MAP inference",
                "approximate Bayesian marginal inference",
                "multilabel attractive model",
                "variational inference technique",
                "high-fidelity multilabel image segmentation dataset",
                "ROC curve"
            ]
        },
        "id": 207,
        "cited_by": []
    },
    {
        "title": "Depth-Based Hand Pose Estimation: Data, Methods, and Challenges",
        "authors": [
            "James S. Supancic",
            "Gr\u00e9gory Rogez",
            "Yi Yang",
            "Jamie Shotton",
            "Deva Ramanan"
        ],
        "abstract": "Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410574",
        "reference_list": [
            {
                "year": "2013",
                "id": 402
            },
            {
                "year": "2013",
                "id": 347
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2013",
                "id": 306
            },
            {
                "year": "2013",
                "id": 431
            }
        ],
        "citation": {
            "ieee": 46,
            "other": 25,
            "total": 71
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Cameras",
                "Training data",
                "Data models",
                "Benchmark testing",
                "Clutter"
            ],
            "INSPEC: Controlled Indexing": [
                "image sensors",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth-based hand pose estimation",
                "commodity depth sensors",
                "single depth frame",
                "cluttered scene analysis",
                "nearest-neighbor baseline",
                "underappreciated point",
                "training data"
            ]
        },
        "id": 208,
        "cited_by": []
    },
    {
        "title": "Adaptive Dither Voting for Robust Spatial Verification",
        "authors": [
            "Xiaomeng Wu",
            "Kunio Kashino"
        ],
        "abstract": "Hough voting in a geometric transformation space allows us to realize spatial verification, but remains sensitive to feature detection errors because of the inflexible quantization of single feature correspondences. To handle this problem, we propose a new method, called adaptive dither voting, for robust spatial verification. For each correspondence, instead of hard-mapping it to a single transformation, the method augments its description by using multiple dithered transformations that are deterministically generated by the other correspondences. The method reduces the probability of losing correspondences during transformation quantization, and provides high robustness as regards mismatches by imposing three geometric constraints on the dithering process. We also propose exploiting the non-uniformity of a Hough histogram as the spatial similarity to handle multiple matching surfaces. Extensive experiments conducted on four datasets show the superiority of our method. The method outperforms its state-of-the-art counterparts in both accuracy and scalability, especially when it comes to the retrieval of small, rotated objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410575",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2013",
                "id": 174
            },
            {
                "year": "2011",
                "id": 185
            },
            {
                "year": "2013",
                "id": 212
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Robustness",
                "Feature extraction",
                "Quantization (signal)",
                "Gaussian distribution",
                "Visualization",
                "Scalability"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "Hough transforms",
                "image matching",
                "image retrieval",
                "probability",
                "quantisation (signal)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive dither voting",
                "robust spatial verification",
                "Hough voting",
                "geometric transformation space",
                "feature detection errors",
                "single feature correspondences",
                "multiple dithered transformations",
                "losing correspondence probability reduction",
                "transformation quantization",
                "geometric constraints",
                "Hough histogram nonuniformity",
                "spatial similarity",
                "multiple matching surface handling",
                "small rotated object retrieval"
            ]
        },
        "id": 209,
        "cited_by": []
    },
    {
        "title": "Alternating Co-Quantization for Cross-Modal Hashing",
        "authors": [
            "Go Irie",
            "Hiroyuki Arai",
            "Yukinobu Taniguchi"
        ],
        "abstract": "This paper addresses the problem of unsupervised learning of binary hash codes for efficient cross-modal retrieval. Many unimodal hashing studies have proven that both similarity preservation of data and maintenance of quantization quality are essential for improving retrieval performance with binary hash codes. However, most existing cross-modal hashing methods mainly have focused on the former, and the latter still remains almost untouched. We propose a method to minimize the binary quantization errors, which is tailored to cross-modal hashing. Our approach, named Alternating Co-Quantization (ACQ), alternately seeks binary quantizers for each modality space with the help of connections to other modality data so that they give minimal quantization errors while preserving data similarities. ACQ can be coupled with various existing cross-modal dimension reduction methods such as Canonical Correlation Analysis (CCA) and substantially boosts their retrieval performance in the Hamming space. Extensive experiments demonstrate that ACQ can outperform several state-of-the-art methods, even when it is combined with simple CCA.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410576",
        "reference_list": [
            {
                "year": "2005",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 7,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Quantization (signal)",
                "Correlation",
                "Symmetric matrices",
                "Computer vision",
                "Databases",
                "Encoding",
                "Binary codes"
            ],
            "INSPEC: Controlled Indexing": [
                "file organisation",
                "Hamming codes",
                "information retrieval",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised learning",
                "binary hash code",
                "cross-modal retrieval",
                "unimodal hashing",
                "similarity data preservation",
                "quantization quality maintenance",
                "cross-modal hashing method",
                "binary quantization error",
                "alternating co-quantization",
                "ACQ",
                "cross-modal dimension reduction method",
                "canonical correlation analysis",
                "CCA",
                "hamming space"
            ]
        },
        "id": 210,
        "cited_by": []
    },
    {
        "title": "Learning Deep Representation with Large-Scale Attributes",
        "authors": [
            "Wanli Ouyang",
            "Hongyang Li",
            "Xingyu Zeng",
            "Xiaogang Wang"
        ],
        "abstract": "Learning strong feature representations from large scale supervision has achieved remarkable success in computer vision as the emergence of deep learning techniques. It is driven by big visual data with rich annotations. This paper contributes a large-scale object attribute database that contains rich attribute annotations (over 300 attributes) for ~180k samples and 494 object classes. Based on the ImageNet object detection dataset, it annotates the rotation, viewpoint, object part location, part occlusion, part existence, common attributes, and class-specific attributes. Then we use this dataset to train deep representations and extensively evaluate how these attributes are useful on the general object detection task. In order to make better use of the attribute annotations, a deep learning scheme is proposed by modeling the relationship of attributes and hierarchically clustering them into semantically meaningful mixture types. Experimental results show that the attributes are helpful in learning better features and improving the object detection accuracy by 2.6% in mAP on the ILSVRC 2014 object detection dataset and 2.4% in mAP on PASCAL VOC 2007 object detection dataset. Such improvement is well generalized across datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410577",
        "reference_list": [
            {
                "year": "2011",
                "id": 155
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2013",
                "id": 15
            },
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 3,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Semantics",
                "Machine learning",
                "Databases",
                "Computer vision",
                "Visualization",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "learning (artificial intelligence)",
                "object detection",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep representation",
                "feature representation",
                "large scale supervision",
                "computer vision",
                "deep learning technique",
                "large-scale object attribute database",
                "rich attribute annotation",
                "ImageNet object detection",
                "rotation annotation",
                "viewpoint annotation",
                "object part location",
                "part occlusion",
                "part existence",
                "common attribute annotation",
                "class-specific attribute annotation"
            ]
        },
        "id": 211,
        "cited_by": [
            {
                "year": "2017",
                "id": 204
            },
            {
                "year": "2017",
                "id": 508
            }
        ]
    },
    {
        "title": "Deep Learning Strong Parts for Pedestrian Detection",
        "authors": [
            "Yonglong Tian",
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Recent advances in pedestrian detection are attained by transferring the learned features of Convolutional Neural Network (ConvNet) to pedestrians. This ConvNet is typically pre-trained with massive general object categories (e.g. ImageNet). Although these features are able to handle variations such as poses, viewpoints, and lightings, they may fail when pedestrian images with complex occlusions are present. Occlusion handling is one of the most important problem in pedestrian detection. Unlike previous deep models that directly learned a single detector for pedestrian detection, we propose DeepParts, which consists of extensive part detectors. DeepParts has several appealing properties. First, DeepParts can be trained on weakly labeled data, i.e. only pedestrian bounding boxes without part annotations are provided. Second, DeepParts is able to handle low IoU positive proposals that shift away from ground truth. Third, each part detector in DeepParts is a strong detector that can detect pedestrian by observing only a part of a proposal. Extensive experiments in Caltech dataset demonstrate the effectiveness of DeepParts, which yields a new state-of-the-art miss rate of 11:89%, outperforming the second best method by 10%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410578",
        "reference_list": [
            {
                "year": "2013",
                "id": 187
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2013",
                "id": 15
            }
        ],
        "citation": {
            "ieee": 94,
            "other": 55,
            "total": 149
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Training",
                "Proposals",
                "Feature extraction",
                "Prototypes",
                "Training data",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "learning (artificial intelligence)",
                "neural nets",
                "object detection",
                "pedestrians"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pedestrian detection",
                "convolutional neural network",
                "ConvNet",
                "massive general object categories",
                "ImageNet",
                "pedestrian images",
                "occlusion handling",
                "DeepParts",
                "extensive part detectors",
                "pedestrian bounding boxes",
                "IoU positive proposals"
            ]
        },
        "id": 212,
        "cited_by": [
            {
                "year": "2017",
                "id": 51
            },
            {
                "year": "2017",
                "id": 295
            },
            {
                "year": "2017",
                "id": 367
            },
            {
                "year": "2017",
                "id": 520
            }
        ]
    },
    {
        "title": "Flowing ConvNets for Human Pose Estimation in Videos",
        "authors": [
            "Tomas Pfister",
            "James Charles",
            "Andrew Zisserman"
        ],
        "abstract": "The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps, (ii) spatial fusion layers that learn an implicit spatial model, (iii) optical flow is used to align heatmap predictions from neighbouring frames, and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also [5, 35] in the high precision region).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410579",
        "reference_list": [
            {
                "year": "2011",
                "id": 52
            },
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 80,
            "other": 40,
            "total": 120
        },
        "keywords": {
            "IEEE Keywords": [
                "Heating",
                "Optical imaging",
                "Videos",
                "Training",
                "Adaptive optics",
                "Computer architecture"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "neural nets",
                "pose estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ConvNets",
                "human pose estimation",
                "temporal context",
                "optical flow",
                "spatial fusion layer",
                "heatmap prediction",
                "parametric pooling layer",
                "pooled confidence map",
                "video pose estimation"
            ]
        },
        "id": 213,
        "cited_by": [
            {
                "year": "2017",
                "id": 82
            },
            {
                "year": "2017",
                "id": 106
            },
            {
                "year": "2017",
                "id": 107
            },
            {
                "year": "2017",
                "id": 390
            },
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2017",
                "id": 415
            },
            {
                "year": "2017",
                "id": 511
            }
        ]
    },
    {
        "title": "Top Rank Supervised Binary Coding for Visual Search",
        "authors": [
            "Dongjin Song",
            "Wei Liu",
            "Rongrong Ji",
            "David A. Meyer",
            "John R. Smith"
        ],
        "abstract": "In recent years, binary coding techniques are becoming increasingly popular because of their high efficiency in handling large-scale computer vision applications. It has been demonstrated that supervised binary coding techniques that leverage supervised information can significantly enhance the coding quality, and hence greatly benefit visual search tasks. Typically, a modern binary coding method seeks to learn a group of coding functions which compress data samples into binary codes. However, few methods pursued the coding functions such that the precision at the top of a ranking list according to Hamming distances of the generated binary codes is optimized. In this paper, we propose a novel supervised binary coding approach, namely Top Rank Supervised Binary Coding (Top-RSBC), which explicitly focuses on optimizing the precision of top positions in a Hamming-distance ranking list towards preserving the supervision information. The core idea is to train the disciplined coding functions, by which the mistakes at the top of a Hamming-distance ranking list are penalized more than those at the bottom. To solve such coding functions, we relax the original discrete optimization objective with a continuous surrogate, and derive a stochastic gradient descent to optimize the surrogate objective. To further reduce the training time cost, we also design an online learning algorithm to optimize the surrogate objective more efficiently. Empirical studies based upon three benchmark image datasets demonstrate that the proposed binary coding approach achieves superior image search accuracy over the state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410580",
        "reference_list": [
            {
                "year": "2013",
                "id": 378
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 5,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Binary codes",
                "Hamming distance",
                "Optimization",
                "Image coding",
                "Visualization",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "gradient methods",
                "Hamming codes",
                "image coding",
                "image retrieval",
                "learning (artificial intelligence)",
                "stochastic programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "top rank supervised binary coding",
                "visual search",
                "large-scale computer vision applications",
                "supervised information",
                "coding quality enhancement",
                "coding functions",
                "data sample compression",
                "Top-RSBC",
                "precision optimization",
                "Hamming-distance ranking list",
                "discrete optimization",
                "stochastic gradient descent method",
                "surrogate objective optimization",
                "training time cost reduction",
                "online learning algorithm",
                "image datasets",
                "image search accuracy"
            ]
        },
        "id": 214,
        "cited_by": []
    },
    {
        "title": "BubbLeNet: Foveated Imaging for Visual Discovery",
        "authors": [
            "Kevin Matzen",
            "Noah Snavely"
        ],
        "abstract": "We propose a new method for turning an Internet-scale corpus of categorized images into a small set of human-interpretable discriminative visual elements using powerful tools based on deep learning. A key challenge with deep learning methods is generating human-interpretable models. To address this, we propose a new technique that uses bubble images -- images where most of the content has been obscured -- to identify spatially localized, discriminative content in each image. By modifying the model training procedure to use both the source imagery and these bubble images, we can arrive at final models which retain much of the original classification performance, but are much more amenable to identifying interpretable visual elements. We apply our algorithm to a wide variety of datasets, including two new Internet-scale datasets of people and places, and show applications to visual mining and discovery. Our method is simple, scalable, and produces visual elements that are highly representative compared to prior work.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410581",
        "reference_list": [
            {
                "year": "2013",
                "id": 424
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Training",
                "Neurons",
                "Machine learning",
                "Market research",
                "Training data",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Internet",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "BubbLeNet",
                "foveated imaging",
                "visual discovery",
                "Internet scale corpus",
                "human interpretable discriminative visual elements",
                "deep learning methods",
                "bubble images",
                "identify spatially localized",
                "training procedure",
                "visual elements",
                "Internet scale datasets",
                "visual mining",
                "computer vision"
            ]
        },
        "id": 215,
        "cited_by": []
    },
    {
        "title": "PQTable: Fast Exact Asymmetric Distance Neighbor Search for Product Quantization Using Hash Tables",
        "authors": [
            "Yusuke Matsui",
            "Toshihiko Yamasaki",
            "Kiyoharu Aizawa"
        ],
        "abstract": "We propose the product quantization table (PQTable), a product quantization-based hash table that is fast and requires neither parameter tuning nor training steps. The PQTable produces exactly the same results as a linear PQ search, and is 102 to 105 times faster when tested on the SIFT1B data. In addition, although state-of-the-art performance can be achieved by previous inverted-indexing-based approaches, such methods do require manually designed parameter setting and much training, whereas our method is free from them. Therefore, PQTable offers a practical and useful solution for real-world problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410582",
        "reference_list": [
            {
                "year": "2013",
                "id": 441
            },
            {
                "year": "2013",
                "id": 426
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 6,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Artificial neural networks",
                "Indexing",
                "Tuning",
                "Training",
                "Quantization (signal)",
                "Data structures"
            ],
            "INSPEC: Controlled Indexing": [
                "database indexing",
                "quantisation (signal)",
                "query processing",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "query vector",
                "inverted-indexing-based approaches",
                "linear PQ search",
                "product quantization-based hash table",
                "hash tables",
                "fast exact asymmetric distance neighbor search",
                "PQTable"
            ]
        },
        "id": 216,
        "cited_by": []
    },
    {
        "title": "Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions",
        "authors": [
            "Sven Bambach",
            "Stefan Lee",
            "David J. Crandall",
            "Chen Yu"
        ],
        "abstract": "Hands appear very often in egocentric video, and their appearance and pose give important cues about what people are doing and what they are paying attention to. But existing work in hand detection has made strong assumptions that work well in only simple scenarios, such as with limited interaction with other people or in lab settings. We develop methods to locate and distinguish between hands in egocentric video using strong appearance models with Convolutional Neural Networks, and introduce a simple candidate region generation approach that outperforms existing techniques at a fraction of the computational cost. We show how these high-quality bounding boxes can be used to create accurate pixelwise hand regions, and as an application, we investigate the extent to which hand segmentation alone can distinguish between different activities. We evaluate these techniques on a new dataset of 48 first-person videos of people interacting in realistic environments, with pixel-level ground truth for over 15,000 hand instances.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410583",
        "reference_list": [
            {
                "year": "2013",
                "id": 327
            }
        ],
        "citation": {
            "ieee": 48,
            "other": 18,
            "total": 66
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Skin",
                "Computer vision",
                "Object detection",
                "Proposals",
                "Context",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "neural nets",
                "object detection",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hand detection",
                "activity recognition",
                "complex egocentric interactions",
                "egocentric video",
                "strong appearance models",
                "convolutional neural networks",
                "candidate region generation approach",
                "hand segmentation",
                "pixel-level ground truth"
            ]
        },
        "id": 217,
        "cited_by": [
            {
                "year": "2017",
                "id": 396
            }
        ]
    },
    {
        "title": "Fast and Accurate Head Pose Estimation via Random Projection Forests",
        "authors": [
            "Donghoon Lee",
            "Ming-Hsuan Yang",
            "Songhwai Oh"
        ],
        "abstract": "In this paper, we consider the problem of estimating the gaze direction of a person from a low-resolution image. Under this condition, reliably extracting facial features is very difficult. We propose a novel head pose estimation algorithm based on compressive sensing. Head image patches are mapped to a large feature space using the proposed extensive, yet efficient filter bank. The filter bank is designed to generate sparse responses of color and gradient information, which can be compressed using random projection, and classified by a random forest. Extensive experiments on challenging datasets show that the proposed algorithm performs favorably against the state-of-the-art methods on head pose estimation in low-resolution images degraded by noise, occlusion, and blurring.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410584",
        "reference_list": [
            {
                "year": "2011",
                "id": 118
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Head",
                "Image color analysis",
                "Image coding",
                "Feature extraction",
                "Vegetation",
                "Compressed sensing"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "face recognition",
                "feature extraction",
                "image resolution",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "head pose estimation",
                "random projection forests",
                "gaze direction estimation",
                "low-resolution image",
                "facial features extraction",
                "head pose estimation algorithm",
                "head image patches",
                "filter bank",
                "random projection",
                "random forest"
            ]
        },
        "id": 218,
        "cited_by": []
    },
    {
        "title": "An MRF-Poselets Model for Detecting Highly Articulated Humans",
        "authors": [
            "Duc Thanh Nguyen",
            "Minh-Khoi Tran",
            "Sai-Kit Yeung"
        ],
        "abstract": "Detecting highly articulated objects such as humans is a challenging problem. This paper proposes a novel part-based model built upon poselets, a notion of parts, and Markov Random Field (MRF) for modelling the human body structure under the variation of human poses and viewpoints. The problem of human detection is then formulated as maximum a posteriori (MAP) estimation in the MRF model. Variational mean field method, a robust statistical inference, is adopted to approximate the MAP estimation. The proposed method was evaluated and compared with existing methods on different test sets including H3D and PASCAL VOC 2007-2009. Experimental results have favourbly shown the robustness of the proposed method in comparison to the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410585",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Computational modeling",
                "Feature extraction",
                "Biological system modeling",
                "Robustness",
                "Deformable models",
                "Legged locomotion"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "inference mechanisms",
                "Markov processes",
                "maximum likelihood estimation",
                "object detection",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "MRF-poselets model",
                "highly articulated human detection",
                "part-based model",
                "Markov random field",
                "human body structure modelling",
                "human pose variation",
                "human viewpoint variation",
                "maximum-a-posteriori estimation",
                "MAP estimation",
                "robust statistical inference",
                "H3D",
                "PASCAL VOC 2007-2009"
            ]
        },
        "id": 219,
        "cited_by": []
    },
    {
        "title": "Beyond Tree Structure Models: A New Occlusion Aware Graphical Model for Human Pose Estimation",
        "authors": [
            "Lianrui Fu",
            "Junge Zhang",
            "Kaiqi Huang"
        ],
        "abstract": "Occlusion is a main challenge for human pose estimation, which is largely ignored in popular tree structure models. The tree structure model is simple and convenient for exact inference, but short in modeling the occlusion coherence especially in the case of self-occlusion. We propose an occlusion aware graphical model which is able to model both self-occlusion and occlusion by the other objects simultaneously. The proposed model structure can encodes the interactions between human body parts and objects, and hence enables it to learn occlusion coherence from data discriminatively. We evaluate our model on several public benchmarks for human pose estimation including challenging subsets featuring significant occlusion. The experimental results show that our method obtains comparable accuracy with the state-of-the-arts, and is robust to occlusion for 2D human pose estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410586",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2013",
                "id": 435
            },
            {
                "year": "2013",
                "id": 235
            },
            {
                "year": "2011",
                "id": 91
            },
            {
                "year": "2009",
                "id": 4
            },
            {
                "year": "2013",
                "id": 25
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Graphical models",
                "Cognition",
                "Message passing",
                "Kinetic theory",
                "Image edge detection",
                "Brain modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tree structure models",
                "new occlusion aware graphical model",
                "human pose estimation",
                "occlusion aware graphical model",
                "public benchmarks"
            ]
        },
        "id": 220,
        "cited_by": []
    },
    {
        "title": "Relaxing from Vocabulary: Robust Weakly-Supervised Deep Learning for Vocabulary-Free Image Tagging",
        "authors": [
            "Jianlong Fu",
            "Yue Wu",
            "Tao Mei",
            "Jinqiao Wang",
            "Hanqing Lu",
            "Yong Rui"
        ],
        "abstract": "The development of deep learning has empowered machines with comparable capability of recognizing limited image categories to human beings. However, most existing approaches heavily rely on human-curated training data, which hinders the scalability to large and unlabeled vocabularies in image tagging. In this paper, we propose a weakly-supervised deep learning model which can be trained from the readily available Web images to relax the dependence on human labors and scale up to arbitrary tags (categories). Specifically, based on the assumption that features of true samples in a category tend to be similar and noises tend to be variant, we embed the feature map of the last deep layer into a new affinity representation, and further minimize the discrepancy between the affinity representation and its low-rank approximation. The discrepancy is finally transformed into the objective function to give relevance feedback to back propagation. Experiments show that we can achieve a performance gain of 14.0% in terms of a semantic-based relevance metric in image tagging with 63,043 tags from the WordNet, against the typical deep model trained on the ImageNet 1,000 vocabulary set.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410587",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Machine learning",
                "Noise measurement",
                "Training data",
                "Vocabulary",
                "Training",
                "Robustness",
                "Tagging"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "backpropagation",
                "feature extraction",
                "image representation",
                "image retrieval",
                "relevance feedback"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "vocabulary-free image tagging",
                "image categories",
                "weakly-supervised deep learning model",
                "Web images",
                "human labors",
                "feature map",
                "affinity representation",
                "low-rank approximation",
                "objective function",
                "relevance feedback",
                "back propagation",
                "semantic-based relevance metric",
                "WordNet",
                "ImageNet"
            ]
        },
        "id": 221,
        "cited_by": [
            {
                "year": "2017",
                "id": 547
            }
        ]
    },
    {
        "title": "Visual Phrases for Exemplar Face Detection",
        "authors": [
            "Vijay Kumar",
            "Anoop Namboodiri",
            "C. V. Jawahar"
        ],
        "abstract": "Recently, exemplar based approaches have been successfully applied for face detection in the wild. Contrary to traditional approaches that model face variations from a large and diverse set of training examples, exemplar-based approaches use a collection of discriminatively trained exemplars for detection. In this paradigm, each exemplar casts a vote using retrieval framework and generalized Hough voting, to locate the faces in the target image. The advantage of this approach is that by having a large database that covers all possible variations, faces in challenging conditions can be detected without having to learn explicit models for different variations. Current schemes, however, make an assumption of independence between the visual words, ignoring their relations in the process. They also ignore the spatial consistency of the visual words. Consequently, every exemplar word contributes equally during voting regardless of its location. In this paper, we propose a novel approach that incorporates higher order information in the voting process. We discover visual phrases that contain semantically related visual words and exploit them for detection along with the visual words. For spatial consistency, we estimate the spatial distribution of visual words and phrases from the entire database and then weigh their occurrence in exemplars. This ensures that a visual word or a phrase in an exemplar makes a major contribution only if it occurs at its semantic location, thereby suppressing the noise significantly. We perform extensive experiments on standard FDDB, AFW and G-album datasets and show significant improvement over previous exemplar approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410588",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            },
            {
                "year": "2011",
                "id": 26
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 8,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Face",
                "Databases",
                "Face detection",
                "Feature extraction",
                "Detectors",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "Hough transforms",
                "interference suppression"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "G-album dataset",
                "AFW",
                "standard FDDB",
                "noise suppression",
                "semantic location",
                "spatial distribution",
                "voting process",
                "spatial consistency",
                "visual word",
                "target image",
                "Hough voting",
                "retrieval framework",
                "discriminatively trained exemplar",
                "face variation",
                "exemplar face detection",
                "visual phrase"
            ]
        },
        "id": 222,
        "cited_by": [
            {
                "year": "2017",
                "id": 20
            },
            {
                "year": "2017",
                "id": 334
            }
        ]
    },
    {
        "title": "Spatial Semantic Regularisation for Large Scale Object Detection",
        "authors": [
            "Damian Mrowca",
            "Marcus Rohrbach",
            "Judy Hoffman",
            "Ronghang Hu",
            "Kate Saenko",
            "Trevor Darrell"
        ],
        "abstract": "Large scale object detection with thousands of classes introduces the problem of many contradicting false positive detections, which have to be suppressed. Class-independent non-maximum suppression has traditionally been used for this step, but it does not scale well as the number of classes grows. Traditional non-maximum suppression does not consider label-and instance-level relationships nor does it allow an exploitation of the spatial layout of detection proposals. We propose a new multi-class spatial semantic regularisation method based on affinity propagation clustering, which simultaneously optimises across all categories and all proposed locations in the image, to improve both the localisation and categorisation of selected detection proposals. Constraints are shared across the labels through the semantic WordNet hierarchy. Our approach proves to be especially useful in large scale settings with thousands of classes, where spatial and semantic interactions are very frequent and only weakly supervised detectors can be built due to a lack of bounding box annotations. Detection experiments are conducted on the ImageNet and COCO dataset, and in settings with thousands of detected categories. Our method provides a significant precision improvement by reducing false positives, while simultaneously improving the recall.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410589",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2009",
                "id": 29
            },
            {
                "year": "2007",
                "id": 224
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Proposals",
                "Detectors",
                "Object detection",
                "Visualization",
                "Clustering algorithms",
                "Message passing"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatial semantic regularisation",
                "large scale object detection",
                "false positive detections",
                "class-independent nonmaximum suppression",
                "instance-level relationships",
                "label-level relationships",
                "multiclass spatial semantic regularisation method",
                "affinity propagation clustering",
                "semantic WordNet hierarchy",
                "semantic interactions",
                "supervised detectors",
                "bounding box annotations",
                "ImageNet dataset",
                "COCO dataset"
            ]
        },
        "id": 223,
        "cited_by": [
            {
                "year": "2017",
                "id": 583
            }
        ]
    },
    {
        "title": "Human Pose Estimation in Videos",
        "authors": [
            "Dong Zhang",
            "Mubarak Shah"
        ],
        "abstract": "In this paper, we present a method to estimate a sequence of human poses in unconstrained videos. In contrast to the commonly employed graph optimization framework, which is NP-hard and needs approximate solutions, we formulate this problem into a unified two stage tree-based optimization problem for which an efficient and exact solution exists. Although the proposed method finds an exact solution, it does not sacrifice the ability to model the spatial and temporal constraints between body parts in the video frames, indeed it even models the symmetric parts better than the existing methods. The proposed method is based on two main ideas: 'Abstraction' and 'Association' to enforce the intra-and inter-frame body part constraints respectively without inducing extra computational complexity to the polynomial time solution. Using the idea of 'Abstraction', a new concept of 'abstract body part' is introduced to model not only the tree based body part structure similar to existing methods, but also extra constraints between symmetric parts. Using the idea of 'Association', the optimal tracklets are generated for each abstract body part, in order to enforce the spatiotemporal constraints between body parts in adjacent frames. Finally, a sequence of the best poses is inferred from the abstract body part tracklets through the tree-based optimization. We evaluated the proposed method on three publicly available video based human pose estimation datasets, and obtained dramatically improved performance compared to the state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410590",
        "reference_list": [
            {
                "year": "2005",
                "id": 60
            },
            {
                "year": "2011",
                "id": 334
            },
            {
                "year": "2013",
                "id": 302
            },
            {
                "year": "2013",
                "id": 413
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 5,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Optimization",
                "Computational modeling",
                "Computer vision",
                "Computational complexity",
                "Spatiotemporal phenomena"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image sequences",
                "optimisation",
                "performance evaluation",
                "pose estimation",
                "spatiotemporal phenomena",
                "trees (mathematics)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human pose sequence estimation",
                "unconstrained videos",
                "unified two stage tree-based optimization problem",
                "spatial constraints",
                "temporal constraints",
                "video frames",
                "abstraction",
                "association",
                "intra-frame body part constraints",
                "inter-frame body part constraints",
                "computational complexity",
                "polynomial time solution",
                "abstract body part",
                "tree based body part structure",
                "optimal tracklet generation",
                "spatiotemporal constraints",
                "performance improvement"
            ]
        },
        "id": 224,
        "cited_by": []
    },
    {
        "title": "Contour Box: Rejecting Object Proposals without Explicit Closed Contours",
        "authors": [
            "Cewu Lu",
            "Shu Liu",
            "Jiaya Jia",
            "Chi-Keung Tang"
        ],
        "abstract": "Closed contour is an important objectness indicator. We propose a new measure subject to the completeness and tightness constraints, where the optimized closed contour should be tightly bounded within an object proposal. The closed contour measure is defined using closed path integral, and we solve the optimization problem efficiently in polar coordinate system with a global optimum guaranteed. Extensive experiments show that our method can reject a large number of false proposals, and achieve over 6% improvement in object recall at the challenging overlap threshold 0.8 on the VOC 2007 test dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410591",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2013",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 5,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Optimization",
                "Image edge detection",
                "Dynamic programming",
                "Object detection",
                "Search problems",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object proposal",
                "closed contour measure",
                "closed path integral",
                "optimization problem",
                "polar coordinate system",
                "overlap threshold",
                "contour box",
                "object detection"
            ]
        },
        "id": 225,
        "cited_by": []
    },
    {
        "title": "Registering Images to Untextured Geometry Using Average Shading Gradients",
        "authors": [
            "Tobias Pl\u00f6tz",
            "Stefan Roth"
        ],
        "abstract": "Many existing approaches for image-to-geometry registration assume that either a textured 3D model or a good initial guess of the 3D pose is available to bootstrap the registration process. In this paper we consider the registration of photographs to 3D models even when no texture information is available. This is very challenging as we cannot rely on texture gradients, and even shading gradients are hard to estimate since the lighting conditions are unknown. To that end, we propose average shading gradients, a rendering technique that estimates the average gradient magnitude over all lighting directions under Lambertian shading. We use this gradient representation as the building block of a registration pipeline based on matching sparse features. To cope with inevitable false matches due to the missing texture information and to increase robustness, the pose of the 3D model is estimated in two stages. Coarse pose hypotheses are first obtained from a single correct match each, subsequently refined using SIFT flow, and finally verified. We apply our algorithm to registering images of real-world objects to untextured 3D meshes of limited accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410592",
        "reference_list": [
            {
                "year": "2013",
                "id": 373
            },
            {
                "year": "2007",
                "id": 276
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Lighting",
                "Rendering (computer graphics)",
                "Cameras",
                "Computational modeling",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "geometry",
                "image matching",
                "image registration",
                "image representation",
                "image texture",
                "mesh generation",
                "pose estimation",
                "rendering (computer graphics)",
                "solid modelling",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image registration",
                "untextured geometry",
                "average shading gradients",
                "image-to-geometry registration",
                "textured 3D model",
                "3D pose",
                "registration process bootstrapping",
                "photograph registration",
                "rendering technique",
                "lighting directions",
                "Lambertian shading",
                "gradient representation",
                "registration pipeline",
                "sparse feature matching",
                "pose estimation",
                "SIFT flow",
                "untextured 3D mesh"
            ]
        },
        "id": 226,
        "cited_by": []
    },
    {
        "title": "Robust Nonrigid Registration by Convex Optimization",
        "authors": [
            "Qifeng Chen",
            "Vladlen Koltun"
        ],
        "abstract": "We present an approach to nonrigid registration of 3D surfaces. We cast isometric embedding as MRF optimization and apply efficient global optimization algorithms based on linear programming relaxations. The Markov random field perspective suggests a natural connection with robust statistics and motivates robust forms of the intrinsic distortion functional. Our approach outperforms a large body of prior work by a significant margin, increasing registration precision on real data by a factor of 3.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410593",
        "reference_list": [
            {
                "year": "2013",
                "id": 145
            },
            {
                "year": "2011",
                "id": 271
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 15,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Robustness",
                "Distortion",
                "Markov processes",
                "Shape",
                "Computational modeling",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "convex programming",
                "image registration",
                "linear programming",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust nonrigid registration",
                "convex optimization",
                "3D surfaces",
                "isometric embedding",
                "MRF optimization",
                "global optimization algorithms",
                "linear programming relaxations",
                "Markov random field",
                "intrinsic distortion functional",
                "registration precision"
            ]
        },
        "id": 227,
        "cited_by": [
            {
                "year": "2017",
                "id": 593
            }
        ]
    },
    {
        "title": "Robust and Optimal Sum-of-Squares-Based Point-to-Plane Registration of Image Sets and Structured Scenes",
        "authors": [
            "Danda Pani Paudel",
            "Adlane Habed",
            "C\u00e9dric Demonceaux",
            "Pascal Vasseur"
        ],
        "abstract": "This paper deals with the problem of registering a known structured 3D scene and its metric Structure-from-Motion (SfM) counterpart. The proposed work relies on a prior plane segmentation of the 3D scene and aligns the data obtained from both modalities by solving the point-to-plane assignment problem. An inliers-maximization approach within a Branch-and-Bound (BnB) search scheme is adopted. For the first time in this paper, a Sum-of-Squares optimization theory framework is employed for identifying point-to-plane mismatches (i.e. outliers) with certainty. This allows us to iteratively build potential inliers sets and converge to the solution satisfied by the largest number of point-to-plane assignments. Furthermore, our approach is boosted by new plane visibility conditions which are also introduced in this paper. Using this framework, we solve the registration problem in two cases: (i) a set of putative point-to-plane correspondences (with possibly overwhelmingly many outliers) is given as input and (ii) no initial correspondences are given. In both cases, our approach yields outstanding results in terms of robustness and optimality.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410594",
        "reference_list": [
            {
                "year": "2009",
                "id": 137
            },
            {
                "year": "2013",
                "id": 181
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Sensors",
                "Cameras",
                "Measurement",
                "Image sensors",
                "Symmetric matrices",
                "Iterative closest point algorithm"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image registration",
                "image segmentation",
                "optimisation",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "putative point-to-plane correspondence",
                "plane visibility condition",
                "potential inliers set",
                "point-to-plane mismatch",
                "sum-of-squares optimization theory framework",
                "BnB search scheme",
                "branch-and-bound search scheme",
                "inliers-maximization approach",
                "point-to-plane assignment problem",
                "prior plane segmentation",
                "SfM counterpart",
                "metric structure-from-motion counterpart",
                "structured 3D scene",
                "structured scene",
                "image set",
                "optimal sum-of-squares-based point-to-plane registration",
                "robust sum-of-squares-based point-to-plane registration"
            ]
        },
        "id": 228,
        "cited_by": [
            {
                "year": "2017",
                "id": 0
            }
        ]
    },
    {
        "title": "MeshStereo: A Global Stereo Model with Mesh Alignment Regularization for View Interpolation",
        "authors": [
            "Chi Zhang",
            "Zhiwei Li",
            "Yanhua Cheng",
            "Rui Cai",
            "Hongyang Chao",
            "Yong Rui"
        ],
        "abstract": "We present a novel global stereo model designed for view interpolation. Unlike existing stereo models which only output a disparity map, our model is able to output a 3D triangular mesh, which can be directly used for view interpolation. To this aim, we partition the input stereo images into 2D triangles with shared vertices. Lifting the 2D triangulation to 3D naturally generates a corresponding mesh. A technical difficulty is to properly split vertices to multiple copies when they appear at depth discontinuous boundaries. To deal with this problem, we formulate our objective as a two-layer MRF, with the upper layer modeling the splitting properties of the vertices and the lower layer optimizing a region-based stereo matching. Experiments on the Middlebury and the Herodion datasets demonstrate that our model is able to synthesize visually coherent new view angles with high PSNR, as well as outputting high quality disparity maps which rank at the first place on the new challenging high resolution Middlebury 3.0 benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410595",
        "reference_list": [
            {
                "year": "2013",
                "id": 294
            },
            {
                "year": "2009",
                "id": 241
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 14,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Rendering (computer graphics)",
                "Three-dimensional displays",
                "Interpolation",
                "Solid modeling",
                "Visualization",
                "Computational modeling",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image resolution",
                "interpolation",
                "mesh generation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "MeshStereo",
                "global stereo model",
                "mesh alignment regularization",
                "3D triangular mesh",
                "stereo image",
                "2D triangle",
                "mesh generation",
                "two-layer MRF",
                "region-based stereo matching",
                "Herodion",
                "PSNR",
                "Middlebury 3.0 benchmark"
            ]
        },
        "id": 229,
        "cited_by": [
            {
                "year": "2017",
                "id": 140
            },
            {
                "year": "2017",
                "id": 162
            }
        ]
    },
    {
        "title": "CV-HAZOP: Introducing Test Data Validation for Computer Vision",
        "authors": [
            "Oliver Zendel",
            "Markus Murschitz",
            "Martin Humenberger",
            "Wolfgang Herzner"
        ],
        "abstract": "Test data plays an important role in computer vision (CV) but is plagued by two questions: Which situations should be covered by the test data and have we tested enough to reach a conclusion? In this paper we propose a new solution answering these questions using a standard procedure devised by the safety community to validate complex systems: The Hazard and Operability Analysis (HAZOP). It is designed to systematically search and identify difficult, performance-decreasing situations and aspects. We introduce a generic CV model that creates the basis for the hazard analysis and, for the first time, apply an extensive HAZOP to the CV domain. The result is a publicly available checklist with more than 900 identified individual hazards. This checklist can be used to evaluate existing test datasets by quantifying the amount of covered hazards. We evaluate our approach by first analyzing and annotating the popular stereo vision test datasets Middlebury and KITTI. Second, we compare the performance of six popular stereo matching algorithms at the identified hazards from our checklist with their average performance and show, as expected, a clear negative influence of the hazards. The presented approach is a useful tool to evaluate and improve test datasets and creates a common basis for future dataset designs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410596",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Hazards",
                "Robustness",
                "Benchmark testing",
                "Context",
                "Mathematical model",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching",
                "search problems",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "CV-HAZOP",
                "test data validation",
                "computer vision",
                "safety community",
                "complex systems",
                "hazard and operability analysis",
                "generic CV model",
                "stereo vision test datasets",
                "Middlebury",
                "KITTI",
                "stereo matching algorithms"
            ]
        },
        "id": 230,
        "cited_by": [
            {
                "year": "2017",
                "id": 350
            }
        ]
    },
    {
        "title": "Structure from Motion Using Structure-Less Resection",
        "authors": [
            "Enliang Zheng",
            "Changchang Wu"
        ],
        "abstract": "This paper proposes a new incremental structure from motion (SfM) algorithm based on a novel structure-less camera resection technique. Traditional methods rely on 2D-3D correspondences to compute the pose of candidate cameras using PnP. In this work, we take the collection of already reconstructed cameras as a generalized camera, and determine the absolute pose of a candidate pinhole camera from pure 2D correspondences, which we call it semi-generalized camera pose problem. We present the minimal solvers of the new problem for both calibrated and partially calibrated (unknown focal length) pinhole cameras. By integrating these new algorithms in an incremental SfM system, we go beyond the state-of-art methods with the capability of reconstructing cameras without 2D-3D correspondences. Large-scale real image experiments show that our new SfM system significantly improves the completeness of 3D reconstruction over the standard approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410597",
        "reference_list": [
            {
                "year": "2009",
                "id": 231
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 8,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Three-dimensional displays",
                "Standards",
                "Matrix decomposition",
                "Conferences"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "computer vision",
                "image motion analysis",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure from motion",
                "incremental SfM system",
                "structure-less camera resection technique",
                "camera reconstruction",
                "calibration"
            ]
        },
        "id": 231,
        "cited_by": []
    },
    {
        "title": "Joint Camera Clustering and Surface Segmentation for Large-Scale Multi-view Stereo",
        "authors": [
            "Runze Zhang",
            "Shiwei Li",
            "Tian Fang",
            "Siyu Zhu",
            "Long Quan"
        ],
        "abstract": "In this paper, we propose an optimal decomposition approach to large-scale multi-view stereo from an initial sparse reconstruction. The success of the approach depends on the introduction of surface-segmentation-based camera clustering rather than sparse-point-based camera clustering, which suffers from the problems of non-uniform reconstruction coverage ratio and high redundancy. In details, we introduce three criteria for camera clustering and surface segmentation for reconstruction, and then we formulate these criteria into an energy minimization problem under constraints. To solve this problem, we propose a joint optimization in a hierarchical framework to obtain the final surface segments and corresponding optimal camera clusters. On each level of the hierarchical framework, the camera clustering problem is formulated as a parameter estimation problem of a probability model solved by a General Expectation-Maximization algorithm and the surface segmentation problem is formulated as a Markov Random Field model based on the probability estimated by the previous camera clustering process. The experiments on several Internet datasets and aerial photo datasets demonstrate that the proposed approach method generates more uniform and complete dense reconstruction with less redundancy, resulting in more efficient multi-view stereo algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410598",
        "reference_list": [
            {
                "year": "2007",
                "id": 245
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Clustering algorithms",
                "Surface reconstruction",
                "Three-dimensional displays",
                "Redundancy",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "expectation-maximisation algorithm",
                "image reconstruction",
                "image segmentation",
                "Markov processes",
                "minimisation",
                "parameter estimation",
                "pattern clustering",
                "random processes",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optimal decomposition approach",
                "large-scale multiview stereo",
                "sparse reconstruction",
                "surface-segmentation-based camera clustering",
                "sparse-point-based camera clustering",
                "nonuniform reconstruction coverage ratio",
                "image reconstruction",
                "energy minimization problem",
                "joint optimization",
                "hierarchical framework",
                "optimal camera clusters",
                "parameter estimation",
                "probability model",
                "general expectation-maximization algorithm",
                "Markov random field model",
                "probability estimation",
                "Internet datasets",
                "aerial photo datasets"
            ]
        },
        "id": 232,
        "cited_by": [
            {
                "year": "2017",
                "id": 3
            },
            {
                "year": "2017",
                "id": 249
            }
        ]
    },
    {
        "title": "Higher-Order CRF Structural Segmentation of 3D Reconstructed Surfaces",
        "authors": [
            "Jingbo Liu",
            "Jinglu Wang",
            "Tian Fang",
            "Chiew-Lan Tai",
            "Long Quan"
        ],
        "abstract": "In this paper, we propose a structural segmentation algorithm to partition multi-view stereo reconstructed surfaces of large-scale urban environments into structural segments. Each segment corresponds to a structural component describable by a surface primitive of up to the second order. This segmentation is for use in subsequent urban object modeling, vectorization, and recognition. To overcome the high geometrical and topological noise levels in the 3D reconstructed urban surfaces, we formulate the structural segmentation as a higher-order Conditional Random Field (CRF) labeling problem. It not only incorporates classical lower-order 2D and 3D local cues, but also encodes contextual geometric regularities to disambiguate the noisy local cues. A general higher-order CRF is difficult to solve. We develop a bottom-up progressive approach through a patch-based surface representation, which iteratively evolves from the initial mesh triangles to the final segmentation. Each iteration alternates between performing a prior discovery step, which finds the contextual regularities of the patch-based representation, and an inference step that leverages the regularities as higher-order priors to construct a more stable and regular segmentation. The efficiency and robustness of the proposed method is extensively demonstrated on real reconstruction models, yielding significantly better performance than classical mesh segmentation methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410599",
        "reference_list": [
            {
                "year": "2011",
                "id": 111
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Image reconstruction",
                "Three-dimensional displays",
                "Surface reconstruction",
                "Image segmentation",
                "Face",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image segmentation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "higher order CRF structural segmentation",
                "3D reconstructed surfaces",
                "structural segmentation algorithm",
                "multiview stereo reconstructed surfaces",
                "large-scale urban environments",
                "structural component",
                "topological noise levels",
                "conditional random field",
                "contextual geometric regularities",
                "initial mesh triangles",
                "patch based representation"
            ]
        },
        "id": 233,
        "cited_by": [
            {
                "year": "2017",
                "id": 445
            }
        ]
    },
    {
        "title": "Hyperpoints and Fine Vocabularies for Large-Scale Location Recognition",
        "authors": [
            "Torsten Sattler",
            "Michal Havlena",
            "Filip Radenovic",
            "Konrad Schindler",
            "Marc Pollefeys"
        ],
        "abstract": "Structure-based localization is the task of finding the absolute pose of a given query image w.r.t. a pre-computed 3D model. While this is almost trivial at small scale, special care must be taken as the size of the 3D model grows, because straight-forward descriptor matching becomes ineffective due to the large memory footprint of the model, as well as the strictness of the ratio test in 3D. Recently, several authors have tried to overcome these problems, either by a smart compression of the 3D model or by clever sampling strategies for geometric verification. Here we explore an orthogonal strategy, which uses all the 3D points and standard sampling, but performs feature matching implicitly, by quantization into a fine vocabulary. We show that although this matching is ambiguous and gives rise to 3D hyperpoints when matching each 2D query feature in isolation, a simple voting strategy, which enforces the fact that the selected 3D points shall be co-visible, can reliably find a locally unique 2D-3D point assignment. Experiments on two large-scale datasets demonstrate that our method achieves state-of-the-art performance, while the memory footprint is greatly reduced, since only visual word labels but no 3D point descriptors need to be stored.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410600",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 0,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Vocabulary",
                "Cameras",
                "Visualization",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "data compression",
                "feature extraction",
                "image matching",
                "image retrieval",
                "image sampling",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D hyperpoints",
                "fine vocabularies",
                "large-scale location recognition",
                "structure-based localization",
                "query image",
                "descriptor matching",
                "3D model smart compression",
                "sampling strategies",
                "geometric verification",
                "orthogonal strategy",
                "3D points",
                "standard sampling",
                "2D query feature matching",
                "voting strategy",
                "2D-3D point assignment",
                "visual word labels"
            ]
        },
        "id": 234,
        "cited_by": [
            {
                "year": "2017",
                "id": 65
            },
            {
                "year": "2017",
                "id": 250
            }
        ]
    },
    {
        "title": "Globally Optimal 2D-3D Registration from Points or Lines without Correspondences",
        "authors": [
            "Mark Brown",
            "David Windridge",
            "Jean-Yves Guillemaut"
        ],
        "abstract": "We present a novel approach to 2D-3D registration from points or lines without correspondences. While there exist established solutions in the case where correspondences are known, there are many situations where it is not possible to reliably extract such correspondences across modalities, thus requiring the use of a correspondence-free registration algorithm. Existing correspondence-free methods rely on local search strategies and consequently have no guarantee of finding the optimal solution. In contrast, we present the first globally optimal approach to 2D-3D registration without correspondences, achieved by a Branch-and-Bound algorithm. Furthermore, a deterministic annealing procedure is proposed to speed up the nested branch-and-bound algorithm used. The theoretical and practical advantages this brings are demonstrated on a range of synthetic and real data where it is observed that the proposed approach is significantly more robust to high proportions of outliers compared to existing approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410601",
        "reference_list": [
            {
                "year": "2005",
                "id": 206
            },
            {
                "year": "2013",
                "id": 181
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Upper bound",
                "Feature extraction",
                "Annealing",
                "Robustness",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "deterministic algorithms",
                "image registration",
                "tree searching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "globally optimal 2D-3D registration",
                "correspondence-free registration algorithm",
                "local search strategies",
                "branch-and-bound algorithm",
                "deterministic annealing procedure"
            ]
        },
        "id": 235,
        "cited_by": [
            {
                "year": "2017",
                "id": 0
            }
        ]
    },
    {
        "title": "The HCI Stereo Metrics: Geometry-Aware Performance Analysis of Stereo Algorithms",
        "authors": [
            "Katrin Honauer",
            "Lena Maier-Hein",
            "Daniel Kondermann"
        ],
        "abstract": "Performance characterization of stereo methods is mandatory to decide which algorithm is useful for which application. Prevalent benchmarks mainly use the root mean squared error (RMS) with respect to ground truth disparity maps to quantify algorithm performance. We show that the RMS is of limited expressiveness for algorithm selection and introduce the HCI Stereo Metrics. These metrics assess stereo results by harnessing three semantic cues: depth discontinuities, planar surfaces, and fine geometric structures. For each cue, we extract the relevant set of pixels from existing ground truth. We then apply our evaluation functions to quantify characteristics such as edge fattening and surface smoothness. We demonstrate that our approach supports practitioners in selecting the most suitable algorithm for their application. Using the new Middlebury dataset, we show that rankings based on our metrics reveal specific algorithm strengths and weaknesses which are not quantified by existing metrics. We finally show how stacked bar charts and radar charts visually support multidimensional performance evaluation. An interactive stereo benchmark based on the proposed metrics and visualizations is available at: http://hci.iwr.uni-heidelberg.de/stereometrics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410602",
        "reference_list": [
            {
                "year": "2001",
                "id": 180
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 3,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Benchmark testing",
                "Object detection",
                "Surface reconstruction",
                "Human computer interaction",
                "Algorithm design and analysis",
                "Performance evaluation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "mean square error methods",
                "performance evaluation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "HCI stereo metrics",
                "geometry-aware performance analysis",
                "stereo algorithms",
                "root mean squared error",
                "ground truth disparity maps",
                "semantic cues",
                "depth discontinuities",
                "planar surfaces",
                "fine geometric structures",
                "edge fattening",
                "surface smoothness",
                "Middlebury dataset",
                "stacked bar charts",
                "radar charts",
                "multidimensional performance evaluation"
            ]
        },
        "id": 236,
        "cited_by": []
    },
    {
        "title": "Merging the Unmatchable: Stitching Visually Disconnected SfM Models",
        "authors": [
            "Andrea Cohen",
            "Torsten Sattler",
            "Marc Pollefeys"
        ],
        "abstract": "Recent advances in Structure-from-Motion not only enable the reconstruction of large scale scenes, but are also able to detect ambiguous structures caused by repeating elements that might result in incorrect reconstructions. Yet, it is not always possible to fully reconstruct a scene. The images required to merge different sub-models might be missing or it might be impossible to acquire such images in the first place due to occlusions or the structure of the scene. The problem of aligning multiple reconstructions that do not have visual overlap is impossible to solve in general. An important variant of this problem is the case in which individual sides of a building can be reconstructed but not joined due to the missing visual overlap. In this paper, we present a combinatorial approach for solving this variant by automatically stitching multiple sides of a building together. Our approach exploits symmetries and semantic information to reason about the possible geometric relations between the individual models. We show that our approach is able to reconstruct complete building models where traditional SfM ends up with disconnected building sides.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410603",
        "reference_list": [
            {
                "year": "2009",
                "id": 9
            },
            {
                "year": "2011",
                "id": 143
            },
            {
                "year": "2013",
                "id": 63
            },
            {
                "year": "2013",
                "id": 350
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 5,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Three-dimensional displays",
                "Buildings",
                "Merging",
                "Computational modeling",
                "Solid modeling",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure-from-motion",
                "SfM models",
                "combinatorial approach",
                "feature matching",
                "image reconstruction"
            ]
        },
        "id": 237,
        "cited_by": [
            {
                "year": "2017",
                "id": 486
            }
        ]
    },
    {
        "title": "3D Fragment Reassembly Using Integrated Template Guidance and Fracture-Region Matching",
        "authors": [
            "Kang Zhang",
            "Wuyi Yu",
            "Mary Manhein",
            "Warren Waggenspack",
            "Xin Li"
        ],
        "abstract": "This paper studies matching of fragmented objects to recompose their original geometry. Solving this geometric reassembly problem has direct applications in archaeology and forensic investigation in the computer-aided restoration of damaged artifacts and evidence. We develop a new algorithm to effectively integrate both guidance from a template and from matching of adjacent pieces' fracture-regions. First, we compute partial matchings between fragments and a template, and pairwise matchings among fragments. Many potential matches are obtained and then selected/refined in a multi-piece matching stage to maximize global groupwise matching consistency. This pipeline is effective in composing fragmented thin-shell objects containing small pieces, whose pairwise matching is usually unreliable and ambiguous and hence their reassembly remains challenging to the existing algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410604",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 8,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Three-dimensional displays",
                "Shape",
                "Geometry",
                "Forensics",
                "Pipelines",
                "Surface cracks"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer graphics",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D fragment reassembly",
                "integrated template guidance",
                "fracture region matching",
                "fragmented object matching",
                "geometric reassembly problem",
                "computer aided restoration",
                "adjacent pieces fracture regions",
                "partial matchings",
                "thin shell objects"
            ]
        },
        "id": 238,
        "cited_by": []
    },
    {
        "title": "Procedural Editing of 3D Building Point Clouds",
        "authors": [
            "Ilke Demir",
            "Daniel G. Aliaga",
            "Bedrich Benes"
        ],
        "abstract": "Thanks to the recent advances in computational photography and remote sensing, point clouds of buildings are becoming increasingly available, yet their processing poses various challenges. In our work, we tackle the problem of point cloud completion and editing and we approach it via inverse procedural modeling. Contrary to the previous work, our approach operates directly on the point cloud without an intermediate triangulation. Our approach consists of 1) semi-automatic segmentation of the input point cloud with segment comparison and template matching to detect repeating structures, 2) a consensus-based voting schema and a pattern extraction algorithm to discover completed terminal geometry and their patterns of usage, all encoded into a context-free grammar, and 3) an interactive editing tool where the user can create new point clouds by using procedural copy and paste operations, and smart resizing. We demonstrate our approach on editing of building models with up to 1.8M points. In our implementation, preprocessing takes up to several minutes and a single editing operation needs from one second to one minute depending on the model size and the operation type.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410605",
        "reference_list": [
            {
                "year": "2009",
                "id": 277
            },
            {
                "year": "2011",
                "id": 135
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Buildings",
                "Solid modeling",
                "Grammar",
                "Computational modeling",
                "Geometry",
                "Surface reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "feature extraction",
                "human computer interaction",
                "image matching",
                "image segmentation",
                "interactive systems",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "procedural editing",
                "3D building point clouds",
                "computational photography",
                "remote sensing",
                "point cloud completion problem",
                "point cloud editing problem",
                "inverse procedural modeling",
                "semi automatic input point cloud segmentation",
                "segment comparison",
                "template matching",
                "repeating structure detection",
                "consensus-based voting schema",
                "pattern extraction algorithm",
                "completed terminal geometry discovery",
                "context-free grammar",
                "interactive editing tool",
                "procedural copy operation",
                "procedural paste operation",
                "smart resizing"
            ]
        },
        "id": 239,
        "cited_by": []
    },
    {
        "title": "Semantically-Aware Aerial Reconstruction from Multi-modal Data",
        "authors": [
            "Randi Cabezas",
            "Julian Straub",
            "John W. Fisher"
        ],
        "abstract": "We consider a methodology for integrating multiple sensors along with semantic information to enhance scene representations. We propose a probabilistic generative model for inferring semantically-informed aerial reconstructions from multi-modal data within a consistent mathematical framework. The approach, called Semantically-Aware Aerial Reconstruction (SAAR), not only exploits inferred scene geometry, appearance, and semantic observations to obtain a meaningful categorization of the data, but also extends previously proposed methods by imposing structure on the prior over geometry, appearance, and semantic labels. This leads to more accurate reconstructions and the ability to fill in missing contextual labels via joint sensor and semantic information. We introduce a new multi-modal synthetic dataset in order to provide quantitative performance analysis. Additionally, we apply the model to real-world data and exploit OpenStreetMap as a source of semantic observations. We show quantitative improvements in reconstruction accuracy of large-scale urban scenes from the combination of LiDAR, aerial photography, and semantic data. Furthermore, we demonstrate the model's ability to fill in for missing sensed data, leading to more interpretable reconstructions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410606",
        "reference_list": [
            {
                "year": "2007",
                "id": 125
            },
            {
                "year": "2013",
                "id": 177
            },
            {
                "year": "2013",
                "id": 176
            },
            {
                "year": "2013",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Three-dimensional displays",
                "Geometry",
                "Image reconstruction",
                "Laser radar",
                "Solid modeling",
                "Probabilistic logic"
            ],
            "INSPEC: Controlled Indexing": [
                "geographic information systems",
                "image reconstruction",
                "image representation",
                "optical radar",
                "photography",
                "sensor fusion"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantically-aware aerial reconstruction",
                "multimodal data",
                "multiple sensor",
                "semantic information",
                "scene representation",
                "probabilistic generative model",
                "semantically-informed aerial reconstruction",
                "consistent mathematical framework",
                "SAAR",
                "scene geometry",
                "semantic observation",
                "data categorization",
                "contextual label",
                "joint sensor",
                "multimodal synthetic dataset",
                "performance analysis",
                "real-world data",
                "OpenStreetMap",
                "quantitative improvement",
                "reconstruction accuracy",
                "large-scale urban scene",
                "LiDAR",
                "aerial photography",
                "semantic data"
            ]
        },
        "id": 240,
        "cited_by": [
            {
                "year": "2017",
                "id": 402
            }
        ]
    },
    {
        "title": "Guaranteed Outlier Removal for Rotation Search",
        "authors": [
            "\u00c1lvaro Parra Bustos",
            "Tat-Jun Chin"
        ],
        "abstract": "Rotation search has become a core routine for solving many computer vision problems. The aim is to rotationally align two input point sets with correspondences. Recently, there is significant interest in developing globally optimal rotation search algorithms. A notable weakness of global algorithms, however, is their relatively high computational cost, especially on large problem sizes and data with a high proportion of outliers. In this paper, we propose a novel outlier removal technique for rotation search. Our method guarantees that any correspondence it discards as an outlier does not exist in the inlier set of the globally optimal rotation for the original data. Based on simple geometric operations, our algorithm is deterministic and fast. Used as a preprocessor to prune a large portion of the outliers from the input data, our method enables substantial speed-up of rotation search algorithms without compromising global optimality. We demonstrate the efficacy of our method in various synthetic and real data experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410607",
        "reference_list": [
            {
                "year": "2009",
                "id": 150
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Upper bound",
                "Search problems",
                "Three-dimensional displays",
                "Uncertainty",
                "Computer vision",
                "Computational efficiency",
                "Runtime"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "geometry",
                "search problems",
                "set theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "guaranteed outlier removal",
                "computer vision problems",
                "globally optimal rotation search algorithms",
                "geometric operations"
            ]
        },
        "id": 241,
        "cited_by": []
    },
    {
        "title": "Peeking Template Matching for Depth Extension",
        "authors": [
            "Simon Korman",
            "Eyal Ofek",
            "Shai Avidan"
        ],
        "abstract": "We propose a method that extends a given depth image into regions in 3D that are not visible from the point of view of the camera. The algorithm detects repeated 3D structures in the visible scene and suggests a set of 3D extension hypotheses, which are then combined together through a global 3D MRF discrete optimization. The recovered global 3D surface is consistent with both the input depth map and the hypotheses. A key component of this work is a novel 3D template matcher that is used to detect repeated 3D structure in the scene and to suggest the hypotheses. A unique property of this matcher is that it can handle depth uncertainty. This is crucial because the matcher is required to \"peek around the corner\", as it operates at the boundaries of the visible 3D scene where depth information is missing. The proposed matcher is fast and is guaranteed to find an approximation to the globally optimal solution. We demonstrate on real-world data that our algorithm is capable of completing a full 3D scene from a single depth image and can synthesize a full depth map from a novel viewpoint of the scene. In addition, we report results on an extensive synthetic set of 3D shapes, which allows us to evaluate the method both qualitatively and quantitatively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410608",
        "reference_list": [
            {
                "year": "2013",
                "id": 267
            },
            {
                "year": "2007",
                "id": 142
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Uncertainty",
                "Shape",
                "Cameras",
                "Geometry",
                "Surface reconstruction",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image matching",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "peeking template matching",
                "depth extension",
                "3D extension hypotheses",
                "global 3D MRF discrete optimization",
                "global 3D surface recovery",
                "input depth map",
                "3D template matcher",
                "repeated 3D structure detection",
                "3D vision",
                "depth image"
            ]
        },
        "id": 242,
        "cited_by": []
    },
    {
        "title": "Deformable 3D Fusion: From Partial Dynamic 3D Observations to Complete 4D Models",
        "authors": [
            "Weipeng Xu",
            "Mathieu Salzmann",
            "Yongtian Wang",
            "Yue Liu"
        ],
        "abstract": "Capturing the 3D motion of dynamic, non-rigid objects has attracted significant attention in computer vision. Existing methods typically require either complete 3D volumetric observations, or a shape template. In this paper, we introduce a template-less 4D reconstruction method that incrementally fuses highly-incomplete 3D observations of a deforming object, and generates a complete, temporally-coherent shape representation of the object. To this end, we design an online algorithm that alternatively registers new observations to the current model estimate and updates the model. We demonstrate the effectiveness of our approach at reconstructing non-rigidly moving objects from highly-incomplete measurements on both sequences of partial 3D point clouds and Kinect videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410609",
        "reference_list": [
            {
                "year": "2009",
                "id": 21
            },
            {
                "year": "2011",
                "id": 247
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Deformable models",
                "Laplace equations",
                "Surface reconstruction",
                "Solid modeling",
                "Dynamics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image capture",
                "image fusion",
                "image motion analysis",
                "image sensors",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deformable 3D fusion",
                "partial dynamic 3D observations",
                "4D models",
                "dynamic nonrigid object 3D motion capturing",
                "computer vision",
                "shape template",
                "template-less 4D reconstruction method",
                "object deformation",
                "temporally-coherent shape representation",
                "nonrigidly moving object reconstruction",
                "partial 3D point clouds",
                "Kinect videos"
            ]
        },
        "id": 243,
        "cited_by": []
    },
    {
        "title": "Non-parametric Structure-Based Calibration of Radially Symmetric Cameras",
        "authors": [
            "Federico Camposeco",
            "Torsten Sattler",
            "Marc Pollefeys"
        ],
        "abstract": "We propose a novel two-step method for estimating the intrinsic and extrinsic calibration of any radially symmetric camera, including non-central systems. The first step consists of estimating the camera pose, given a Structure from Motion (SfM) model, up to the translation along the optical axis. As a second step, we obtain the calibration by finding the translation of the camera center using an ordering constraint. The method makes use of the 1D radial camera model, which allows us to effectively handle any radially symmetric camera, including non-central ones. Using this ordering constraint, we show that the we are able to calibrate several different (central and non-central) Wide Field of View (WFOV) cameras, including fisheye, hyper-catadioptric and spherical catadioptric cameras, as well as pinhole cameras, using a single image or jointly solving for several views.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410610",
        "reference_list": [
            {
                "year": "2013",
                "id": 351
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Distortion",
                "Optical distortion",
                "Optical imaging",
                "Three-dimensional displays",
                "Distortion measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric structure-based calibration",
                "radially symmetric cameras",
                "extrinsic calibration",
                "intrinsic calibration",
                "noncentral systems",
                "camera pose estimation",
                "structure from motion model",
                "SfM model",
                "camera center translation",
                "ordering constraint",
                "1D radial camera model",
                "wide field of view cameras",
                "WFOV cameras",
                "fisheye cameras",
                "hypercatadioptric cameras",
                "spherical catadioptric cameras",
                "pinhole cameras"
            ]
        },
        "id": 244,
        "cited_by": []
    },
    {
        "title": "Exploiting Object Similarity in 3D Reconstruction",
        "authors": [
            "Chen Zhou",
            "Fatma G\u00fcney",
            "Yizhou Wang",
            "Andreas Geiger"
        ],
        "abstract": "Despite recent progress, reconstructing outdoor scenes in 3D from movable platforms remains a highly difficult endeavour. Challenges include low frame rates, occlusions, large distortions and difficult lighting conditions. In this paper, we leverage the fact that the larger the reconstructed area, the more likely objects of similar type and shape will occur in the scene. This is particularly true for outdoor scenes where buildings and vehicles often suffer from missing texture or reflections, but share similarity in 3D shape. We take advantage of this shape similarity by localizing objects using detectors and jointly reconstructing them while learning a volumetric model of their shape. This allows us to reduce noise while completing missing surfaces as objects of similar shape benefit from all observations for the respective category. We evaluate our approach with respect to LIDAR ground truth on a novel challenging suburban dataset and show its advantages over the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410611",
        "reference_list": [
            {
                "year": "2013",
                "id": 423
            },
            {
                "year": "2001",
                "id": 49
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2013",
                "id": 4
            },
            {
                "year": "2011",
                "id": 329
            },
            {
                "year": "2013",
                "id": 407
            },
            {
                "year": "2013",
                "id": 58
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Image reconstruction",
                "Solid modeling",
                "Surface reconstruction",
                "Buildings",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "learning (artificial intelligence)",
                "optical radar",
                "radar imaging"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object similarity",
                "3D reconstruction",
                "outdoor scene reconstruction",
                "movable platforms",
                "3D shape",
                "shape similarity",
                "object localization",
                "volumetric model learning",
                "noise reduction",
                "LIDAR ground truth",
                "suburban dataset"
            ]
        },
        "id": 245,
        "cited_by": []
    },
    {
        "title": "You are Here: Mimicking the Human Thinking Process in Reading Floor-Plans",
        "authors": [
            "Hang Chu",
            "Dong Ki Kim",
            "Tsuhan Chen"
        ],
        "abstract": "A human can easily find his or her way in an unfamiliar building, by walking around and reading the floor-plan. We try to mimic and automate this human thinking process. More precisely, we introduce a new and useful task of locating an user in the floor-plan, by using only a camera and a floor-plan without any other prior information. We address the problem with a novel matching-localization algorithm that is inspired by human logic. We demonstrate through experiments that our method outperforms state-of-the-art floor-plan-based localization methods by a large margin, while also being highly efficient for real-time applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410612",
        "reference_list": [
            {
                "year": "2013",
                "id": 180
            },
            {
                "year": "2013",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Buildings",
                "Image reconstruction",
                "Reliability",
                "Computer vision",
                "Sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "biomimetics",
                "buildings (structures)",
                "floors",
                "structural engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human thinking process",
                "floor-plans",
                "unfamiliar building",
                "matching-localization algorithm",
                "human logic",
                "floor-plan-based localization methods"
            ]
        },
        "id": 246,
        "cited_by": [
            {
                "year": "2017",
                "id": 231
            }
        ]
    },
    {
        "title": "MAP Disparity Estimation Using Hidden Markov Trees",
        "authors": [
            "Eric T. Psota",
            "Jedrzej Kowalczuk",
            "Mateusz Mittek",
            "Lance C. P\u00e9rez"
        ],
        "abstract": "A new method is introduced for stereo matching that operates on minimum spanning trees (MSTs) generated from the images. Disparity maps are represented as a collection of hidden states on MSTs, and each MST is modeled as a hidden Markov tree. An efficient recursive message-passing scheme designed to operate on hidden Markov trees, known as the upward-downward algorithm, is used to compute the maximum a posteriori (MAP) disparity estimate at each pixel. The messages processed by the upward-downward algorithm involve two types of probabilities: the probability of a pixel having a particular disparity given a set of per-pixel matching costs, and the probability of a disparity transition between a pair of connected pixels given their similarity. The distributions of these probabilities are modeled from a collection of images with ground truth disparities. Performance evaluation using the Middlebury stereo benchmark version 3 demonstrates that the proposed method ranks second and third in terms of overall accuracy when evaluated on the training and test image sets, respectively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410613",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 3,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Message passing",
                "Estimation",
                "Image edge detection",
                "Image color analysis",
                "Computational modeling",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden Markov models",
                "image matching",
                "message passing",
                "stereo image processing",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "MAP disparity estimation",
                "hidden Markov trees",
                "stereo matching",
                "minimum spanning trees",
                "recursive message-passing scheme",
                "upward-downward algorithm",
                "maximum a posteriori disparity",
                "Middlebury stereo benchmark version 3"
            ]
        },
        "id": 247,
        "cited_by": [
            {
                "year": "2017",
                "id": 140
            }
        ]
    },
    {
        "title": "Wide Baseline Stereo Matching with Convex Bounded Distortion Constraints",
        "authors": [
            "Meirav Galun",
            "Tal Amir",
            "Tal Hassner",
            "Ronen Basri",
            "Yaron Lipman"
        ],
        "abstract": "Finding correspondences in wide baseline setups is a challenging problem. Existing approaches have focused largely on developing better feature descriptors for correspondence and on accurate recovery of epipolar line constraints. This paper focuses on the challenging problem of finding correspondences once approximate epipolar constraints are given. We introduce a novel method that integrates a deformation model. Specifically, we formulate the problem as finding the largest number of corresponding points related by a bounded distortion map that obeys the given epipolar constraints. We show that, while the set of bounded distortion maps is not convex, the subset of maps that obey the epipolar line constraints is convex, allowing us to introduce an efficient algorithm for matching. We further utilize a robust cost function for matching and employ majorization-minimization for its optimization. Our experiments indicate that our method finds significantly more accurate maps than existing approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410614",
        "reference_list": [
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2001",
                "id": 191
            },
            {
                "year": "2003",
                "id": 156
            },
            {
                "year": "2003",
                "id": 80
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Distortion",
                "Deformable models",
                "Image analysis",
                "Robustness",
                "Optimization",
                "Transmission line matrix methods",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "feature extraction",
                "image matching",
                "minimisation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "wide baseline stereo matching",
                "convex bounded distortion constraints",
                "wide baseline setups correspondences",
                "feature descriptors",
                "approximate epipolar line constraints",
                "deformation model",
                "bounded distortion map",
                "cost function",
                "majorization-minimization",
                "optimization",
                "point correspondences"
            ]
        },
        "id": 248,
        "cited_by": []
    },
    {
        "title": "Interactive Visual Hull Refinement for Specular and Transparent Object Surface Reconstruction",
        "authors": [
            "Xinxin Zuo",
            "Chao Du",
            "Sen Wang",
            "Jiangbin Zheng",
            "Ruigang Yang"
        ],
        "abstract": "In this paper we present a method of using standard multi-view images for 3D surface reconstruction of non-Lambertian objects. We extend the original visual hull concept to incorporate 3D cues presented by internal occluding contours, i.e., occluding contours that are inside the object's silhouettes. We discovered that these internal contours, which are results of convex parts on an object's surface, can lead to a tighter fit than the original visual hull. We formulated a new visual hull refinement scheme -- Locally Convex Carving that can completely reconstruct concavity caused by two or more intersecting convex surfaces. In addition we develop a novel approach for contour tracking given labeled contours in sparse key frames. It is designed specifically for highly specular or transparent objects, for which assumptions made in traditional contour detection/tracking methods, such as highest gradient and stationary texture edges, are no longer valid. It is formulated as an energy minimization function where several novel terms are developed to increase robustness. Based on the two core algorithms, we have developed an interactive system for 3D modeling. We have validated our system, both quantitatively and qualitatively, with four datasets of different object materials. Results show that we are able to generate visually pleasing models for very challenging cases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410615",
        "reference_list": [
            {
                "year": "2011",
                "id": 315
            },
            {
                "year": "2007",
                "id": 163
            },
            {
                "year": "2007",
                "id": 114
            },
            {
                "year": "2013",
                "id": 312
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 5,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Surface reconstruction",
                "Three-dimensional displays",
                "Shape",
                "Image edge detection",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image texture",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interactive visual hull refinement",
                "transparent object surface reconstruction",
                "standard multiview images",
                "specular object surface reconstruction",
                "3D surface reconstruction",
                "non-Lambertian objects",
                "3D cues",
                "internal occluding contours",
                "convex carving",
                "contour tracking",
                "sparse key frames",
                "gradient edges",
                "stationary texture edges",
                "energy minimization function",
                "object materials"
            ]
        },
        "id": 249,
        "cited_by": []
    },
    {
        "title": "Hierarchical Higher-Order Regression Forest Fields: An Application to 3D Indoor Scene Labelling",
        "authors": [
            "Trung T. Pham",
            "Ian Reid",
            "Yasir Latif",
            "Stephen Gould"
        ],
        "abstract": "This paper addresses the problem of semantic segmentation of 3D indoor scenes reconstructed from RGB-D images. Traditionally label prediction for 3D points is tackled by employing graphical models that capture scene features and complex relations between different class labels. However, the existing work is restricted to pairwise conditional random fields, which are insufficient when encoding rich scene context. In this work we propose models with higher-order potentials to describe complex relational information from the 3D scenes. Specifically, we relax the labelling problem to a regression, and generalize the higher-order associative P n Potts model to a new family of arbitrary higher-order models based on regression forests. We show that these models, like the robust P n models, can still be decomposed into the sum of pairwise terms by introducing auxiliary variables. Moreover, our proposed higher-order models also permit extension to hierarchical random fields, which allows for the integration of scene context and features computed at different scales. Our potential functions are constructed based on regression forests encoding Gaussian densities that admit efficient inference. The parameters of our model are learned from training data using a structured learning approach. Results on two datasets show clear improvements over current state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410616",
        "reference_list": [
            {
                "year": "2013",
                "id": 382
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2011",
                "id": 295
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 4,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Labeling",
                "Semantics",
                "Solid modeling",
                "Computational modeling",
                "Robustness",
                "Context modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image colour analysis",
                "image reconstruction",
                "image segmentation",
                "learning (artificial intelligence)",
                "Potts model",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical higher-order regression forest fields",
                "3D indoor scene labelling",
                "semantic segmentation",
                "3D indoor scenes reconstruction",
                "RGB-D images",
                "label prediction",
                "3D points",
                "graphical models",
                "pairwise conditional random fields",
                "rich scene context",
                "Potts model",
                "Gaussian densities",
                "structured learning"
            ]
        },
        "id": 250,
        "cited_by": []
    },
    {
        "title": "Classical Scaling Revisited",
        "authors": [
            "Gil Shamai",
            "Yonathan Aflalo",
            "Michael Zibulevsky",
            "Ron Kimmel"
        ],
        "abstract": "Multidimensional-scaling (MDS) is an information analysis tool. It involves the evaluation of distances between data points, which is a quadratic space-time problem. Then, MDS procedures find an embedding of the points in a low dimensional Euclidean (flat) domain, optimizing for the similarity of inter-points distances. We present an efficient solver for Classical Scaling (a specific MDS model) by extending the distances measured from a subset of the points to the rest, while exploiting the smoothness property of the distance functions. The smoothness is measured by the L2 norm of the Laplace-Beltrami operator applied to the unknown distance function. The Laplace Beltrami reflects the local differential relations between points, and can be computed in linear time. Classical-scaling is thereby reformulated into a quasi-linear space-time complexities procedure.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410617",
        "reference_list": [
            {
                "year": "2003",
                "id": 187
            },
            {
                "year": "2001",
                "id": 181
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Matrix decomposition",
                "Interpolation",
                "Manifolds",
                "Complexity theory",
                "Spectral analysis",
                "Sparse matrices",
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "information analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multidimensional-scaling",
                "information analysis tool",
                "quadratic space-time problem",
                "MDS procedure",
                "dimensional Euclidean domain",
                "smoothness property",
                "Laplace-Beltrami operator",
                "distance function",
                "Laplace Beltrami",
                "quasi-linear space-time complexity procedure"
            ]
        },
        "id": 251,
        "cited_by": []
    },
    {
        "title": "Dense Continuous-Time Tracking and Mapping with Rolling Shutter RGB-D Cameras",
        "authors": [
            "Christian Kerl",
            "J\u00f6rg St\u00fcckler",
            "Daniel Cremers"
        ],
        "abstract": "We propose a dense continuous-time tracking and mapping method for RGB-D cameras. We parametrize the camera trajectory using continuous B-splines and optimize the trajectory through dense, direct image alignment. Our method also directly models rolling shutter in both RGB and depth images within the optimization, which improves tracking and reconstruction quality for low-cost CMOS sensors. Using a continuous trajectory representation has a number of advantages over a discrete-time representation (e.g. camera poses at the frame interval). With splines, less variables need to be optimized than with a discrete representation, since the trajectory can be represented with fewer control points than frames. Splines also naturally include smoothness constraints on derivatives of the trajectory estimate. Finally, the continuous trajectory representation allows to compensate for rolling shutter effects, since a pose estimate is available at any exposure time of an image. Our approach demonstrates superior quality in tracking and reconstruction compared to approaches with discrete-time or global shutter assumptions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410618",
        "reference_list": [
            {
                "year": "2013",
                "id": 251
            },
            {
                "year": "2011",
                "id": 199
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 10,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Trajectory",
                "Splines (mathematics)",
                "Simultaneous localization and mapping",
                "Three-dimensional displays",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image representation",
                "optimisation",
                "pose estimation",
                "SLAM (robots)",
                "splines (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense continuous-time tracking",
                "simultaneous localization and mapping",
                "SLAM",
                "pose estimation",
                "trajectory estimation",
                "continuous trajectory representation",
                "image alignment",
                "trajectory optimization",
                "B-spline",
                "camera trajectory representation",
                "rolling shutter RGB-D camera"
            ]
        },
        "id": 252,
        "cited_by": []
    },
    {
        "title": "Dense Image Registration and Deformable Surface Reconstruction in Presence of Occlusions and Minimal Texture",
        "authors": [
            "Dat Tien Ngo",
            "Sanghyuk Park",
            "Anne Jorstad",
            "Alberto Crivellaro",
            "Chang D. Yoo",
            "Pascal Fua"
        ],
        "abstract": "Deformable surface tracking from monocular images is well-known to be under-constrained. Occlusions often make the task even more challenging, and can result in failure if the surface is not sufficiently textured. In this work, we explicitly address the problem of 3D reconstruction of poorly textured, occluded surfaces, proposing a framework based on a template-matching approach that scales dense robust features by a relevancy score. Our approach is extensively compared to current methods employing both local feature matching and dense template alignment. We test on standard datasets as well as on a new dataset (that will be made publicly available) of a sparsely textured, occluded surface. Our framework achieves state-of-the-art results for both well and poorly textured, occluded surfaces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410619",
        "reference_list": [
            {
                "year": "2009",
                "id": 4
            },
            {
                "year": "2009",
                "id": 134
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 7,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Robustness",
                "Surface texture",
                "Three-dimensional displays",
                "Shape",
                "Image reconstruction",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image reconstruction",
                "image registration",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense image registration",
                "deformable surface reconstruction",
                "minimal texture",
                "deformable surface tracking",
                "monocular images",
                "3D reconstruction",
                "template-matching approach",
                "local feature matching",
                "dense template alignment",
                "sparsely textured surface",
                "occluded surface"
            ]
        },
        "id": 253,
        "cited_by": []
    },
    {
        "title": "The Likelihood-Ratio Test and Efficient Robust Estimation",
        "authors": [
            "Andrea Cohen",
            "Christopher Zach"
        ],
        "abstract": "Robust estimation of model parameters in the presence of outliers is a key problem in computer vision. RANSAC inspired techniques are widely used in this context, although their application might be limited due to the need of a priori knowledge on the inlier noise level. We propose a new approach for jointly optimizing over model parameters and the inlier noise level based on the likelihood ratio test. This allows control over the type I error incurred. We also propose an early bailout strategy for efficiency. Tests on both synthetic and real data show that our method outperforms the state-of-the-art in a fraction of the time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410620",
        "reference_list": [
            {
                "year": "2005",
                "id": 225
            },
            {
                "year": "2011",
                "id": 164
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise level",
                "Data models",
                "Robustness",
                "Maximum likelihood estimation",
                "Computational modeling",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust estimation",
                "likelihood-ratio test",
                "computer vision",
                "RANSAC inspired techniques"
            ]
        },
        "id": 254,
        "cited_by": []
    },
    {
        "title": "Reflection Modeling for Passive Stereo",
        "authors": [
            "Rahul Nair",
            "Andrew Fitzgibbon",
            "Daniel Kondermann",
            "Carsten Rother"
        ],
        "abstract": "Stereo reconstruction in presence of reality faces many challenges that still need to be addressed. This paper considers reflections, which introduce incorrect matches due to the observation violating the diffuse-world assumption underlying the majority of stereo techniques. Unlike most existing work, which employ regularization or robust data terms to suppress such errors, we derive two least squares models from first principles that generalize diffuse world stereo and explicitly take reflections into account. These models are parametrized by depth, orientation and material properties, resulting in a total of up to 5 parameters per pixel that have to be estimated. Additionally large non-local interactions between viewed and reflected surface have to be taken into account. These two properties make inference of the model appear prohibitive, but we present evidence that inference is actually possible using a variant of patch match stereo.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410621",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Cameras",
                "Rough surfaces",
                "Surface roughness",
                "Robustness",
                "Optimization",
                "Mirrors"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image reconstruction",
                "least mean squares methods",
                "optimisation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo reconstruction",
                "robust data",
                "least square model",
                "material property",
                "optimization",
                "patch match stereo matching"
            ]
        },
        "id": 255,
        "cited_by": []
    },
    {
        "title": "Detailed Full-Body Reconstructions of Moving People from Monocular RGB-D Sequences",
        "authors": [
            "Federica Bogo",
            "Michael J. Black",
            "Matthew Loper",
            "Javier Romero"
        ],
        "abstract": "We accurately estimate the 3D geometry and appearance of the human body from a monocular RGB-D sequence of a user moving freely in front of the sensor. Range data in each frame is first brought into alignment with a multi-resolution 3D body model in a coarse-to-fine process. The method then uses geometry and image texture over time to obtain accurate shape, pose, and appearance information despite unconstrained motion, partial views, varying resolution, occlusion, and soft tissue deformation. Our novel body model has variable shape detail, allowing it to capture faces with a high-resolution deformable head model and body shape with lower-resolution. Finally we combine range data from an entire sequence to estimate a high-resolution displacement map that captures fine shape details. We compare our recovered models with high-resolution scans from a professional system and with avatars created by a commercial product. We extract accurate 3D avatars from challenging motion sequences and even capture soft tissue dynamics.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410622",
        "reference_list": [
            {
                "year": "2009",
                "id": 21
            },
            {
                "year": "2003",
                "id": 120
            },
            {
                "year": "2011",
                "id": 247
            }
        ],
        "citation": {
            "ieee": 30,
            "other": 25,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three-dimensional displays",
                "Sensors",
                "Deformable models",
                "Avatars",
                "Image reconstruction",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "geometry",
                "image colour analysis",
                "image motion analysis",
                "image reconstruction",
                "image resolution",
                "image sequences",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion sequences",
                "soft tissue deformation",
                "occlusion",
                "image texture",
                "coarse-to-fine process",
                "multiresolution 3D body model",
                "3D geometry",
                "monocular RGB-D sequences",
                "moving people",
                "detailed full-body reconstructions"
            ]
        },
        "id": 256,
        "cited_by": [
            {
                "year": "2017",
                "id": 94
            },
            {
                "year": "2017",
                "id": 267
            }
        ]
    },
    {
        "title": "Efficient Solution to the Epipolar Geometry for Radially Distorted Cameras",
        "authors": [
            "Zuzana Kukelova",
            "Jan Heller",
            "Martin Bujnak",
            "Andrew Fitzgibbon",
            "Tomas Pajdla"
        ],
        "abstract": "The estimation of the epipolar geometry of two cameras from image matches is a fundamental problem of computer vision with many applications. While the closely related problem of estimating relative pose of two different uncalibrated cameras with radial distortion is of particular importance, none of the previously published methods is suitable for practical applications. These solutions are either numerically unstable, sensitive to noise, based on a large number of point correspondences, or simply too slow for real-time applications. In this paper, we present a new efficient solution to this problem that uses 10 image correspondences. By manipulating ten input polynomial equations, we derive a degree 10 polynomial equation in one variable. The solutions to this equation are efficiently found using the Sturm sequences method. In the experiments, we show that the proposed solution is stable, noise resistant, and fast, and as such efficiently usable in a practical Structure-from-Motion pipeline.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410623",
        "reference_list": [
            {
                "year": "2005",
                "id": 80
            },
            {
                "year": "2013",
                "id": 351
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Distortion",
                "Geometry",
                "Mathematical model",
                "Estimation",
                "Eigenvalues and eigenfunctions",
                "Real-time systems"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "image matching",
                "image sequences",
                "polynomials",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "epipolar geometry",
                "radially distorted camera",
                "image matching",
                "computer vision",
                "polynomial equation",
                "Sturm sequence method",
                "structure-from-motion pipeline"
            ]
        },
        "id": 257,
        "cited_by": [
            {
                "year": "2017",
                "id": 244
            }
        ]
    },
    {
        "title": "Learning a Descriptor-Specific 3D Keypoint Detector",
        "authors": [
            "Samuele Salti",
            "Federico Tombari",
            "Riccardo Spezialetti",
            "Luigi Di Stefano"
        ],
        "abstract": "Keypoint detection represents the first stage in the majority of modern computer vision pipelines based on automatically established correspondences between local descriptors. However, no standard solution has emerged yet in the case of 3D data such as point clouds or meshes, which exhibit high variability in level of detail and noise. More importantly, existing proposals for 3D keypoint detection rely on geometric saliency functions that attempt to maximize repeatability rather than distinctiveness of the selected regions, which may lead to sub-optimal performance of the overall pipeline. To overcome these shortcomings, we cast 3D keypoint detection as a binary classification between points whose support can be correctly matched by a predefined 3D descriptor or not, thereby learning a descriptor-specific detector that adapts seamlessly to different scenarios. Through experiments on several public datasets, we show that this novel approach to the design of a keypoint detector represents a flexible solution that, nonetheless, can provide state-of-the-art descriptor matching performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410624",
        "reference_list": [
            {
                "year": "2011",
                "id": 324
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Three-dimensional displays",
                "Feature extraction",
                "Training",
                "Computer vision",
                "Standards",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "geometry",
                "image classification",
                "image matching",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "descriptor-specific 3D keypoint detector learning",
                "computer vision pipelines",
                "local descriptors",
                "3D keypoint detection",
                "geometric saliency functions",
                "suboptimal performance",
                "binary classification",
                "public datasets",
                "descriptor matching performance"
            ]
        },
        "id": 258,
        "cited_by": []
    },
    {
        "title": "Component-Wise Modeling of Articulated Objects",
        "authors": [
            "Valsamis Ntouskos",
            "Marta Sanzari",
            "Bruno Cafaro",
            "Federico Nardi",
            "Fabrizio Natola",
            "Fiora Pirri",
            "Manuel Ruiz"
        ],
        "abstract": "We introduce a novel framework for modeling articulated objects based on the aspects of their components. By decomposing the object into components, we divide the problem in smaller modeling tasks. After obtaining 3D models for each component aspect by employing a shape deformation paradigm, we merge them together, forming the object components. The final model is obtained by assembling the components using an optimization scheme which fits the respective 3D models to the corresponding apparent contours in a reference pose. The results suggest that our approach can produce realistic 3D models of articulated objects in reasonable time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410625",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Three-dimensional displays",
                "Solid modeling",
                "Shape",
                "Deformable models",
                "Surface treatment",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "optimisation",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "component-wise modeling",
                "articulated objects",
                "3D models",
                "shape deformation paradigm",
                "optimization scheme"
            ]
        },
        "id": 259,
        "cited_by": []
    },
    {
        "title": "A Collaborative Filtering Approach to Real-Time Hand Pose Estimation",
        "authors": [
            "Chiho Choi",
            "Ayan Sinha",
            "Joon Hee Choi",
            "Sujin Jang",
            "Karthik Ramani"
        ],
        "abstract": "Collaborative filtering aims to predict unknown user ratings in a recommender system by collectively assessing known user preferences. In this paper, we first draw analogies between collaborative filtering and the pose estimation problem. Specifically, we recast the hand pose estimation problem as the cold-start problem for a new user with unknown item ratings in a recommender system. Inspired by fast and accurate matrix factorization techniques for collaborative filtering, we develop a real-time algorithm for estimating the hand pose from RGB-D data of a commercial depth camera. First, we efficiently identify nearest neighbors using local shape descriptors in the RGB-D domain from a library of hand poses with known pose parameter values. We then use this information to evaluate the unknown pose parameters using a joint matrix factorization and completion (JMFC) approach. Our quantitative and qualitative results suggest that our approach is robust to variation in hand configurations while achieving real time performance (\u2248 29 FPS) on a standard computer.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410626",
        "reference_list": [
            {
                "year": "2005",
                "id": 196
            },
            {
                "year": "2013",
                "id": 431
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 7,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three-dimensional displays",
                "Libraries",
                "Recommender systems",
                "Real-time systems",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "collaborative filtering",
                "image colour analysis",
                "image retrieval",
                "matrix decomposition",
                "pose estimation",
                "real-time systems",
                "recommender systems",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time hand pose estimation",
                "collaborative filtering",
                "user ratings",
                "recommender system",
                "user preferences",
                "cold-start problem",
                "item ratings",
                "real-time algorithm",
                "RGB-D data",
                "depth camera",
                "nearest neighbors",
                "local shape descriptors",
                "pose parameter values",
                "joint matrix factorization and completion approach",
                "JMFC approach"
            ]
        },
        "id": 260,
        "cited_by": [
            {
                "year": "2017",
                "id": 327
            },
            {
                "year": "2017",
                "id": 329
            }
        ]
    },
    {
        "title": "On the Equivalence of Moving Entrance Pupil and Radial Distortion for Camera Calibration",
        "authors": [
            "Avinash Kumar",
            "Narendra Ahuja"
        ],
        "abstract": "Radial distortion for ordinary (non-fisheye) camera lenses has traditionally been modeled as an infinite series function of radial location of an image pixel from the image center. While there has been enough empirical evidence to show that such a model is accurate and sufficient for radial distortion calibration, there has not been much analysis on the geometric/physical understanding of radial distortion from a camera calibration perspective. In this paper, we show using a thick-lens imaging model, that the variation of entrance pupil location as a function of incident image ray angle is directly responsible for radial distortion in captured images. Thus, unlike as proposed in the current state-of-the-art in camera calibration, radial distortion and entrance pupil movement are equivalent and need not be modeled together. By modeling only entrance pupil motion instead of radial distortion, we achieve two main benefits, first, we obtain comparable if not better pixel re-projection error than traditional methods, second, and more importantly, we directly back-project a radially distorted image pixel along the true image ray which formed it. Using a thick-lens setting, we show that such a back-projection is more accurate than the two-step method of undistorting an image pixel and then back-projecting it. We have applied this calibration method to the problem of generative depth-from-focus using focal stack to get accurate depth estimates.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410627",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Distortion",
                "Calibration",
                "Lenses",
                "Cameras",
                "Apertures",
                "Analytical models"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image processing",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "moving entrance pupil",
                "camera calibration",
                "ordinary non-fisheye camera lenses",
                "infinite series function",
                "radial location",
                "image pixel",
                "image center",
                "radial distortion calibration",
                "camera calibration perspective",
                "thick lens imaging model",
                "entrance pupil location",
                "image ray angle",
                "thick-lens setting"
            ]
        },
        "id": 261,
        "cited_by": []
    },
    {
        "title": "A Linear Generalized Camera Calibration from Three Intersecting Reference Planes",
        "authors": [
            "Mai Nishimura",
            "Shohei Nobuhara",
            "Takashi Matsuyama",
            "Shinya Shimizu",
            "Kensaku Fujii"
        ],
        "abstract": "This paper presents a new generalized (or ray-pixel, raxel) camera calibration algorithm for camera systems involving distortions by unknown refraction and reflection processes. The key idea is use of intersections of calibration planes, while conventional methods utilized collinearity constraints of points on the planes. We show that intersections of calibration planes can realize a simple linear algorithm, and that our method can be applied to any ray-distributions while conventional methods require knowing the ray-distribution class in advance. Evaluations using synthesized and real datasets demonstrate the performance of our method quantitatively and qualitatively.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410628",
        "reference_list": [
            {
                "year": "2011",
                "id": 293
            },
            {
                "year": "2005",
                "id": 54
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Three-dimensional displays",
                "Distortion",
                "Geometry",
                "Solid modeling",
                "Linear systems"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "cameras",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ray-distributions",
                "calibration plane intersections",
                "collinearity constraints",
                "raxel camera calibration algorithm",
                "ray-pixel camera calibration algorithm",
                "intersecting reference planes",
                "linear generalized camera calibration"
            ]
        },
        "id": 262,
        "cited_by": []
    },
    {
        "title": "Towards Pointless Structure from Motion: 3D Reconstruction and Camera Parameters from General 3D Curves",
        "authors": [
            "Irina Nurutdinova",
            "Andrew Fitzgibbon"
        ],
        "abstract": "Modern structure from motion (SfM) remains dependent on point features to recover camera positions, meaning that reconstruction is severely hampered in low-texture environments, for example scanning a plain coffee cup on an uncluttered table. We show how 3D curves can be used to refine camera position estimation in challenging low-texture scenes. In contrast to previous work, we allow the curves to be partially observed in all images, meaning that for the first time, curve-based SfM can be demonstrated in realistic scenes. The algorithm is based on bundle adjustment, so needs an initial estimate, but even a poor estimate from a few point correspondences can be substantially improved by including curves, suggesting that this method would benefit many existing systems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410629",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 5,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Image reconstruction",
                "Shape",
                "Splines (mathematics)",
                "Data models",
                "Calibration"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image reconstruction",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure from motion",
                "3D reconstruction",
                "camera parameters",
                "3D curves",
                "camera position estimation",
                "low-texture scenes",
                "curve-based SfM",
                "bundle adjustment"
            ]
        },
        "id": 263,
        "cited_by": [
            {
                "year": "2017",
                "id": 324
            }
        ]
    },
    {
        "title": "Attributed Grammars for Joint Estimation of Human Attributes, Part and Pose",
        "authors": [
            "Seyoung Park",
            "Song-Chun Zhu"
        ],
        "abstract": "In this paper, we are interested in developing compositional models to explicit representing pose, parts and attributes and tackling the tasks of attribute recognition, pose estimation and part localization jointly. This is different from the recent trend of using CNN-based approaches for training and testing on these tasks separately with a large amount of data. Conventional attribute models typically use a large number of region-based attribute classifiers on parts of pre-trained pose estimator without explicitly detecting the object or its parts, or considering the correlations between attributes. In contrast, our approach jointly represents both the object parts and their semantic attributes within a unified compositional hierarchy. We apply our attributed grammar model to the task of human parsing by simultaneously performing part localization and attribute recognition. We show our modeling helps performance improvements on pose-estimation task and also outperforms on other existing methods on attribute prediction task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410630",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2013",
                "id": 89
            },
            {
                "year": "2011",
                "id": 61
            },
            {
                "year": "2013",
                "id": 435
            },
            {
                "year": "2013",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Grammar",
                "Torso",
                "Geometry",
                "Training",
                "Hair",
                "Data models",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "attribute grammars",
                "object recognition",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint estimation",
                "human attributes",
                "attribute recognition",
                "pose estimation",
                "CNN",
                "pretrained pose estimator",
                "attributed grammar model"
            ]
        },
        "id": 264,
        "cited_by": []
    },
    {
        "title": "Real-Time Pose Estimation Piggybacked on Object Detection",
        "authors": [
            "Roman Jur\u00e1nek",
            "Adam Herout",
            "Mark\u00e9ta Dubsk\u00e1",
            "Pavel Zemc\u00edk"
        ],
        "abstract": "We present an object detector coupled with pose estimation directly in a single compact and simple model, where the detector shares extracted image features with the pose estimator. The output of the classification of each candidate window consists of both object score and likelihood map of poses. This extension introduces negligible overhead during detection so that the detector is still capable of real time operation. We evaluated the proposed approach on the problem of vehicle detection. We used existing datasets with viewpoint/pose annotation (WCVP, 3D objects, KITTI). Besides that, we collected a new traffic surveillance dataset COD20k which fills certain gaps of the existing datasets and we make it public. The experimental results show that the proposed approach is comparable with state-of-the-art approaches in terms of accuracy, but it is considerably faster - easily operating in real time (Matlab with C++ code). The source codes and the collected COD20k dataset are made public along with the paper.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410631",
        "reference_list": [
            {
                "year": "2013",
                "id": 188
            },
            {
                "year": "2011",
                "id": 124
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Detectors",
                "Training",
                "Feature extraction",
                "Real-time systems",
                "Three-dimensional displays",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "object detection",
                "road vehicles",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time pose estimation",
                "object detection",
                "image features extraction",
                "vehicle detection",
                "pose annotation",
                "viewpoint annotation",
                "WCVP",
                "3D objects",
                "KITTI",
                "COD20k",
                "Matlab",
                "C++ code"
            ]
        },
        "id": 265,
        "cited_by": []
    },
    {
        "title": "Understanding and Predicting Image Memorability at a Large Scale",
        "authors": [
            "Aditya Khosla",
            "Akhil S. Raju",
            "Antonio Torralba",
            "Aude Oliva"
        ],
        "abstract": "Progress in estimating visual memorability has been limited by the small scale and lack of variety of benchmark data. Here, we introduce a novel experimental procedure to objectively measure human memory, allowing us to build LaMem, the largest annotated image memorability dataset to date (containing 60,000 images from diverse sources). Using Convolutional Neural Networks (CNNs), we show that fine-tuned deep features outperform all other features by a large margin, reaching a rank correlation of 0.64, near human consistency (0.68). Analysis of the responses of the high-level CNN layers shows which objects and regions are positively, and negatively, correlated with memorability, allowing us to create memorability maps for each image and provide a concrete method to perform image memorability manipulation. This work demonstrates that one can now robustly estimate the memorability of images from many different classes, positioning memorability and deep memorability features as prime candidates to estimate the utility of information for cognitive systems. Our model and data are available at: http://memorability.csail.mit.edu.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410632",
        "reference_list": [
            {
                "year": "2015",
                "id": 121
            },
            {
                "year": "2013",
                "id": 399
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 27,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Games",
                "Correlation",
                "Benchmark testing",
                "Delay effects",
                "Delays",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "correlation methods",
                "feature extraction",
                "image retrieval",
                "neural nets",
                "prediction theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image memorability understanding",
                "image memorability prediction",
                "visual memorability",
                "human memory",
                "LaMem",
                "largest annotated image memorability dataset",
                "convolutional neural networks",
                "rank correlation",
                "human consistency",
                "high-level CNN layers",
                "memorability maps",
                "deep memorability features"
            ]
        },
        "id": 266,
        "cited_by": [
            {
                "year": "2015",
                "id": 121
            }
        ]
    },
    {
        "title": "Multiple Granularity Descriptors for Fine-Grained Categorization",
        "authors": [
            "Dequan Wang",
            "Zhiqiang Shen",
            "Jie Shao",
            "Wei Zhang",
            "Xiangyang Xue",
            "Zheng Zhang"
        ],
        "abstract": "Fine-grained categorization, which aims to distinguish subordinate-level categories such as bird species or dog breeds, is an extremely challenging task. This is due to two main issues: how to localize discriminative regions for recognition and how to learn sophisticated features for representation. Neither of them is easy to handle if there is insufficient labeled data. We leverage the fact that a subordinate-level object already has other labels in its ontology tree. These \"free\" labels can be used to train a series of CNN-based classifiers, each specialized at one grain level. The internal representations of these networks have different region of interests, allowing the construction of multi-grained descriptors that encode informative and discriminative features covering all the grain levels. Our multiple granularity framework can be learned with the weakest supervision, requiring only image-level label and avoiding the use of labor-intensive bounding box or part annotations. Experimental results on three challenging fine-grained image datasets demonstrate that our approach outperforms state-of-the-art algorithms, including those requiring strong labels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410633",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2011",
                "id": 20
            },
            {
                "year": "2013",
                "id": 213
            },
            {
                "year": "2013",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 14,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Heating",
                "Feature extraction",
                "Ontologies",
                "Computer vision",
                "Birds",
                "Vegetation",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "ontologies (artificial intelligence)",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple granularity descriptors",
                "fine-grained categorization",
                "ontology tree",
                "CNN-based classifiers",
                "labor-intensive bounding box",
                "part annotations",
                "fine-grained image datasets"
            ]
        },
        "id": 267,
        "cited_by": [
            {
                "year": "2017",
                "id": 56
            },
            {
                "year": "2017",
                "id": 395
            },
            {
                "year": "2017",
                "id": 547
            }
        ]
    },
    {
        "title": "Guiding the Long-Short Term Memory Model for Image Caption Generation",
        "authors": [
            "Xu Jia",
            "Efstratios Gavves",
            "Basura Fernando",
            "Tinne Tuytelaars"
        ],
        "abstract": "In this work we focus on the problem of image caption generation. We propose an extension of the long short term memory (LSTM) model, which we coin gLSTM for short. In particular, we add semantic information extracted from the image as extra input to each unit of the LSTM block, with the aim of guiding the model towards solutions that are more tightly coupled to the image content. Additionally, we explore different length normalization strategies for beam search to avoid bias towards short sentences. On various benchmark datasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on par with or better than the current state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410634",
        "reference_list": [],
        "citation": {
            "ieee": 46,
            "other": 46,
            "total": 92
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Computer architecture",
                "Logic gates",
                "Microprocessors",
                "Visualization",
                "Training",
                "Pipelines"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image processing",
                "modelling",
                "semantic networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "long-short term memory model",
                "LSTM model",
                "image caption generation",
                "semantic information extraction",
                "length normalization strategy"
            ]
        },
        "id": 268,
        "cited_by": [
            {
                "year": "2017",
                "id": 47
            },
            {
                "year": "2017",
                "id": 360
            },
            {
                "year": "2017",
                "id": 510
            },
            {
                "year": "2017",
                "id": 531
            }
        ]
    },
    {
        "title": "Just Noticeable Differences in Visual Attributes",
        "authors": [
            "Aron Yu",
            "Kristen Grauman"
        ],
        "abstract": "We explore the problem of predicting \"just noticeable differences\" in a visual attribute. While some pairs of images have a clear ordering for an attribute (e.g., A is more sporty than B), for others the difference may be indistinguishable to human observers. However, existing relative attribute models are unequipped to infer partial orders on novel data. Attempting to map relative attribute ranks to equality predictions is non-trivial, particularly since the span of indistinguishable pairs in attribute space may vary in different parts of the feature space. We develop a Bayesian local learning strategy to infer when images are indistinguishable for a given attribute. On the UT-Zap50K shoes and LFW-10 faces datasets, we outperform a variety of alternative methods. In addition, we show the practical impact on fine-grained visual search.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410635",
        "reference_list": [
            {
                "year": "2013",
                "id": 341
            },
            {
                "year": "2013",
                "id": 428
            },
            {
                "year": "2013",
                "id": 37
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2013",
                "id": 269
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 7,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Image color analysis",
                "Visualization",
                "Bayes methods",
                "Observers",
                "Footwear",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "face recognition",
                "feature extraction",
                "image retrieval",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "just noticeable differences prediction",
                "visual attributes",
                "relative attribute mapping",
                "attribute space",
                "feature space",
                "Bayesian local learning strategy",
                "UT-Zap50K shoes dataset",
                "LFW-10 faces dataset",
                "fine-grained visual search"
            ]
        },
        "id": 269,
        "cited_by": [
            {
                "year": "2017",
                "id": 40
            },
            {
                "year": "2017",
                "id": 584
            }
        ]
    },
    {
        "title": "VQA: Visual Question Answering",
        "authors": [
            "Stanislaw Antol",
            "Aishwarya Agrawal",
            "Jiasen Lu",
            "Margaret Mitchell",
            "Dhruv Batra",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "abstract": "We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ~0.25M images, ~0.76M questions, and ~10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410636",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2013",
                "id": 338
            },
            {
                "year": "2015",
                "id": 0
            },
            {
                "year": "2013",
                "id": 54
            },
            {
                "year": "2015",
                "id": 283
            },
            {
                "year": "2015",
                "id": 274
            },
            {
                "year": "2013",
                "id": 209
            }
        ],
        "citation": {
            "ieee": 232,
            "other": 154,
            "total": 386
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Image color analysis",
                "Knowledge discovery",
                "Cognition",
                "Measurement",
                "Glass"
            ],
            "INSPEC: Controlled Indexing": [
                "question answering (information retrieval)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "free-form VQA",
                "open-ended visual question answering",
                "natural language answer",
                "generic image captions",
                "open-ended answers",
                "multiple-choice format",
                "human performance"
            ]
        },
        "id": 270,
        "cited_by": [
            {
                "year": "2017",
                "id": 43
            },
            {
                "year": "2017",
                "id": 47
            },
            {
                "year": "2017",
                "id": 54
            },
            {
                "year": "2017",
                "id": 64
            },
            {
                "year": "2017",
                "id": 83
            },
            {
                "year": "2017",
                "id": 85
            },
            {
                "year": "2017",
                "id": 132
            },
            {
                "year": "2017",
                "id": 133
            },
            {
                "year": "2017",
                "id": 135
            },
            {
                "year": "2017",
                "id": 147
            },
            {
                "year": "2017",
                "id": 191
            },
            {
                "year": "2017",
                "id": 192
            },
            {
                "year": "2017",
                "id": 199
            },
            {
                "year": "2017",
                "id": 203
            },
            {
                "year": "2017",
                "id": 207
            },
            {
                "year": "2017",
                "id": 262
            },
            {
                "year": "2017",
                "id": 275
            },
            {
                "year": "2017",
                "id": 302
            },
            {
                "year": "2017",
                "id": 311
            },
            {
                "year": "2017",
                "id": 315
            },
            {
                "year": "2017",
                "id": 360
            },
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2017",
                "id": 431
            },
            {
                "year": "2017",
                "id": 432
            },
            {
                "year": "2017",
                "id": 438
            },
            {
                "year": "2017",
                "id": 439
            },
            {
                "year": "2017",
                "id": 442
            },
            {
                "year": "2017",
                "id": 495
            },
            {
                "year": "2017",
                "id": 510
            }
        ]
    },
    {
        "title": "Localize Me Anywhere, Anytime: A Multi-task Point-Retrieval Approach",
        "authors": [
            "Guoyu Lu",
            "Yan Yan",
            "Li Ren",
            "Jingkuan Song",
            "Nicu Sebe",
            "Chandra Kambhamettu"
        ],
        "abstract": "Image-based localization is an essential complement to GPS localization. Current image-based localization methods are based on either 2D-to-3D or 3D-to-2D to find the correspondences, which ignore the real scene geometric attributes. The main contribution of our paper is that we use a 3D model reconstructed by a short video as the query to realize 3D-to-3D localization under a multi-task point retrieval framework. Firstly, the use of a 3D model as the query enables us to efficiently select location candidates. Furthermore, the reconstruction of 3D model exploits the correlations among different images, based on the fact that images captured from different views for SfM share information through matching features. By exploring shared information (matching features) across multiple related tasks (images of the same scene captured from different views), the visual feature's view-invariance property can be improved in order to get to a higher point retrieval accuracy. More specifically, we use multi-task point retrieval framework to explore the relationship between descriptors and the 3D points, which extracts the discriminant points for more accurate 3D-to-3D correspondences retrieval. We further apply multi-task learning (MTL) retrieval approach on thermal images to prove that our MTL retrieval framework also provides superior performance for the thermal domain. This application is exceptionally helpful to cope with the localization problem in an environment with limited light sources.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410637",
        "reference_list": [
            {
                "year": "2011",
                "id": 84
            },
            {
                "year": "2013",
                "id": 146
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 1,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Image reconstruction",
                "Solid modeling",
                "Cameras",
                "Training",
                "Surface reconstruction",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "feature extraction",
                "image matching",
                "image reconstruction",
                "video retrieval",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "localization problem",
                "MTL retrieval framework",
                "thermal images",
                "multitask learning retrieval",
                "3D-to-3D correspondences retrieval",
                "3D points",
                "matching features",
                "SfM",
                "multitask point retrieval framework",
                "3D-to-3D localization",
                "short video",
                "3D model reconstruction",
                "real scene geometric attributes",
                "3D-to-2D",
                "2D-to-3D",
                "image-based localization methods",
                "GPS localization"
            ]
        },
        "id": 271,
        "cited_by": []
    },
    {
        "title": "Dense Optical Flow Prediction from a Static Image",
        "authors": [
            "Jacob Walker",
            "Abhinav Gupta",
            "Martial Hebert"
        ],
        "abstract": "Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410638",
        "reference_list": [
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2013",
                "id": 443
            },
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 10,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical imaging",
                "Videos",
                "Predictive models",
                "Optical losses",
                "Neural networks",
                "Context",
                "Trajectory"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "motion estimation",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "future motion prediction",
                "motion prediction",
                "CNN",
                "convolutional neural network",
                "static image",
                "dense optical flow prediction"
            ]
        },
        "id": 272,
        "cited_by": [
            {
                "year": "2017",
                "id": 197
            },
            {
                "year": "2017",
                "id": 216
            },
            {
                "year": "2017",
                "id": 316
            },
            {
                "year": "2017",
                "id": 351
            },
            {
                "year": "2017",
                "id": 585
            }
        ]
    },
    {
        "title": "Unsupervised Domain Adaptation for Zero-Shot Learning",
        "authors": [
            "Elyor Kodirov",
            "Tao Xiang",
            "Zhenyong Fu",
            "Shaogang Gong"
        ],
        "abstract": "Zero-shot learning (ZSL) can be considered as a special case of transfer learning where the source and target domains have different tasks/label spaces and the target domain is unlabelled, providing little guidance for the knowledge transfer. A ZSL method typically assumes that the two domains share a common semantic representation space, where a visual feature vector extracted from an image/video can be projected/embedded using a projection function. Existing approaches learn the projection function from the source domain and apply it without adaptation to the target domain. They are thus based on naive knowledge transfer and the learned projections are prone to the domain shift problem. In this paper a novel ZSL method is proposed based on unsupervised domain adaptation. Specifically, we formulate a novel regularised sparse coding framework which uses the target domain class labels' projections in the semantic space to regularise the learned target domain projection thus effectively overcoming the projection domain shift problem. Extensive experiments on four object and action recognition benchmark datasets show that the proposed ZSL method significantly outperforms the state-of-the-arts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410639",
        "reference_list": [
            {
                "year": "2013",
                "id": 322
            },
            {
                "year": "2013",
                "id": 369
            },
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2011",
                "id": 321
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 47,
            "other": 29,
            "total": 76
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Visualization",
                "Encoding",
                "Adaptation models",
                "Prototypes",
                "Feature extraction",
                "Birds"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "unsupervised learning",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "regularised sparse coding",
                "ZSL method",
                "naive knowledge transfer",
                "projection function",
                "visual feature vector",
                "semantic representation space",
                "transfer learning",
                "zero-shot learning",
                "unsupervised domain adaptation"
            ]
        },
        "id": 273,
        "cited_by": [
            {
                "year": "2017",
                "id": 141
            },
            {
                "year": "2017",
                "id": 377
            },
            {
                "year": "2017",
                "id": 439
            },
            {
                "year": "2017",
                "id": 443
            },
            {
                "year": "2017",
                "id": 466
            }
        ]
    },
    {
        "title": "Visual Madlibs: Fill in the Blank Description Generation and Question Answering",
        "authors": [
            "Licheng Yu",
            "Eunbyung Park",
            "Alexander C. Berg",
            "Tamara L. Berg"
        ],
        "abstract": "In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410640",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 13,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Natural languages",
                "Context",
                "Computer vision",
                "Knowledge discovery",
                "Explosions",
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "natural language processing",
                "question answering (information retrieval)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fill in the blank description generation",
                "natural language descriptions",
                "visual Madlibs dataset",
                "fill-in-the-blank templates",
                "targeted descriptions",
                "description generation tasks",
                "multiple-choice question-answering"
            ]
        },
        "id": 274,
        "cited_by": [
            {
                "year": "2017",
                "id": 360
            },
            {
                "year": "2017",
                "id": 439
            },
            {
                "year": "2017",
                "id": 442
            },
            {
                "year": "2015",
                "id": 270
            }
        ]
    },
    {
        "title": "Actions and Attributes from Wholes and Parts",
        "authors": [
            "Georgia Gkioxari",
            "Ross Girshick",
            "Jitendra Malik"
        ],
        "abstract": "We investigate the importance of parts for the tasks of action and attribute classification. We develop a part-based approach by leveraging convolutional network features inspired by recent advances in computer vision. Our part detectors are a deep version of poselets and capture parts of the human body under a distinct set of poses. For the tasks of action and attribute classification, we train holistic convolutional neural networks and show that adding parts leads to top-performing results for both tasks. We observe that for deeper networks parts are less significant. In addition, we demonstrate the effectiveness of our approach when we replace an oracle person detector, as is the default in the current evaluation protocol for both tasks, with a state-of-the-art person detection system.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410641",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 18,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Detectors",
                "Training",
                "Legged locomotion",
                "Object detection",
                "Birds",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convolution",
                "feature extraction",
                "image classification",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action classification",
                "attribute classification",
                "part-based approach",
                "convolutional network network",
                "CNN",
                "computer vision",
                "part detection",
                "person detection system"
            ]
        },
        "id": 275,
        "cited_by": [
            {
                "year": "2017",
                "id": 357
            },
            {
                "year": "2015",
                "id": 120
            }
        ]
    },
    {
        "title": "DeepBox: Learning Objectness with Convolutional Networks",
        "authors": [
            "Weicheng Kuo",
            "Bharath Hariharan",
            "Jitendra Malik"
        ],
        "abstract": "Existing object proposal approaches use primarily bottom-up cues to rank proposals, while we believe that \"objectness\" is in fact a high level construct. We argue for a data-driven, semantic approach for ranking object proposals. Our framework, which we call DeepBox, uses convolutional neural networks (CNNs) to rerank proposals from a bottom-up method. We use a novel four-layer CNN architecture that is as good as much larger networks on the task of evaluating objectness while being much faster. We show that DeepBox significantly improves over the bottom-up ranking, achieving the same recall with 500 proposals as achieved by bottom-up methods with 2000. This improvement generalizes to categories the CNN has never seen before and leads to a 4.5-point gain in detection mAP. Our implementation achieves this performance while running at 260 ms per image.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410642",
        "reference_list": [
            {
                "year": "2015",
                "id": 160
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2011",
                "id": 133
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 19,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Image edge detection",
                "Training",
                "Computer architecture",
                "Image segmentation",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "neural nets",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "DeepBox",
                "learning objectness",
                "convolutional neural network",
                "object proposals",
                "four-layer CNN architecture",
                "bottom-up ranking"
            ]
        },
        "id": 276,
        "cited_by": [
            {
                "year": "2017",
                "id": 189
            }
        ]
    },
    {
        "title": "Active Object Localization with Deep Reinforcement Learning",
        "authors": [
            "Juan C. Caicedo",
            "Svetlana Lazebnik"
        ],
        "abstract": "We present an active detection model for localizing objects in scenes. The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object. This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning. The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410643",
        "reference_list": [
            {
                "year": "2013",
                "id": 373
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2011",
                "id": 165
            }
        ],
        "citation": {
            "ieee": 68,
            "other": 27,
            "total": 95
        },
        "keywords": {
            "IEEE Keywords": [
                "Transforms",
                "Proposals",
                "Search problems",
                "Computational modeling",
                "Prediction algorithms",
                "History",
                "Learning (artificial intelligence)"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object detection",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active object localization",
                "deep reinforcement learning",
                "active detection model",
                "Pascal VOC 2007"
            ]
        },
        "id": 277,
        "cited_by": [
            {
                "year": "2017",
                "id": 11
            },
            {
                "year": "2017",
                "id": 414
            },
            {
                "year": "2017",
                "id": 430
            }
        ]
    },
    {
        "title": "Scene-Domain Active Part Models for Object Representation",
        "authors": [
            "Zhou Ren",
            "Chaohui Wang",
            "Alan Yuille"
        ],
        "abstract": "In this paper, we are interested in enhancing the expressivity and robustness of part-based models for object representation, in the common scenario where the training data are based on 2D images. To this end, we propose scene-domain active part models (SDAPM), which reconstruct and characterize the 3D geometric statistics between object's parts in 3D scene-domain by using 2D training data in the image-domain alone. And on top of this, we explicitly model and handle occlusions in SDAPM. Together with the developed learning and inference algorithms, such a model provides rich object descriptions, including 2D object and parts localization, 3D landmark shape and camera viewpoint, which offers an effective representation to various image understanding tasks, such as object and parts detection, 3D landmark shape and viewpoint estimation from images. Experiments on the above tasks show that SDAPM outperforms previous part-based models, and thus demonstrates the potential of the proposed technique.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410644",
        "reference_list": [
            {
                "year": "2009",
                "id": 172
            },
            {
                "year": "2009",
                "id": 194
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2011",
                "id": 180
            },
            {
                "year": "2013",
                "id": 217
            },
            {
                "year": "2011",
                "id": 91
            },
            {
                "year": "2013",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Three-dimensional displays",
                "Data models",
                "Deformable models",
                "Training data",
                "Shape",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image representation",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "object detection",
                "statistics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene-domain active part models",
                "object representation",
                "2D images",
                "3D geometric statistics",
                "3D scene-domain",
                "learning algorithms",
                "inference algorithms",
                "2D object localization",
                "image understanding tasks",
                "object detection",
                "3D landmark shape and viewpoint estimation"
            ]
        },
        "id": 278,
        "cited_by": []
    },
    {
        "title": "A Unified Multiplicative Framework for Attribute Learning",
        "authors": [
            "Kongming Liang",
            "Hong Chang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "abstract": "Attributes are mid-level semantic properties of objects. Recent research has shown that visual attributes can benefit many traditional learning problems in computer vision community. However, attribute learning is still a challenging problem as the attributes may not always be predictable directly from input images and the variation of visual attributes is sometimes large across categories. In this paper, we propose a unified multiplicative framework for attribute learning, which tackles the key problems. Specifically, images and category information are jointly projected into a shared feature space, where the latent factors are disentangled and multiplied for attribute prediction. The resulting attribute classifier is category-specific instead of being shared by all categories. Moreover, our method can leverage auxiliary data to enhance the predictive ability of attribute classifiers, reducing the effort of instance-level attribute annotation to some extent. Experimental results show that our method achieves superior performance on both instance-level and category-level attribute prediction. For zero-shot learning based on attributes, our method significantly improves the state-of-the-art performance on AwA dataset and achieves comparable performance on CUB dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410645",
        "reference_list": [
            {
                "year": "2013",
                "id": 264
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Predictive models",
                "Semantics",
                "Training",
                "Correlation",
                "Object recognition",
                "Learning systems"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "attribute learning",
                "unified multiplicative framework",
                "computer vision community",
                "attribute classifier",
                "category-level attribute prediction",
                "instance-level",
                "zero-shot learning",
                "AwA dataset",
                "CUB dataset"
            ]
        },
        "id": 279,
        "cited_by": []
    },
    {
        "title": "Contractive Rectifier Networks for Nonlinear Maximum Margin Classification",
        "authors": [
            "Senjian An",
            "Munawar Hayat",
            "Salman H. Khan",
            "Mohammed Bennamoun",
            "Farid Boussaid",
            "Ferdous Sohel"
        ],
        "abstract": "To find the optimal nonlinear separating boundary with maximum margin in the input data space, this paper proposes Contractive Rectifier Networks (CRNs), wherein the hidden-layer transformations are restricted to be contraction mappings. The contractive constraints ensure that the achieved separating margin in the input space is larger than or equal to the separating margin in the output layer. The training of the proposed CRNs is formulated as a linear support vector machine (SVM) in the output layer, combined with two or more contractive hidden layers. Effective algorithms have been proposed to address the optimization challenges arising from contraction constraints. Experimental results on MNIST, CIFAR-10, CIFAR-100 and MIT-67 datasets demonstrate that the proposed contractive rectifier networks consistently outperform their conventional unconstrained rectifier network counterparts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410646",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Training",
                "Neurons",
                "Australia",
                "Aerospace electronics",
                "Nonlinear distortion"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "neural nets",
                "nonlinear programming",
                "object recognition",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object classification",
                "deep learning network",
                "optimization",
                "SVM",
                "support vector machine",
                "contraction mapping",
                "hidden-layer transformation",
                "nonlinear maximum margin classification",
                "CRN",
                "contractive rectifier network"
            ]
        },
        "id": 280,
        "cited_by": [
            {
                "year": "2017",
                "id": 591
            }
        ]
    },
    {
        "title": "Augmenting Strong Supervision Using Web Data for Fine-Grained Categorization",
        "authors": [
            "Zhe Xu",
            "Shaoli Huang",
            "Ya Zhang",
            "Dacheng Tao"
        ],
        "abstract": "We propose a new method for fine-grained object recognition that employs part-level annotations and deep convolutional neural networks (CNNs) in a unified framework. Although both schemes have been widely used to boost recognition performance, due to the difficulty in acquiring detailed part annotations, strongly supervised fine-grained datasets are usually too small to keep pace with the rapid evolution of CNN architectures. In this paper, we solve this problem by exploiting inexhaustible web data. The proposed method improves classification accuracy in two ways: more discriminative CNN feature representations are generated using a training set augmented by collecting a large number of part patches from weakly supervised web images, and more robust object classifiers are learned using a multi-instance learning algorithm jointly on the strong and weak datasets. Despite its simplicity, the proposed method delivers a remarkable performance improvement on the CUB200-2011 dataset compared to baseline part-based R-CNN methods, and achieves the highest accuracy on this dataset even in the absence of test image annotations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410647",
        "reference_list": [
            {
                "year": "2011",
                "id": 232
            },
            {
                "year": "2013",
                "id": 40
            },
            {
                "year": "2013",
                "id": 213
            },
            {
                "year": "2013",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 9,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Training",
                "Detectors",
                "Robustness",
                "Training data",
                "Proposals",
                "Computer architecture"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image retrieval",
                "Internet",
                "learning (artificial intelligence)",
                "neural net architecture",
                "object recognition",
                "performance evaluation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "strong supervision augmentation",
                "Web data",
                "fine-grained categorization",
                "fine-grained object recognition",
                "part-level annotations",
                "deep convolutional neural networks",
                "recognition performance",
                "strongly supervised fine-grained datasets",
                "CNN architectures",
                "classification accuracy improvement",
                "discriminative CNN feature representations",
                "weakly supervised Web images",
                "robust object classifiers",
                "multiinstance learning algorithm",
                "performance improvement",
                "CUB200-2011 dataset"
            ]
        },
        "id": 281,
        "cited_by": []
    },
    {
        "title": "Learning Like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images",
        "authors": [
            "Junhua Mao",
            "Xu Wei",
            "Yi Yang",
            "Jiang Wang",
            "Zhiheng Huang",
            "Alan L. Yuille"
        ],
        "abstract": "In this paper, we address the task of learning novel visual concepts, and their interactions with other concepts, from a few images with sentence descriptions. Using linguistic context and visual features, our method is able to efficiently hypothesize the semantic meaning of new words and add them to its word dictionary so that they can be used to describe images which contain these novel concepts. Our method has an image captioning module based on with several improvements. In particular, we propose a transposed weight sharing scheme, which not only improves performance on image captioning, but also makes the model more suitable for the novel concept learning task. We propose methods to prevent overfitting the new concepts. In addition, three novel concept datasets are constructed for this new task, and are publicly available on the project page. In the experiments, we show that our method effectively learns novel visual concepts from a few examples without disturbing the previously learned concepts. The project page is: www.stat.ucla.edu/junhua. mao/projects/child_learning.html.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410648",
        "reference_list": [
            {
                "year": "2013",
                "id": 322
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 15,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Visualization",
                "Semantics",
                "Dictionaries",
                "Computer vision",
                "Computational modeling",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "learning (artificial intelligence)",
                "semantic networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weight sharing scheme",
                "image captioning module",
                "word dictionary",
                "semantic meaning",
                "image sentence description",
                "visual concept learning"
            ]
        },
        "id": 282,
        "cited_by": [
            {
                "year": "2017",
                "id": 370
            }
        ]
    },
    {
        "title": "Learning Common Sense through Visual Abstraction",
        "authors": [
            "Ramakrishna Vedantam",
            "Xiao Lin",
            "Tanmay Batra",
            "C. Lawrence Zitnick",
            "Devi Parikh"
        ],
        "abstract": "Common sense is essential for building intelligent machines. While some commonsense knowledge is explicitly stated in human-generated text and can be learnt by mining the web, much of it is unwritten. It is often unnecessary and even unnatural to write about commonsense facts. While unwritten, this commonsense knowledge is not unseen! The visual world around us is full of structure modeled by commonsense knowledge. Can machines learn common sense simply by observing our visual world? Unfortunately, this requires automatic and accurate detection of objects, their attributes, poses, and interactions between objects, which remain challenging problems. Our key insight is that while visual common sense is depicted in visual content, it is the semantic features that are relevant and not low-level pixel information. In other words, photorealism is not necessary to learn common sense. We explore the use of human-generated abstract scenes made from clipart for learning common sense. In particular, we reason about the plausibility of an interaction or relation between a pair of nouns by measuring the similarity of the relation and nouns with other relations and nouns we have seen in abstract scenes. We show that the commonsense knowledge we learn is complementary to what can be learnt from sources of text.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410649",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2013",
                "id": 209
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 9,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Libraries",
                "Semantics",
                "Cognition",
                "Data mining",
                "Grounding",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "data mining",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "common sense learning",
                "visual abstraction",
                "intelligent machines",
                "human-generated text",
                "Web mining",
                "visual common sense",
                "visual content",
                "semantic features"
            ]
        },
        "id": 283,
        "cited_by": [
            {
                "year": "2015",
                "id": 270
            }
        ]
    },
    {
        "title": "Domain Generalization for Object Recognition with Multi-task Autoencoders",
        "authors": [
            "Muhammad Ghifary",
            "W. Bastiaan Kleijn",
            "Mengjie Zhang",
            "David Balduzzi"
        ],
        "abstract": "The problem of domain generalization is to take knowledge acquired from a number of related domains, where training data is available, and to then successfully apply it to previously unseen domains. We propose a new feature learning algorithm, Multi-Task Autoencoder (MTAE), that provides good generalization performance for cross-domain object recognition. The algorithm extends the standard denoising autoencoder framework by substituting artificially induced corruption with naturally occurring inter-domain variability in the appearance of objects. Instead of reconstructing images from noisy versions, MTAE learns to transform the original image into analogs in multiple related domains. It thereby learns features that are robust to variations across domains. The learnt features are then used as inputs to a classifier. We evaluated the performance of the algorithm on benchmark image recognition datasets, where the task is to learn features from multiple datasets and to then predict the image label from unseen datasets. We found that (denoising) MTAE outperforms alternative autoencoder-based models as well as the current state-of-the-art algorithms for domain generalization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410650",
        "reference_list": [
            {
                "year": "2013",
                "id": 206
            }
        ],
        "citation": {
            "ieee": 22,
            "other": 20,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Object recognition",
                "Noise reduction",
                "Feature extraction",
                "Standards",
                "Robustness",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image denoising",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "domain generalization",
                "multitask autoencoder",
                "feature learning algorithm",
                "cross-domain object recognition",
                "standard denoising autoencoder",
                "MTAE",
                "image recognition"
            ]
        },
        "id": 284,
        "cited_by": [
            {
                "year": "2017",
                "id": 581
            },
            {
                "year": "2017",
                "id": 599
            }
        ]
    },
    {
        "title": "Square Localization for Efficient and Accurate Object Detection",
        "authors": [
            "Cewu Lu",
            "Yongyi Lu",
            "Hao Chen",
            "Chi-Keung Tang"
        ],
        "abstract": "The key contribution of this paper is the compact square object localization, which relaxes the exhaustive sliding window from testing all windows of different combinations of aspect ratios. Square object localization is category scalable. By using a binary search strategy, the number of scales to test is further reduced empirically to only O(log(min{H, W})) rounds of sliding CNNs, where H and W are respectively the image height and width. In the training phase, square CNN models and object co-presence priors are learned. In the testing phase, sliding CNN models are applied which produces a set of response maps that can be effectively filtered by the learned co-presence prior to output the final bounding boxes for localizing an object. We performed extensive experimental evaluation on the VOC 2007 and 2012 datasets to demonstrate that while efficient, square localization can output precise bounding boxes to improve the final detection result.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410651",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Testing",
                "Search problems",
                "Training",
                "Proposals",
                "Graphics processing units",
                "Computer vision",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "image filtering",
                "neural nets",
                "object detection",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "response map filtering",
                "binary search strategy",
                "sliding CNN model",
                "convolutional neural network",
                "object detection",
                "square object localization"
            ]
        },
        "id": 285,
        "cited_by": []
    },
    {
        "title": "Box Aggregation for Proposal Decimation: Last Mile of Object Detection",
        "authors": [
            "Shu Liu",
            "Cewu Lu",
            "Jiaya Jia"
        ],
        "abstract": "Regions-with-convolutional-neural-network (RCNN) is now a commonly employed object detection pipeline. Its main steps, i.e., proposal generation and convolutional neural network (CNN) feature extraction, have been intensively investigated. We focus on the last step of the system to aggregate thousands of scored box proposals into final object prediction, which we call proposal decimation. We show this step can be enhanced with a very simple box aggregation function by considering statistical properties of proposals with respect to ground truth objects. Our method is with extremely light-weight computation, while it yields an improvement of 3.7% in mAP on PASCAL VOC 2007 test. We explain why it works using some statistics in this paper.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410652",
        "reference_list": [
            {
                "year": "2011",
                "id": 238
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Object detection",
                "Correlation",
                "Feature extraction",
                "Computational modeling",
                "Mercury (metals)",
                "Pipelines"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "feature extraction",
                "neural nets",
                "object detection",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "proposal decimation",
                "object detection",
                "regions-with-convolutional-neural-network",
                "RCNN",
                "feature extraction",
                "object prediction",
                "box aggregation function",
                "PASCAL VOC 2007 test",
                "statistical properties",
                "ground truth objects"
            ]
        },
        "id": 286,
        "cited_by": []
    },
    {
        "title": "DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers",
        "authors": [
            "Amir Ghodrati",
            "Ali Diba",
            "Marco Pedersoli",
            "Tinne Tuytelaars",
            "Luc Van Gool"
        ],
        "abstract": "In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the generation of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate, it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the-art detection performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410653",
        "reference_list": [
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2015",
                "id": 160
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2013",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 18,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Feature extraction",
                "Detectors",
                "Image edge detection",
                "Object detection",
                "Pipelines",
                "Aggregates"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "neural nets",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "DeepProposal",
                "deep convolutional layers",
                "activation layer quality evaluation",
                "convolutional neural network",
                "object proposal generation",
                "hypothesis generation",
                "feature map coarseness",
                "object localization",
                "CNN features",
                "inverse cascade",
                "feature extraction",
                "integral images",
                "CNN-based detector",
                "state-of-the-art detection performance"
            ]
        },
        "id": 287,
        "cited_by": [
            {
                "year": "2017",
                "id": 189
            }
        ]
    },
    {
        "title": "Semantic Segmentation with Object Clique Potential",
        "authors": [
            "Xiaojuan Qi",
            "Jianping Shi",
            "Shu Liu",
            "Renjie Liao",
            "Jiaya Jia"
        ],
        "abstract": "We propose an object clique potential for semantic segmentation. Our object clique potential addresses the misclassified object-part issues arising in solutions based on fully-convolutional networks. Our object clique set, compared to that yielded from segment-proposal-based approaches, is with a significantly smaller size, making our method consume notably less computation. Regarding system design and model formation, our object clique potential can be regarded as a functional complement to local-appearance-based CRF models and works in synergy with these effective approaches for further performance improvement. Extensive experiments verify our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410654",
        "reference_list": [
            {
                "year": "2009",
                "id": 0
            },
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2009",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 1,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Semantics",
                "Proposals",
                "Object detection",
                "Training",
                "Computational modeling",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image segmentation",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "synergy",
                "local-appearance-based CRF models",
                "model formation",
                "object clique set",
                "fully-convolutional networks",
                "object clique potential",
                "semantic segmentation"
            ]
        },
        "id": 288,
        "cited_by": []
    },
    {
        "title": "Automatic Concept Discovery from Parallel Text and Visual Corpora",
        "authors": [
            "Chen Sun",
            "Chuang Gan",
            "Ram Nevatia"
        ],
        "abstract": "Humans connect language and vision to perceive the world. How to build a similar connection for computers? One possible way is via visual concepts, which are text terms that relate to visually discriminative entities. We propose an automatic visual concept discovery algorithm using parallel text and visual corpora, it filters text terms based on the visual discriminative power of the associated images, and groups them into concepts using visual and semantic similarities. We illustrate the applications of the discovered concepts using bidirectional image and sentence retrieval task and image tagging task, and show that the discovered concepts not only outperform several large sets of manually selected concepts significantly, but also achieves the state-of-the-art performance in the retrieval task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410655",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2013",
                "id": 338
            },
            {
                "year": "2013",
                "id": 345
            },
            {
                "year": "2013",
                "id": 54
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 10,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Roads",
                "Bicycles",
                "Semantics",
                "Detectors",
                "Vocabulary"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "information filtering",
                "parallel processing",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parallel text corpora",
                "parallel visual corpora",
                "visually discriminative entities",
                "automatic visual concept discovery algorithm",
                "text term filtering",
                "visual similarities",
                "semantic similarities",
                "bidirectional image",
                "sentence retrieval task",
                "image tagging task"
            ]
        },
        "id": 289,
        "cited_by": [
            {
                "year": "2017",
                "id": 153
            },
            {
                "year": "2017",
                "id": 191
            },
            {
                "year": "2017",
                "id": 553
            }
        ]
    },
    {
        "title": "Simpler Non-Parametric Methods Provide as Good or Better Results to Multiple-Instance Learning",
        "authors": [
            "Ragav Venkatesan",
            "Parag Shridhar Chandakkar",
            "Baoxin Li"
        ],
        "abstract": "Multiple-instance learning (MIL) is a unique learning problem in which training data labels are available only for collections of objects (called bags) instead of individual objects (called instances). A plethora of approaches have been developed to solve this problem in the past years. Popular methods include the diverse density, MILIS and DD-SVM. While having been widely used, these methods, particularly those in computer vision have attempted fairly sophisticated solutions to solve certain unique and particular configurations of the MIL space. In this paper, we analyze the MIL feature space using modified versions of traditional non-parametric techniques like the Parzen window and k-nearest-neighbour, and develop a learning approach employing distances to k-nearest neighbours of a point in the feature space. We show that these methods work as well, if not better than most recently published methods on benchmark datasets. We compare and contrast our analysis with the well-established diverse-density approach and its variants in recent literature, using benchmark datasets including the Musk, Andrews' and Corel datasets, along with a diabetic retinopathy pathology diagnosis dataset. Experimental results demonstrate that, while enjoying an intuitive interpretation and supporting fast learning, these method have the potential of delivering improved performance even for complex data arising from real-world applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410656",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Prototypes",
                "Pathology",
                "Noise measurement",
                "Computer vision",
                "Support vector machines",
                "Benchmark testing",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "learning (artificial intelligence)",
                "optimisation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric method",
                "multiple-instance learning",
                "MIL",
                "support vector machine",
                "SVM",
                "computer vision",
                "image classification",
                "optimization"
            ]
        },
        "id": 290,
        "cited_by": []
    },
    {
        "title": "Monocular Object Instance Segmentation and Depth Ordering with CNNs",
        "authors": [
            "Ziyu Zhang",
            "Alexander G. Schwing",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "In this paper we tackle the problem of instance-level segmentation and depth ordering from a single monocular image. Towards this goal, we take advantage of convolutional neural nets and train them to directly predict instance-level segmentations where the instance ID encodes the depth ordering within image patches. To provide a coherent single explanation of an image we develop a Markov random field which takes as input the predictions of convolutional neural nets applied at overlapping patches of different resolutions, as well as the output of a connected component algorithm. It aims to predict accurate instance-level segmentation and depth ordering. We demonstrate the effectiveness of our approach on the challenging KITTI benchmark and show good performance on both tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410657",
        "reference_list": [
            {
                "year": "2007",
                "id": 144
            },
            {
                "year": "2013",
                "id": 380
            },
            {
                "year": "2013",
                "id": 373
            },
            {
                "year": "2013",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 30,
            "other": 11,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Labeling",
                "Three-dimensional displays",
                "Neural networks",
                "Automobiles",
                "Image resolution",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "image segmentation",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "monocular object instance segmentation",
                "depth ordering",
                "CNN",
                "single monocular image",
                "instance ID encode",
                "image patch",
                "coherent single explanation",
                "Markov random field",
                "connected component algorithm",
                "KITTI benchmark"
            ]
        },
        "id": 291,
        "cited_by": [
            {
                "year": "2017",
                "id": 81
            }
        ]
    },
    {
        "title": "Multimodal Convolutional Neural Networks for Matching Image and Sentence",
        "authors": [
            "Lin Ma",
            "Zhengdong Lu",
            "Lifeng Shang",
            "Hang Li"
        ],
        "abstract": "In this paper, we propose multimodal convolutional neural networks (m-CNNs) for matching image and sentence. Our m-CNN provides an end-to-end framework with convolutional architectures to exploit image representation, word composition, and the matching relations between the two modalities. More specifically, it consists of one image CNN encoding the image content and one matching CNN modeling the joint representation of image and sentence. The matching CNN composes different semantic fragments from words and learns the inter-modal relations between image and the composed fragments at different levels, thus fully exploit the matching relations between image and sentence. Experimental results demonstrate that the proposed m-CNNs can effectively capture the information necessary for image and sentence matching. More specifically, our proposed m-CNNs significantly outperform the state-of-the-art approaches for bidirectional image and sentence retrieval on the Flickr8K and Flickr30K datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410658",
        "reference_list": [
            {
                "year": "2013",
                "id": 209
            }
        ],
        "citation": {
            "ieee": 38,
            "other": 36,
            "total": 74
        },
        "keywords": {
            "IEEE Keywords": [
                "Convolution",
                "Image representation",
                "Semantics",
                "Neural networks",
                "Computer architecture",
                "Natural languages",
                "Grounding"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image representation",
                "image retrieval",
                "natural language processing",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multimodal convolutional neural network",
                "image matching",
                "sentence matching",
                "m-CNN",
                "image representation",
                "word composition",
                "image encoding",
                "image content",
                "sentence retrieval",
                "image retrieval",
                "Flickr8K dataset",
                "Flickr30K dataset"
            ]
        },
        "id": 292,
        "cited_by": [
            {
                "year": "2017",
                "id": 61
            },
            {
                "year": "2017",
                "id": 203
            },
            {
                "year": "2017",
                "id": 275
            },
            {
                "year": "2017",
                "id": 431
            },
            {
                "year": "2017",
                "id": 432
            }
        ]
    },
    {
        "title": "Structural Kernel Learning for Large Scale Multiclass Object Co-detection",
        "authors": [
            "Zeeshan Hayder",
            "Xuming He",
            "Mathieu Salzmann"
        ],
        "abstract": "Exploiting contextual relationships across images has recently proven key to improve object detection. The resulting object co-detection algorithms, however, fail to exploit the correlations between multiple classes and, for scalability reasons are limited to modeling object instance similarity with relatively low-dimensional hand-crafted features. Here, we address the problem of multiclass object co-detection for large scale datasets. To this end, we formulate co-detection as the joint multiclass labeling of object candidates obtained in a class-independent manner. To exploit the correlations between objects, we build a fully-connected CRF on the candidates, which explicitly incorporates both geometric layout relations across object classes and similarity relations across multiple images. We then introduce a structural boosting algorithm that lets us exploits rich, high-dimensional deep network features to learn object similarity within our fully-connected CRF. Our experiments on PASCAL VOC 2007 and 2012 evidences the benefits of our approach over object detection with RCNN, single-image CRF methods and state-of-the-art co-detection algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410659",
        "reference_list": [
            {
                "year": "2009",
                "id": 29
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 3,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Conferences",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structural kernel learning",
                "multiclass object co-detection",
                "joint multiclass labeling",
                "fully-connected CRF",
                "structural boosting algorithm",
                "object similarity learning",
                "RCNN",
                "single-image CRF methods",
                "conditional random fields"
            ]
        },
        "id": 293,
        "cited_by": []
    },
    {
        "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models",
        "authors": [
            "Bryan A. Plummer",
            "Liwei Wang",
            "Chris M. Cervantes",
            "Juan C. Caicedo",
            "Julia Hockenmaier",
            "Svetlana Lazebnik"
        ],
        "abstract": "The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding. We present experiments demonstrating the usefulness of our annotations for text-to-image reference resolution, or the task of localizing textual entity mentions in an image, and for bidirectional image-sentence retrieval. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and a caption remains really challenging.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410660",
        "reference_list": [],
        "citation": {
            "ieee": 57,
            "other": 47,
            "total": 104
        },
        "keywords": {
            "IEEE Keywords": [
                "Standards",
                "Benchmark testing",
                "Image resolution",
                "Grounding",
                "Glass",
                "Training",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "image retrieval",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "region-to-phrase correspondences",
                "image-to-sentence models",
                "Flickr30k dataset",
                "sentence-based image description",
                "Flickr30k entities",
                "coreference chains",
                "manually annotated bounding boxes",
                "automatic image description",
                "grounded language understanding",
                "text-to-image reference resolution",
                "textual entity mentions",
                "bidirectional image-sentence retrieval"
            ]
        },
        "id": 294,
        "cited_by": [
            {
                "year": "2017",
                "id": 85
            },
            {
                "year": "2017",
                "id": 128
            },
            {
                "year": "2017",
                "id": 130
            },
            {
                "year": "2017",
                "id": 152
            },
            {
                "year": "2017",
                "id": 191
            },
            {
                "year": "2017",
                "id": 198
            },
            {
                "year": "2017",
                "id": 442
            },
            {
                "year": "2017",
                "id": 544
            },
            {
                "year": "2017",
                "id": 553
            },
            {
                "year": "2017",
                "id": 608
            }
        ]
    },
    {
        "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture",
        "authors": [
            "David Eigen",
            "Rob Fergus"
        ],
        "abstract": "In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410661",
        "reference_list": [
            {
                "year": "2013",
                "id": 423
            }
        ],
        "citation": {
            "ieee": 317,
            "other": 173,
            "total": 490
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Estimation",
                "Labeling",
                "Image segmentation",
                "Adaptation models",
                "Spatial resolution"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convolution",
                "prediction theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale convolutional architecture",
                "computer vision tasks",
                "depth prediction",
                "surface normal estimation",
                "semantic labeling",
                "multiscale convolutional network"
            ]
        },
        "id": 295,
        "cited_by": []
    },
    {
        "title": "AttentionNet: Aggregating Weak Directions for Accurate Object Detection",
        "authors": [
            "Donggeun Yoo",
            "Sunggyun Park",
            "Joon-Young Lee",
            "Anthony S. Paek",
            "In So Kweon"
        ],
        "abstract": "We present a novel detection method using a deep convolutional neural network (CNN), named AttentionNet. We cast an object detection problem as an iterative classification problem, which is the most suitable form of a CNN. AttentionNet provides quantized weak directions pointing a target object and the ensemble of iterative predictions from AttentionNet converges to an accurate object boundary box. Since AttentionNet is a unified network for object detection, it detects objects without any separated models from the object proposal to the post bounding-box regression. We evaluate AttentionNet by a human detection task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 with an 8-layered architecture only.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410662",
        "reference_list": [
            {
                "year": "2013",
                "id": 370
            },
            {
                "year": "2015",
                "id": 114
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 8,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Object detection",
                "Training",
                "Agriculture",
                "Computer vision",
                "Computer architecture",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "learning (artificial intelligence)",
                "neural nets",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "AttentionNet",
                "object detection",
                "deep convolutional neural network",
                "CNN"
            ]
        },
        "id": 296,
        "cited_by": [
            {
                "year": "2017",
                "id": 430
            }
        ]
    },
    {
        "title": "Common Subspace for Model and Similarity: Phrase Learning for Caption Generation from Images",
        "authors": [
            "Yoshitaka Ushiku",
            "Masataka Yamaguchi",
            "Yusuke Mukuta",
            "Tatsuya Harada"
        ],
        "abstract": "Generating captions to describe images is a fundamental problem that combines computer vision and natural language processing. Recent works focus on descriptive phrases, such as \"a white dog\" to explain the visual composites of an input image. The phrases can not only express objects, attributes, events, and their relations but can also reduce visual complexity. A caption for an input image can be generated by connecting estimated phrases using a grammar model. However, because phrases are combinations of various words, the number of phrases is much larger than the number of single words. Consequently, the accuracy of phrase estimation suffers from too few training samples per phrase. In this paper, we propose a novel phrase-learning method: Common Subspace for Model and Similarity (CoSMoS). In order to overcome the shortage of training samples, CoSMoS obtains a subspace in which (a) all feature vectors associated with the same phrase are mapped as mutually close, (b) classifiers for each phrase are learned, and (c) training samples are shared among co-occurring phrases. Experimental results demonstrate that our system is more accurate than those in earlier work and that the accuracy increases when the dataset from the web increases.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410663",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 5,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Visualization",
                "Learning systems",
                "Neural networks",
                "Grammar",
                "Scalability",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "caption generation",
                "grammar model",
                "phrase-learning method",
                "common subspace for model and similarity",
                "CoSMoS",
                "feature vectors",
                "cooccurring phrases"
            ]
        },
        "id": 297,
        "cited_by": []
    },
    {
        "title": "3D-Assisted Feature Synthesis for Novel Views of an Object",
        "authors": [
            "Hao Su",
            "Fan Wang",
            "Eric Yi",
            "Leonidas Guibas"
        ],
        "abstract": "Comparing two images from different views has been a long-standing challenging problem in computer vision, as visual features are not stable under large view point changes. In this paper, given a single input image of an object, we synthesize its features for other views, leveraging an existing modestly-sized 3D model collection of related but not identical objects. To accomplish this, we study the relationship of image patches between different views of the same object, seeking what we call surrogate patches -- patches in one view whose feature content predicts well the features of a patch in another view. Based upon these surrogate relationships, we can create feature sets for all views of the latent object on a per patch basis, providing us an augmented multi-view representation of the object. We provide theoretical and empirical analysis of the feature synthesis process, and evaluate the augmented features in fine-grained image retrieval/recognition and instance retrieval tasks. Experimental results show that our synthesized features do enable view-independent comparison between images and perform significantly better than other traditional approaches in this respect.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410664",
        "reference_list": [
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 10,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Solid modeling",
                "Correlation",
                "Computer vision",
                "Visualization",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image representation",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D-assisted feature synthesis",
                "image patches",
                "image retrieval",
                "instance retrieval",
                "image recognition",
                "object augmented multiview representation"
            ]
        },
        "id": 298,
        "cited_by": []
    },
    {
        "title": "Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views",
        "authors": [
            "Hao Su",
            "Charles R. Qi",
            "Yangyan Li",
            "Leonidas J. Guibas"
        ],
        "abstract": "Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs (Convolutional Neural Networks). We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410665",
        "reference_list": [
            {
                "year": "2005",
                "id": 172
            },
            {
                "year": "2013",
                "id": 373
            }
        ],
        "citation": {
            "ieee": 100,
            "other": 79,
            "total": 179
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Solid modeling",
                "Estimation",
                "Training",
                "Deformable models",
                "Computational modeling",
                "Pipelines"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "neural nets",
                "rendering (computer graphics)",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "CNN",
                "3D model view rendering",
                "object viewpoint estimation",
                "computer vision",
                "render-based image synthesis",
                "convolutional neural networks",
                "overfit-resistant image synthesis pipeline",
                "viewpoint estimation task"
            ]
        },
        "id": 299,
        "cited_by": [
            {
                "year": "2017",
                "id": 6
            },
            {
                "year": "2017",
                "id": 60
            },
            {
                "year": "2017",
                "id": 125
            },
            {
                "year": "2017",
                "id": 131
            },
            {
                "year": "2017",
                "id": 136
            },
            {
                "year": "2017",
                "id": 167
            },
            {
                "year": "2017",
                "id": 548
            }
        ]
    },
    {
        "title": "Lost Shopping! Monocular Localization in Large Indoor Spaces",
        "authors": [
            "Shenlong Wang",
            "Sanja Fidler",
            "Raquel Urtasun"
        ],
        "abstract": "In this paper we propose a novel approach to localization in very large indoor spaces (i.e., 200+ store shopping malls) that takes a single image and a floor plan of the environment as input. We formulate the localization problem as inference in a Markov random field, which jointly reasons about text detection (localizing shop's names in the image with precise bounding boxes), shop facade segmentation, as well as camera's rotation and translation within the entire shopping mall. The power of our approach is that it does not use any prior information about appearance and instead exploits text detections corresponding to the shop names. This makes our method applicable to a variety of domains and robust to store appearance variation across countries, seasons, and illumination conditions. We demonstrate the performance of our approach in a new dataset we collected of two very large shopping malls, and show the power of holistic reasoning.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410666",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2009",
                "id": 10
            },
            {
                "year": "2009",
                "id": 237
            },
            {
                "year": "2011",
                "id": 84
            },
            {
                "year": "2013",
                "id": 44
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 8,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Layout",
                "Image segmentation",
                "Robustness",
                "Lighting",
                "Global Positioning System"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "cartography",
                "image segmentation",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "holistic reasoning",
                "illumination condition",
                "store appearance variation",
                "camera translation",
                "camera rotation",
                "shop facade segmentation",
                "bounding box",
                "text detection",
                "Markov random field",
                "localization problem",
                "floor plan",
                "store shopping mall",
                "indoor space",
                "monocular localization",
                "lost shopping"
            ]
        },
        "id": 300,
        "cited_by": [
            {
                "year": "2017",
                "id": 231
            }
        ]
    },
    {
        "title": "Camera Pose Voting for Large-Scale Image-Based Localization",
        "authors": [
            "Bernhard Zeisl",
            "Torsten Sattler",
            "Marc Pollefeys"
        ],
        "abstract": "Image-based localization approaches aim to determine the camera pose from which an image was taken. Finding correct 2D-3D correspondences between query image features and 3D points in the scene model becomes harder as the size of the model increases. Current state-of-the-art methods therefore combine elaborate matching schemes with camera pose estimation techniques that are able to handle large fractions of wrong matches. In this work we study the benefits and limitations of spatial verification compared to appearance-based filtering. We propose a voting-based pose estimation strategy that exhibits O(n) complexity in the number of matches and thus facilitates to consider much more matches than previous approaches - whose complexity grows at least quadratically. This new outlier rejection formulation enables us to evaluate pose estimation for 1-to-many matches and to surpass the state-of-the-art. At the same time, we show that using more matches does not automatically lead to a better performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410667",
        "reference_list": [
            {
                "year": "2007",
                "id": 52
            },
            {
                "year": "2011",
                "id": 84
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 34,
            "other": 13,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Shape",
                "Solid modeling",
                "Computational modeling",
                "Gravity"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computational complexity",
                "image filtering",
                "image matching",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera pose voting",
                "large-scale image-based localization",
                "2D-3D correspondences",
                "query image features",
                "image matching schemes",
                "camera pose estimation techniques",
                "spatial verification",
                "appearance-based filtering",
                "voting-based pose estimation strategy",
                "outlier rejection formulation"
            ]
        },
        "id": 301,
        "cited_by": [
            {
                "year": "2017",
                "id": 65
            },
            {
                "year": "2017",
                "id": 250
            }
        ]
    },
    {
        "title": "MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking",
        "authors": [
            "Thibaut Durand",
            "Nicolas Thome",
            "Matthieu Cord"
        ],
        "abstract": "In this work, we propose a novel Weakly Supervised Learning (WSL) framework dedicated to learn discriminative part detectors from images annotated with a global label. Our WSL method encompasses three main contributions. Firstly, we introduce a new structured output latent variable model, Minimum mAximum lateNt sTRucturAl SVM (MANTRA), which prediction relies on a pair of latent variables: h + (resp. h - ) provides positive (resp. negative) evidence for a given output y. Secondly, we instantiate MANTRA for two different visual recognition tasks: multi-class classification and ranking. For ranking, we propose efficient solutions to exactly solve the inference and the loss-augmented problems. Finally, extensive experiments highlight the relevance of the proposed method: MANTRA outperforms state-of-the art results on five different datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410668",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 0,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Optimization",
                "Support vector machines",
                "Detectors",
                "Predictive models",
                "Libraries",
                "Visualization"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "image classification",
                "learning (artificial intelligence)",
                "minimax techniques",
                "neural nets",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "CNN",
                "convolutional neural network",
                "WSL framework",
                "weakly supervised learning",
                "image ranking",
                "image classification",
                "support vector machine",
                "minimum maximum latent structural SVM",
                "MANTRA"
            ]
        },
        "id": 302,
        "cited_by": []
    },
    {
        "title": "DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving",
        "authors": [
            "Chenyi Chen",
            "Ari Seff",
            "Alain Kornhauser",
            "Jianxiong Xiao"
        ],
        "abstract": "Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410669",
        "reference_list": [
            {
                "year": "2013",
                "id": 172
            },
            {
                "year": "2013",
                "id": 381
            }
        ],
        "citation": {
            "ieee": 167,
            "other": 91,
            "total": 258
        },
        "keywords": {
            "IEEE Keywords": [
                "Roads",
                "Games",
                "Automobiles",
                "Training",
                "Neural networks",
                "Robots",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer games",
                "computer vision",
                "neural nets",
                "road traffic",
                "traffic engineering computing",
                "virtual reality"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "KITTI dataset",
                "car distance estimation",
                "virtual environments",
                "video game",
                "convolutional neural network",
                "direct perception representation",
                "behavior reflex",
                "traffic state",
                "road state",
                "mediated perception approaches",
                "vision-based autonomous driving systems"
            ]
        },
        "id": 303,
        "cited_by": [
            {
                "year": "2017",
                "id": 50
            },
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2017",
                "id": 310
            },
            {
                "year": "2017",
                "id": 317
            }
        ]
    },
    {
        "title": "Active Transfer Learning with Zero-Shot Priors: Reusing Past Datasets for Future Tasks",
        "authors": [
            "E. Gavves",
            "T. Mensink",
            "T. Tommasi",
            "C. G. M. Snoek",
            "T. Tuytelaars"
        ],
        "abstract": "How can we reuse existing knowledge, in the form of available datasets, when solving a new and apparently unrelated target task from a set of unlabeled data? In this work we make a first contribution to answer this question in the context of image classification. We frame this quest as an active learning problem and use zero-shot classifiers to guide the learning process by linking the new task to the the existing classifiers. By revisiting the dual formulation of adaptive SVM, we reveal two basic conditions to choose greedily only the most relevant samples to be annotated. On this basis we propose an effective active learning algorithm which learns the best possible target classification model with minimum human labeling effort. Extensive experiments on two challenging datasets show the value of our approach compared to the state-of-the-art active learning methodologies, as well as its potential to reuse past datasets with minimal effort for future tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410670",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2011",
                "id": 177
            },
            {
                "year": "2011",
                "id": 236
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 15,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Training",
                "Computer vision",
                "Labeling",
                "Focusing",
                "Predictive models",
                "Context"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "active learning methodology",
                "human labeling effort",
                "target classification model",
                "active learning algorithm",
                "adaptive SVM",
                "dual formulation",
                "learning process",
                "zero-shot classifier",
                "active learning problem",
                "image classification",
                "unlabeled data",
                "target task",
                "zero-shot prior",
                "active transfer learning"
            ]
        },
        "id": 304,
        "cited_by": [
            {
                "year": "2017",
                "id": 555
            }
        ]
    },
    {
        "title": "HD-CNN: Hierarchical Deep Convolutional Neural Networks for Large Scale Visual Recognition",
        "authors": [
            "Zhicheng Yan",
            "Hao Zhang",
            "Robinson Piramuthu",
            "Vignesh Jagadeesh",
            "Dennis DeCoste",
            "Wei Di",
            "Yizhou Yu"
        ],
        "abstract": "In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a two-level category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HDCNN training, component-wise pretraining is followed by global fine-tuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for largescale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different two-level HD-CNNs, and they lower the top-1 error of the standard CNNs by 2:65%, 3:1%, and 1:1%.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410671",
        "reference_list": [
            {
                "year": "2011",
                "id": 263
            },
            {
                "year": "2007",
                "id": 224
            }
        ],
        "citation": {
            "ieee": 46,
            "other": 30,
            "total": 76
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Visualization",
                "Feature extraction",
                "Neural networks",
                "Probabilistic logic",
                "Training data",
                "Computer architecture"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "image classification",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "HD-CNN",
                "hierarchical deep convolutional neural networks",
                "large scale visual recognition",
                "image classification",
                "visual separability",
                "flat N-way classifiers",
                "coarse category classifier",
                "fine category classifiers",
                "layer parameter compression",
                "CIFAR100 datasets",
                "ImageNet 1000-class benchmark datasets"
            ]
        },
        "id": 305,
        "cited_by": [
            {
                "year": "2017",
                "id": 52
            },
            {
                "year": "2017",
                "id": 56
            },
            {
                "year": "2017",
                "id": 337
            }
        ]
    },
    {
        "title": "Learning the Structure of Deep Convolutional Networks",
        "authors": [
            "Jiashi Feng",
            "Trevor Darrell"
        ],
        "abstract": "In this work, we develop a novel method for automatically learning aspects of the structure of a deep model, in order to improve its performance, especially when labeled training data are scarce. We propose a new convolutional neural network model with the Indian Buffet Process (IBP) prior, termed ibpCNN. The ibpCNN automatically adapts its structure to provided training data, achieves an optimal balance among model complexity, data fidelity and training loss, and thus offers better generalization performance. The proposed ibpCNN captures complicated data distribution in an unsupervised generative way. Therefore, ibpCNN can exploit unlabeled data -- which can be collected at low cost -- to learn its structure. After determining the structure, ibpCNN further learns its parameters according to specified tasks, in an end-to-end fashion, and produces discriminative yet compact representations. We evaluate the performance of ibpCNN, on fully-and semi-supervised image classification tasks, ibpCNN surpasses standard CNN models on benchmark datasets, with much smaller size and higher efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410672",
        "reference_list": [
            {
                "year": "2011",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Adaptation models",
                "Training data",
                "Complexity theory",
                "Training",
                "Neural networks",
                "Convolutional codes"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "image classification",
                "learning (artificial intelligence)",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep convolutional networks",
                "convolutional neural network model",
                "Indian Buffet Process",
                "IBP prior",
                "ibpCNN",
                "model complexity",
                "data fidelity",
                "training loss",
                "data distribution",
                "semisupervised image classification",
                "CNN models"
            ]
        },
        "id": 306,
        "cited_by": [
            {
                "year": "2017",
                "id": 323
            }
        ]
    },
    {
        "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
        "authors": [
            "Alexey Dosovitskiy",
            "Philipp Fischer",
            "Eddy Ilg",
            "Philip H\u00e4usser",
            "Caner Hazirbas",
            "Vladimir Golkov",
            "Patrick van der Smagt",
            "Daniel Cremers",
            "Thomas Brox"
        ],
        "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410673",
        "reference_list": [
            {
                "year": "2013",
                "id": 214
            },
            {
                "year": "2009",
                "id": 213
            },
            {
                "year": "2013",
                "id": 172
            },
            {
                "year": "2011",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 244,
            "other": 205,
            "total": 449
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical imaging",
                "Computer architecture",
                "Image resolution",
                "Correlation",
                "Optical fiber networks",
                "Neural networks",
                "Optical computing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "learning (artificial intelligence)",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "FlowNet",
                "optical flow learning",
                "convolutional neural network",
                "CNN",
                "computer vision",
                "opticalflow estimation problem",
                "supervised learning",
                "generic architecture",
                "ground truth data",
                "synthetic flying chair dataset"
            ]
        },
        "id": 307,
        "cited_by": [
            {
                "year": "2017",
                "id": 27
            },
            {
                "year": "2017",
                "id": 32
            },
            {
                "year": "2017",
                "id": 42
            },
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2017",
                "id": 110
            },
            {
                "year": "2017",
                "id": 123
            },
            {
                "year": "2017",
                "id": 148
            },
            {
                "year": "2017",
                "id": 164
            },
            {
                "year": "2017",
                "id": 182
            },
            {
                "year": "2017",
                "id": 184
            },
            {
                "year": "2017",
                "id": 188
            },
            {
                "year": "2017",
                "id": 189
            },
            {
                "year": "2017",
                "id": 220
            },
            {
                "year": "2017",
                "id": 233
            },
            {
                "year": "2017",
                "id": 237
            },
            {
                "year": "2017",
                "id": 270
            },
            {
                "year": "2017",
                "id": 282
            },
            {
                "year": "2017",
                "id": 291
            },
            {
                "year": "2017",
                "id": 306
            },
            {
                "year": "2017",
                "id": 320
            },
            {
                "year": "2017",
                "id": 338
            },
            {
                "year": "2017",
                "id": 428
            },
            {
                "year": "2017",
                "id": 457
            },
            {
                "year": "2017",
                "id": 468
            },
            {
                "year": "2017",
                "id": 468
            },
            {
                "year": "2017",
                "id": 469
            },
            {
                "year": "2017",
                "id": 496
            },
            {
                "year": "2017",
                "id": 529
            },
            {
                "year": "2017",
                "id": 594
            }
        ]
    },
    {
        "title": "Learning Semi-Supervised Representation Towards a Unified Optimization Framework for Semi-Supervised Learning",
        "authors": [
            "Chun-Guang Li",
            "Zhouchen Lin",
            "Honggang Zhang",
            "Jun Guo"
        ],
        "abstract": "State of the art approaches for Semi-Supervised Learning (SSL) usually follow a two-stage framework -- constructing an affinity matrix from the data and then propagating the partial labels on this affinity matrix to infer those unknown labels. While such a two-stage framework has been successful in many applications, solving two subproblems separately only once is still suboptimal because it does not fully exploit the correlation between the affinity and the labels. In this paper, we formulate the two stages of SSL into a unified optimization framework, which learns both the affinity matrix and the unknown labels simultaneously. In the unified framework, both the given labels and the estimated labels are used to learn the affinity matrix and to infer the unknown labels. We solve the unified optimization problem via an alternating direction method of multipliers combined with label propagation. Extensive experiments on a synthetic data set and several benchmark data sets demonstrate the effectiveness of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410674",
        "reference_list": [
            {
                "year": "2009",
                "id": 40
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 6,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Sparse matrices",
                "Semisupervised learning",
                "Buildings",
                "Correlation",
                "Heating",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "optimisation",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "data classification",
                "label propagation",
                "multipliers alternating direction method",
                "affinity matrix",
                "unified optimization framework",
                "SSL",
                "semisupervised representation learning"
            ]
        },
        "id": 308,
        "cited_by": []
    },
    {
        "title": "Context-Guided Diffusion for Label Propagation on Graphs",
        "authors": [
            "Kwang In Kim",
            "James Tompkin",
            "Hanspeter Pfister",
            "Christian Theobalt"
        ],
        "abstract": "Existing approaches for diffusion on graphs, e.g., for label propagation, are mainly focused on isotropic diffusion, which is induced by the commonly-used graph Laplacian regularizer. Inspired by the success of diffusivity tensors for anisotropic diffusion in image processing, we presents anisotropic diffusion on graphs and the corresponding label propagation algorithm. We develop positive definite diffusivity operators on the vector bundles of Riemannian manifolds, and discretize them to diffusivity operators on graphs. This enables us to easily define new robust diffusivity operators which significantly improve semi-supervised learning performance over existing diffusion algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410675",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Laplace equations",
                "Manifolds",
                "Anisotropic magnetoresistance",
                "Semisupervised learning",
                "Diffusion processes",
                "Image edge detection",
                "Eigenvalues and eigenfunctions"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image processing",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "context-guided diffusion",
                "label propagation",
                "graph Laplacian regularizer",
                "diffusivity tensors",
                "anisotropic diffusion",
                "image processing",
                "positive definite diffusivity operators",
                "vector bundles",
                "Riemannian manifolds",
                "semisupervised learning performance"
            ]
        },
        "id": 309,
        "cited_by": []
    },
    {
        "title": "Learning to Rank Based on Subsequences",
        "authors": [
            "Basura Fernando",
            "Efstratios Gavves",
            "Damien Muselet",
            "Tinne Tuytelaars"
        ],
        "abstract": "We present a supervised learning to rank algorithm that effectively orders images by exploiting the structure in image sequences. Most often in the supervised learning to rank literature, ranking is approached either by analysing pairs of images or by optimizing a list-wise surrogate loss function on full sequences. In this work we propose MidRank, which learns from moderately sized sub-sequences instead. These sub-sequences contain useful structural ranking information that leads to better learnability during training and better generalization during testing. By exploiting sub-sequences, the proposed MidRank improves ranking accuracy considerably on an extensive array of image ranking applications and datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410676",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Loss measurement",
                "Supervised learning",
                "Testing",
                "Image sequences",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "supervised learning",
                "rank algorithm",
                "image sequence",
                "rank literature",
                "list-wise surrogate loss function",
                "MidRank",
                "structural ranking information",
                "learnability",
                "ranking accuracy",
                "image ranking application"
            ]
        },
        "id": 310,
        "cited_by": []
    },
    {
        "title": "Unsupervised Learning of Visual Representations Using Videos",
        "authors": [
            "Xiaolong Wang",
            "Abhinav Gupta"
        ],
        "abstract": "Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a Convolutional Neural Network (CNN)? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of CNN. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that visual tracking provides the supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this CNN representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52% mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We also show that our unsupervised network can perform competitively in other tasks such as surface-normal estimation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410677",
        "reference_list": [
            {
                "year": "2015",
                "id": 158
            },
            {
                "year": "2013",
                "id": 423
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 129,
            "other": 64,
            "total": 193
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Visualization",
                "Training",
                "Unsupervised learning",
                "Semantics",
                "Clustering algorithms",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "neural nets",
                "unsupervised learning",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised learning",
                "visual representation",
                "convolutional neural network",
                "CNN",
                "semantically-labeled images",
                "deep feature space",
                "Siamese-triplet network",
                "ImageNet",
                "VOC 2012 dataset"
            ]
        },
        "id": 311,
        "cited_by": [
            {
                "year": "2017",
                "id": 63
            },
            {
                "year": "2017",
                "id": 84
            },
            {
                "year": "2017",
                "id": 108
            },
            {
                "year": "2017",
                "id": 139
            },
            {
                "year": "2017",
                "id": 164
            },
            {
                "year": "2017",
                "id": 184
            },
            {
                "year": "2017",
                "id": 190
            },
            {
                "year": "2017",
                "id": 195
            },
            {
                "year": "2017",
                "id": 216
            },
            {
                "year": "2017",
                "id": 277
            },
            {
                "year": "2017",
                "id": 298
            },
            {
                "year": "2017",
                "id": 316
            },
            {
                "year": "2017",
                "id": 338
            },
            {
                "year": "2017",
                "id": 451
            },
            {
                "year": "2017",
                "id": 461
            },
            {
                "year": "2017",
                "id": 468
            },
            {
                "year": "2017",
                "id": 618
            },
            {
                "year": "2015",
                "id": 158
            }
        ]
    },
    {
        "title": "A Nonparametric Bayesian Approach toward Stacked Convolutional Independent Component Analysis",
        "authors": [
            "Sotirios P. Chatzis",
            "Dimitrios Kosmopoulos"
        ],
        "abstract": "Unsupervised feature learning algorithms based on convolutional formulations of independent components analysis (ICA) have been demonstrated to yield state-of-the-art results in several action recognition benchmarks. However, existing approaches do not allow for the number of latent components (features) to be automatically inferred from the data in an unsupervised manner. This is a significant disadvantage of the state-of-the-art, as it results in considerable burden imposed on researchers and practitioners, who must resort to tedious cross-validation procedures to obtain the optimal number of latent features. To resolve these issues, in this paper we introduce a convolutional nonparametric Bayesian sparse ICA architecture for overcomplete feature learning from high-dimensional data. Our method utilizes an Indian buffet process prior to facilitate inference of the appropriate number of latent features under a hybrid variational inference algorithm, scalable to massive datasets. As we show, our model can be naturally used to obtain deep unsupervised hierarchical feature extractors, by greedily stacking successive model layers, similar to existing approaches. In addition, inference for this model is completely heuristics-free, thus, it obviates the need of tedious parameter tuning, which is a major challenge most deep learning approaches are faced with. We evaluate our method on several action recognition benchmarks, and exhibit its advantages over the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410678",
        "reference_list": [
            {
                "year": "2007",
                "id": 147
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 3,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Inference algorithms",
                "Training",
                "Data models",
                "Adaptation models",
                "Bayes methods",
                "Machine learning"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "convolution",
                "feature extraction",
                "image recognition",
                "independent component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonparametric Bayesian approach",
                "stacked convolutional independent component analysis",
                "unsupervised feature learning algorithms",
                "convolutional formulations",
                "action recognition benchmarks",
                "cross-validation procedures",
                "convolutional nonparametric Bayesian sparse ICA architecture",
                "overcomplete feature learning",
                "high-dimensional data",
                "Indian buffet process",
                "hybrid variational inference algorithm",
                "massive datasets",
                "deep unsupervised hierarchical feature extractors",
                "successive model layers",
                "heuristics-free",
                "parameter tuning",
                "deep learning"
            ]
        },
        "id": 312,
        "cited_by": []
    },
    {
        "title": "Robust Principal Component Analysis on Graphs",
        "authors": [
            "Nauman Shahid",
            "Vassilis Kalofolias",
            "Xavier Bresson",
            "Michael Bronstein",
            "Pierre Vandergheynst"
        ],
        "abstract": "Principal Component Analysis (PCA) is the most widely used tool for linear dimensionality reduction and clustering. Still it is highly sensitive to outliers and does not scale well with respect to the number of data samples. Robust PCA solves the first issue with a sparse penalty term. The second issue can be handled with the matrix factorization model, which is however non-convex. Besides, PCA based clustering can also be enhanced by using a graph of data similarity. In this article, we introduce a new model called 'Robust PCA on Graphs' which incorporates spectral graph regularization into the Robust PCA framework. Our proposed model benefits from 1) the robustness of principal components to occlusions and missing values, 2) enhanced low-rank recovery, 3) improved clustering property due to the graph smoothness assumption on the low-rank matrix, and 4) convexity of the resulting optimization problem. Extensive experiments on 8 benchmark, 3 video and 2 artificial datasets with corruptions clearly reveal that our model outperforms 10 other state-of-the-art models in its clustering and low-rank recovery tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410679",
        "reference_list": [],
        "citation": {
            "ieee": 25,
            "other": 7,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis",
                "Robustness",
                "Data models",
                "Sparse matrices",
                "Manifolds",
                "Laplace equations",
                "Benchmark testing"
            ],
            "INSPEC: Controlled Indexing": [
                "data reduction",
                "graph theory",
                "matrix decomposition",
                "optimisation",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust principal component analysis",
                "linear dimensionality reduction and clustering",
                "robust PCA",
                "matrix factorization model",
                "data similarity",
                "spectral graph regularization"
            ]
        },
        "id": 313,
        "cited_by": [
            {
                "year": "2017",
                "id": 453
            }
        ]
    },
    {
        "title": "Projection Bank: From High-Dimensional Data to Medium-Length Binary Codes",
        "authors": [
            "Li Liu",
            "Mengyang Yu",
            "Ling Shao"
        ],
        "abstract": "Recently, very high-dimensional feature representations, e.g., Fisher Vector, have achieved excellent performance for visual recognition and retrieval. However, these lengthy representations always cause extremely heavy computational and storage costs and even become unfeasible in some large-scale applications. A few existing techniques can transfer very high-dimensional data into binary codes, but they still require the reduced code length to be relatively long to maintain acceptable accuracies. To target a better balance between computational efficiency and accuracies, in this paper, we propose a novel embedding method called Binary Projection Bank (BPB), which can effectively reduce the very high-dimensional representations to medium-dimensional binary codes without sacrificing accuracies. Instead of using conventional single linear or bilinear projections, the proposed method learns a bank of small projections via the max-margin constraint to optimally preserve the intrinsic data similarity. We have systematically evaluated the proposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing competitive retrieval and recognition accuracies compared with state-of-the-art approaches, but with a significantly smaller memory footprint and lower coding complexity.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410680",
        "reference_list": [
            {
                "year": "2013",
                "id": 375
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 1,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Binary codes",
                "Encoding",
                "Complexity theory",
                "Videos",
                "Visualization",
                "Image coding",
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "binary codes",
                "computational complexity",
                "feature extraction",
                "image coding",
                "image recognition",
                "image retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "high-dimensional data",
                "medium-length binary codes",
                "high-dimensional feature representation",
                "Fisher vector",
                "visual recognition",
                "visual retrieval",
                "storage costs",
                "computational costs",
                "large-scale applications",
                "computational efficiency",
                "computational accuracies",
                "embedding method",
                "binary projection bank",
                "BPB",
                "very high-dimensional representation reduction",
                "medium-dimensional binary codes",
                "max-margin constraint",
                "intrinsic data similarity",
                "Flickr 1M datasets",
                "ILSVR2010 datasets",
                "UCF101 datasets",
                "coding complexity"
            ]
        },
        "id": 314,
        "cited_by": []
    },
    {
        "title": "Robust Optimization for Deep Regression",
        "authors": [
            "Vasileios Belagiannis",
            "Christian Rupprecht",
            "Gustavo Carneiro",
            "Nassir Navab"
        ],
        "abstract": "Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. In our experiments, we demonstrate faster convergence and better generalization of our robust loss function for the tasks of human pose estimation and age estimation from face images. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in four publicly available human pose estimation datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410681",
        "reference_list": [
            {
                "year": "2011",
                "id": 282
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 15,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Robustness",
                "Machine learning",
                "Convergence",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convergence",
                "face recognition",
                "image resolution",
                "neural nets",
                "optimisation",
                "pose estimation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "regression model",
                "ConvNets",
                "convolutional neural networks",
                "biweight function",
                "M-estimator",
                "image resolution",
                "convergence",
                "loss function",
                "human pose estimation",
                "face image age estimation",
                "computer vision"
            ]
        },
        "id": 315,
        "cited_by": [
            {
                "year": "2017",
                "id": 378
            }
        ]
    },
    {
        "title": "Multi-class Multi-annotator Active Learning with Robust Gaussian Process for Visual Recognition",
        "authors": [
            "Chengjiang Long",
            "Gang Hua"
        ],
        "abstract": "Active learning is an effective way to relieve the tedious work of manual annotation in many applications of visual recognition. However, less research attention has been focused on multi-class active learning. In this paper, we propose a novel Gaussian process classifier model with multiple annotators for multi-class visual recognition. Expectation propagation (EP) is adopted for efficient approximate Bayesian inference of our probabilistic model for classification. Based on the EP approximation inference, a generalized Expectation Maximization (GEM) algorithm is derived to estimate both the parameters for instances and the quality of each individual annotator. Also, we incorporate the idea of reinforcement learning to actively select both the informative samples and the high-quality annotators, which better explores the trade-off between exploitation and exploration. The experiments clearly demonstrate the efficacy of the proposed model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410682",
        "reference_list": [
            {
                "year": "2013",
                "id": 150
            },
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2011",
                "id": 177
            },
            {
                "year": "2013",
                "id": 98
            },
            {
                "year": "2013",
                "id": 374
            },
            {
                "year": "2003",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 5,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Gaussian processes",
                "Visualization",
                "Bayes methods",
                "Learning (artificial intelligence)",
                "Noise measurement",
                "Mathematical model",
                "Probabilistic logic"
            ],
            "INSPEC: Controlled Indexing": [
                "expectation-maximisation algorithm",
                "Gaussian processes",
                "image classification",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "GEM algorithm",
                "generalized expectation maximization algorithm",
                "EP approximation inference",
                "probabilistic classification model",
                "approximate Bayesian inference",
                "EP",
                "expectation propagation",
                "novel Gaussian process classifier model",
                "visual recognition",
                "multiclass multiannotator active learning"
            ]
        },
        "id": 316,
        "cited_by": []
    },
    {
        "title": "Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose Estimation",
        "authors": [
            "Sijin Li",
            "Weichen Zhang",
            "Antoni B. Chan"
        ],
        "abstract": "This paper focuses on structured-output learning using deep neural networks for 3D human pose estimation from monocular images. Our network takes an image and 3D pose as inputs and outputs a score value, which is high when the image-pose pair matches and low otherwise. The network structure consists of a convolutional neural network for image feature extraction, followed by two sub-networks for transforming the image features and pose into a joint embedding. The score function is then the dot-product between the image and pose embeddings. The image-pose embedding and score function are jointly trained using a maximum-margin cost function. Our proposed framework can be interpreted as a special form of structured support vector machines where the joint feature space is discriminatively learned using deep neural networks. We test our framework on the Human3.6m dataset and obtain state-of-the-art results compared to other recent methods. Finally, we present visualizations of the image-pose embedding space, demonstrating the network has learned a high-level embedding of body-orientation and pose-configuration.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410683",
        "reference_list": [
            {
                "year": "2009",
                "id": 148
            },
            {
                "year": "2011",
                "id": 282
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 20,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Three-dimensional displays",
                "Neural networks",
                "Cost function",
                "Support vector machines",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image matching",
                "learning (artificial intelligence)",
                "neural nets",
                "pose estimation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pose-configuration",
                "body-orientation",
                "image-pose embedding space visualizations",
                "Human3.6m dataset",
                "joint feature space",
                "structured support vector machines",
                "maximum-margin cost function",
                "dot-product",
                "score function",
                "image feature extraction",
                "convolutional neural network",
                "image-pose pair matching",
                "monocular images",
                "deep neural networks",
                "structured-output learning",
                "3D human pose estimation",
                "maximum-margin structured learning"
            ]
        },
        "id": 317,
        "cited_by": [
            {
                "year": "2017",
                "id": 274
            },
            {
                "year": "2017",
                "id": 278
            },
            {
                "year": "2017",
                "id": 363
            },
            {
                "year": "2017",
                "id": 415
            },
            {
                "year": "2017",
                "id": 579
            }
        ]
    },
    {
        "title": "An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections",
        "authors": [
            "Yu Cheng",
            "Felix X. Yu",
            "Rogerio S. Feris",
            "Sanjiv Kumar",
            "Alok Choudhary",
            "Shi-Fu Chang"
        ],
        "abstract": "We explore the redundancy of parameters in deep neural networks by replacing the conventional linear projection in fully-connected layers with the circulant projection. The circulant structure substantially reduces memory footprint and enables the use of the Fast Fourier Transform to speed up the computation. Considering a fully-connected neural network layer with d input nodes, and d output nodes, this method improves the time complexity from O(d 2 ) to O(dlogd) and space complexity from O(d 2 ) to O(d). The space savings are particularly important for modern deep convolutional neural network architectures, where fully-connected layers typically contain more than 90% of the network parameters. We further show that the gradient computation and optimization of the circulant projections can be performed very efficiently. Our experiments on three standard datasets show that the proposed approach achieves this significant gain in storage and efficiency with minimal increase in error rate compared to neural networks with unstructured projections.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410684",
        "reference_list": [
            {
                "year": "2013",
                "id": 97
            },
            {
                "year": "2013",
                "id": 344
            },
            {
                "year": "2011",
                "id": 238
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 17,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Neural networks",
                "Computational modeling",
                "Training",
                "Complexity theory",
                "Sparse matrices",
                "Computer architecture",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "fast Fourier transforms",
                "feedforward neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parameter redundancy exploration",
                "deep convolutional neural network architecture",
                "circulant projection",
                "circulant structure",
                "memory footprint reduction",
                "fast Fourier transform",
                "fully-connected neural network layer",
                "time complexity",
                "space complexity",
                "gradient computation",
                "unstructured projection"
            ]
        },
        "id": 318,
        "cited_by": [
            {
                "year": "2017",
                "id": 591
            },
            {
                "year": "2015",
                "id": 326
            }
        ]
    },
    {
        "title": "Additive Nearest Neighbor Feature Maps",
        "authors": [
            "Zhenzhen Wang",
            "Xiao-Tong Yuan",
            "Qingshan Liu",
            "Shuicheng Yan"
        ],
        "abstract": "In this paper, we present a concise framework to approximately construct feature maps for nonlinear additive kernels such as the Intersection, Hellinger's, and X 2 kernels. The core idea is to construct for each individual feature a set of anchor points and assign to every query the feature map of its nearest neighbor or the weighted combination of those of its k-nearest neighbors in the anchors. The resultant feature maps can be compactly stored by a group of nearest neighbor (binary) indication vectors along with the anchor feature maps. The approximation error of such an anchored feature mapping approach is analyzed. We evaluate the performance of our approach on large-scale nonlinear support vector machines~(SVMs) learning tasks in the context of visual object classification. Experimental results on several benchmark data sets show the superiority of our method over existing feature mapping methods in achieving reasonable trade-off between training time and testing accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410685",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Additives",
                "Training",
                "Computer vision",
                "Approximation error",
                "Optimization",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "additive nearest neighbor feature maps",
                "nonlinear additive kernels",
                "k-nearest neighbors",
                "nearest neighbor binary indication vectors",
                "anchor feature maps",
                "nonlinear SVM learning",
                "support vector machines",
                "visual object classification"
            ]
        },
        "id": 319,
        "cited_by": []
    },
    {
        "title": "Understanding Deep Features with Computer-Generated Imagery",
        "authors": [
            "Mathieu Aubry",
            "Bryan C. Russell"
        ],
        "abstract": "We introduce an approach for analyzing the variation of features generated by convolutional neural networks (CNNs) trained on large image datasets with respect to scene factors that occur in natural images. Such factors may include object style, 3D viewpoint, color, and scene lighting configuration. Our approach analyzes CNN feature responses with respect to different scene factors by controlling for them via rendering using a large database of 3D CAD models. The rendered images are presented to a trained CNN and responses for different layers are studied with respect to the input scene factors. We perform a linear decomposition of the responses based on knowledge of the input scene factors and analyze the resulting components. In particular, we quantify their relative importance in the CNN responses and visualize them using principal component analysis. We show qualitative and quantitative results of our study on three trained CNNs: AlexNet [18], Places [43], and Oxford VGG [8]. We observe important differences across the different networks and CNN layers with respect to different scene factors and object categories. Finally, we demonstrate that our analysis based on computer-generated imagery translates to the network representation of natural images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410686",
        "reference_list": [
            {
                "year": "2011",
                "id": 161
            },
            {
                "year": "2013",
                "id": 0
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 7,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Three-dimensional displays",
                "Principal component analysis",
                "Lighting",
                "Computational modeling",
                "Rendering (computer graphics)",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image colour analysis",
                "image representation",
                "neural nets",
                "principal component analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "understanding deep features",
                "computer generated imagery",
                "convolutional neural networks",
                "image datasets",
                "object style",
                "3D viewpoint",
                "scene lighting configuration",
                "CNN feature analysis",
                "3D CAD models",
                "rendered images",
                "input scene factors",
                "linear decomposition",
                "principal component analysis",
                "object categories",
                "network representation"
            ]
        },
        "id": 320,
        "cited_by": []
    },
    {
        "title": "Interpolation on the Manifold of K Component GMMs",
        "authors": [
            "Hyunwoo J. Kim",
            "Nagesh Adluru",
            "Monami Banerjee",
            "Baba C. Vemuri",
            "Vikas Singh"
        ],
        "abstract": "Probability density functions (PDFs) are fundamental \"objects\" in mathematics with numerous applications in computer vision, machine learning and medical imaging. The feasibility of basic operations such as computing the distance between two PDFs and estimating a mean of a set of PDFs is a direct function of the representation we choose to work with. In this paper, we study the Gaussian mixture model (GMM) representation of the PDFs motivated by its numerous attractive features. (1) GMMs are arguably more interpretable than, say, square root parameterizations (2) the model complexity can be explicitly controlled by the number of components and (3) they are already widely used in many applications. The main contributions of this paper are numerical algorithms to enable basic operations on such objects that strictly respect their underlying geometry. For instance, when operating with a set of k component GMMs, a first order expectation is that the result of simple operations like interpolation and averaging should provide an object that is also a k component GMM. The literature provides very little guidance on enforcing such requirements systematically. It turns out that these tasks are important internal modules for analysis and processing of a field of ensemble average propagators (EAPs), common in diffusion weighted magnetic resonance imaging. We provide proof of principle experiments showing how the proposed algorithms for interpolation can facilitate statistical analysis of such data, essential to many neuroimaging studies. Separately, we also derive interesting connections of our algorithm with functional spaces of Gaussians, that may be of independent interest.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410687",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Interpolation",
                "Measurement",
                "Probability density function",
                "Computer vision",
                "Manifolds",
                "Geometry",
                "Entropy"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "interpolation",
                "mixture models",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "K component GMM manifolds",
                "Gaussian mixture model",
                "GMM representation",
                "probability density functions",
                "model complexity",
                "numerical algorithms",
                "first order expectation",
                "interpolation",
                "ensemble average propagators",
                "diffusion weighted magnetic resonance imaging",
                "EAP",
                "Gaussian functional spaces"
            ]
        },
        "id": 321,
        "cited_by": []
    },
    {
        "title": "Context-Aware CNNs for Person Head Detection",
        "authors": [
            "Tuan-Hung Vu",
            "Anton Osokin",
            "Ivan Laptev"
        ],
        "abstract": "Person detection is a key problem for many computer vision tasks. While face detection has reached maturity, detecting people under full variation of camera view-points, human poses, lighting conditions and occlusions is still a difficult challenge. In this work we focus on detecting human heads in natural scenes. Starting from the recent R-CNN object detector, we extend it in two ways. First, we leverage person-scene relations and propose a global CNN model trained to predict positions and scales of heads directly from the full image. Second, we explicitly model pairwise relations among the objects via energy-based model where the potentials are computed with a CNN framework. Our full combined model complements R-CNN with contextual cues derived from the scene. To train and test our model, we introduce a large dataset with 369,846 human heads annotated in 224,740 movie frames. We evaluate our method and demonstrate improvements of person head detection compared to several recent baselines on three datasets. We also show improvements of the detection speed provided by our model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410688",
        "reference_list": [
            {
                "year": "2011",
                "id": 238
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 15,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Head",
                "Training",
                "Context modeling",
                "Videos",
                "Object detection",
                "Computational modeling",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "natural scenes",
                "neural nets",
                "object detection",
                "prediction theory",
                "ubiquitous computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "context-aware CNNS",
                "person head detection",
                "computer vision",
                "human heads detection",
                "natural scenes",
                "R-CNN object detector",
                "person-scene relations",
                "global CNN model",
                "positions prediction",
                "head scales",
                "object pairwise relations",
                "energy-based model",
                "contextual cues",
                "movie frames",
                "convolutional neural network"
            ]
        },
        "id": 322,
        "cited_by": [
            {
                "year": "2017",
                "id": 536
            }
        ]
    },
    {
        "title": "Mode-Seeking on Hypergraphs for Robust Geometric Model Fitting",
        "authors": [
            "Hanzi Wang",
            "Guobao Xiao",
            "Yan Yan",
            "David Suter"
        ],
        "abstract": "In this paper, we propose a novel geometric model fitting method, called Mode-Seeking on Hypergraphs (MSH), to deal with multi-structure data even in the presence of severe outliers. The proposed method formulates geometric model fitting as a mode seeking problem on a hypergraph in which vertices represent model hypotheses and hyperedges denote data points. MSH intuitively detects model instances by a simple and effective mode seeking algorithm. In addition to the mode seeking algorithm, MSH includes a similarity measure between vertices on the hypergraph and a \"weight-aware sampling\" technique. The proposed method not only alleviates sensitivity to the data distribution, but also is scalable to large scale problems. Experimental results further demonstrate that the proposed method has significant superiority over the state-of-the-art fitting methods on both synthetic data and real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410689",
        "reference_list": [
            {
                "year": "2009",
                "id": 52
            },
            {
                "year": "2013",
                "id": 438
            },
            {
                "year": "2011",
                "id": 132
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Computational modeling",
                "Robustness",
                "Mathematical model",
                "Computer vision",
                "Weight measurement",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "graph theory",
                "image sampling",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "real image",
                "synthetic data",
                "data distribution",
                "weight-aware sampling technique",
                "similarity measure",
                "model instance detection",
                "data points",
                "hyperedge",
                "model hypotheses",
                "multistructure data",
                "MSH",
                "mode-seeking on hypergraph",
                "robust geometric model fitting"
            ]
        },
        "id": 323,
        "cited_by": []
    },
    {
        "title": "Highly-Expressive Spaces of Well-Behaved Transformations: Keeping it Simple",
        "authors": [
            "Oren Freifeld",
            "S\u00f8ren Hauberg",
            "Kayhan Batmanghelich",
            "John W. Fisher"
        ],
        "abstract": "We propose novel finite-dimensional spaces of R n \u2192 R n transformations, n \u2208 {1, 2, 3}, derived from (continuously-defined) parametric stationary velocity fields. Particularly, we obtain these transformations, which are diffeomorphisms, by fast and highly-accurate integration of continuous piecewise-affine velocity fields, we also provide an exact solution for n = 1. The simple-yet-highly-expressive proposed representation handles optional constraints (e.g., volume preservation) easily and supports convenient modeling choices and rapid likelihood evaluations (facilitating tractable inference over latent transformations). Its applications include, but are not limited to: unconstrained optimization over monotonic functions, modeling cumulative distribution functions or histograms, time warping, image registration, landmark-based warping, real-time diffeomorphic image editing. Our code is available at https://github.com/freifeld/cpabDiffeo.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410690",
        "reference_list": [
            {
                "year": "2013",
                "id": 107
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Computational modeling",
                "Histograms",
                "Computer vision",
                "Geometry",
                "Optimization",
                "Distribution functions"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "highly-expressive spaces",
                "finite-dimensional spaces",
                "parametric stationary velocity fields",
                "piecewise-affine velocity fields",
                "rapid likelihood evaluations",
                "unconstrained optimization",
                "monotonic functions",
                "cumulative distribution functions",
                "histograms",
                "time warping",
                "image registration",
                "landmark-based warping",
                "real-time diffeomorphic image editing"
            ]
        },
        "id": 324,
        "cited_by": []
    },
    {
        "title": "Entropy-Based Latent Structured Output Prediction",
        "authors": [
            "Diane Bouchacourt",
            "Sebastian Nowozin",
            "M. Pawan Kumar"
        ],
        "abstract": "Recently several generalizations of the popular latent structural SVM framework have been proposed in the literature. Broadly speaking, the generalizations can be divided into two categories: (i) those that predict the output variables while either marginalizing the latent variables or estimating their most likely values, and (ii) those that predict the output variables by minimizing an entropy-based uncertainty measure over the latent space. In order to aid their application in computer vision, we study these generalizations with the aim of identifying their strengths and weaknesses. To this end, we propose a novel prediction criterion that includes as special cases all previous prediction criteria that have been used in the literature. Specifically, our framework's prediction criterion minimizes the Acz\u00e9l and Dar\u00f3czy entropy of the output. This in turn allows us to design a learning objective that provides a unified framework (UF) for latent structured prediction. We develop a single optimization algorithm and empirically show that it is as effective as the more complex approaches that have been previously employed for latent structured prediction. Using this algorithm, we provide empirical evidence that lends support to prediction via the minimization of the latent space uncertainty.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410691",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Uncertainty",
                "Computer vision",
                "Prediction algorithms",
                "Loss measurement",
                "Predictive models",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "entropy",
                "generalisation (artificial intelligence)",
                "minimisation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "entropy-based latent structured output prediction",
                "latent structural SVM framework",
                "output variables",
                "latent variables",
                "entropy-based uncertainty measure",
                "computer vision",
                "framework prediction criterion",
                "Aczel entropy",
                "Daroczy entropy",
                "unified framework",
                "latent space uncertainty minimization"
            ]
        },
        "id": 325,
        "cited_by": []
    },
    {
        "title": "Fast Orthogonal Projection Based on Kronecker Product",
        "authors": [
            "Xu Zhang",
            "Felix X. Yu",
            "Ruiqi Guo",
            "Sanjiv Kumar",
            "Shengjin Wang",
            "Shi-Fu Chang"
        ],
        "abstract": "We propose a family of structured matrices to speed up orthogonal projections for high-dimensional data commonly seen in computer vision applications. In this, a structured matrix is formed by the Kronecker product of a series of smaller orthogonal matrices. This achieves O(d log d) computational complexity and O(log d) space complexity for d-dimensional data, a drastic improvement over the standard unstructured projections whose computational and space complexities are both O(d 2 ). The proposed structured matrices are applicable to a number of application domains, and are faster and more compact than other structured matrices used in the past. We also introduce an efficient learning procedure for optimizing such matrices in a data dependent fashion. We demonstrate the significant advantages of the proposed approach in solving the approximate nearest neighbor (ANN) image search problem with both binary embedding and quantization. We find that the orthogonality plays a very important role in solving ANN problem, since the random orthogonal Kronecker projection has already provided promising performance. Comprehensive experiments show that the proposed approach can achieve similar or better accuracy as the existing state-of-the-art but with significantly less time and memory.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410692",
        "reference_list": [
            {
                "year": "2015",
                "id": 318
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 4,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Sparse matrices",
                "Quantization (signal)",
                "Matrix converters",
                "Binary codes",
                "Computational complexity",
                "Error correction",
                "Error correction codes"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "computer vision",
                "image retrieval",
                "learning (artificial intelligence)",
                "matrix multiplication"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast orthogonal projection",
                "Kronecker product",
                "structured matrices",
                "high-dimensional data",
                "computer vision applications",
                "O(d log d) computational complexity",
                "O(log d) space complexity",
                "d-dimensional data",
                "approximate nearest neighbor image search problem",
                "quantization",
                "binary embedding",
                "ANN image search problem"
            ]
        },
        "id": 326,
        "cited_by": []
    },
    {
        "title": "PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization",
        "authors": [
            "Alex Kendall",
            "Matthew Grimes",
            "Roberto Cipolla"
        ],
        "abstract": "We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degrees accuracy for large scale outdoor scenes and 0.5m and 5 degrees accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410693",
        "reference_list": [
            {
                "year": "2011",
                "id": 295
            }
        ],
        "citation": {
            "ieee": 194,
            "other": 70,
            "total": 264
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Training",
                "Neural networks",
                "Robot vision systems",
                "Simultaneous localization and mapping",
                "Real-time systems",
                "Quaternions"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "image motion analysis",
                "neural nets",
                "real-time systems",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "PoseNet",
                "real-time 6-DOF camera relocalization",
                "monocular six degree of freedom relocalization system",
                "single RGB image",
                "convolutional neural network",
                "large scale outdoor scenes",
                "image plane regression problems",
                "transfer learning",
                "large scale classification data",
                "motion blur",
                "camera intrinsics",
                "point based SIFT registration",
                "pose feature"
            ]
        },
        "id": 327,
        "cited_by": [
            {
                "year": "2017",
                "id": 65
            },
            {
                "year": "2017",
                "id": 159
            },
            {
                "year": "2017",
                "id": 403
            },
            {
                "year": "2017",
                "id": 406
            },
            {
                "year": "2017",
                "id": 579
            }
        ]
    },
    {
        "title": "Predicting Multiple Structured Visual Interpretations",
        "authors": [
            "Debadeepta Dey",
            "Varun Ramakrishna",
            "Martial Hebert",
            "J. Andrew Bagnell"
        ],
        "abstract": "We present a simple approach for producing a small number of structured visual outputs which have high recall, for a variety of tasks including monocular pose estimation and semantic scene segmentation. Current state-of-the-art approaches learn a single model and modify inference procedures to produce a small number of diverse predictions. We take the alternate route of modifying the learning procedure to directly optimize for good, high recall sequences of structured-output predictors. Our approach introduces no new parameters, naturally learns diverse predictions and is not tied to any specific structured learning or inference procedure. We leverage recent advances in the contextual submodular maximization literature to learn a sequence of predictors and empirically demonstrate the simplicity and performance of our approach on multiple challenging vision tasks including achieving state-of-the-art results on multiple predictions for monocular pose-estimation and image foreground/background segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410694",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Predictive models",
                "Labeling",
                "Adaptation models",
                "Computer vision",
                "Semantics",
                "Inference algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image segmentation",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "optimisation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple structured visual interpretation",
                "structured visual output",
                "monocular pose estimation",
                "semantic scene segmentation",
                "inference procedure",
                "learning procedure",
                "structured-output predictor",
                "diverse prediction",
                "structured learning",
                "submodular maximization literature",
                "vision task",
                "monocular pose-estimation",
                "image foreground/background segmentation"
            ]
        },
        "id": 328,
        "cited_by": [
            {
                "year": "2017",
                "id": 378
            }
        ]
    },
    {
        "title": "Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks",
        "authors": [
            "Chunshui Cao",
            "Xianming Liu",
            "Yi Yang",
            "Yinan Yu",
            "Jiang Wang",
            "Zilei Wang",
            "Yongzhen Huang",
            "Liang Wang",
            "Chang Huang",
            "Wei Xu",
            "Deva Ramanan",
            "Thomas S. Huang"
        ],
        "abstract": "While feedforward deep convolutional neural networks (CNNs) have been a great success in computer vision, it is important to note that the human visual cortex generally contains more feedback than feedforward connections. In this paper, we will briefly introduce the background of feedbacks in the human visual cortex, which motivates us to develop a computational feedback mechanism in deep neural networks. In addition to the feedforward inference in traditional neural networks, a feedback loop is introduced to infer the activation status of hidden layer neurons according to the \"goal\" of the network, e.g., high-level semantic labels. We analogize this mechanism as \"Look and Think Twice.\" The feedback networks help better visualize and understand how deep neural networks work, and capture visual attention on expected objects, even in images with cluttered background and multiple objects. Experiments on ImageNet dataset demonstrate its effectiveness in solving tasks such as image classification and object localization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410695",
        "reference_list": [
            {
                "year": "2001",
                "id": 13
            },
            {
                "year": "2011",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 43,
            "other": 20,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Neurons",
                "Visualization",
                "Biological neural networks",
                "Feedforward neural networks",
                "Semantics",
                "Feedback loop",
                "Logic gates"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "convolution",
                "feedback",
                "feedforward neural nets",
                "inference mechanisms",
                "learning (artificial intelligence)",
                "semantic networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "top-down visual attention",
                "feedback convolutional neural network",
                "CNN",
                "computer vision",
                "computational feedback mechanism",
                "deep neural network",
                "feedforward inference",
                "feedback loop",
                "high-level semantic label"
            ]
        },
        "id": 329,
        "cited_by": [
            {
                "year": "2017",
                "id": 361
            }
        ]
    },
    {
        "title": "Matrix Backpropagation for Deep Networks with Structured Layers",
        "authors": [
            "Catalin Ionescu",
            "Orestis Vantzos",
            "Cristian Sminchisescu"
        ],
        "abstract": "Deep neural network architectures have recently produced excellent results in a variety of areas in artificial intelligence and visual recognition, well surpassing traditional shallow architectures trained using hand-designed features. The power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields, and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation. An open problem is the inclusion of layers that perform global, structured matrix computations like segmentation (e.g. normalized cuts) or higher-order pooling (e.g. log-tangent space metrics defined over the manifold of symmetric positive definite matrices) while preserving the validity and efficiency of an end-to-end deep training framework. In this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures. At the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations. We perform segmentation experiments using the BSDS and MSCOCO benchmarks and demonstrate that deep networks relying on second-order pooling and normalized cuts layers, trained end-to-end using matrix backpropagation, outperform counterparts that do not take advantage of such global layers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410696",
        "reference_list": [
            {
                "year": "2015",
                "id": 170
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 25,
            "total": 60
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer architecture",
                "Backpropagation",
                "Computational modeling",
                "Computer vision",
                "Standards",
                "Covariance matrices",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "backpropagation",
                "graph theory",
                "image segmentation",
                "matrix algebra",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "matrix backpropagation",
                "structured layers",
                "deep neural network architectures",
                "artificial intelligence",
                "visual recognition",
                "gradient-descent training procedure",
                "structured matrix computations",
                "mathematical apparatus",
                "deep computation architectures",
                "adjoint matrix variations",
                "BSDS benchmark",
                "MSCOCO benchmark",
                "second-order pooling",
                "normalized cuts layers"
            ]
        },
        "id": 330,
        "cited_by": [
            {
                "year": "2017",
                "id": 218
            },
            {
                "year": "2017",
                "id": 448
            }
        ]
    },
    {
        "title": "Introducing Geometry in Active Learning for Image Segmentation",
        "authors": [
            "Ksenia Konyushkova",
            "Raphael Sznitman",
            "Pascal Fua"
        ],
        "abstract": "We propose an Active Learning approach to training a segmentation classifier that exploits geometric priors to streamline the annotation process in 3D image volumes. To this end, we use these priors not only to select voxels most in need of annotation but to guarantee that they lie on 2D planar patch, which makes it much easier to annotate than if they were randomly distributed in the volume. A simplified version of this approach is effective in natural 2D images. We evaluated our approach on Electron Microscopy and Magnetic Resonance image volumes, as well as on natural images. Comparing our approach against several accepted baselines demonstrates a marked performance increase.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410697",
        "reference_list": [
            {
                "year": "2013",
                "id": 26
            },
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2013",
                "id": 374
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 6,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Uncertainty",
                "Three-dimensional displays",
                "Entropy",
                "Image segmentation",
                "Training",
                "Labeling",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "image classification",
                "image segmentation",
                "learning (artificial intelligence)",
                "magnetic resonance imaging",
                "natural scenes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometry",
                "active learning",
                "image segmentation",
                "segmentation classifier",
                "geometric prior",
                "annotation process",
                "3D image volume",
                "2D planar patch",
                "natural 2D image",
                "electron microscopy",
                "magnetic resonance image volume",
                "natural image"
            ]
        },
        "id": 331,
        "cited_by": []
    },
    {
        "title": "Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition",
        "authors": [
            "Heechul Jung",
            "Sihaeng Lee",
            "Junho Yim",
            "Sunjeong Park",
            "Junmo Kim"
        ],
        "abstract": "Temporal information has useful features for recognizing facial expressions. However, to manually design useful features requires a lot of effort. In this paper, to reduce this effort, a deep learning technique, which is regarded as a tool to automatically extract useful features from raw data, is adopted. Our deep network is based on two different models. The first deep network extracts temporal appearance features from image sequences, while the other deep network extracts temporal geometry features from temporal facial landmark points. These two models are combined using a new integration method in order to boost the performance of the facial expression recognition. Through several experiments, we show that the two models cooperate with each other. As a result, we achieve superior performance to other state-of-the-art methods in the CK+ and Oulu-CASIA databases. Furthermore, we show that our new integration method gives more accurate results than traditional methods, such as a weighted summation and a feature concatenation method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410698",
        "reference_list": [],
        "citation": {
            "ieee": 80,
            "other": 59,
            "total": 139
        },
        "keywords": {
            "IEEE Keywords": [
                "Image sequences",
                "Three-dimensional displays",
                "Face recognition",
                "Feature extraction",
                "Databases",
                "Image recognition",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "face recognition",
                "feature extraction",
                "image sequences",
                "learning (artificial intelligence)",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint fine-tuning",
                "deep neural networks",
                "facial expression recognition",
                "temporal information",
                "image sequences",
                "weighted summation",
                "feature concatenation method"
            ]
        },
        "id": 332,
        "cited_by": [
            {
                "year": "2017",
                "id": 335
            }
        ]
    },
    {
        "title": "Direct Intrinsics: Learning Albedo-Shading Decomposition by Convolutional Regression",
        "authors": [
            "Takuya Narihira",
            "Michael Maire",
            "Stella X. Yu"
        ],
        "abstract": "We introduce a new approach to intrinsic image decomposition, the task of decomposing a single image into albedo and shading components. Our strategy, which we term direct intrinsics, is to learn a convolutional neural network (CNN) that directly predicts output albedo and shading channels from an input RGB image patch. Direct intrinsics is a departure from classical techniques for intrinsic image decomposition, which typically rely on physically-motivated priors and graph-based inference algorithms. The large-scale synthetic ground-truth of the MPI Sintel dataset plays the key role in training direct intrinsics. We demonstrate results on both the synthetic images of Sintel and the real images of the classic MIT intrinsic image dataset. On Sintel, direct intrinsics, using only RGB input, outperforms all prior work, including methods that rely on RGB+Depth input. Direct intrinsics also generalizes across modalities, our Sintel-trained CNN produces quite reasonable decompositions on the real images of the MIT dataset. Our results indicate that the marriage of CNNs with synthetic training data may be a powerful new technique for tackling classic problems in computer vision.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410699",
        "reference_list": [
            {
                "year": "2013",
                "id": 30
            },
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2015",
                "id": 387
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 17,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Training data",
                "Computer vision",
                "Neural networks",
                "Color",
                "Terminology",
                "Deconvolution"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "inference mechanisms",
                "neural nets",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "direct intrinsics",
                "learning Albedo-shading decomposition",
                "convolutional regression",
                "intrinsic image decomposition",
                "convolutional neural network",
                "CNN",
                "shading channels",
                "RGB image patch",
                "graph based inference algorithms",
                "MPI Sintel dataset",
                "RGB input",
                "reasonable decompositions",
                "computer vision"
            ]
        },
        "id": 333,
        "cited_by": [
            {
                "year": "2017",
                "id": 238
            },
            {
                "year": "2017",
                "id": 543
            },
            {
                "year": "2015",
                "id": 387
            }
        ]
    },
    {
        "title": "Face Flow",
        "authors": [
            "Patrick Snape",
            "Anastasios Roussos",
            "Yannis Panagakis",
            "Stefanos Zafeiriou"
        ],
        "abstract": "In this paper, we propose a method for the robust and efficient computation of multi-frame optical flow in an expressive sequence of facial images. We formulate a novel energy minimisation problem for establishing dense correspondences between a neutral template and every frame of a sequence. We exploit the highly correlated nature of human expressions by representing dense facial motion using a deformation basis. Furthermore, we exploit the even higher correlation between deformations in a given input sequence by imposing a low-rank prior on the coefficients of the deformation basis, yielding temporally consistent optical flow. Our proposed model-based formulation, in conjunction with the inverse compositional strategy and low-rank matrix optimisation that we adopt, leads to a highly efficient algorithm for calculating facial flow. As experimental evaluation, we show quantitative experiments on a challenging novel benchmark of face sequences, with dense ground truth optical flow provided by motion capture data. We also provide qualitative results on a real sequence displaying fast motion and occlusions. Extensive quantitative and qualitative comparisons demonstrate that the proposed method outperforms state-of-the-art optical flow and dense non-rigid registration techniques, whilst running an order of magnitude faster.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410700",
        "reference_list": [
            {
                "year": "2011",
                "id": 292
            },
            {
                "year": "2013",
                "id": 307
            },
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical imaging",
                "Face",
                "Integrated optics",
                "Adaptive optics",
                "Estimation",
                "Optical variables control",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image capture",
                "image motion analysis",
                "image representation",
                "image sequences",
                "inverse problems",
                "matrix algebra",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiframe optical flow",
                "facial image sequence",
                "energy minimisation problem",
                "facial motion representation",
                "deformation basis",
                "inverse compositional strategy",
                "low-rank matrix optimisation",
                "motion capture data"
            ]
        },
        "id": 334,
        "cited_by": []
    },
    {
        "title": "Discriminative Low-Rank Tracking",
        "authors": [
            "Yao Sui",
            "Yafei Tang",
            "Li Zhang"
        ],
        "abstract": "Good tracking performance is in general attributed to accurate representation over previously obtained targets or reliable discrimination between the target and the surrounding background. In this work, we exploit the advantages of the both approaches to achieve a robust tracker. We construct a subspace to represent the target and the neighboring background, and simultaneously propagate their class labels via the learned subspace. Moreover, we propose a novel criterion to identify the target from numerous target candidates on each frame, which takes into account both discrimination reliability and representation accuracy. In addition, with the proposed criterion, the ambiguity in the class labels of the neighboring background samples, which often influences the reliability of discriminative tracking model, is effectively alleviated, while the training set is still kept small. Extensive experiments demonstrate that our tracker performs favourably against many other state-of-the-art trackers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410701",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2011",
                "id": 151
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 10,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Training",
                "Visualization",
                "Robustness",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "optical tracking",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative low-rank tracking",
                "target surrounding background",
                "robust tracker",
                "target representation",
                "discrimination reliability",
                "representation accuracy"
            ]
        },
        "id": 335,
        "cited_by": []
    },
    {
        "title": "SOWP: Spatially Ordered and Weighted Patch Descriptor for Visual Tracking",
        "authors": [
            "Han-Ul Kim",
            "Dae-Youn Lee",
            "Jae-Young Sim",
            "Chang-Su Kim"
        ],
        "abstract": "A simple yet effective object descriptor for visual tracking is proposed in this paper. We first decompose the bounding box of a target object into multiple patches, which are described by color and gradient histograms. Then, we concatenate the features of the spatially ordered patches to represent the object appearance. Moreover, to alleviate the impacts of background information possibly included in the bounding box, we determine patch weights using random walk with restart (RWR) simulations. The patch weights represent the importance of each patch in the description of foreground information, and are used to construct an object descriptor, called spatially ordered and weighted patch (SOWP) descriptor. We incorporate the proposed SOWP descriptor into the structured output tracking framework. Experimental results demonstrate that the proposed algorithm yields significantly better performance than the state-of-the-art trackers on a recent benchmark dataset, and also excels in another recent benchmark dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410702",
        "reference_list": [
            {
                "year": "2013",
                "id": 138
            },
            {
                "year": "2013",
                "id": 309
            },
            {
                "year": "2011",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 8,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Target tracking",
                "Visualization",
                "Color",
                "Training",
                "Benchmark testing",
                "Labeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image representation",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SOWP descriptor",
                "foreground information description",
                "patch weights representation",
                "RWR simulations",
                "random walk with restart",
                "background information",
                "gradient histograms",
                "color",
                "target object bounding box decomposition",
                "visual tracking",
                "spatially ordered and weighted patch descriptor"
            ]
        },
        "id": 336,
        "cited_by": [
            {
                "year": "2017",
                "id": 120
            },
            {
                "year": "2017",
                "id": 379
            }
        ]
    },
    {
        "title": "Live Repetition Counting",
        "authors": [
            "Ofir Levy",
            "Lior Wolf"
        ],
        "abstract": "The task of counting the number of repetitions of approximately the same action in an input video sequence is addressed. The proposed method runs online and not on the complete pre-captured video. It analyzes sequentially blocks of 20 non-consecutive frames. The cycle length within each block is evaluated using a convolutional neural network and the information is then integrated over time. The entropy of the network's predictions is used in order to automatically start and stop the repetition counter and to select the appropriate time scale. Coupled with a region of interest detection mechanism, the method is robust enough to handle real world videos, even when the camera is moving. A unique property of our method is that it is shown to successfully train on entirely unrealistic data created by synthesizing moving random patches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410703",
        "reference_list": [
            {
                "year": "2005",
                "id": 105
            },
            {
                "year": "2011",
                "id": 217
            },
            {
                "year": "2005",
                "id": 3
            },
            {
                "year": "2013",
                "id": 185
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Motion segmentation",
                "Training",
                "Convolution",
                "Detectors",
                "Computer vision",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "live repetition counting",
                "input video sequence",
                "cycle length",
                "convolutional neural network"
            ]
        },
        "id": 337,
        "cited_by": []
    },
    {
        "title": "Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor",
        "authors": [
            "Wongun Choi"
        ],
        "abstract": "In this paper, we tackle two key aspects of multiple target tracking problem: 1) designing an accurate affinity measure to associate detections and 2) implementing an efficient and accurate (near) online multiple target tracking algorithm. As for the first contribution, we introduce a novel Aggregated Local Flow Descriptor (ALFD) that encodes the relative motion pattern between a pair of temporally distant detections using long term interest point trajectories (IPTs). Leveraging on the IPTs, the ALFD provides a robust affinity measure for estimating the likelihood of matching detections regardless of the application scenarios. As for another contribution, we present a Near-Online Multi-target Tracking (NOMT) algorithm. The tracking problem is formulated as a data-association between targets and detections in a temporal window, that is performed repeatedly at every frame. While being efficient, NOMT achieves robustness via integrating multiple cues including ALFD metric, target dynamics, appearance similarity, and long term trajectory regularization into the model. Our ablative analysis verifies the superiority of the ALFD metric over the other conventional affinity metrics. We run a comprehensive experimental evaluation on two challenging tracking datasets, KITTI [16] and MOT [2] datasets. The NOMT method combined with ALFD metric achieves the best accuracy in both datasets with significant margins (about 10% higher MOTA) over the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410704",
        "reference_list": [
            {
                "year": "2009",
                "id": 194
            },
            {
                "year": "2001",
                "id": 22
            },
            {
                "year": "2009",
                "id": 33
            },
            {
                "year": "2013",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 79,
            "other": 22,
            "total": 101
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Trajectory",
                "Robustness",
                "Algorithm design and analysis",
                "Optical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "image fusion",
                "image motion analysis",
                "object detection",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "near-online multitarget tracking",
                "aggregated local flow descriptor",
                "temporally distant target detections",
                "relative motion pattern",
                "long term interest point trajectories",
                "NOMT algorithm",
                "data-association",
                "ALFD metric",
                "target dynamics",
                "appearance similarity",
                "long term trajectory regularization",
                "KITTI datasets",
                "MOT datasets"
            ]
        },
        "id": 338,
        "cited_by": [
            {
                "year": "2017",
                "id": 30
            },
            {
                "year": "2017",
                "id": 31
            },
            {
                "year": "2017",
                "id": 268
            },
            {
                "year": "2017",
                "id": 508
            }
        ]
    },
    {
        "title": "Multi-kernel Correlation Filter for Visual Tracking",
        "authors": [
            "Ming Tang",
            "Jiayi Feng"
        ],
        "abstract": "Correlation filter based trackers are ranked top in terms of performances. Nevertheless, they only employ a single kernel at a time. In this paper, we will derive a multi-kernel correlation filter (MKCF) based tracker which fully takes advantage of the invariance-discriminative power spectrums of various features to further improve the performance. Moreover, it may easily introduce location and representation errors to search several discrete scales for the proper one of the object bounding box, because normally the discrete candidate scales are determined and the corresponding feature pyramid are generated ahead of searching. In this paper, we will propose a novel and efficient scale estimation method based on optimal bisection search and fast evaluation of features. Our scale estimation method is the first one that uses the truly minimal number of layers of feature pyramid and avoids constructing the pyramid before searching for proper scales.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410705",
        "reference_list": [
            {
                "year": "2013",
                "id": 383
            },
            {
                "year": "2009",
                "id": 28
            },
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 344
            },
            {
                "year": "2007",
                "id": 36
            },
            {
                "year": "2009",
                "id": 77
            }
        ],
        "citation": {
            "ieee": 35,
            "other": 27,
            "total": 62
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Correlation",
                "Estimation",
                "Target tracking",
                "Visualization",
                "Training",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "estimation theory",
                "filtering theory",
                "object tracking",
                "spectral analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multikernel correlation filter",
                "visual tracking",
                "correlation filter based tracker",
                "MKCF based tracker",
                "invariance-discriminative power spectrum",
                "location error",
                "representation error",
                "object bounding box",
                "discrete candidate scale",
                "feature pyramid",
                "scale estimation method",
                "optimal bisection search"
            ]
        },
        "id": 339,
        "cited_by": []
    },
    {
        "title": "Joint Probabilistic Data Association Revisited",
        "authors": [
            "Seyed Hamid Rezatofighi",
            "Anton Milan",
            "Zhen Zhang",
            "Qinfeng Shi",
            "Anthony Dick",
            "Ian Reid"
        ],
        "abstract": "In this paper, we revisit the joint probabilistic data association (JPDA) technique and propose a novel solution based on recent developments in finding the m-best solutions to an integer linear program. The key advantage of this approach is that it makes JPDA computationally tractable in applications with high target and/or clutter density, such as spot tracking in fluorescence microscopy sequences and pedestrian tracking in surveillance footage. We also show that our JPDA algorithm embedded in a simple tracking framework is surprisingly competitive with state-of-the-art global tracking methods in these two applications, while needing considerably less processing time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410706",
        "reference_list": [
            {
                "year": "2009",
                "id": 194
            },
            {
                "year": "2007",
                "id": 97
            },
            {
                "year": "2013",
                "id": 362
            }
        ],
        "citation": {
            "ieee": 53,
            "other": 18,
            "total": 71
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Probabilistic logic",
                "Clutter",
                "Surveillance",
                "Kalman filters",
                "Noise measurement",
                "Time measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "integer programming",
                "linear programming",
                "object tracking",
                "probability",
                "sensor fusion"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint probabilistic data association",
                "JPDA technique",
                "integer linear program",
                "clutter density",
                "global tracking method"
            ]
        },
        "id": 340,
        "cited_by": [
            {
                "year": "2017",
                "id": 268
            },
            {
                "year": "2017",
                "id": 379
            },
            {
                "year": "2017",
                "id": 508
            },
            {
                "year": "2017",
                "id": 540
            }
        ]
    },
    {
        "title": "Tracking-by-Segmentation with Online Gradient Boosting Decision Tree",
        "authors": [
            "Jeany Son",
            "Ilchae Jung",
            "Kayoung Park",
            "Bohyung Han"
        ],
        "abstract": "We propose an online tracking algorithm that adaptively models target appearances based on an online gradient boosting decision tree. Our algorithm is particularly useful for non-rigid and/or articulated objects since it handles various deformations of the target effectively by integrating a classifier operating on individual patches and provides segmentation masks of the target as final results. The posterior of the target state is propagated over time by particle filtering, where the likelihood is computed based mainly on patch-level confidence map associated with a latent target state corresponding to each sample. Once tracking is completed in each frame, our gradient boosting decision tree is updated to adapt new data in a recursive manner. For effective evaluation of segmentation-based tracking algorithms, we construct a new ground-truth that contains pixel-level annotation of segmentation mask. We evaluate the performance of our tracking algorithm based on the measures for segmentation masks, where our algorithm illustrates superior accuracy compared to the state-of-the-art segmentation-based tracking methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410707",
        "reference_list": [
            {
                "year": "2013",
                "id": 309
            },
            {
                "year": "2011",
                "id": 10
            },
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2011",
                "id": 196
            },
            {
                "year": "2011",
                "id": 167
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 13,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Target tracking",
                "Decision trees",
                "Training",
                "Visualization",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "image classification",
                "image filtering",
                "image segmentation",
                "object tracking",
                "particle filtering (numerical methods)",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "tracking-by-segmentation",
                "online gradient boosting decision tree",
                "classifier",
                "target state posterior propagation",
                "particle filtering",
                "patch-level confidence map",
                "pixel-level segmentation masks annotation"
            ]
        },
        "id": 341,
        "cited_by": []
    },
    {
        "title": "Exploring Causal Relationships in Visual Object Tracking",
        "authors": [
            "Karel Lebeda",
            "Simon Hadfield",
            "Richard Bowden"
        ],
        "abstract": "Causal relationships can often be found in visual object tracking between the motions of the camera and that of the tracked object. This object motion may be an effect of the camera motion, e.g. an unsteady handheld camera. But it may also be the cause, e.g. the cameraman framing the object. In this paper we explore these relationships, and provide statistical tools to detect and quantify them, these are based on transfer entropy and stem from information theory. The relationships are then exploited to make predictions about the object location. The approach is shown to be an excellent measure for describing such relationships. On the VOT2013 dataset the prediction accuracy is increased by 62 % over the best non-causal predictor. We show that the location predictions are robust to camera shake and sudden motion, which is invaluable for any tracking algorithm and demonstrate this by applying causal prediction to two state-of-the-art trackers. Both of them benefit, Struck gaining a 7 % accuracy and 22 % robustness increase on the VTB1.1 benchmark, becoming the new state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410708",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Cameras",
                "Computer vision",
                "Visualization",
                "Object tracking",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "object tracking",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual object tracking",
                "causal relationships",
                "camera motion",
                "object motion",
                "statistical tools",
                "transfer entropy",
                "information theory",
                "object location",
                "VOT2013 dataset"
            ]
        },
        "id": 342,
        "cited_by": []
    },
    {
        "title": "Hierarchical Convolutional Features for Visual Tracking",
        "authors": [
            "Chao Ma",
            "Jia-Bin Huang",
            "Xiaokang Yang",
            "Ming-Hsuan Yang"
        ],
        "abstract": "Visual object tracking is challenging as target objects often undergo significant appearance changes caused by deformation, abrupt motion, background clutter and occlusion. In this paper, we exploit features extracted from deep convolutional neural networks trained on object recognition datasets to improve tracking accuracy and robustness. The outputs of the last convolutional layers encode the semantic information of targets and such representations are robust to significant appearance variations. However, their spatial resolution is too coarse to precisely localize targets. In contrast, earlier convolutional layers provide more precise localization but are less invariant to appearance changes. We interpret the hierarchies of convolutional layers as a nonlinear counterpart of an image pyramid representation and exploit these multiple levels of abstraction for visual tracking. Specifically, we adaptively learn correlation filters on each convolutional layer to encode the target appearance. We hierarchically infer the maximum response of each layer to locate targets. Extensive experimental results on a largescale benchmark dataset show that the proposed algorithm performs favorably against state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410709",
        "reference_list": [
            {
                "year": "2013",
                "id": 383
            },
            {
                "year": "2011",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 337,
            "other": 186,
            "total": 523
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Target tracking",
                "Correlation",
                "Feature extraction",
                "Semantics",
                "Spatial resolution",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "correlation methods",
                "feature extraction",
                "image coding",
                "image filtering",
                "image representation",
                "image resolution",
                "neural nets",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convolutional layer",
                "correlation filters",
                "image pyramid representation",
                "spatial resolution",
                "semantic information encoding",
                "object recognition datasets",
                "deep convolutional neural networks",
                "feature extraction",
                "visual object tracking",
                "hierarchical convolutional features"
            ]
        },
        "id": 343,
        "cited_by": [
            {
                "year": "2017",
                "id": 11
            },
            {
                "year": "2017",
                "id": 33
            },
            {
                "year": "2017",
                "id": 119
            },
            {
                "year": "2017",
                "id": 120
            },
            {
                "year": "2017",
                "id": 186
            },
            {
                "year": "2017",
                "id": 228
            },
            {
                "year": "2017",
                "id": 269
            },
            {
                "year": "2017",
                "id": 320
            },
            {
                "year": "2017",
                "id": 508
            },
            {
                "year": "2017",
                "id": 575
            }
        ]
    },
    {
        "title": "Robust Non-rigid Motion Tracking and Surface Reconstruction Using L0 Regularization",
        "authors": [
            "Kaiwen Guo",
            "Feng Xu",
            "Yangang Wang",
            "Yebin Liu",
            "Qionghai Dai"
        ],
        "abstract": "We present a new motion tracking method to robustly reconstruct non-rigid geometries and motions from single view depth inputs captured by a consumer depth sensor. The idea comes from the observation of the existence of intrinsic articulated subspace in most of non-rigid motions. To take advantage of this characteristic, we propose a novel L0 based motion regularizer with an iterative optimization solver that can implicitly constrain local deformation only on joints with articulated motions, leading to reduced solution space and physical plausible deformations. The L0 strategy is integrated into the available non-rigid motion tracking pipeline, forming the proposed L0-L2 non-rigid motion tracking method that can adaptively stop the tracking error propagation. Extensive experiments over complex human body motions with occlusions, face and hand motions demonstrate that our approach substantially improves tracking robustness and surface reconstruction accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410710",
        "reference_list": [
            {
                "year": "2009",
                "id": 21
            },
            {
                "year": "2011",
                "id": 265
            },
            {
                "year": "2011",
                "id": 92
            },
            {
                "year": "2013",
                "id": 58
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 16,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Skeleton",
                "Surface reconstruction",
                "Robustness",
                "Deformable models",
                "Three-dimensional displays",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "geometry",
                "image motion analysis",
                "iterative methods",
                "optimisation",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface reconstruction",
                "L0 regularization",
                "nonrigid geometries",
                "consumer depth sensor",
                "intrinsic articulated subspace",
                "iterative optimization solver",
                "local deformation",
                "articulated motions",
                "solution space reduction",
                "physical plausible deformations",
                "motion tracking pipeline",
                "L0-L2 nonrigid motion tracking method",
                "complex human body motions",
                "hand motions"
            ]
        },
        "id": 344,
        "cited_by": [
            {
                "year": "2017",
                "id": 94
            }
        ]
    },
    {
        "title": "Online Object Tracking with Proposal Selection",
        "authors": [
            "Yang Hua",
            "Karteek Alahari",
            "Cordelia Schmid"
        ],
        "abstract": "Tracking-by-detection approaches are some of the most successful object trackers in recent years. Their success is largely determined by the detector model they learn initially and then update over time. However, under challenging conditions where an object can undergo transformations, e.g., severe rotation, these methods are found to be lacking. In this paper, we address this problem by formulating it as a proposal selection task and making two contributions. The first one is introducing novel proposals estimated from the geometric transformations undergone by the object, and building a rich candidate set for predicting the object location. The second one is devising a novel selection strategy using multiple cues, i.e., detection score and edgeness score computed from state-of-the-art object edges and motion boundaries. We extensively evaluate our approach on the visual object tracking 2014 challenge and online tracking benchmark datasets, and show the best performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410711",
        "reference_list": [
            {
                "year": "2007",
                "id": 116
            },
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2011",
                "id": 10
            },
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2011",
                "id": 151
            },
            {
                "year": "2013",
                "id": 347
            },
            {
                "year": "2013",
                "id": 29
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 20,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Detectors",
                "Image edge detection",
                "Training",
                "Target tracking",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object detection",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online object tracking",
                "proposal selection",
                "tracking-by-detection approach",
                "object trackers",
                "detector model",
                "learning",
                "geometric transformation",
                "object location prediction",
                "detection score",
                "edgeness score",
                "object edges",
                "motion boundaries",
                "visual object tracking 2014 challenge",
                "online tracking benchmark datasets"
            ]
        },
        "id": 345,
        "cited_by": [
            {
                "year": "2017",
                "id": 33
            },
            {
                "year": "2017",
                "id": 575
            }
        ]
    },
    {
        "title": "Understanding and Diagnosing Visual Tracking Systems",
        "authors": [
            "Naiyan Wang",
            "Jianping Shi",
            "Dit-Yan Yeung",
            "Jiaya Jia"
        ],
        "abstract": "Several benchmark datasets for visual tracking research have been created in recent years. Despite their usefulness, whether they are sufficient for understanding and diagnosing the strengths and weaknesses of different trackers remains questionable. To address this issue, we propose a framework by breaking a tracker down into five constituent parts, namely, motion model, feature extractor, observation model, model updater, and ensemble post-processor. We then conduct ablative experiments on each component to study how it affects the overall result. Surprisingly, our findings are discrepant with some common beliefs in the visual tracking research community. We find that the feature extractor plays the most important role in a tracker. On the other hand, although the observation model is the focus of many studies, we find that it often brings no significant improvement. Moreover, the motion model and model updater contain many details that could affect the result. Also, the ensemble post-processor can improve the result substantially when the constituent trackers have high diversity. Based on our findings, we put together some very elementary building blocks to give a basic tracker which is competitive in performance to the state-of-the-art trackers. We believe our framework can provide a solid baseline when conducting controlled experiments for visual tracking research.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410712",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 347
            },
            {
                "year": "2013",
                "id": 81
            },
            {
                "year": "2013",
                "id": 82
            }
        ],
        "citation": {
            "ieee": 77,
            "other": 50,
            "total": 127
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Feature extraction",
                "Benchmark testing",
                "Visualization",
                "Adaptation models"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image motion analysis",
                "modelling",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual tracking system",
                "motion model",
                "feature extractor",
                "observation model",
                "model updater",
                "ensemble postprocessor"
            ]
        },
        "id": 346,
        "cited_by": [
            {
                "year": "2017",
                "id": 33
            },
            {
                "year": "2017",
                "id": 186
            }
        ]
    },
    {
        "title": "Integrating Dashcam Views through Inter-Video Mapping",
        "authors": [
            "Hsin-I Chen",
            "Yi-Ling Chen",
            "Wei-Tse Lee",
            "Fan Wang",
            "Bing-Yu Chen"
        ],
        "abstract": "In this paper, an inter-video mapping approach is proposed to integrate video footages from two dashcams installed on a preceding and its following vehicle to provide the illusion that the driver of the following vehicle can see-through the preceding one. The key challenge is to adapt the perspectives of the two videos based on a small number of common features since a large portion of the common region in the video captured by the following vehicle is occluded by the preceding one. Inspired by the observation that images with the most similar viewpoints yield dense and high-quality matches, the proposed inter-video mapping estimates spatially-varying motions across two videos utilizing images of very similar contents. Specifically, we estimate frame-to-frame motions of each two consecutive images and incrementally add new views into a merged representation. In this way, long-range motion estimation is achieved, and the observed perspective discrepancy between the two videos can be well approximated our motion estimation. Once the inter-video mapping is established, the correspondences can be updated incrementally, so the proposed method is suitable for on-line applications. Our experiments demonstrate the effectiveness of our approach on real-world challenging videos demonstrate the effectiveness of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410713",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Vehicles",
                "Trajectory",
                "Bridges",
                "Tracking",
                "Cameras",
                "Motion estimation",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "image capture",
                "motion estimation",
                "road vehicles",
                "traffic engineering computing",
                "video cameras",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dashcam views",
                "inter-video mapping",
                "video footages",
                "dashcams",
                "preceding vehicle",
                "following vehicle",
                "video capture",
                "viewpoints",
                "spatially-varying motions",
                "frame-to-frame motions estimation",
                "long-range motion estimation",
                "perspective discrepancy"
            ]
        },
        "id": 347,
        "cited_by": []
    },
    {
        "title": "Visual Tracking with Fully Convolutional Networks",
        "authors": [
            "Lijun Wang",
            "Wanli Ouyang",
            "Xiaogang Wang",
            "Huchuan Lu"
        ],
        "abstract": "We propose a new approach for general object tracking with fully convolutional neural network. Instead of treating convolutional neural network (CNN) as a black-box feature extractor, we conduct in-depth study on the properties of CNN features offline pre-trained on massive image data and classification task on ImageNet. The discoveries motivate the design of our tracking system. It is found that convolutional layers in different levels characterize the target from different perspectives. A top layer encodes more semantic features and serves as a category detector, while a lower layer carries more discriminative information and can better separate the target from distracters with similar appearance. Both layers are jointly used with a switch mechanism during tracking. It is also found that for a tracking target, only a subset of neurons are relevant. A feature map selection method is developed to remove noisy and irrelevant feature maps, which can reduce computation redundancy and improve tracking accuracy. Extensive evaluation on the widely used tracking benchmark [36] shows that the proposed tacker outperforms the state-of-the-art significantly.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410714",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 212,
            "other": 127,
            "total": 339
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Visualization",
                "Feature extraction",
                "Semantics",
                "Neurons",
                "Noise measurement",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "feature extraction",
                "feature selection",
                "image classification",
                "neural nets",
                "semantic networks",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual tracking",
                "convolutional neural network",
                "CNN",
                "black-box feature extractor",
                "image classification",
                "target tracking",
                "semantic feature",
                "category detector",
                "switch mechanism",
                "feature map selection method"
            ]
        },
        "id": 348,
        "cited_by": [
            {
                "year": "2017",
                "id": 11
            },
            {
                "year": "2017",
                "id": 21
            },
            {
                "year": "2017",
                "id": 42
            },
            {
                "year": "2017",
                "id": 118
            },
            {
                "year": "2017",
                "id": 119
            },
            {
                "year": "2017",
                "id": 120
            },
            {
                "year": "2017",
                "id": 228
            },
            {
                "year": "2017",
                "id": 269
            },
            {
                "year": "2017",
                "id": 320
            },
            {
                "year": "2017",
                "id": 508
            }
        ]
    },
    {
        "title": "Multiple Feature Fusion via Weighted Entropy for Visual Tracking",
        "authors": [
            "Lin Ma",
            "Jiwen Lu",
            "Jianjiang Feng",
            "Jie Zhou"
        ],
        "abstract": "It is desirable to combine multiple feature descriptors to improve the visual tracking performance because different features can provide complementary information to describe objects of interest. However, how to effectively fuse multiple features remains a challenging problem in visual tracking, especially in a data-driven manner. In this paper, we propose a new data-adaptive visual tracking approach by using multiple feature fusion via weighted entropy. Unlike existing visual trackers which simply concatenate multiple feature vectors together for object representation, we employ the weighted entropy to evaluate the dissimilarity between the object state and the background state, and seek the optimal feature combination by minimizing the weighted entropy, so that more complementary information can be exploited for object representation. Experimental results demonstrate the effectiveness of our approach in tackling various challenges for visual object tracking.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410715",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 81
            },
            {
                "year": "2007",
                "id": 194
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 9,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Entropy",
                "Visualization",
                "Computational modeling",
                "Robustness",
                "Object tracking",
                "Principal component analysis",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "entropy",
                "feature extraction",
                "image representation",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple feature fusion",
                "weighted entropy",
                "data-adaptive visual tracking approach",
                "visual trackers",
                "multiple feature vectors",
                "object representation",
                "object state",
                "background state",
                "optimal feature combination",
                "visual object tracking"
            ]
        },
        "id": 349,
        "cited_by": []
    },
    {
        "title": "Pedestrian Travel Time Estimation in Crowded Scenes",
        "authors": [
            "Shuai Yi",
            "Hongsheng Li",
            "Xiaogang Wang"
        ],
        "abstract": "In this paper, we target on the problem of estimating the statistic of pedestrian travel time within a period from an entrance to a destination in a crowded scene. Such estimation is based on the global distributions of crowd densities and velocities instead of complete trajectories of pedestrians, which cannot be obtained in crowded scenes. The proposed method is motivated by our statistical investigation into the correlations between travel time and global properties of crowded scenes. Active regions are created for each source-destination pair to model the probable walking regions over the corresponding source-destination traffic flow. Two sets of scene features are specially designed for modeling moving and stationary persons inside the active regions and their influences on pedestrian travel time. The estimation of pedestrian travel time provides valuable information for both crowd scene understanding and pedestrian behavior analysis, but was not sufficiently studied in literature. The effectiveness of the proposed pedestrian travel time estimation model is demonstrated through several surveillance applications, including dynamic scene monitoring, localization of regions blocking traffics, and detection of abnormal pedestrian behaviors. Many more valuable applications based on our method are to be explored in the future.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410716",
        "reference_list": [
            {
                "year": "2009",
                "id": 33
            },
            {
                "year": "2009",
                "id": 48
            },
            {
                "year": "2009",
                "id": 214
            },
            {
                "year": "2011",
                "id": 90
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 7,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Legged locomotion",
                "Correlation",
                "Estimation",
                "Trajectory",
                "Feature extraction",
                "Computer vision",
                "Surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "behavioural sciences",
                "feature extraction",
                "image motion analysis",
                "object detection",
                "pedestrians",
                "road traffic",
                "statistical analysis",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pedestrian travel time estimation",
                "statistic",
                "global distributions",
                "crowd densities",
                "crowd velocities",
                "active regions",
                "walking regions",
                "source-destination traffic flow",
                "scene features",
                "moving persons modeling",
                "stationary persons modeling",
                "crowd scene understanding",
                "pedestrian behavior analysis",
                "surveillance applications",
                "dynamic scene monitoring",
                "regions blocking traffics localization",
                "abnormal pedestrian behavior detection"
            ]
        },
        "id": 350,
        "cited_by": []
    },
    {
        "title": "Unsupervised Synchrony Discovery in Human Interaction",
        "authors": [
            "Wen-Sheng Chu",
            "Jiabei Zeng",
            "Fernando De la Torre",
            "Jeffrey F. Cohn",
            "Daniel S. Messinger"
        ],
        "abstract": "People are inherently social. Social interaction plays an important and natural role in human behavior. Most computational methods focus on individuals alone rather than in social context. They also require labelled training data. We present an unsupervised approach to discover interpersonal synchrony, referred as to two or more persons preforming common actions in overlapping video frames or segments. For computational efficiency, we develop a branch-and-bound (B&B) approach that affords exhaustive search while guaranteeing a globally optimal solution. The proposed method is entirely general. It takes from two or more videos any multi-dimensional signal that can be represented as a histogram. We derive three novel bounding functions and provide efficient extensions, including multi-synchrony detection and accelerated search, using a warm-start strategy and parallelism. We evaluate the effectiveness of our approach in multiple databases, including human actions using the CMU Mocap dataset [1], spontaneous facial behaviors using group-formation task dataset [37] and parent-infant interaction dataset [28].",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410717",
        "reference_list": [
            {
                "year": "2011",
                "id": 57
            },
            {
                "year": "2009",
                "id": 191
            },
            {
                "year": "2001",
                "id": 114
            },
            {
                "year": "2007",
                "id": 147
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Feature extraction",
                "Hidden Markov models",
                "Training data",
                "Assistive technology",
                "Time series analysis",
                "Acceleration"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "learning (artificial intelligence)",
                "tree searching",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised synchrony discovery",
                "human interaction",
                "interpersonal synchrony",
                "overlapping video frames",
                "branch-and-bound approach",
                "multi-synchrony detection and accelerated search",
                "warm-start strategy",
                "parallelism",
                "multiple databases",
                "CMU Mocap dataset",
                "spontaneous facial behaviors",
                "group-formation task dataset",
                "parent-infant interaction dataset"
            ]
        },
        "id": 351,
        "cited_by": []
    },
    {
        "title": "Efficient Video Segmentation Using Parametric Graph Partitioning",
        "authors": [
            "Chen-Ping Yu",
            "Hieu Le",
            "Gregory Zelinsky",
            "Dimitris Samaras"
        ],
        "abstract": "Video segmentation is the task of grouping similar pixels in the spatio-temporal domain, and has become an important preprocessing step for subsequent video analysis. Most video segmentation and supervoxel methods output a hierarchy of segmentations, but while this provides useful multiscale information, it also adds difficulty in selecting the appropriate level for a task. In this work, we propose an efficient and robust video segmentation framework based on parametric graph partitioning (PGP), a fast, almost parameter free graph partitioning method that identifies and removes between-cluster edges to form node clusters. Apart from its computational efficiency, PGP performs clustering of the spatio-temporal volume without requiring a pre-specified cluster number or bandwidth parameters, thus making video segmentation more practical to use in applications. The PGP framework also allows processing sub-volumes, which further improves performance, contrary to other streaming video segmentation methods where sub-volume processing reduces performance. We evaluate the PGP method using the SegTrack v2 and Chen Xiph.org datasets, and show that it outperforms related state-of-the-art algorithms in 3D segmentation metrics and running time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410718",
        "reference_list": [
            {
                "year": "2011",
                "id": 310
            },
            {
                "year": "2011",
                "id": 268
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2013",
                "id": 279
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Streaming media",
                "Image segmentation",
                "Mixture models",
                "Measurement",
                "Clustering algorithms",
                "Three-dimensional displays",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "video signal processing",
                "video streaming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parametric graph partitioning",
                "spatio-temporal domain",
                "preprocessing step",
                "subsequent video analysis",
                "supervoxel method",
                "multiscale information",
                "video segmentation framework",
                "PGP",
                "parameter free graph partitioning method",
                "between-cluster edge",
                "node cluster",
                "computational efficiency",
                "spatio-temporal volume",
                "bandwidth parameter",
                "streaming video segmentation method",
                "subvolume processing",
                "SegTrack v2",
                "Chen Xiph.org dataset",
                "3D segmentation metrics"
            ]
        },
        "id": 352,
        "cited_by": []
    },
    {
        "title": "Learning to Track for Spatio-Temporal Action Localization",
        "authors": [
            "Philippe Weinzaepfel",
            "Zaid Harchaoui",
            "Cordelia Schmid"
        ],
        "abstract": "We propose an effective approach for spatio-temporal action localization in realistic videos. The approach first detects proposals at the frame-level and scores them with a combination of static and motion CNN features. It then tracks high-scoring proposals throughout the video using a tracking-by-detection approach. Our tracker relies simultaneously on instance-level and class-level detectors. The tracks are scored using a spatio-temporal motion histogram, a descriptor at the track level, in combination with the CNN features. Finally, we perform temporal localization of the action using a sliding-window approach at the track level. We present experimental results for spatio-temporal localization on the UCF-Sports, J-HMDB and UCF-101 action localization datasets, where our approach outperforms the state of the art with a margin of 15%, 7% and 12% respectively in mAP.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410719",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 398
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2013",
                "id": 342
            }
        ],
        "citation": {
            "ieee": 62,
            "other": 30,
            "total": 92
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Tracking",
                "Videos",
                "Feature extraction",
                "Detectors",
                "Object detection",
                "Optical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "learning (artificial intelligence)",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatiotemporal action localization",
                "realistic videos",
                "frame-level",
                "static features",
                "motion CNN features",
                "tracking-by-detection approach",
                "instance-level detectors",
                "class-level detectors",
                "motion histogram",
                "sliding-window approach",
                "J-HMDB datasets",
                "UCF-101 datasets",
                "UCF-Sports datasets"
            ]
        },
        "id": 353,
        "cited_by": [
            {
                "year": "2017",
                "id": 30
            },
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 152
            },
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2017",
                "id": 306
            },
            {
                "year": "2017",
                "id": 307
            },
            {
                "year": "2017",
                "id": 382
            },
            {
                "year": "2017",
                "id": 462
            },
            {
                "year": "2017",
                "id": 466
            },
            {
                "year": "2017",
                "id": 607
            },
            {
                "year": "2017",
                "id": 609
            },
            {
                "year": "2017",
                "id": 610
            }
        ]
    },
    {
        "title": "Unsupervised Object Discovery and Tracking in Video Collections",
        "authors": [
            "Suha Kwak",
            "Minsu Cho",
            "Ivan Laptev",
            "Jean Ponce",
            "Cordelia Schmid"
        ],
        "abstract": "This paper addresses the problem of automatically localizing dominant objects as spatio-temporal tubes in a noisy collection of videos with minimal or even no supervision. We formulate the problem as a combination of two complementary processes: discovery and tracking. The first one establishes correspondences between prominent regions across videos, and the second one associates similar object regions within the same video. Interestingly, our algorithm also discovers the implicit topology of frames associated with instances of the same object class across different videos, a role normally left to supervisory information in the form of class labels in conventional image and video understanding methods. Indeed, as demonstrated by our experiments, our method can handle video collections featuring multiple object classes, and substantially outperforms the state of the art in colocalization, even though it tackles a broader problem with much less supervision.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410720",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 286
            },
            {
                "year": "2011",
                "id": 276
            },
            {
                "year": "2013",
                "id": 316
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2013",
                "id": 221
            },
            {
                "year": "2013",
                "id": 372
            },
            {
                "year": "2013",
                "id": 105
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 17,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Electron tubes",
                "Coherence",
                "Noise measurement",
                "Object tracking",
                "Robustness",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "object tracking",
                "topology",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised object discovery",
                "object tracking",
                "automatic dominant objects localization",
                "spatio-temporal tubes",
                "video noisy collection",
                "similar object regions",
                "implicit topology",
                "object class",
                "class labels",
                "image understanding methods",
                "video understanding methods"
            ]
        },
        "id": 354,
        "cited_by": [
            {
                "year": "2017",
                "id": 30
            },
            {
                "year": "2017",
                "id": 190
            },
            {
                "year": "2017",
                "id": 227
            },
            {
                "year": "2017",
                "id": 247
            }
        ]
    },
    {
        "title": "Car that Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models",
        "authors": [
            "Ashesh Jain",
            "Hema S. Koppula",
            "Bharad Raghavan",
            "Shane Soh",
            "Ashutosh Saxena"
        ],
        "abstract": "Advanced Driver Assistance Systems (ADAS) have made driving safer over the last decade. They prepare vehicles for unsafe road conditions and alert drivers if they perform a dangerous maneuver. However, many accidents are unavoidable because by the time drivers are alerted, it is already too late. Anticipating maneuvers beforehand can alert drivers before they perform the maneuver and also give ADAS more time to avoid or prepare for the danger. In this work we anticipate driving maneuvers a few seconds before they occur. For this purpose we equip a car with cameras and a computing device to capture the driving context from both inside and outside of the car. We propose an Autoregressive Input-Output HMM to model the contextual information alongwith the maneuvers. We evaluate our approach on a diverse data set with 1180 miles of natural freeway and city driving and show that we can anticipate maneuvers 3.5 seconds before they occur with over 80% F1-score in real-time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410721",
        "reference_list": [],
        "citation": {
            "ieee": 43,
            "other": 12,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Vehicles",
                "Context",
                "Context modeling",
                "Cameras",
                "Face",
                "Global Positioning System"
            ],
            "INSPEC: Controlled Indexing": [
                "automobiles",
                "autoregressive processes",
                "cameras",
                "driver information systems",
                "hidden Markov models",
                "image motion analysis",
                "learning (artificial intelligence)",
                "road accidents",
                "road safety"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "car",
                "learning",
                "temporal driving models",
                "advanced driver assistance systems",
                "ADAS",
                "safe driving",
                "vehicles",
                "dangerous maneuver",
                "accidents",
                "driving maneuvers anticipation",
                "cameras",
                "computing device",
                "autoregressive input-output HMM",
                "natural freeway",
                "city driving",
                "F1-score",
                "autoregressive input-output hidden Markov model"
            ]
        },
        "id": 355,
        "cited_by": [
            {
                "year": "2017",
                "id": 5
            },
            {
                "year": "2017",
                "id": 352
            }
        ]
    },
    {
        "title": "Activity Auto-Completion: Predicting Human Activities from Partial Videos",
        "authors": [
            "Zhen Xu",
            "Laiyun Qing",
            "Jun Miao"
        ],
        "abstract": "In this paper, we propose an activity auto-completion (AAC) model for human activity prediction by formulating activity prediction as a query auto-completion (QAC) problem in information retrieval. First, we extract discriminative patches in frames of videos. A video is represented based on these patches and divided into a collection of segments, each of which is regarded as a character typed in the search box. Then a partially observed video is considered as an activity prefix, consisting of one or more characters. Finally, the missing observation of an activity is predicted as the activity candidates provided by the auto-completion model. The candidates are matched against the activity prefix on-the-fly and ranked by a learning-to-rank algorithm. We validate our method on UT-Interaction Set #1 and Set #2 [19]. The experimental results show that the proposed activity auto-completion model achieves promising performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410722",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2011",
                "id": 131
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 12,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Support vector machines",
                "Detectors",
                "Training",
                "Firing",
                "Indexes",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "information retrieval",
                "learning (artificial intelligence)",
                "query processing",
                "search problems",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human activity prediction",
                "partial videos",
                "activity autocompletion model",
                "AAC model",
                "query auto-completion problem",
                "information retrieval",
                "search box",
                "activity prefix",
                "auto-completion model",
                "learning-to-rank algorithm"
            ]
        },
        "id": 356,
        "cited_by": []
    },
    {
        "title": "Person Re-Identification with Correspondence Structure Learning",
        "authors": [
            "Yang Shen",
            "Weiyao Lin",
            "Junchi Yan",
            "Mingliang Xu",
            "Jianxin Wu",
            "Jingdong Wang"
        ],
        "abstract": "This paper addresses the problem of handling spatial misalignments due to camera-view changes or human-pose variations in person re-identification. We first introduce a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. The learned correspondence structure can not only capture the spatial correspondence pattern between cameras but also handle the viewpoint or human-pose variation in individual images. We further introduce a global-based matching process. It integrates a global matching constraint over the learned correspondence structure to exclude cross-view misalignments during the image patch matching process, hence achieving a more reliable matching score between images. Experimental results on various datasets demonstrate the effectiveness of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410723",
        "reference_list": [
            {
                "year": "2007",
                "id": 179
            },
            {
                "year": "2013",
                "id": 393
            },
            {
                "year": "2015",
                "id": 22
            },
            {
                "year": "2013",
                "id": 315
            },
            {
                "year": "2015",
                "id": 124
            }
        ],
        "citation": {
            "ieee": 37,
            "other": 26,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Probes",
                "Correlation",
                "Reliability",
                "Pattern matching",
                "Measurement",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "learning (artificial intelligence)",
                "pose estimation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "person reidentification",
                "correspondence structure learning",
                "spatial misalignment handling problem",
                "camera-view changes",
                "human-pose variations",
                "boosting-based approach",
                "patch-wise matching probabilities",
                "spatial correspondence pattern",
                "cameras",
                "viewpoint handling",
                "global-based matching process",
                "global matching constraint",
                "cross-view misalignments",
                "image patch matching process"
            ]
        },
        "id": 357,
        "cited_by": [
            {
                "year": "2017",
                "id": 103
            },
            {
                "year": "2017",
                "id": 339
            },
            {
                "year": "2017",
                "id": 417
            },
            {
                "year": "2015",
                "id": 22
            },
            {
                "year": "2015",
                "id": 124
            }
        ]
    },
    {
        "title": "Adaptive Exponential Smoothing for Online Filtering of Pixel Prediction Maps",
        "authors": [
            "Kang Dang",
            "Jiong Yang",
            "Junsong Yuan"
        ],
        "abstract": "We propose an efficient online video filtering method, called adaptive exponential filtering (AES) to refine pixel prediction maps. Assuming each pixel is associated with a discriminative prediction score, the proposed AES applies exponentially decreasing weights over time to smooth the prediction score of each pixel, similar to classic exponential smoothing. However, instead of fixing the spatial pixel location to perform temporal filtering, we trace each pixel in the past frames by finding the optimal path that can bring the maximum exponential smoothing score, thus performing adaptive and non-linear filtering. Thanks to the pixel tracing, AES can better address object movements and avoid over-smoothing. To enable real-time filtering, we propose a linear-complexity dynamic programming scheme that can trace all pixels simultaneously. We apply the proposed filtering method to improve both saliency detection maps and scene parsing maps. The comparisons with average and exponential filtering, as well as state-of-the-art methods, validate that our AES can effectively refine the pixel prediction maps, without using the original video again.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410724",
        "reference_list": [
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2013",
                "id": 221
            },
            {
                "year": "2013",
                "id": 279
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Smoothing methods",
                "Streaming media",
                "Filtering",
                "Real-time systems",
                "Noise reduction",
                "Measurement",
                "Probabilistic logic"
            ],
            "INSPEC: Controlled Indexing": [
                "adaptive filters",
                "computational complexity",
                "dynamic programming",
                "image filtering",
                "nonlinear filters",
                "smoothing methods",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "average filtering",
                "scene parsing map improvement",
                "saliency detection map improvement",
                "linear-complexity dynamic programming scheme",
                "adaptive exponential smoothing",
                "online filtering",
                "pixel prediction maps",
                "online video filtering method",
                "adaptive exponential filtering",
                "AES",
                "discriminative prediction score",
                "exponentially-decreasing weights",
                "spatial pixel location",
                "temporal filtering",
                "optimal path",
                "maximum exponential smoothing score",
                "nonlinear filtering",
                "pixel tracing",
                "object movements"
            ]
        },
        "id": 358,
        "cited_by": []
    },
    {
        "title": "P-CNN: Pose-Based CNN Features for Action Recognition",
        "authors": [
            "Guilhem Ch\u00e9ron",
            "Ivan Laptev",
            "Cordelia Schmid"
        ],
        "abstract": "This work targets human action recognition in video. While recent methods typically represent actions by statistics of local video features, here we argue for the importance of a representation derived from human pose. To this end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN) for action recognition. The descriptor aggregates motion and appearance information along tracks of human body parts. We investigate different schemes of temporal aggregation and experiment with P-CNN features obtained both for automatically estimated and manually annotated human poses. We evaluate our method on the recent and challenging JHMDB and MPII Cooking datasets. For both datasets our method shows consistent improvement over the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410725",
        "reference_list": [
            {
                "year": "2013",
                "id": 398
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 48,
            "total": 81
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Tracking",
                "Head",
                "Neural networks",
                "Dynamics",
                "Face recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation",
                "neural nets",
                "pose estimation",
                "statistical analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pose-based CNN features",
                "human action recognition",
                "local video features",
                "pose-based convolutional neural network descriptor",
                "human body part tracking",
                "P-CNN features",
                "manually annotated human poses",
                "JHMDB cooking datasets",
                "MPII cooking datasets"
            ]
        },
        "id": 359,
        "cited_by": [
            {
                "year": "2017",
                "id": 29
            },
            {
                "year": "2017",
                "id": 306
            },
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2017",
                "id": 392
            }
        ]
    },
    {
        "title": "Fully Connected Object Proposals for Video Segmentation",
        "authors": [
            "Federico Perazzi",
            "Oliver Wang",
            "Markus Gross",
            "Alexander Sorkine-Hornung"
        ],
        "abstract": "We present a novel approach to video segmentation using multiple object proposals. The problem is formulated as a minimization of a novel energy function defined over a fully connected graph of object proposals. Our model combines appearance with long-range point tracks, which is key to ensure robustness with respect to fast motion and occlusions over longer video sequences. As opposed to previous approaches based on object proposals, we do not seek the best per-frame object hypotheses to perform the segmentation. Instead, we combine multiple, potentially imperfect proposals to improve overall segmentation accuracy and ensure robustness to outliers. Overall, the basic algorithm consists of three steps. First, we generate a very large number of object proposals for each video frame using existing techniques. Next, we perform an SVM-based pruning step to retain only high quality proposals with sufficiently discriminative power. Finally, we determine the fore-and background classification by solving for the maximum a posteriori of a fully connected conditional random field, defined using our novel energy function. Experimental results on a well established dataset demonstrate that our method compares favorably to several recent state-of-the-art approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410726",
        "reference_list": [
            {
                "year": "2009",
                "id": 106
            },
            {
                "year": "2003",
                "id": 8
            },
            {
                "year": "2011",
                "id": 253
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2013",
                "id": 221
            }
        ],
        "citation": {
            "ieee": 50,
            "other": 23,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Proposals",
                "Motion segmentation",
                "Support vector machines",
                "Robustness",
                "Feature extraction",
                "Tracking",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image segmentation",
                "support vector machines",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fully connected object proposals",
                "video segmentation",
                "fully connected graph",
                "long-range point tracks",
                "video sequences",
                "potentially imperfect proposals",
                "object proposals",
                "video frame",
                "SVM-based pruning step",
                "fully connected conditional random field",
                "energy function"
            ]
        },
        "id": 360,
        "cited_by": [
            {
                "year": "2017",
                "id": 175
            },
            {
                "year": "2017",
                "id": 228
            },
            {
                "year": "2017",
                "id": 379
            }
        ]
    },
    {
        "title": "Video Segmentation with Just a Few Strokes",
        "authors": [
            "Naveen Shankar Nagaraja",
            "Frank R. Schmidt",
            "Thomas Brox"
        ],
        "abstract": "As the use of videos is becoming more popular in computer vision, the need for annotated video datasets increases. Such datasets are required either as training data or simply as ground truth for benchmark datasets. A particular challenge in video segmentation is due to disocclusions, which hamper frame-to-frame propagation, in conjunction with non-moving objects. We show that a combination of motion from point trajectories, as known from motion segmentation, along with minimal supervision can largely help solve this problem. Moreover, we integrate a new constraint that enforces consistency of the color distribution in successive frames. We quantify user interaction effort with respect to segmentation quality on challenging ego motion videos. We compare our approach to a diverse set of algorithms in terms of user effort and in terms of performance on common video segmentation benchmarks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410727",
        "reference_list": [
            {
                "year": "2009",
                "id": 106
            },
            {
                "year": "2009",
                "id": 196
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2013",
                "id": 290
            },
            {
                "year": "2013",
                "id": 221
            },
            {
                "year": "2009",
                "id": 99
            },
            {
                "year": "2009",
                "id": 186
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 11,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Motion segmentation",
                "Computer vision",
                "Image color analysis",
                "Optical propagation",
                "Optical imaging",
                "Benchmark testing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image colour analysis",
                "image segmentation",
                "motion estimation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "annotated video datasets",
                "benchmark datasets",
                "frame-to-frame propagation",
                "nonmoving objects",
                "motion combination",
                "color distribution",
                "user interaction effort",
                "segmentation quality",
                "ego motion videos",
                "video segmentation benchmarks"
            ]
        },
        "id": 361,
        "cited_by": [
            {
                "year": "2017",
                "id": 71
            },
            {
                "year": "2017",
                "id": 175
            },
            {
                "year": "2017",
                "id": 222
            }
        ]
    },
    {
        "title": "Actionness-Assisted Recognition of Actions",
        "authors": [
            "Ye Luo",
            "Loong-Fah Cheong",
            "An Tran"
        ],
        "abstract": "We elicit from a fundamental definition of action low-level attributes that can reveal agency and intentionality. These descriptors are mainly trajectory-based, measuring sudden changes, temporal synchrony, and repetitiveness. The actionness map can be used to localize actions in a way that is generic across action and agent types. Furthermore, it also groups interacting regions into a useful unit of analysis, which is crucial for recognition of actions involving interactions. We then implement an actionness-driven pooling scheme to improve action recognition performance. Experimental results on three datasets show the advantages of our method on both action detection and action recognition comparing with other state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410728",
        "reference_list": [
            {
                "year": "2011",
                "id": 99
            },
            {
                "year": "2013",
                "id": 337
            },
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2013",
                "id": 443
            },
            {
                "year": "2013",
                "id": 444
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 1,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Computer vision",
                "Biology",
                "Adaptive optics",
                "Optical imaging",
                "Optical sensors",
                "Dynamics"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "actionness-assisted recognition",
                "low-level attributes",
                "intentionality",
                "descriptors",
                "temporal synchrony",
                "repetitiveness",
                "agent types",
                "actionness-driven pooling scheme",
                "action recognition performance",
                "action detection"
            ]
        },
        "id": 362,
        "cited_by": []
    },
    {
        "title": "COUNT Forest: CO-Voting Uncertain Number of Targets Using Random Forest for Crowd Density Estimation",
        "authors": [
            "Viet-Quoc Pham",
            "Tatsuo Kozakaya",
            "Osamu Yamaguchi",
            "Ryuzo Okada"
        ],
        "abstract": "This paper presents a patch-based approach for crowd density estimation in public scenes. We formulate the problem of estimating density in a structured learning framework applied to random decision forests. Our approach learns the mapping between patch features and relative locations of all objects inside each patch, which contribute to generate the patch density map through Gaussian kernel density estimation. We build the forest in a coarse-to-fine manner with two split node layers, and further propose a crowdedness prior and an effective forest reduction method to improve the estimation accuracy and speed. Moreover, we introduce a semi-automatic training method to learn the estimator for a specific scene. We achieved state-of-the-art results on the public Mall dataset and UCSD dataset, and also proposed two potential applications in traffic counts and scene understanding with promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410729",
        "reference_list": [
            {
                "year": "2013",
                "id": 281
            },
            {
                "year": "2009",
                "id": 257
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 18,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Estimation",
                "Vegetation",
                "Kernel",
                "Computational modeling",
                "Computer vision",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Gaussian kernel density estimation",
                "coarse-to-fine manner",
                "split node layers",
                "crowdedness prior",
                "forest reduction method",
                "public mall dataset",
                "UCSD dataset",
                "traffic counts",
                "scene understanding",
                "patch density map",
                "patch features",
                "random decision forests",
                "structured learning framework",
                "public scenes",
                "patch-based approach",
                "crowd density estimation",
                "uncertain target number covoting",
                "COUNT forest"
            ]
        },
        "id": 363,
        "cited_by": [
            {
                "year": "2017",
                "id": 196
            },
            {
                "year": "2017",
                "id": 541
            },
            {
                "year": "2017",
                "id": 551
            }
        ]
    },
    {
        "title": "Multi-cue Structure Preserving MRF for Unconstrained Video Segmentation",
        "authors": [
            "Saehoon Yi",
            "Vladimir Pavlovic"
        ],
        "abstract": "Video segmentation is a stepping stone to understanding video context. Video segmentation enables one to represent a video by decomposing it into coherent regions which comprise whole or parts of objects. However, the challenge originates from the fact that most of the video segmentation algorithms are based on unsupervised learning due to expensive cost of pixelwise video annotation and intra-class variability within similar unconstrained video classes. We propose a Markov Random Field model for unconstrained video segmentation that relies on tight integration of multiple cues: vertices are defined from contour based superpixels, unary potentials from temporally smooth label likelihood and pairwise potentials from global structure of a video. Multi-cue structure is a breakthrough to extracting coherent object regions for unconstrained videos in absence of supervision. Our experiments on VSB100 dataset show that the proposed model significantly outperforms competing state-of-the-art algorithms. Qualitative analysis illustrates that video segmentation result of the proposed model is consistent with human perception of objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410730",
        "reference_list": [
            {
                "year": "2013",
                "id": 250
            },
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2013",
                "id": 221
            },
            {
                "year": "2009",
                "id": 58
            },
            {
                "year": "2011",
                "id": 200
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Trajectory",
                "Color",
                "Proposals",
                "Image color analysis",
                "Image edge detection",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "Markov processes",
                "unsupervised learning",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human object perception",
                "VSB100 dataset",
                "coherent object regions",
                "global video structure",
                "pairwise potentials",
                "temporally smooth label likelihood",
                "unary potentials",
                "contour based superpixels",
                "Markov random field model",
                "unconstrained video classes",
                "intraclass variability",
                "pixelwise video annotation",
                "unsupervised learning",
                "video segmentation algorithms",
                "video context",
                "unconstrained video segmentation",
                "multicue structure preserving MRF"
            ]
        },
        "id": 364,
        "cited_by": []
    },
    {
        "title": "Motion Trajectory Segmentation via Minimum Cost Multicuts",
        "authors": [
            "Margret Keuper",
            "Bjoern Andres",
            "Thomas Brox"
        ],
        "abstract": "For the segmentation of moving objects in videos, the analysis of long-term point trajectories has been very popular recently. In this paper, we formulate the segmentation of a video sequence based on point trajectories as a minimum cost multicut problem. Unlike the commonly used spectral clustering formulation, the minimum cost multicut formulation gives natural rise to optimize not only for a cluster assignment but also for the number of clusters while allowing for varying cluster sizes. In this setup, we provide a method to create a long-term point trajectory graph with attractive and repulsive binary terms and outperform state-of-the-art methods based on spectral clustering on the FBMS-59 dataset and on the motion subtask of the VSB100 dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410731",
        "reference_list": [
            {
                "year": "2011",
                "id": 332
            },
            {
                "year": "2009",
                "id": 110
            },
            {
                "year": "2015",
                "id": 195
            },
            {
                "year": "2013",
                "id": 170
            },
            {
                "year": "2011",
                "id": 200
            },
            {
                "year": "2013",
                "id": 385
            },
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 26,
            "other": 14,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Motion segmentation",
                "Computer vision",
                "Labeling",
                "Color",
                "Image segmentation",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "image motion analysis",
                "image segmentation",
                "image sequences",
                "pattern clustering",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion trajectory segmentation",
                "video sequence",
                "minimum cost multicut problem",
                "cluster assignment",
                "point trajectory graph",
                "binary terms"
            ]
        },
        "id": 365,
        "cited_by": [
            {
                "year": "2017",
                "id": 123
            },
            {
                "year": "2017",
                "id": 175
            },
            {
                "year": "2017",
                "id": 445
            },
            {
                "year": "2017",
                "id": 470
            },
            {
                "year": "2015",
                "id": 195
            }
        ]
    },
    {
        "title": "Action Localization in Videos through Context Walk",
        "authors": [
            "Khurram Soomro",
            "Haroon Idrees",
            "Mubarak Shah"
        ],
        "abstract": "This paper presents an efficient approach for localizing actions by learning contextual relations, in the form of relative locations between different video regions. We begin by over-segmenting the videos into supervoxels, which have the ability to preserve action boundaries and also reduce the complexity of the problem. Context relations are learned during training which capture displacements from all the supervoxels in a video to those belonging to foreground actions. Then, given a testing video, we select a supervoxel randomly and use the context information acquired during training to estimate the probability of each supervoxel belonging to the foreground action. The walk proceeds to a new supervoxel and the process is repeated for a few steps. This \"context walk\" generates a conditional distribution of an action over all the supervoxels. A Conditional Random Field is then used to find action proposals in the video, whose confidences are obtained using SVMs. We validated the proposed approach on several datasets and show that context in the form of relative displacements between supervoxels can be extremely useful for action localization. This also results in significantly fewer evaluations of the classifier, in sharp contrast to the alternate sliding window approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410732",
        "reference_list": [
            {
                "year": "2009",
                "id": 248
            },
            {
                "year": "2009",
                "id": 16
            },
            {
                "year": "2013",
                "id": 398
            },
            {
                "year": "2007",
                "id": 171
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2013",
                "id": 342
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 18,
            "other": 4,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Context",
                "Training",
                "Testing",
                "Proposals",
                "Electron tubes",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image segmentation",
                "learning (artificial intelligence)",
                "probability",
                "support vector machines",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action localization",
                "context walk",
                "contextual relation learning",
                "video regions",
                "video over-segmentation",
                "complexity reduction",
                "action boundary preservation",
                "supervoxel selection",
                "probability estimation",
                "conditional random field",
                "SVM",
                "support vector machines"
            ]
        },
        "id": 366,
        "cited_by": [
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 152
            },
            {
                "year": "2017",
                "id": 383
            },
            {
                "year": "2017",
                "id": 463
            },
            {
                "year": "2017",
                "id": 466
            }
        ]
    },
    {
        "title": "RGB-W: When Vision Meets Wireless",
        "authors": [
            "Alexandre Alahi",
            "Albert Haque",
            "Li Fei-Fei"
        ],
        "abstract": "Inspired by the recent success of RGB-D cameras, we propose the enrichment of RGB data with an additional \"quasi-free\" modality, namely, the wireless signal (e.g., wifi or Bluetooth) emitted by individuals' cell phones, referred to as RGB-W. The received signal strength acts as a rough proxy for depth and a reliable cue on their identity. Although the measured signals are highly noisy (more than 2m average localization error), we demonstrate that the combination of visual and wireless data significantly improves the localization accuracy. We introduce a novel image-driven representation of wireless data which embeds all received signals onto a single image. We then indicate the ability of this additional data to (i) locate persons within a sparsity-driven framework and to (ii) track individuals with a new confidence measure on the data association problem. Our solution outperforms existing localization methods by a significant margin. It can be applied to the millions of currently installed RGB cameras to better analyze human behavior and offer the next generation of high-accuracy location-based services.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410733",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 0,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Noise measurement",
                "Antennas",
                "Wireless communication",
                "Hidden Markov models",
                "Bluetooth",
                "Fuses"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image fusion",
                "image representation",
                "mobile computing",
                "RSSI"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "RGB-W",
                "RGB-D cameras",
                "quasifree modality",
                "wireless signal",
                "cell phones",
                "received signal strength",
                "wireless data image-driven representation",
                "sparsity-driven framework",
                "data association problem",
                "high-accuracy location-based services"
            ]
        },
        "id": 367,
        "cited_by": [
            {
                "year": "2017",
                "id": 31
            }
        ]
    },
    {
        "title": "Action Detection by Implicit Intentional Motion Clustering",
        "authors": [
            "Wei Chen",
            "Jason J. Corso"
        ],
        "abstract": "Explicitly using human detection and pose estimation has found limited success in action recognition problems. This may be due to the complexity in the articulated motion human exhibit. Yet, we know that action requires an actor and intention. This paper hence seeks to understand the spatiotemporal properties of intentional movement and how to capture such intentional movement without relying on challenging human detection and tracking. We conduct a quantitative analysis of intentional movement, and our findings motivate a new approach for implicit intentional movement extraction that is based on spatiotemporal trajectory clustering by leveraging the properties of intentional movement. The intentional movement clusters are then used as action proposals for detection. Our results on three action detection benchmarks indicate the relevance of focusing on intentional movement for action detection, our method significantly outperforms the state of the art on the challenging MSR-II multi-action video benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410734",
        "reference_list": [
            {
                "year": "2013",
                "id": 398
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2009",
                "id": 156
            },
            {
                "year": "2013",
                "id": 443
            },
            {
                "year": "2009",
                "id": 193
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 10,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Proposals",
                "Spatiotemporal phenomena",
                "Benchmark testing",
                "Computer vision",
                "Tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "graph theory",
                "image motion analysis",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "trajectory graph",
                "spatiotemporal trajectory clustering",
                "intentional movement extraction",
                "intentional motion clustering",
                "action detection"
            ]
        },
        "id": 368,
        "cited_by": [
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 152
            },
            {
                "year": "2017",
                "id": 462
            },
            {
                "year": "2017",
                "id": 466
            },
            {
                "year": "2017",
                "id": 609
            }
        ]
    },
    {
        "title": "Simultaneous Foreground Detection and Classification with Hybrid Features",
        "authors": [
            "Jaemyun Kim",
            "Ad\u00edn Ram\u00edrez Rivera",
            "Byungyong Ryu",
            "Oksam Chae"
        ],
        "abstract": "In this paper, we propose a hybrid background model that relies on edge and non-edge features of the image to produce the model. We encode these features into a coding scheme, that we called Local Hybrid Pattern (LHP), that selectively models edges and non-edges features of each pixel. Furthermore, we model each pixel with an adaptive code dictionary to represent the background dynamism, and update it by adding stable codes and discarding unstable ones. We weight each code in the dictionary to enhance its description of the pixel it models. The foreground is detected as the incoming codes that deviate from the dictionary. We can detect (as foreground or background) and classify (as edge or inner region) each pixel simultaneously. We tested our proposed method in existing databases with promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410735",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Adaptation models",
                "Dictionaries",
                "Lighting",
                "Computational modeling",
                "Encoding",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "image classification",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "foreground detection",
                "foreground classification",
                "hybrid background model",
                "coding scheme",
                "local hybrid pattern",
                "adaptive code dictionary"
            ]
        },
        "id": 369,
        "cited_by": []
    },
    {
        "title": "Training a Feedback Loop for Hand Pose Estimation",
        "authors": [
            "Markus Oberweger",
            "Paul Wohlhart",
            "Vincent Lepetit"
        ],
        "abstract": "We propose an entirely data-driven approach to estimating the 3D pose of a hand given a depth image. We show that we can correct the mistakes made by a Convolutional Neural Network trained to predict an estimate of the 3D pose by using a feedback loop. The components of this feedback loop are also Deep Networks, optimized using training data. They remove the need for fitting a 3D model to the input data, which requires both a carefully designed fitting function and algorithm. We show that our approach outperforms state-of-the-art methods, and is efficient as our implementation runs at over 400 fps on a single GPU.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410736",
        "reference_list": [
            {
                "year": "2011",
                "id": 265
            },
            {
                "year": "2013",
                "id": 306
            },
            {
                "year": "2013",
                "id": 402
            },
            {
                "year": "2013",
                "id": 431
            },
            {
                "year": "2011",
                "id": 256
            }
        ],
        "citation": {
            "ieee": 60,
            "other": 29,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization",
                "Three-dimensional displays",
                "Solid modeling",
                "Synthesizers",
                "Data models",
                "Training",
                "Feedback loop"
            ],
            "INSPEC: Controlled Indexing": [
                "neural nets",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single GPU",
                "deep networks",
                "convolutional neural network",
                "3D pose estimation",
                "hand pose estimation",
                "feedback loop training"
            ]
        },
        "id": 370,
        "cited_by": [
            {
                "year": "2017",
                "id": 88
            },
            {
                "year": "2017",
                "id": 121
            },
            {
                "year": "2017",
                "id": 267
            },
            {
                "year": "2017",
                "id": 327
            },
            {
                "year": "2017",
                "id": 329
            },
            {
                "year": "2017",
                "id": 403
            },
            {
                "year": "2017",
                "id": 511
            },
            {
                "year": "2017",
                "id": 515
            }
        ]
    },
    {
        "title": "Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose",
        "authors": [
            "Danhang Tang",
            "Jonathan Taylor",
            "Pushmeet Kohli",
            "Cem Keskin",
            "Tae-Kyun Kim",
            "Jamie Shotton"
        ],
        "abstract": "We address the problem of hand pose estimation, formulated as an inverse problem. Typical approaches optimize an energy function over pose parameters using a 'black box' image generation procedure. This procedure knows little about either the relationships between the parameters or the form of the energy function. In this paper, we show that we can significantly improving upon black box optimization by exploiting high-level knowledge of the structure of the parameters and using a local surrogate energy function. Our new framework, hierarchical sampling optimization, consists of a sequence of predictors organized into a kinematic hierarchy. Each predictor is conditioned on its ancestors, and generates a set of samples over a subset of the pose parameters. The highly-efficient surrogate energy is used to select among samples. Having evaluated the full hierarchy, the partial pose samples are concatenated to generate a full-pose hypothesis. Several hypotheses are generated using the same procedure, and finally the original full energy function selects the best result. Experimental evaluation on three publically available datasets show that our method is particularly impressive in low-compute scenarios where it significantly outperforms all other state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410737",
        "reference_list": [
            {
                "year": "2011",
                "id": 334
            },
            {
                "year": "2013",
                "id": 402
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 20,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Kinematics",
                "Optimization",
                "Silver",
                "Inverse problems",
                "Three-dimensional displays",
                "Rendering (computer graphics)"
            ],
            "INSPEC: Controlled Indexing": [
                "image sampling",
                "optimisation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human hand pose estimation",
                "hierarchical sampling optimization",
                "black box image generation procedure",
                "parameter structure",
                "local surrogate energy function",
                "kinematic hierarchy",
                "partial pose samples",
                "full-pose hypothesis",
                "low-compute scenarios"
            ]
        },
        "id": 371,
        "cited_by": [
            {
                "year": "2017",
                "id": 121
            },
            {
                "year": "2017",
                "id": 267
            },
            {
                "year": "2017",
                "id": 327
            }
        ]
    },
    {
        "title": "Panoptic Studio: A Massively Multiview System for Social Motion Capture",
        "authors": [
            "Hanbyul Joo",
            "Hao Liu",
            "Lei Tan",
            "Lin Gui",
            "Bart Nabbe",
            "Iain Matthews",
            "Takeo Kanade",
            "Shohei Nobuhara",
            "Yaser Sheikh"
        ],
        "abstract": "We present an approach to capture the 3D structure and motion of a group of people engaged in a social interaction. The core challenges in capturing social interactions are: (1) occlusion is functional and frequent, (2) subtle motion needs to be measured over a space large enough to host a social group, and (3) human appearance and configuration variation is immense. The Panoptic Studio is a system organized around the thesis that social interactions should be measured through the perceptual integration of a large variety of view points. We present a modularized system designed around this principle, consisting of integrated structural, hardware, and software innovations. The system takes, as input, 480 synchronized video streams of multiple people engaged in social activities, and produces, as output, the labeled time-varying 3D structure of anatomical landmarks on individuals in the space. The algorithmic contributions include a hierarchical approach for generating skeletal trajectory proposals, and an optimization framework for skeletal reconstruction with trajectory re-association.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410738",
        "reference_list": [
            {
                "year": "2011",
                "id": 138
            },
            {
                "year": "2013",
                "id": 437
            },
            {
                "year": "2011",
                "id": 120
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 15,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Proposals",
                "Trajectory",
                "Three-dimensional displays",
                "Synchronization",
                "Face",
                "Sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "massively multiview system",
                "social motion capture",
                "panoptic studio",
                "3D structure",
                "social interaction",
                "social group",
                "human appearance",
                "configuration variation",
                "software innovations",
                "hardware innovations",
                "integrated structural innovations",
                "perceptual integration",
                "synchronized video streams",
                "labeled time-varying 3D structure",
                "hierarchical approach",
                "skeletal trajectory proposals",
                "optimization framework",
                "skeletal reconstruction",
                "trajectory re-association"
            ]
        },
        "id": 372,
        "cited_by": []
    },
    {
        "title": "Where to Buy It: Matching Street Clothing Photos in Online Shops",
        "authors": [
            "M. Hadi Kiapour",
            "Xufeng Han",
            "Svetlana Lazebnik",
            "Alexander C. Berg",
            "Tamara L. Berg"
        ],
        "abstract": "In this paper, we define a new task, Exact Street to Shop, where our goal is to match a real-world example of a garment item to the same item in an online shop. This is an extremely challenging task due to visual differences between street photos (pictures of people wearing clothing in everyday uncontrolled settings) and online shop photos (pictures of clothing items on people, mannequins, or in isolation, captured by professionals in more controlled settings). We collect a new dataset for this application containing 404,683 shop photos collected from 25 different online retailers and 20,357 street photos, providing a total of 39,479 clothing item matches between street and shop photos. We develop three different methods for Exact Street to Shop retrieval, including two deep learning baseline methods, and a method to learn a similarity measure between the street and shop domains. Experiments demonstrate that our learned similarity significantly outperforms our baselines that use existing deep learning based representations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410739",
        "reference_list": [
            {
                "year": "2013",
                "id": 425
            },
            {
                "year": "2013",
                "id": 369
            },
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2013",
                "id": 373
            },
            {
                "year": "2011",
                "id": 137
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2011",
                "id": 194
            },
            {
                "year": "2013",
                "id": 439
            }
        ],
        "citation": {
            "ieee": 50,
            "other": 49,
            "total": 99
        },
        "keywords": {
            "IEEE Keywords": [
                "Clothing",
                "Machine learning",
                "Computer vision",
                "Lighting",
                "Image retrieval",
                "Visualization",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "clothing",
                "image matching",
                "image representation",
                "image retrieval",
                "Internet",
                "learning (artificial intelligence)",
                "retail data processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "street clothing photo matching",
                "visual differences",
                "online shop photos",
                "online retailers",
                "exact street to shop retrieval",
                "deep learning baseline method",
                "deep learning based representations"
            ]
        },
        "id": 373,
        "cited_by": [
            {
                "year": "2017",
                "id": 40
            },
            {
                "year": "2017",
                "id": 153
            },
            {
                "year": "2017",
                "id": 273
            },
            {
                "year": "2017",
                "id": 299
            },
            {
                "year": "2017",
                "id": 441
            }
        ]
    },
    {
        "title": "Multi-task Recurrent Neural Network for Immediacy Prediction",
        "authors": [
            "Xiao Chu",
            "Wanli Ouyang",
            "Wei Yang",
            "Xiaogang Wang"
        ],
        "abstract": "In this paper, we propose to predict immediacy for interacting persons from still images. A complete immediacy set includes interactions, relative distance, body leaning direction and standing orientation. These measures are found to be related to the attitude, social relationship, social interaction, action, nationality, and religion of the communicators. A large-scale dataset with 10,000 images is constructed, in which all the immediacy measures and the human poses are annotated. We propose a rich set of immediacy representations that help to predict immediacy from imperfect 1-person and 2-person pose estimation results. A multi-task deep recurrent neural network is constructed to take the proposed rich immediacy representation as input and learn the complex relationship among immediacy predictions multiple steps of refinement. The effectiveness of the proposed approach is proved through extensive experiments on the large scale dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410740",
        "reference_list": [
            {
                "year": "2013",
                "id": 435
            },
            {
                "year": "2011",
                "id": 91
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 6,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Shoulder",
                "Feature extraction",
                "Recurrent neural networks",
                "Correlation",
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "data analysis",
                "pose estimation",
                "recurrent neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "immediacy prediction",
                "still images",
                "interactions",
                "relative distance",
                "body leaning direction",
                "standing orientation",
                "attitude",
                "social relationship",
                "social interaction",
                "action",
                "nationality",
                "religion",
                "large-scale dataset",
                "imperfect 1-person pose estimation",
                "2-person pose estimation",
                "multitask deep recurrent neural network"
            ]
        },
        "id": 374,
        "cited_by": [
            {
                "year": "2017",
                "id": 200
            }
        ]
    },
    {
        "title": "Learning Complexity-Aware Cascades for Deep Pedestrian Detection",
        "authors": [
            "Zhaowei Cai",
            "Mohammad Saberian",
            "Nuno Vasconcelos"
        ],
        "abstract": "The design of complexity-aware cascaded detectors, combining features of very different complexities, is considered. A new cascade design procedure is introduced, by formulating cascade learning as the Lagrangian optimization of a risk that accounts for both accuracy and complexity. A boosting algorithm, denoted as complexity aware cascade training (CompACT), is then derived to solve this optimization. CompACT cascades are shown to seek an optimal trade-off between accuracy and complexity by pushing features of higher complexity to the later cascade stages, where only a few difficult candidate patches remain to be classified. This enables the use of features of vastly different complexities in a single detector. In result, the feature pool can be expanded to features previously impractical for cascade design, such as the responses of a deep convolutional neural network (CNN). This is demonstrated through the design of a pedestrian detector with a pool of features whose complexities span orders of magnitude. The resulting cascade generalizes the combination of a CNN with an object proposal mechanism: rather than a pre-processing stage, CompACT cascades seamlessly integrate CNNs in their stages. This enables state of the art performance on the Caltech and KITTI datasets, at fairly fast speeds.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410741",
        "reference_list": [
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2007",
                "id": 21
            },
            {
                "year": "2013",
                "id": 256
            },
            {
                "year": "2011",
                "id": 238
            },
            {
                "year": "2013",
                "id": 2
            },
            {
                "year": "2007",
                "id": 203
            },
            {
                "year": "2003",
                "id": 94
            }
        ],
        "citation": {
            "ieee": 89,
            "other": 33,
            "total": 122
        },
        "keywords": {
            "IEEE Keywords": [
                "Complexity theory",
                "Detectors",
                "Feature extraction",
                "Boosting",
                "Proposals",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "neural nets",
                "pedestrians"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning complexity-aware cascade",
                "deep pedestrian detection",
                "Lagrangian optimization",
                "boosting algorithm",
                "complexity aware cascade training",
                "CompACT cascade",
                "deep convolutional neural network",
                "deep CNN"
            ]
        },
        "id": 375,
        "cited_by": [
            {
                "year": "2017",
                "id": 367
            },
            {
                "year": "2017",
                "id": 369
            },
            {
                "year": "2017",
                "id": 520
            }
        ]
    },
    {
        "title": "Polarized 3D: High-Quality Depth Sensing with Polarization Cues",
        "authors": [
            "Achuta Kadambi",
            "Vage Taamazyan",
            "Boxin Shi",
            "Ramesh Raskar"
        ],
        "abstract": "Coarse depth maps can be enhanced by using the shape information from polarization cues. We propose a framework to combine surface normals from polarization (hereafter polarization normals) with an aligned depth map. Polarization normals have not been used for depth enhancement before. This is because polarization normals suffer from physics-based artifacts, such as azimuthal ambiguity, refractive distortion and fronto-parallel signal degradation. We propose a framework to overcome these key challenges, allowing the benefits of polarization to be used to enhance depth maps. Our results demonstrate improvement with respect to state-of-the-art 3D reconstruction techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410742",
        "reference_list": [
            {
                "year": "2005",
                "id": 40
            },
            {
                "year": "2013",
                "id": 201
            },
            {
                "year": "2007",
                "id": 181
            },
            {
                "year": "2003",
                "id": 129
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 12,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Three-dimensional displays",
                "Robustness",
                "Sensors",
                "Cameras",
                "Azimuth",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "image enhancement",
                "image reconstruction",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "polarized 3D",
                "high-quality depth sensing",
                "polarization cues",
                "coarse depth map enhancement",
                "physics-based artifacts",
                "azimuthal ambiguity",
                "refractive distortion",
                "fronto-parallel signal degradation",
                "3D reconstruction techniques",
                "3D cameras"
            ]
        },
        "id": 376,
        "cited_by": [
            {
                "year": "2017",
                "id": 240
            },
            {
                "year": "2017",
                "id": 245
            }
        ]
    },
    {
        "title": "Airborne Three-Dimensional Cloud Tomography",
        "authors": [
            "Aviad Levis",
            "Yoav Y. Schechner",
            "Amit Aides",
            "Anthony B. Davis"
        ],
        "abstract": "We seek to sense the three dimensional (3D) volumetric distribution of scatterers in a heterogenous medium. An important case study for such a medium is the atmosphere. Atmospheric contents and their role in Earth's radiation balance have significant uncertainties with regards to scattering components: aerosols and clouds. Clouds, made of water droplets, also lead to local effects as precipitation and shadows. Our sensing approach is computational tomography using passive multi-angular imagery. For light-matter interaction that accounts for multiple-scattering, we use the 3D radiative transfer equation as a forward model. Volumetric recovery by inverting this model suffers from a computational bottleneck on large scales, which include many unknowns. Steps taken make this tomography tractable, without approximating the scattering order or angle range.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410743",
        "reference_list": [
            {
                "year": "2005",
                "id": 137
            },
            {
                "year": "2005",
                "id": 205
            },
            {
                "year": "2005",
                "id": 54
            },
            {
                "year": "2009",
                "id": 296
            },
            {
                "year": "2011",
                "id": 149
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 7,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Clouds",
                "Scattering",
                "Atmospheric modeling",
                "Three-dimensional displays",
                "Tomography",
                "Mathematical model",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "clouds",
                "computerised tomography",
                "geophysical image processing",
                "light scattering",
                "radiative transfer",
                "remote sensing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "airborne three-dimensional cloud tomography",
                "scatterers 3D volumetric distribution",
                "water droplets",
                "computational tomography",
                "passive multiangular imagery",
                "light-matter interaction",
                "3D radiative transfer equation",
                "forward model",
                "volumetric recovery"
            ]
        },
        "id": 377,
        "cited_by": []
    },
    {
        "title": "Leave-One-Out Kernel Optimization for Shadow Detection",
        "authors": [
            "Tom\u00e1s F. Yago Vicente",
            "Minh Hoai",
            "Dimitris Samaras"
        ],
        "abstract": "The objective of this work is to detect shadows in images. We pose this as the problem of labeling image regions, where each region corresponds to a group of superpixels. To predict the label of each region, we train a kernel Least-Squares SVM for separating shadow and non-shadow regions. The parameters of the kernel and the classifier are jointly learned to minimize the leave-one-out cross validation error. Optimizing the leave-one-out cross validation error is typically difficult, but it can be done efficiently in our framework. Experiments on two challenging shadow datasets, UCF and UIUC, show that our region classifier outperforms more complex methods. We further enhance the performance of the region classifier by embedding it in an MRF framework and adding pairwise contextual cues. This leads to a method that significantly outperforms the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410744",
        "reference_list": [
            {
                "year": "2011",
                "id": 113
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 5,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Training",
                "Support vector machines",
                "Training data",
                "Error analysis",
                "Image segmentation",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image resolution",
                "least squares approximations",
                "object detection",
                "optimisation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "leave-one-out kernel optimization",
                "shadow detection",
                "image region labeling problem",
                "kernel least-squares SVM",
                "leave-one-out cross validtion error",
                "shadow datasets",
                "UCF",
                "UIUC",
                "region classifier",
                "complex methods",
                "MRF framework"
            ]
        },
        "id": 378,
        "cited_by": [
            {
                "year": "2017",
                "id": 473
            }
        ]
    },
    {
        "title": "Removing Rain from a Single Image via Discriminative Sparse Coding",
        "authors": [
            "Yu Luo",
            "Yong Xu",
            "Hui Ji"
        ],
        "abstract": "Visual distortions on images caused by bad weather conditions can have a negative impact on the performance of many outdoor vision systems. One often seen bad weather is rain which causes significant yet complex local intensity fluctuations in images. The paper aims at developing an effective algorithm to remove visual effects of rain from a single rainy image, i.e. separate the rain layer and the de-rained image layer from an rainy image. Built upon a non-linear generative model of rainy image, namely screen blend mode, we proposed a dictionary learning based algorithm for single image de-raining. The basic idea is to sparsely approximate the patches of two layers by very high discriminative codes over a learned dictionary with strong mutual exclusivity property. Such discriminative sparse codes lead to accurate separation of two layers from their non-linear composite. The experiments showed that the proposed method outperformed the existing single image de-raining methods on tested rain images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410745",
        "reference_list": [
            {
                "year": "2013",
                "id": 245
            },
            {
                "year": "2013",
                "id": 78
            }
        ],
        "citation": {
            "ieee": 57,
            "other": 11,
            "total": 68
        },
        "keywords": {
            "IEEE Keywords": [
                "Rain",
                "Dictionaries",
                "Visual effects",
                "Image coding",
                "Visualization",
                "Machine vision"
            ],
            "INSPEC: Controlled Indexing": [
                "distortion",
                "image coding",
                "nonlinear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image deraining",
                "sparse coding",
                "visual distortion",
                "image fluctuation",
                "nonlinear generative model",
                "dictionary learning"
            ]
        },
        "id": 379,
        "cited_by": [
            {
                "year": "2017",
                "id": 179
            },
            {
                "year": "2017",
                "id": 181
            },
            {
                "year": "2017",
                "id": 265
            },
            {
                "year": "2017",
                "id": 266
            }
        ]
    },
    {
        "title": "Mutual-Structure for Joint Filtering",
        "authors": [
            "Xiaoyong Shen",
            "Chao Zhou",
            "Li Xu",
            "Jiaya Jia"
        ],
        "abstract": "Previous joint/guided filters directly transfer the structural information in the reference image to the target one. In this paper, we first analyze its major drawback -- that is, there may be completely different edges in the two images. Simply passing all patterns to the target could introduce significant errors. To address this issue, we propose the concept of mutual-structure, which refers to the structural information that is contained in both images and thus can be safely enhanced by joint filtering, and an untraditional objective function that can be efficiently optimized to yield mutual structure. Our method results in necessary and important edge preserving, which greatly benefits depth completion, optical flow estimation, image enhancement, stereo matching, to name a few.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410746",
        "reference_list": [
            {
                "year": "2013",
                "id": 6
            },
            {
                "year": "2011",
                "id": 205
            },
            {
                "year": "2013",
                "id": 191
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 11,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Optical filters",
                "Image restoration",
                "Optimization",
                "Filtering",
                "Linear programming",
                "Optical imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "image enhancement",
                "image filtering",
                "image matching",
                "image sequences",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "mutual-structure",
                "joint filtering",
                "structural information",
                "image enhancement",
                "objective function",
                "mutual structure",
                "edge preservation",
                "depth completion",
                "optical flow estimation",
                "stereo matching"
            ]
        },
        "id": 380,
        "cited_by": [
            {
                "year": "2017",
                "id": 347
            }
        ]
    },
    {
        "title": "Photometric Stereo in a Scattering Medium",
        "authors": [
            "Zak Murez",
            "Tali Treibitz",
            "Ravi Ramamoorthi",
            "David Kriegman"
        ],
        "abstract": "Photometric stereo is widely used for 3D reconstruction. However, its use in scattering media such as water, biological tissue and fog has been limited until now, because of forward scattered light from both the source and object, as well as light scattered back from the medium (backscatter). Here we make three contributions to address the key modes of light propagation, under the common single scattering assumption for dilute media. First, we show through extensive simulations that single-scattered light from a source can be approximated by a point light source with a single direction. This alleviates the need to handle light source blur explicitly. Next, we model the blur due to scattering of light from the object. We measure the object point-spread function and introduce a simple deconvolution method. Finally, we show how imaging fluorescence emission where available, eliminates the backscatter component and increases the signal-to-noise ratio. Experimental results in a water tank, with different concentrations of scattering media added, show that deconvolution produces higher-quality 3D reconstructions than previous techniques, and that when combined with fluorescence, can produce results similar to that in clear water even for highly turbid media.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410747",
        "reference_list": [
            {
                "year": "2005",
                "id": 54
            },
            {
                "year": "2005",
                "id": 78
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 7,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Backscatter",
                "Scattering",
                "Light sources",
                "Cameras",
                "Three-dimensional displays",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "backscatter",
                "computer graphics",
                "deconvolution",
                "fluorescence",
                "image reconstruction",
                "light scattering",
                "optical transfer function",
                "photometric light sources",
                "photometry",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "higher-quality 3D reconstructions",
                "signal-to-noise ratio",
                "imaging fluorescence emission",
                "deconvolution method",
                "point-spread function",
                "light source blur handling",
                "point light source",
                "single-scattered light",
                "dilute media",
                "light propagation",
                "backscatter",
                "light scattering",
                "scattering medium",
                "photometric stereo"
            ]
        },
        "id": 381,
        "cited_by": [
            {
                "year": "2017",
                "id": 253
            }
        ]
    },
    {
        "title": "Resolving Scale Ambiguity via XSlit Aspect Ratio Analysis",
        "authors": [
            "Wei Yang",
            "Haiting Lin",
            "Sing Bing Kang",
            "Jingyi Yu"
        ],
        "abstract": "In perspective cameras, images of a frontal-parallel 3D object preserve its aspect ratio invariant to its depth. Such an invariance is useful in photography but is unique to perspective projection. In this paper, we show that alternative non-perspective cameras such as the crossed-slit or XSlit cameras exhibit a different depth-dependent aspect ratio (DDAR) property that can be used to 3D recovery. We first conduct a comprehensive analysis to characterize DDAR, infer object depth from its AR, and model recoverable depth range, sensitivity, and error. We show that repeated shape patterns in real Manhattan World scenes can be used for 3D reconstruction using a single XSlit image. We also extend our analysis to model slopes of lines. Specifically, parallel 3D lines exhibit depth-dependent slopes (DDS) on their images which can also be used to infer their depths. We validate our analyses using real XSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show that DDAR and DDS provide important depth cues and enable effective single-image scene reconstruction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410748",
        "reference_list": [
            {
                "year": "2005",
                "id": 84
            },
            {
                "year": "2007",
                "id": 244
            },
            {
                "year": "2007",
                "id": 291
            },
            {
                "year": "2001",
                "id": 2
            },
            {
                "year": "2013",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Three-dimensional displays",
                "Sensitivity",
                "Geometry",
                "Image reconstruction",
                "Analytical models"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single-image scene reconstruction",
                "depth cues",
                "catadioptric mirrors",
                "XSlit panoramas",
                "XSlit cameras",
                "DDS",
                "depth-dependent slopes",
                "parallel 3D lines",
                "3D reconstruction",
                "Manhattan World scenes",
                "repeated shape pattern",
                "recoverable depth range",
                "3D recovery",
                "DDAR property",
                "depth-dependent aspect ratio property",
                "crossed-slit camera",
                "frontal-parallel 3D object image",
                "perspective cameras",
                "XSlit aspect ratio analysis",
                "scale ambiguity"
            ]
        },
        "id": 382,
        "cited_by": []
    },
    {
        "title": "Single-Shot Specular Surface Reconstruction with Gonio-Plenoptic Imaging",
        "authors": [
            "Lingfei Meng",
            "Liyang Lu",
            "Noah Bedard",
            "Kathrin Berkner"
        ],
        "abstract": "We present a gonio-plenoptic imaging system that realizes a single-shot shape measurement for specular surfaces. The system is comprised of a collimated illumination source and a plenoptic camera. Unlike a conventional plenoptic camera, our system captures the BRDF variation of the object surface in a single image in addition to the light field information from the scene, which allows us to recover very fine 3D structures of the surface. The shape of the surface is reconstructed based on the reflectance property of the material rather than the parallax between different views. Since only a single-shot is required to reconstruct the whole surface, our system is able to capture dynamic surface deformation in a video mode. We also describe a novel calibration technique that maps the light field viewing directions from the object space to subpixels on the sensor plane. The proposed system is evaluated using a concave mirror with known curvature, and is compared to a parabolic mirror scanning system as well as a multi-illumination photometric stereo approach based on simulations and experiments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410749",
        "reference_list": [
            {
                "year": "2011",
                "id": 149
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Lenses",
                "Cameras",
                "Lighting",
                "Image reconstruction",
                "Microoptics"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single-shot specular surface reconstruction",
                "gonio-plenoptic imaging",
                "collimated illumination source",
                "plenoptic camera",
                "BRDF variation",
                "light field information",
                "3D structure",
                "dynamic surface deformation",
                "video mode",
                "subpixel",
                "sensor plane",
                "concave mirror",
                "parabolic mirror scanning system",
                "multiillumination photometric stereo approach"
            ]
        },
        "id": 383,
        "cited_by": []
    },
    {
        "title": "TransCut: Transparent Object Segmentation from a Light-Field Image",
        "authors": [
            "Yichao Xu",
            "Hajime Nagahara",
            "Atsushi Shimada",
            "Rin-ichiro Taniguchi"
        ],
        "abstract": "The segmentation of transparent objects can be very useful in computer vision applications. However, because they borrow texture from their background and have a similar appearance to their surroundings, transparent objects are not handled well by regular image segmentation methods. We propose a method that overcomes these problems using the consistency and distortion properties of a light-field image. Graph-cut optimization is applied for the pixel labeling problem. The light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or Lambertian background, and the occlusion detector is used to find the occlusion boundary. We acquire a light field dataset for the transparent object, and use this dataset to evaluate our method. The results demonstrate that the proposed method successfully segments transparent objects from the background.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410750",
        "reference_list": [
            {
                "year": "2003",
                "id": 135
            },
            {
                "year": "2011",
                "id": 315
            },
            {
                "year": "2013",
                "id": 221
            },
            {
                "year": "2011",
                "id": 149
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 2,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Glass",
                "Object segmentation",
                "Detectors",
                "Cameras",
                "Computer vision",
                "Linearity"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "graph theory",
                "image segmentation",
                "image texture",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "TransCut",
                "transparent object segmentation",
                "light-field image",
                "computer vision",
                "image texture",
                "image segmentation method",
                "graph-cut optimization",
                "pixel labeling problem",
                "light-field linearity",
                "occlusion boundary"
            ]
        },
        "id": 384,
        "cited_by": []
    },
    {
        "title": "Depth Recovery from Light Field Using Focal Stack Symmetry",
        "authors": [
            "Haiting Lin",
            "Can Chen",
            "Sing Bing Kang",
            "Jingyi Yu"
        ],
        "abstract": "We describe a technique to recover depth from a light field (LF) using two proposed features of the LF focal stack. One feature is the property that non-occluding pixels exhibit symmetry along the focal depth dimension centered at the in-focus slice. The other is a data consistency measure based on analysis-by-synthesis, i.e., the difference between the synthesized focal stack given the hypothesized depth map and that from the LF. These terms are used in an iterative optimization framework to extract scene depth. Experimental results on real Lytro and Raytrix data demonstrate that our technique outperforms state-of-the-art solutions and is significantly more robust to noise and under-sampling.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410751",
        "reference_list": [
            {
                "year": "2013",
                "id": 123
            },
            {
                "year": "2013",
                "id": 83
            },
            {
                "year": "2013",
                "id": 348
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 18,
            "total": 54
        },
        "keywords": {
            "IEEE Keywords": [
                "Color",
                "Robustness",
                "Cameras",
                "Image color analysis",
                "Noise measurement",
                "Pipelines",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "data integrity",
                "feature extraction",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth recovery",
                "focal stack symmetry",
                "focal depth dimension",
                "data consistency measure",
                "synthesized focal stack",
                "hypothesized depth map",
                "iterative optimization framework",
                "scene depth extraction"
            ]
        },
        "id": 385,
        "cited_by": [
            {
                "year": "2017",
                "id": 1
            },
            {
                "year": "2017",
                "id": 486
            }
        ]
    },
    {
        "title": "Depth Map Estimation and Colorization of Anaglyph Images Using Local Color Prior and Reverse Intensity Distribution",
        "authors": [
            "Williem",
            "Ramesh Raskar",
            "In Kyu Park"
        ],
        "abstract": "In this paper, we present a joint iterative anaglyph stereo matching and colorization framework for obtaining a set of disparity maps and colorized images. Conventional stereo matching algorithms fail when addressing anaglyph images that do not have similar intensities on their two respective view images. To resolve this problem, we propose two novel data costs using local color prior and reverse intensity distribution factor for obtaining accurate depth maps. To colorize an anaglyph image, each pixel in one view is warped to another view using the obtained disparity values of non-occluded regions. A colorization algorithm using optimization is then employed with additional constraint to colorize the remaining occluded regions. Experimental results confirm that the proposed unified framework is robust and produces accurate depth maps and colorized stereo images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410752",
        "reference_list": [
            {
                "year": "2003",
                "id": 136
            },
            {
                "year": "2005",
                "id": 55
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image color analysis",
                "Image reconstruction",
                "Three-dimensional displays",
                "Robustness",
                "Glass",
                "Computer vision",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image colour analysis",
                "image matching",
                "iterative methods",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth map estimation",
                "anaglyph image colorization",
                "local color prior",
                "reverse intensity distribution",
                "joint iterative anaglyph stereo matching",
                "disparity maps",
                "colorized images",
                "data costs",
                "reverse intensity distribution factor",
                "accurate depth maps",
                "occluded regions",
                "colorized stereo images"
            ]
        },
        "id": 386,
        "cited_by": []
    },
    {
        "title": "Learning Data-Driven Reflectance Priors for Intrinsic Image Decomposition",
        "authors": [
            "Tinghui Zhou",
            "Philipp Kr\u00e4henb\u00fchl",
            "Alexei A. Efros"
        ],
        "abstract": "We propose a data-driven approach for intrinsic image decomposition, which is the process of inferring the confounding factors of reflectance and shading in an image. We pose this as a two-stage learning problem. First, we train a model to predict relative reflectance ordering between image patches ('brighter', 'darker', 'same') from large-scale human annotations, producing a data-driven reflectance prior. Second, we show how to naturally integrate this learned prior into existing energy minimization frame-works for intrinsic image decomposition. We compare our method to the state-of-the-art approach of Bell et al. [7] on both decomposition and image relighting tasks, demonstrating the benefits of the simple relative reflectance prior, especially for scenes under challenging lighting conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410753",
        "reference_list": [
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2015",
                "id": 333
            },
            {
                "year": "2015",
                "id": 43
            }
        ],
        "citation": {
            "ieee": 29,
            "other": 9,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Image decomposition",
                "Lighting",
                "Image color analysis",
                "Minimization",
                "Optimization",
                "Streaming media",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "learning (artificial intelligence)",
                "lighting",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning data-driven reflectance prior",
                "intrinsic image decomposition",
                "confounding factor",
                "image shading",
                "two-stage learning problem",
                "relative reflectance ordering",
                "image patches",
                "large-scale human annotation",
                "learned prior",
                "energy minimization framework",
                "image relighting task",
                "relative reflectance prior",
                "lighting condition"
            ]
        },
        "id": 387,
        "cited_by": [
            {
                "year": "2017",
                "id": 2
            },
            {
                "year": "2017",
                "id": 238
            },
            {
                "year": "2015",
                "id": 43
            },
            {
                "year": "2015",
                "id": 333
            }
        ]
    },
    {
        "title": "Photometric Stereo with Small Angular Variations",
        "authors": [
            "Jian Wang",
            "Yasuyuki Matsushita",
            "Boxin Shi",
            "Aswin C. Sankaranarayanan"
        ],
        "abstract": "Most existing successful photometric stereo setups require large angular variations in illumination directions, which results in acquisition rigs that have large spatial extent. For many applications, especially involving mobile devices, it is important that the device be spatially compact. This naturally implies smaller angular variations in the illumination directions. This paper studies the effect of small angular variations in illumination directions to photometric stereo. We explore both theoretical justification and practical issues in the design of a compact and portable photometric stereo device on which a camera is surrounded by a ring of point light sources. We first derive the relationship between the estimation error of surface normal and the baseline of the point light sources. Armed with this theoretical insight, we develop a small baseline photometric stereo prototype to experimentally examine the theory and its practicality.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410754",
        "reference_list": [
            {
                "year": "2005",
                "id": 222
            },
            {
                "year": "2007",
                "id": 100
            },
            {
                "year": "2009",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise measurement",
                "Lighting",
                "Cameras",
                "Light sources",
                "Solids",
                "Estimation error"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image sensors",
                "photometry",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photometric stereo",
                "angular variation",
                "illumination direction",
                "mobile device",
                "surface normal estimation error",
                "point light sources"
            ]
        },
        "id": 388,
        "cited_by": []
    },
    {
        "title": "Occlusion-Aware Depth Estimation Using Light-Field Cameras",
        "authors": [
            "Ting-Chun Wang",
            "Alexei A. Efros",
            "Ravi Ramamoorthi"
        ],
        "abstract": "Consumer-level and high-end light-field cameras are now widely available. Recent work has demonstrated practical methods for passive depth estimation from light-field images. However, most previous approaches do not explicitly model occlusions, and therefore cannot capture sharp transitions around object boundaries. A common assumption is that a pixel exhibits photo-consistency when focused to its correct depth, i.e., all viewpoints converge to a single (Lambertian) point in the scene. This assumption does not hold in the presence of occlusions, making most current approaches unreliable precisely where accurate depth information is most important - at depth discontinuities. In this paper, we develop a depth estimation algorithm that treats occlusion explicitly, the method also enables identification of occlusion edges, which may be useful in other applications. We show that, although pixels at occlusions do not preserve photo-consistency in general, they are still consistent in approximately half the viewpoints. Moreover, the line separating the two view regions (correct depth vs. occluder) has the same orientation as the occlusion edge has in the spatial domain. By treating these two regions separately, depth estimation can be improved. Occlusion predictions can also be computed and used for regularization. Experimental results show that our method outperforms current state-of-the-art light-field depth estimation algorithms, especially near occlusion boundaries.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410755",
        "reference_list": [
            {
                "year": "2013",
                "id": 83
            },
            {
                "year": "2013",
                "id": 348
            }
        ],
        "citation": {
            "ieee": 82,
            "other": 31,
            "total": 113
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image edge detection",
                "Estimation",
                "Mathematical model",
                "Three-dimensional displays",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "edge detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "light-field depth estimation algorithms",
                "occlusion edge identification",
                "depth estimation algorithm",
                "light-field cameras",
                "occlusion-aware depth estimation"
            ]
        },
        "id": 389,
        "cited_by": [
            {
                "year": "2017",
                "id": 1
            },
            {
                "year": "2017",
                "id": 253
            },
            {
                "year": "2017",
                "id": 486
            }
        ]
    },
    {
        "title": "Oriented Light-Field Windows for Scene Flow",
        "authors": [
            "Pratul P. Srinivasan",
            "Michael W. Tao",
            "Ren Ng",
            "Ravi Ramamoorthi"
        ],
        "abstract": "2D spatial image windows are used for comparing pixel values in computer vision applications such as correspondence for optical flow and 3D reconstruction, bilateral filtering, and image segmentation. However, pixel window comparisons can suffer from varying defocus blur and perspective at different depths, and can also lead to a loss of precision. In this paper, we leverage the recent use of light-field cameras to propose alternative oriented light-field windows that enable more robust and accurate pixel comparisons. For Lambertian surfaces focused to the correct depth, the 2D distribution of angular rays from a pixel remains consistent. We build on this idea to develop an oriented 4D light-field window that accounts for shearing (depth), translation (matching), and windowing. Our main application is to scene flow, a generalization of optical flow to the 3D vector field describing the motion of each point in the scene. We show significant benefits of oriented light-field windows over standard 2D spatial windows. We also demonstrate additional applications of oriented light-field windows for bilateral filtering and image segmentation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410756",
        "reference_list": [
            {
                "year": "2007",
                "id": 160
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2013",
                "id": 83
            },
            {
                "year": "2013",
                "id": 171
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 1,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Optical imaging",
                "Image segmentation",
                "Computer vision",
                "Robustness",
                "Optical sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "filtering theory",
                "image reconstruction",
                "image segmentation",
                "image sequences",
                "vectors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "oriented light-field windows",
                "scene flow",
                "2D spatial image windows",
                "computer vision",
                "optical flow",
                "3D reconstruction",
                "bilateral filtering",
                "image segmentation",
                "pixel window",
                "defocus blur",
                "light-field camera",
                "Lambertian surface",
                "angular rays",
                "oriented 4D light-field window",
                "3D vector field"
            ]
        },
        "id": 390,
        "cited_by": [
            {
                "year": "2017",
                "id": 236
            }
        ]
    },
    {
        "title": "Extended Depth of Field Catadioptric Imaging Using Focal Sweep",
        "authors": [
            "Ryunosuke Yokoya",
            "Shree K. Nayar"
        ],
        "abstract": "Catadioptric imaging systems use curved mirrors to capture wide fields of view. However, due to the curvature of the mirror, these systems tend to have very limited depth of field (DOF), with the point spread function (PSF) varying dramatically over the field of view and as a function of scene depth. In recent years, focal sweep has been used extensively to extend the DOF of conventional imaging systems. It has been shown that focal sweep produces an integrated point spread function (IPSF) that is nearly space-invariant and depth-invariant, enabling the recovery of an extended depth of field (EDOF) image by deconvolving the captured focal sweep image with a single IPSF. In this paper, we use focal sweep to extend the DOF of a catadioptric imaging system. We show that while the IPSF is spatially varying when a curved mirror is used, it remains quasi depth-invariant over the wide field of view of the imaging system. We have developed a focal sweep system where mirrors of different shapes can be used to capture wide field of view EDOF images. In particular, we show experimental results using spherical and paraboloidal mirrors.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410757",
        "reference_list": [
            {
                "year": "2007",
                "id": 312
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Mirrors",
                "Lenses",
                "Cameras",
                "Apertures",
                "Optical imaging",
                "Optical sensors"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "extended depth of field",
                "catadioptric imaging systems",
                "scene depth",
                "integrated point spread function",
                "IPSF",
                "space-invariant image",
                "depth-invariant image",
                "focal sweep image",
                "EDOF images",
                "spherical mirrors",
                "paraboloidal mirrors"
            ]
        },
        "id": 391,
        "cited_by": []
    },
    {
        "title": "Intrinsic Depth: Improving Depth Transfer with Intrinsic Images",
        "authors": [
            "Naejin Kong",
            "Michael J. Black"
        ],
        "abstract": "We formulate the estimation of dense depth maps from video sequences as a problem of intrinsic image estimation. Our approach synergistically integrates the estimation of multiple intrinsic images including depth, albedo, shading, optical flow, and surface contours. We build upon an example-based framework for depth estimation that uses label transfer from a database of RGB and depth pairs. We combine this with a method that extracts consistent albedo and shading from video. In contrast to raw RGB values, albedo and shading provide a richer, more physical, foundation for depth transfer. Additionally we train a new contour detector to predict surface boundaries from albedo, shading, and pixel values and use this to improve the estimation of depth boundaries. We also integrate sparse structure from motion with our method to improve the metric accuracy of the estimated depth maps. We evaluate our Intrinsic Depth method quantitatively by estimating depth from videos in the NYU RGB-D and SUN3D datasets. We find that combining the estimation of multiple intrinsic images improves depth estimation relative to the baseline method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410758",
        "reference_list": [
            {
                "year": "2013",
                "id": 30
            },
            {
                "year": "2011",
                "id": 295
            },
            {
                "year": "2013",
                "id": 172
            },
            {
                "year": "2013",
                "id": 202
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Optical imaging",
                "Databases",
                "Lighting",
                "Video sequences",
                "Cameras",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image colour analysis",
                "image motion analysis",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pixel value",
                "structure from motion",
                "contour detector",
                "RGB",
                "label transfer",
                "example-based framework",
                "surface contour",
                "optical flow",
                "shading",
                "albedo",
                "intrinsic image estimation",
                "video sequences",
                "dense depth maps",
                "depth transfer",
                "intrinsic depth estimation"
            ]
        },
        "id": 392,
        "cited_by": []
    },
    {
        "title": "Separating Fluorescent and Reflective Components by Using a Single Hyperspectral Image",
        "authors": [
            "Yinqiang Zheng",
            "Ying Fu",
            "Antony Lam",
            "Imari Sato",
            "Yoichi Sato"
        ],
        "abstract": "This paper introduces a novel method to separate fluorescent and reflective components in the spectral domain. In contrast to existing methods, which require to capture two or more images under varying illuminations, we aim to achieve this separation task by using a single hyperspectral image. After identifying the critical hurdle in single-image component separation, we mathematically design the optimal illumination spectrum, which is shown to contain substantial high-frequency components in the frequency domain. This observation, in turn, leads us to recognize a key difference between reflectance and fluorescence in response to the frequency modulation effect of illumination, which fundamentally explains the feasibility of our method. On the practical side, we successfully find an off-the-shelf lamp as the light source, which is strong in irradiance intensity and cheap in cost. A fast linear separation algorithm is developed as well. Experiments using both synthetic data and real images have confirmed the validity of the selected illuminant and the accuracy of our separation algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410759",
        "reference_list": [
            {
                "year": "2013",
                "id": 56
            },
            {
                "year": "2007",
                "id": 250
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Hyperspectral imaging",
                "Fluorescence",
                "Image color analysis",
                "Light sources",
                "Frequency modulation"
            ],
            "INSPEC: Controlled Indexing": [
                "frequency-domain analysis",
                "hyperspectral imaging",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "fast linear separation algorithm",
                "frequency domain",
                "single hyperspectral image",
                "reflective components separation",
                "fluorescent components separation"
            ]
        },
        "id": 393,
        "cited_by": []
    },
    {
        "title": "Frequency-Based Environment Matting by Compressive Sensing",
        "authors": [
            "Yiming Qian",
            "Minglun Gong",
            "Yee-Hong Yang"
        ],
        "abstract": "Extracting environment mattes using existing approaches often requires either thousands of captured images or a long processing time, or both. In this paper, we propose a novel approach to capturing and extracting the matte of a real scene effectively and efficiently. Grown out of the traditional frequency-based signal analysis, our approach can accurately locate contributing sources. By exploiting the recently developed compressive sensing theory, we simplify the data acquisition process of frequency-based environment matting. Incorporating phase information in a frequency signal into data acquisition further accelerates the matte extraction procedure. Compared with the state-of-the-art method, our approach achieves superior performance on both synthetic and real data, while consuming only a fraction of the processing time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410760",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Discrete Fourier transforms",
                "Data acquisition",
                "Compressed sensing",
                "Frequency-domain analysis",
                "Image reconstruction",
                "Lighting",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "compressed sensing",
                "feature extraction",
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "frequency-based environment matting",
                "compressive sensing",
                "environment matte extraction",
                "real scene",
                "frequency-based signal analysis",
                "data acquisition process"
            ]
        },
        "id": 394,
        "cited_by": []
    },
    {
        "title": "Complementary Sets of Shutter Sequences for Motion Deblurring",
        "authors": [
            "Hae-Gon Jeon",
            "Joon-Young Lee",
            "Yudeog Han",
            "Seon Joo Kim",
            "In So Kweon"
        ],
        "abstract": "In this paper, we present a novel multi-image motion deblurring method utilizing the coded exposure technique. The key idea of our work is to capture video frames with a set of complementary fluttering patterns to preserve spatial frequency details. We introduce an algorithm for generating a complementary set of binary sequences based on the modern communication theory and implement the coded exposure video system with an off-the-shelf machine vision camera. The effectiveness of our method is demonstrated on various challenging examples with quantitative and qualitative comparisons to other computational image capturing methods used for image deblurring.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410761",
        "reference_list": [
            {
                "year": "2013",
                "id": 124
            },
            {
                "year": "2011",
                "id": 27
            },
            {
                "year": "2009",
                "id": 210
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Upper bound",
                "Image restoration",
                "Correlation",
                "Cameras",
                "Deconvolution",
                "Radar imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shutter sequences",
                "multi-image motion deblurring method",
                "video frames",
                "complementary fluttering patterns",
                "spatial frequency details",
                "binary sequences",
                "modern communication theory",
                "off-the-shelf machine vision camera",
                "coded exposure video system",
                "computational image capturing methods"
            ]
        },
        "id": 395,
        "cited_by": []
    },
    {
        "title": "Hyperspectral Compressive Sensing Using Manifold-Structured Sparsity Prior",
        "authors": [
            "Lei Zhang",
            "Wei Wei",
            "Yanning Zhang",
            "Fei Li",
            "Chunhua Shen",
            "Qinfeng Shi"
        ],
        "abstract": "To reconstruct hyperspectral image (HSI) accurately from a few noisy compressive measurements, we present a novel manifold-structured sparsity prior based hyperspectral compressive sensing (HCS) method in this study. A matrix based hierarchical prior is first proposed to represent the spectral structured sparsity and spatial unknown manifold structure of HSI simultaneously. Then, a latent variable Bayes model is introduced to learn the sparsity prior and estimate the noise jointly from measurements. The learned prior can fully represent the inherent 3D structure of HSI and regulate its shape based on the estimated noise level. Thus, with this learned prior, the proposed method improves the reconstruction accuracy significantly and shows strong robustness to unknown noise in HCS. Experiments on four real hyperspectral datasets show that the proposed method outperforms several state-of-the-art methods on the reconstruction accuracy of HSI.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410762",
        "reference_list": [
            {
                "year": "2011",
                "id": 159
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 1,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Three-dimensional displays",
                "Noise measurement",
                "Sparse matrices",
                "Manifolds",
                "Hyperspectral imaging",
                "Correlation"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "compressed sensing",
                "geophysical image processing",
                "image coding",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hyperspectral compressive sensing method",
                "manifold-structured sparsity prior",
                "hyperspectral image reconstruction",
                "matrix based hierarchical prior",
                "latent variable Bayes model"
            ]
        },
        "id": 396,
        "cited_by": []
    },
    {
        "title": "A Gaussian Process Latent Variable Model for BRDF Inference",
        "authors": [
            "Stamatios Georgoulis",
            "Vincent Vanweddingen",
            "Marc Proesmans",
            "Luc Van Gool"
        ],
        "abstract": "The problem of estimating a full BRDF from partial observations has already been studied using either parametric or non-parametric approaches. The goal in each case is to best match this sparse set of input measurements. In this paper we address the problem of inferring higher order reflectance information starting from the minimal input of a single BRDF slice. We begin from the prototypical case of a homogeneous sphere, lit by a head-on light source, which only holds information about less than 0.001% of the whole BRDF domain. We propose a novel method to infer the higher dimensional properties of the material's BRDF, based on the statistical distribution of known material characteristics observed in real-life samples. We evaluated our method based on a large set of experiments generated from real-world BRDFs and newly measured materials. Although inferring higher dimensional BRDFs from such modest training is not a trivial problem, our method performs better than state-of-the-art parametric, semi-parametric and non-parametric approaches. Finally, we discuss interesting applications on material re-lighting, and flash-based photography.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410763",
        "reference_list": [
            {
                "year": "2011",
                "id": 136
            },
            {
                "year": "2009",
                "id": 60
            },
            {
                "year": "2001",
                "id": 80
            },
            {
                "year": "2013",
                "id": 28
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Three-dimensional displays",
                "Manifolds",
                "Computational modeling",
                "Shape",
                "Cameras",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian processes",
                "image processing",
                "statistical distributions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Gaussian process latent variable model",
                "BRDF inference",
                "partial observations",
                "higher order reflectance information",
                "homogeneous sphere",
                "statistical distribution",
                "material r-lighting",
                "flash-based photography"
            ]
        },
        "id": 397,
        "cited_by": [
            {
                "year": "2017",
                "id": 543
            }
        ]
    },
    {
        "title": "Active One-Shot Scan for Wide Depth Range Using a Light Field Projector Based on Coded Aperture",
        "authors": [
            "Hiroshi Kawasaki",
            "Satoshi Ono",
            "Yuki Horita",
            "Yuki Shiba",
            "Ryo Furukawa",
            "Shinsaku Hiura"
        ],
        "abstract": "The central projection model commonly used to model cameras as well as projectors, results in similar advantages and disadvantages in both types of system. Considering the case of active stereo systems using a projector and camera setup, a central projection model creates several problems, among them, narrow depth range and necessity of wide baseline are crucial. In the paper, we solve the problems by introducing a light field projector, which can project a depth-dependent pattern. The light field projector is realized by attaching a coded aperture with a high frequency mask in front of the lens of the video projector, which also projects a high frequency pattern. Because the light field projector cannot be approximated by a thin lens model and a precise calibration method is not established yet, an image-based approach is proposed to apply a stereo technique to the system. Although image-based techniques usually require a large database and often imply heavy computational costs, we propose a hierarchical approach and a feature-based search for solution. In the experiments, it is confirmed that our method can accurately recover the dense shape of curved and textured objects for a wide range of depths from a single captured image.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410764",
        "reference_list": [
            {
                "year": "2011",
                "id": 242
            },
            {
                "year": "2009",
                "id": 41
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Apertures",
                "Cameras",
                "Shape",
                "Lenses",
                "Image reconstruction",
                "Optical imaging",
                "Convolution"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image coding",
                "stereo image processing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "central projection model",
                "active stereo systems",
                "camera setup",
                "active one-shot scan",
                "wide depth range",
                "light field projector",
                "coded aperture",
                "feature-based search",
                "hierarchical approach",
                "image-based techniques",
                "image-based approach",
                "precise calibration method",
                "thin lens model",
                "video projector",
                "depth-dependent pattern",
                "narrow depth range"
            ]
        },
        "id": 398,
        "cited_by": [
            {
                "year": "2017",
                "id": 12
            },
            {
                "year": "2017",
                "id": 487
            }
        ]
    },
    {
        "title": "Model-Based Tracking at 300Hz Using Raw Time-of-Flight Observations",
        "authors": [
            "Jan St\u00fchmer",
            "Sebastian Nowozin",
            "Andrew Fitzgibbon",
            "Richard Szeliski",
            "Travis Perry",
            "Sunil Acharya",
            "Daniel Cremers",
            "Jamie Shotton"
        ],
        "abstract": "Consumer depth cameras have dramatically improved our ability to track rigid, articulated, and deformable 3D objects in real-time. However, depth cameras have a limited temporal resolution (frame-rate) that restricts the accuracy and robustness of tracking, especially for fast or unpredictable motion. In this paper, we show how to perform model-based object tracking which allows to reconstruct the object's depth at an order of magnitude higher frame-rate through simple modifications to an off-the-shelf depth camera. We focus on phase-based time-of-flight (ToF) sensing, which reconstructs each low frame-rate depth image from a set of short exposure 'raw' infrared captures. These raw captures are taken in quick succession near the beginning of each depth frame, and differ in the modulation of their active illumination. We make two contributions. First, we detail how to perform model-based tracking against these raw captures. Second, we show that by reprogramming the camera to space the raw captures uniformly in time, we obtain a 10x higher frame-rate, and thereby improve the ability to track fast-moving objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410765",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 1,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Sensors",
                "Computational modeling",
                "Image reconstruction",
                "Cameras",
                "Lighting",
                "Three-dimensional displays"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image capture",
                "image motion analysis",
                "image reconstruction",
                "image resolution",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based tracking",
                "raw time-of-flight observation",
                "consumer depth camera",
                "3D object tracking",
                "temporal resolution",
                "unpredictable motion",
                "model-based object tracking",
                "magnitude higher frame-rate",
                "off-the-shelf depth camera",
                "phase-based time-of-flight sensing",
                "ToF sensing",
                "frame-rate depth image reconstruction",
                "raw infrared capture",
                "raw capture",
                "active illumination",
                "fast-moving object"
            ]
        },
        "id": 399,
        "cited_by": []
    },
    {
        "title": "Hyperspectral Super-Resolution by Coupled Spectral Unmixing",
        "authors": [
            "Charis Lanaras",
            "Emmanuel Baltsavias",
            "Konrad Schindler"
        ],
        "abstract": "Hyperspectral cameras capture images with many narrow spectral channels, which densely sample the electromagnetic spectrum. The detailed spectral resolution is useful for many image analysis problems, but it comes at the cost of much lower spatial resolution. Hyperspectral super-resolution addresses this problem, by fusing a low-resolution hyperspectral image and a conventional high-resolution image into a product of both high spatial and high spectral resolution. In this paper, we propose a method which performs hyperspectral super-resolution by jointly unmixing the two input images into the pure reflectance spectra of the observed materials and the associated mixing coefficients. The formulation leads to a coupled matrix factorisation problem, with a number of useful constraints imposed by elementary physical properties of spectral mixing. In experiments with two benchmark datasets we show that the proposed approach delivers improved hyperspectral super-resolution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410766",
        "reference_list": [],
        "citation": {
            "ieee": 56,
            "other": 14,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatial resolution",
                "Hyperspectral imaging",
                "Cameras",
                "Signal resolution",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "hyperspectral imaging",
                "image resolution",
                "matrix decomposition",
                "spectral analysers",
                "spectral analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hyperspectral super-resolution",
                "coupled spectral unmixing",
                "hyperspectral cameras",
                "image capture",
                "narrow spectral channels",
                "electromagnetic spectrum",
                "detailed spectral resolution",
                "image analysis problems",
                "low-resolution hyperspectral image",
                "reflectance spectra",
                "mixing coefficients",
                "matrix factorisation problem",
                "physical properties"
            ]
        },
        "id": 400,
        "cited_by": []
    },
    {
        "title": "Depth Selective Camera: A Direct, On-Chip, Programmable Technique for Depth Selectivity in Photography",
        "authors": [
            "Ryuichi Tadano",
            "Adithya Kumar Pediredla",
            "Ashok Veeraraghavan"
        ],
        "abstract": "Time of flight (ToF) cameras use a temporally modulated light source and measure correlation between the reflected light and a sensor modulation pattern, in order to infer scene depth. In this paper, we show that such correlational sensors can also be used to selectively accept or reject light rays from certain scene depths. The basic idea is to carefully select illumination and sensor modulation patterns such that the correlation is non-zero only in the selected depth range - thus light reflected from objects outside this depth range do not affect the correlational measurements. We demonstrate a prototype depth-selective camera and highlight two potential applications: imaging through scattering media and virtual blue screening. This depth-selectivity can be used to reject back-scattering and reflection from media in front of the subjects of interest, thereby significantly enhancing the ability to image through scattering media-critical for applications such as car navigation in fog and rain. Similarly, such depth selectivity can also be utilized as a virtual blue-screen in cinematography by rejecting light reflecting from background, while selectively retaining light contributions from the foreground subject.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410767",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Lighting",
                "Scattering",
                "Rain",
                "Light sources",
                "Modulation"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "correlation methods",
                "image sensors",
                "photography"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "depth selective camera",
                "on-chip technique",
                "programmable technique",
                "depth selectivity",
                "photography",
                "time of flight",
                "ToF cameras",
                "temporally modulated light source",
                "reflected light",
                "correlational sensors",
                "illumination",
                "sensor modulation patterns",
                "prototype depth-selective camera",
                "scattering media",
                "virtual blue screening",
                "back-scattering",
                "car navigation",
                "fog",
                "rain",
                "virtual blue-screen",
                "cinematography"
            ]
        },
        "id": 401,
        "cited_by": []
    },
    {
        "title": "A Groupwise Multilinear Correspondence Optimization for 3D Faces",
        "authors": [
            "Timo Bolkart",
            "Stefanie Wuhrer"
        ],
        "abstract": "Multilinear face models are widely used to model the space of human faces with expressions. For databases of 3D human faces of different identities performing multiple expressions, these statistical shape models decouple identity and expression variations. To compute a high-quality multilinear face model, the quality of the registration of the database of 3D face scans used for training is essential. Meanwhile, a multilinear face model can be used as an effective prior to register 3D face scans, which are typically noisy and incomplete. Inspired by the minimum description length approach, we propose the first method to jointly optimize a multilinear model and the registration of the 3D scans used for training. Given an initial registration, our approach fully automatically improves the registration by optimizing an objective function that measures the compactness of the multilinear model, resulting in a sparse model. We choose a continuous representation for each face shape that allows to use a quasi-Newton method in parameter space for optimization. We show that our approach is computationally significantly more efficient and leads to correspondences of higher quality than existing methods based on linear statistical models. This allows us to evaluate our approach on large standard 3D face databases and in the presence of noisy initializations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410768",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 8,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Computational modeling",
                "Solid modeling",
                "Shape",
                "Optimization",
                "Tensile stress",
                "Principal component analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "emotion recognition",
                "face recognition",
                "image registration",
                "Newton method",
                "statistical analysis",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "groupwise multilinear correspondence optimization",
                "multilinear face models",
                "statistical shape models",
                "identity variations",
                "expression variations",
                "high-quality multilinear face model",
                "3D face scan registration",
                "minimum description length approach",
                "quasi-Newton method",
                "parameter space"
            ]
        },
        "id": 402,
        "cited_by": []
    },
    {
        "title": "Selective Encoding for Recognizing Unreliably Localized Faces",
        "authors": [
            "Ang Li",
            "Vlad I. Morariu",
            "Larry S. Davis"
        ],
        "abstract": "Most existing face verification systems rely on precise face detection and registration. However, these two components are fallible under unconstrained scenarios (e.g., mobile face authentication) due to partial occlusions, pose variations, lighting conditions and limited view-angle coverage of mobile cameras. We address the unconstrained face verification problem by encoding face images directly without any explicit models of detection or registration. We propose a selective encoding framework which injects relevance information (e.g., foreground/background probabilities) into each cluster of a descriptor codebook. An additional selector component also discards distractive image patches and improves spatial robustness. We evaluate our framework using Gaussian mixture models and Fisher vectors on challenging face verification datasets. We apply selective encoding to Fisher vector features, which in our experiments degrade quickly with inaccurate face localization, our framework improves robustness with no extra test time computation. We also apply our approach to mobile based active face authentication task, demonstrating its utility in real scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410769",
        "reference_list": [
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2009",
                "id": 247
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Encoding",
                "Face recognition",
                "Robustness",
                "Face detection",
                "Videos",
                "Mobile communication",
                "Authentication"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "Gaussian processes",
                "image registration",
                "image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "selective encoding",
                "recognizing unreliably localized faces",
                "face verification systems",
                "face detection",
                "face registration",
                "mobile face authentication",
                "partial occlusions",
                "pose variations",
                "lighting conditions",
                "limited view angle coverage",
                "mobile cameras",
                "unconstrained face verification problem",
                "selective encoding framework",
                "descriptor codebook",
                "image patches",
                "spatial robustness",
                "Gaussian mixture models",
                "Fisher vectors"
            ]
        },
        "id": 403,
        "cited_by": []
    },
    {
        "title": "Confidence Preserving Machine for Facial Action Unit Detection",
        "authors": [
            "Jiabei Zeng",
            "Wen-Sheng Chu",
            "Fernando De la Torre",
            "Jeffrey F. Cohn",
            "Zhang Xiong"
        ],
        "abstract": "Varied sources of error contribute to the challenge of facial action unit detection. Previous approaches address specific and known sources. However, many sources are unknown. To address the ubiquity of error, we propose a Confident Preserving Machine (CPM) that follows an easy-to-hard classification strategy. During training, CPM learns two confident classifiers. A confident positive classifier separates easily identified positive samples from all else, a confident negative classifier does same for negative samples. During testing, CPM then learns a person-specific classifier using \"virtual labels\" provided by confident classifiers. This step is achieved using a quasi-semi-supervised (QSS) approach. Hard samples are typically close to the decision boundary, and the QSS approach disambiguates them using spatio-temporal constraints. To evaluate CPM, we compared it with a baseline single-margin classifier and state-of-the-art semi-supervised learning, transfer learning, and boosting methods in three datasets of spontaneous facial behavior. With few exceptions, CPM outperformed baseline and state-of-the art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410770",
        "reference_list": [
            {
                "year": "2013",
                "id": 299
            },
            {
                "year": "2013",
                "id": 369
            },
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2011",
                "id": 183
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 7,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Gold",
                "Support vector machines",
                "Magnetic heads",
                "Manifolds",
                "Testing",
                "Boosting"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial action unit detection",
                "confident preserving machine",
                "CPM",
                "easy-to-hard classification strategy",
                "quasi-semisupervised approach",
                "QSS approach",
                "decision boundary",
                "spatio-temporal constraint",
                "baseline single-margin classifier",
                "semisupervised learning",
                "transfer learning",
                "boosting method",
                "spontaneousfacial behavior"
            ]
        },
        "id": 404,
        "cited_by": [
            {
                "year": "2017",
                "id": 418
            }
        ]
    },
    {
        "title": "Learning Social Relation Traits from Face Images",
        "authors": [
            "Zhanpeng Zhang",
            "Ping Luo",
            "Chen-Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "Social relation defines the association, e.g., warm, friendliness, and dominance, between two or more people. Motivated by psychological studies, we investigate if such fine grained and high-level relation traits can be characterised and quantified from face images in the wild. To address this challenging problem we propose a deep model that learns a rich face representation to capture gender, expression, head pose, and age-related attributes, and then performs pairwise-face reasoning for relation prediction. To learn from heterogeneous attribute sources, we formulate a new network architecture with a bridging layer to leverage the inherent correspondences among these datasets. It can also cope with missing target attribute labels. Extensive experiments show that our approach is effective for fine-grained social relation learning in images and videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410771",
        "reference_list": [
            {
                "year": "2011",
                "id": 88
            },
            {
                "year": "2015",
                "id": 416
            },
            {
                "year": "2013",
                "id": 357
            },
            {
                "year": "2013",
                "id": 185
            },
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 13,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Psychology",
                "Computer vision",
                "Face recognition",
                "Cognition",
                "Videos"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image representation",
                "psychology",
                "social sciences computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "social relation trait learning",
                "face images",
                "psychological studies",
                "high-level relation traits",
                "deep model",
                "face representation",
                "pairwise-face reasoning",
                "relation prediction",
                "heterogeneous attribute sources",
                "network architecture",
                "fine-grained social relation learning"
            ]
        },
        "id": 405,
        "cited_by": [
            {
                "year": "2017",
                "id": 279
            }
        ]
    },
    {
        "title": "Robust Heart Rate Measurement from Video Using Select Random Patches",
        "authors": [
            "Antony Lam",
            "Yoshinori Kuno"
        ],
        "abstract": "The ability to remotely measure heart rate from videos without requiring any special setup is beneficial to many applications. In recent years, a number of papers on heart rate (HR) measurement from videos have been proposed. However, these methods typically require the human subject to be stationary and for the illumination to be controlled. For methods that do take into account motion and illumination changes, strong assumptions are still made about the environment (e.g. background can be used for illumination rectification). In this paper, we propose an HR measurement method that is robust to motion, illumination changes, and does not require use of an environment's background. We present conditions under which cardiac activity extraction from local regions of the face can be treated as a linear Blind Source Separation problem and propose a simple but robust algorithm for selecting good local regions. The independent HR estimates from multiple local regions are then combined in a majority voting scheme that robustly recovers the HR. We validate our algorithm on a large database of challenging videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410772",
        "reference_list": [
            {
                "year": "2013",
                "id": 242
            }
        ],
        "citation": {
            "ieee": 23,
            "other": 12,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Skin",
                "Face",
                "Heart rate",
                "Cameras",
                "Robustness",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "blind source separation",
                "cardiology",
                "image motion analysis",
                "medical image processing",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "majority voting scheme",
                "linear blind source separation",
                "cardiac activity extraction",
                "illumination",
                "select random patches",
                "video processing",
                "robust heart rate measurement"
            ]
        },
        "id": 406,
        "cited_by": []
    },
    {
        "title": "Robust Model-Based 3D Head Pose Estimation",
        "authors": [
            "Gregory P. Meyer",
            "Shalini Gupta",
            "Iuri Frosio",
            "Dikpal Reddy",
            "Jan Kautz"
        ],
        "abstract": "We introduce a method for accurate three dimensional head pose estimation using a commodity depth camera. We perform pose estimation by registering a morphable face model to the measured depth data, using a combination of particle swarm optimization (PSO) and the iterative closest point (ICP) algorithm, which minimizes a cost function that includes a 3D registration and a 2D overlap term. The pose is estimated on the fly without requiring an explicit initialization or training phase. Our method handles large pose angles and partial occlusions by dynamically adapting to the reliable visible parts of the face. It is robust and generalizes to different depth sensors without modification. On the Biwi Kinect dataset, we achieve best-in-class performance, with average angular errors of 2.1, 2.1 and 2.4 degrees for yaw, pitch, and roll, respectively, and an average translational error of 5.9 mm, while running at 6 fps on a graphics processing unit.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410773",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 7,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Face",
                "Solid modeling",
                "Iterative closest point algorithm",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "face recognition",
                "image morphing",
                "image registration",
                "iterative methods",
                "particle swarm optimisation",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust model-based 3D head pose estimation",
                "three dimensional head pose estimation",
                "commodity depth camera",
                "morphable face model",
                "measured depth data",
                "particle swarm optimization",
                "PSO",
                "iterative closest point algorithm",
                "ICP algorithm",
                "cost function",
                "3D registration",
                "2D overlap term",
                "pose angle",
                "partial occlusion",
                "depth sensor",
                "Biwi Kinect dataset",
                "best-in-class performance",
                "angular error",
                "translational error",
                "graphics processing unit"
            ]
        },
        "id": 407,
        "cited_by": []
    },
    {
        "title": "Robust Facial Landmark Detection Under Significant Head Poses and Occlusion",
        "authors": [
            "Yue Wu",
            "Qiang Ji"
        ],
        "abstract": "There have been tremendous improvements for facial landmark detection on general \"in-the-wild\" images. However, it is still challenging to detect the facial landmarks on images with severe occlusion and images with large head poses (e.g. profile face). In fact, the existing algorithms usually can only handle one of them. In this work, we propose a unified robust cascade regression framework that can handle both images with severe occlusion and images with large head poses. Specifically, the method iteratively predicts the landmark occlusions and the landmark locations. For occlusion estimation, instead of directly predicting the binary occlusion vectors, we introduce a supervised regression method that gradually updates the landmark visibility probabilities in each iteration to achieve robustness. In addition, we explicitly add occlusion pattern as a constraint to improve the performance of occlusion prediction. For landmark detection, we combine the landmark visibility probabilities, the local appearances, and the local shapes to iteratively update their positions. The experimental results show that the proposed method is significantly better than state-of-the-art works on images with severe occlusion and images with large head poses. It is also comparable to other methods on general \"in-the-wild\" images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410774",
        "reference_list": [
            {
                "year": "2013",
                "id": 188
            },
            {
                "year": "2013",
                "id": 242
            },
            {
                "year": "2013",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 10,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Robustness",
                "Predictive models",
                "Mathematical model",
                "Estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "performance evaluation",
                "pose estimation",
                "probability",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust facial landmark detection",
                "head poses",
                "in-the-wild images",
                "unified robust cascade regression framework",
                "landmark occlusion prediction",
                "landmark location prediction",
                "occlusion estimation",
                "binary occlusion vector prediction",
                "supervised regression method",
                "landmark visibility probabilities",
                "occlusion pattern",
                "performance improvement",
                "local appearances",
                "local shapes"
            ]
        },
        "id": 408,
        "cited_by": []
    },
    {
        "title": "Conditional Convolutional Neural Network for Modality-Aware Face Recognition",
        "authors": [
            "Chao Xiong",
            "Xiaowei Zhao",
            "Danhang Tang",
            "Karlekar Jayashree",
            "Shuicheng Yan",
            "Tae-Kyun Kim"
        ],
        "abstract": "Faces in the wild are usually captured with various poses, illuminations and occlusions, and thus inherently multimodally distributed in many tasks. We propose a conditional Convolutional Neural Network, named as c-CNN, to handle multimodal face recognition. Different from traditional CNN that adopts fixed convolution kernels, samples in c-CNN are processed with dynamically activated sets of kernels. In particular, convolution kernels within each layer are only sparsely activated when a sample is passed through the network. For a given sample, the activations of convolution kernels in a certain layer are conditioned on its present intermediate representation and the activation status in the lower layers. The activated kernels across layers define the sample-specific adaptive routes that reveal the distribution of underlying modalities. Consequently, the proposed framework does not rely on any prior knowledge of modalities in contrast with most existing methods. To substantiate the generic framework, we introduce a special case of c-CNN via incorporating the conditional routing of the decision tree, which is evaluated with two problems of multimodality - multi-view face identification and occluded face verification. Extensive experiments demonstrate consistent improvements over the counterparts unaware of modalities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410775",
        "reference_list": [
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 6,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Face",
                "Convolution",
                "Feature extraction",
                "Face recognition",
                "Training",
                "Decision trees"
            ],
            "INSPEC: Controlled Indexing": [
                "decision trees",
                "face recognition",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "occluded face verification",
                "multiview face identification",
                "multimodality",
                "decision tree",
                "conditional routing",
                "generic framework",
                "sample-specific adaptive routes",
                "activated kernels",
                "activation status",
                "dynamically activated sets",
                "fixed convolution kernels",
                "multimodal face recognition",
                "c-CNN",
                "occlusions",
                "illuminations",
                "modality-aware face recognition",
                "conditional convolutional neural network"
            ]
        },
        "id": 409,
        "cited_by": [
            {
                "year": "2017",
                "id": 257
            }
        ]
    },
    {
        "title": "From Facial Parts Responses to Face Detection: A Deep Learning Approach",
        "authors": [
            "Shuo Yang",
            "Ping Luo",
            "Chen-Change Loy",
            "Xiaoou Tang"
        ],
        "abstract": "In this paper, we propose a novel deep convolutional network (DCN) that achieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically, our method achieves a high recall rate of 90.99% on the challenging FDDB benchmark, outperforming the state-of-the-art method [23] by a large margin of 2.91%. Importantly, we consider finding faces from a new perspective through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variation, which are the main difficulty and bottleneck of most existing face detection approaches. We show that despite the use of DCN, our network can achieve practical runtime speed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410776",
        "reference_list": [
            {
                "year": "2013",
                "id": 98
            }
        ],
        "citation": {
            "ieee": 121,
            "other": 62,
            "total": 183
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Face detection",
                "Proposals",
                "Hair",
                "Mouth",
                "Detectors",
                "Nose"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial parts responses",
                "face detection",
                "deep learning approach",
                "novel deep convolutional network",
                "DCN",
                "FDDB",
                "PASCAL Face",
                "AFW",
                "scoring facial parts",
                "spatial structure",
                "spatial arrangement",
                "unconstrained pose variation"
            ]
        },
        "id": 410,
        "cited_by": [
            {
                "year": "2017",
                "id": 20
            },
            {
                "year": "2017",
                "id": 334
            },
            {
                "year": "2017",
                "id": 399
            },
            {
                "year": "2017",
                "id": 512
            },
            {
                "year": "2017",
                "id": 512
            }
        ]
    },
    {
        "title": "Efficient PSD Constrained Asymmetric Metric Learning for Person Re-Identification",
        "authors": [
            "Shengcai Liao",
            "Stan Z. Li"
        ],
        "abstract": "Person re-identification is becoming a hot research topic due to its value in both machine learning research and video surveillance applications. For this challenging problem, distance metric learning is shown to be effective in matching person images. However, existing approaches either require a heavy computation due to the positive semidefinite (PSD) constraint, or ignore the PSD constraint and learn a free distance function that makes the learned metric potentially noisy. We argue that the PSD constraint provides a useful regularization to smooth the solution of the metric, and hence the learned metric is more robust than without the PSD constraint. Another problem with metric learning algorithms is that the number of positive sample pairs is very limited, and the learning process is largely dominated by the large amount of negative sample pairs. To address the above issues, we derive a logistic metric learning approach with the PSD constraint and an asymmetric sample weighting strategy. Besides, we successfully apply the accelerated proximal gradient approach to find a global minimum solution of the proposed formulation, with a convergence rate of O(1/t^2) where t is the number of iterations. The proposed algorithm termed MLAPG is shown to be computationally efficient and able to perform low rank selection. We applied the proposed method for person re-identification, achieving state-of-the-art performance on four challenging databases (VIPeR, QMUL GRID, CUHK Campus, and CUHK03), compared to existing metric learning methods as well as published results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410777",
        "reference_list": [
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2013",
                "id": 315
            }
        ],
        "citation": {
            "ieee": 96,
            "other": 49,
            "total": 145
        },
        "keywords": {
            "IEEE Keywords": [
                "Measurement",
                "Logistics",
                "Convergence",
                "Acceleration",
                "Linear programming",
                "Optimization",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "convergence",
                "gradient methods",
                "image matching",
                "learning (artificial intelligence)",
                "logistics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "CUHK03 database",
                "CUHK Campus database",
                "QMUL GRID database",
                "VIPeR database",
                "low rank selection",
                "MLAPG",
                "convergence rate",
                "global minimum solution",
                "accelerated proximal gradient approach",
                "asymmetric sample weighting strategy",
                "logistic metric learning approach",
                "PSD constraint",
                "positive semidefinite constraint",
                "person image matching",
                "distance metric learning",
                "video surveillance applications",
                "machine learning research",
                "person reidentification",
                "PSD constrained asymmetric metric learning process"
            ]
        },
        "id": 411,
        "cited_by": [
            {
                "year": "2017",
                "id": 31
            },
            {
                "year": "2017",
                "id": 39
            },
            {
                "year": "2017",
                "id": 103
            },
            {
                "year": "2017",
                "id": 255
            },
            {
                "year": "2017",
                "id": 258
            },
            {
                "year": "2017",
                "id": 339
            },
            {
                "year": "2017",
                "id": 400
            },
            {
                "year": "2017",
                "id": 417
            },
            {
                "year": "2017",
                "id": 497
            },
            {
                "year": "2017",
                "id": 540
            },
            {
                "year": "2017",
                "id": 565
            }
        ]
    },
    {
        "title": "Pose-Invariant 3D Face Alignment",
        "authors": [
            "Amin Jourabloo",
            "Xiaoming Liu"
        ],
        "abstract": "Face alignment aims to estimate the locations of a set of landmarks for a given image. This problem has received much attention as evidenced by the recent advancement in both the methodology and performance. However, most of the existing works neither explicitly handle face images with arbitrary poses, nor perform large-scale experiments on non-frontal and profile face images. In order to address these limitations, this paper proposes a novel face alignment algorithm that estimates both 2D and 3D landmarks and their 2D visibilities for a face image with an arbitrary pose. By integrating a 3D point distribution model, a cascaded coupled-regressor approach is designed to estimate both the camera projection matrix and the 3D landmarks. Furthermore, the 3D model also allows us to automatically estimate the 2D landmark visibilities via surface normal. We use a substantially larger collection of all-pose face images to evaluate our algorithm and demonstrate superior performances than the state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410778",
        "reference_list": [
            {
                "year": "2009",
                "id": 132
            },
            {
                "year": "2013",
                "id": 73
            },
            {
                "year": "2011",
                "id": 40
            },
            {
                "year": "2013",
                "id": 241
            },
            {
                "year": "2013",
                "id": 242
            }
        ],
        "citation": {
            "ieee": 40,
            "other": 20,
            "total": 60
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Three-dimensional displays",
                "Shape",
                "Solid modeling",
                "Estimation",
                "Cameras",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "face recognition",
                "pose estimation",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pose-invariant 3D face alignment",
                "locations estimation",
                "image landmarks",
                "2D landmarks estimation",
                "3D landmarks estimation",
                "arbitrary pose",
                "3D point distribution model",
                "cascaded coupled-regressor approach",
                "camera projection matrix",
                "3D model",
                "2D landmark visibilities",
                "surface normal",
                "all-pose face images"
            ]
        },
        "id": 412,
        "cited_by": [
            {
                "year": "2017",
                "id": 171
            },
            {
                "year": "2017",
                "id": 337
            },
            {
                "year": "2017",
                "id": 390
            },
            {
                "year": "2017",
                "id": 419
            },
            {
                "year": "2017",
                "id": 420
            },
            {
                "year": "2017",
                "id": 496
            }
        ]
    },
    {
        "title": "From Emotions to Action Units with Hidden and Semi-Hidden-Task Learning",
        "authors": [
            "Adria Ruiz",
            "Joost Van de Weijer",
            "Xavier Binefa"
        ],
        "abstract": "Limited annotated training data is a challenging problem in Action Unit recognition. In this paper, we investigate how the use of large databases labelled according to the 6 universal facial expressions can increase the generalization ability of Action Unit classifiers. For this purpose, we propose a novel learning framework: Hidden-Task Learning. HTL aims to learn a set of Hidden-Tasks (Action Units) for which samples are not available but, in contrast, training data is easier to obtain from a set of related Visible-Tasks (Facial Expressions). To that end, HTL is able to exploit prior knowledge about the relation between Hidden and Visible-Tasks. In our case, we base this prior knowledge on empirical psychological studies providing statistical correlations between Action Units and universal facial expressions. Additionally, we extend HTL to Semi-Hidden Task Learning (SHTL) assuming that Action Unit training samples are also provided. Performing exhaustive experiments over four different datasets, we show that HTL and SHTL improve the generalization ability of AU classifiers by training them with additional facial expression data. Additionally, we show that SHTL achieves competitive performance compared with state-of-the-art Transductive Learning approaches which face the problem of limited training data by using unlabelled test samples during training.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410779",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Gold",
                "Training data",
                "Face recognition",
                "Databases",
                "Psychology",
                "Hidden Markov models"
            ],
            "INSPEC: Controlled Indexing": [
                "computer based training",
                "emotion recognition",
                "image motion analysis",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "emotions",
                "semi-hidden-task learning",
                "training data",
                "action unit recognition",
                "universal facial expressions",
                "action unit classifiers",
                "hidden task learning",
                "visible tasks facial expressions",
                "statistical correlations",
                "SHTL",
                "action unit training samples",
                "transductive learning"
            ]
        },
        "id": 413,
        "cited_by": [
            {
                "year": "2017",
                "id": 416
            }
        ]
    },
    {
        "title": "Automated Facial Trait Judgment and Election Outcome Prediction: Social Dimensions of Face",
        "authors": [
            "Jungseock Joo",
            "Francis F. Steen",
            "Song-Chun Zhu"
        ],
        "abstract": "The human face is a primary medium of human communication and a prominent source of information used to infer various attributes. In this paper, we study a fully automated system that can infer the perceived traits of a person from his face -- social dimensions, such as \"intelligence,\" \"honesty,\" and \"competence\" -- and how those traits can be used to predict the outcomes of real-world social events that involve long-term commitments, such as political elections, job hires, and marriage engagements. To this end, we propose a hierarchical model for enduring traits inferred from faces, incorporating high-level perceptions and intermediate-level attributes. We show that our trained model can successfully classify the outcomes of two important political events, only using the photographs of politicians' faces. Firstly, it classifies the winners of a series of recent U. S. elections with the accuracy of 67.9% (Governors) and 65.5% (Senators). We also reveal that the different political offices require different types of preferred traits. Secondly, our model can categorize the political party affiliations of politicians, i.e., Democrats vs. Republicans, with the accuracy of 62.6% (male) and 60.1% (female). To the best of our knowledge, our paper is the first to use automated visual trait analysis to predict the outcomes of real-world social events. This approach is more scalable and objective than the prior behavioral studies, and opens for a range of new applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410780",
        "reference_list": [
            {
                "year": "2013",
                "id": 89
            },
            {
                "year": "2011",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Nominations and elections",
                "History",
                "Computer vision",
                "Visualization",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "politics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automated visual trait analysis",
                "United States elections",
                "political elections",
                "face social dimensions",
                "election outcome prediction",
                "automated facial trait judgment"
            ]
        },
        "id": 414,
        "cited_by": []
    },
    {
        "title": "Simultaneous Local Binary Feature Learning and Encoding for Face Recognition",
        "authors": [
            "Jiwen Lu",
            "Venice Erin Liong",
            "Jie Zhou"
        ],
        "abstract": "In this paper, we propose a simultaneous local binary feature learning and encoding (SLBFLE) method for face recognition. Different from existing hand-crafted face descriptors such as local binary pattern (LBP) and Gabor features which require strong prior knowledge, our SLBFLE is an unsupervised feature learning approach which is automatically learned from raw pixels. Unlike existing binary face descriptors such as the LBP and discriminant face descriptor (DFD) which use a two-stage feature extraction approach, our SLBFLE jointly learns binary codes for local face patches and the codebook for feature encoding so that discriminative information from raw pixels can be simultaneously learned with a one-stage procedure. Experimental results on four widely used face datasets including LFW, YouTube Face (YTF), FERET and PaSC clearly demonstrate the effectiveness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410781",
        "reference_list": [
            {
                "year": "2013",
                "id": 244
            },
            {
                "year": "2013",
                "id": 400
            },
            {
                "year": "2005",
                "id": 101
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 5,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Feature extraction",
                "Encoding",
                "Binary codes",
                "Dictionaries",
                "Face recognition",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "Gabor filters",
                "image coding",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "simultaneous local binary feature learning and encoding method",
                "SLBFLE method",
                "hand-crafted face descriptors",
                "local binary pattern",
                "LBP",
                "Gabor features",
                "unsupervised feature learning approach",
                "binary face descriptors",
                "discriminant face descriptor",
                "DFD",
                "two-stage feature extraction approach",
                "binary codes",
                "local face patches",
                "LFW",
                "YouTube Face",
                "FERET",
                "PaSC"
            ]
        },
        "id": 415,
        "cited_by": []
    },
    {
        "title": "Deep Learning Face Attributes in the Wild",
        "authors": [
            "Ziwei Liu",
            "Ping Luo",
            "Xiaogang Wang",
            "Xiaoou Tang"
        ],
        "abstract": "Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410782",
        "reference_list": [
            {
                "year": "2011",
                "id": 195
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2013",
                "id": 357
            }
        ],
        "citation": {
            "ieee": 355,
            "other": 204,
            "total": 559
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Feature extraction",
                "Training",
                "Face recognition",
                "Machine learning",
                "Support vector machines",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feedforward neural nets",
                "image representation",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep learning face attribute prediction",
                "complex face variations",
                "CNN",
                "LNet",
                "ANet",
                "image-level attribute tags",
                "face identities",
                "face representation learning",
                "face localization",
                "pretraining strategies",
                "image-level annotations",
                "attribute recognition",
                "automatic semantic concept discovery"
            ]
        },
        "id": 416,
        "cited_by": [
            {
                "year": "2017",
                "id": 26
            },
            {
                "year": "2017",
                "id": 114
            },
            {
                "year": "2017",
                "id": 160
            },
            {
                "year": "2017",
                "id": 177
            },
            {
                "year": "2017",
                "id": 195
            },
            {
                "year": "2017",
                "id": 215
            },
            {
                "year": "2017",
                "id": 235
            },
            {
                "year": "2017",
                "id": 292
            },
            {
                "year": "2017",
                "id": 338
            },
            {
                "year": "2017",
                "id": 387
            },
            {
                "year": "2017",
                "id": 391
            },
            {
                "year": "2017",
                "id": 394
            },
            {
                "year": "2017",
                "id": 457
            },
            {
                "year": "2017",
                "id": 571
            },
            {
                "year": "2015",
                "id": 405
            }
        ]
    },
    {
        "title": "Multi-Task Learning with Low Rank Attribute Embedding for Person Re-Identification",
        "authors": [
            "Chi Su",
            "Fan Yang",
            "Shiliang Zhang",
            "Qi Tian",
            "Larry S. Davis",
            "Wen Gao"
        ],
        "abstract": "We propose a novel Multi-Task Learning with Low Rank Attribute Embedding (MTL-LORAE) framework for person re-identification. Re-identifications from multiple cameras are regarded as related tasks to exploit shared information to improve re-identification accuracy. Both low level features and semantic/data-driven attributes are utilized. Since attributes are generally correlated, we introduce a low rank attribute embedding into the MTL formulation to embed original binary attributes to a continuous attribute space, where incorrect and incomplete attributes are rectified and recovered to better describe people. The learning objective function consists of a quadratic loss regarding class labels and an attribute embedding error, which is solved by an alternating optimization procedure. Experiments on three person re-identification datasets have demonstrated that MTL-LORAE outperforms existing approaches by a large margin and produces state-of-the-art results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410783",
        "reference_list": [
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2013",
                "id": 445
            },
            {
                "year": "2013",
                "id": 315
            },
            {
                "year": "2015",
                "id": 124
            }
        ],
        "citation": {
            "ieee": 39,
            "other": 29,
            "total": 68
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Correlation",
                "Measurement",
                "Linear programming",
                "Semantics",
                "Probes",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multitask learning with low rank attribute embedding",
                "MTL-LORAE framework",
                "person re-identification",
                "low level features",
                "semantic-driven attributes",
                "learning objective function",
                "quadratic loss",
                "attribute embedding error",
                "alternating optimization procedure",
                "data-driven attributes"
            ]
        },
        "id": 417,
        "cited_by": [
            {
                "year": "2017",
                "id": 339
            },
            {
                "year": "2015",
                "id": 124
            }
        ]
    },
    {
        "title": "Regressing a 3D Face Shape from a Single Image",
        "authors": [
            "Sergey Tulyakov",
            "Nicu Sebe"
        ],
        "abstract": "In this work we present a method to estimate a 3D face shape from a single image. Our method is based on a cascade regression framework that directly estimates face landmarks locations in 3D. We include the knowledge that a face is a 3D object into the learning pipeline and show how this information decreases localization errors while keeping the computational time low. We predict the actual positions of the landmarks even if they are occluded due to face rotation. To support the ability of our method to reliably reconstruct 3D shapes, we introduce a simple method for head pose estimation using a single image that reaches higher accuracy than the state of the art. Comparison of 3D face landmarks localization with the available state of the art further supports the feasibility of a single-step face shape estimation. The code, trained models and our 3D annotations will be made available to the research community.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410784",
        "reference_list": [
            {
                "year": "2013",
                "id": 188
            },
            {
                "year": "2013",
                "id": 450
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 11,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Three-dimensional displays",
                "Training",
                "Solid modeling",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image reconstruction",
                "pose estimation",
                "regression analysis",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D face shape regression",
                "cascade regression framework",
                "3D shape reconstruction",
                "head pose estimation",
                "3D face landmarks localization",
                "single-step face shape estimation"
            ]
        },
        "id": 418,
        "cited_by": [
            {
                "year": "2017",
                "id": 337
            }
        ]
    },
    {
        "title": "Rendering of Eyes for Eye-Shape Registration and Gaze Estimation",
        "authors": [
            "Erroll Wood",
            "Tadas Baltruaitis",
            "Xucong Zhang",
            "Yusuke Sugano",
            "Peter Robinson",
            "Andreas Bulling"
        ],
        "abstract": "Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniquesto build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410785",
        "reference_list": [
            {
                "year": "2011",
                "id": 290
            }
        ],
        "citation": {
            "ieee": 25,
            "other": 26,
            "total": 51
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Estimation",
                "Training data",
                "Head",
                "Geometry",
                "Three-dimensional displays",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "eye",
                "gaze tracking",
                "image registration",
                "pose estimation",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "eyes rendering",
                "eye-shape registration",
                "cross-dataset appearance-based gaze estimation",
                "computer vision problems",
                "photorealistic training data synthesis",
                "computer graphics techniques",
                "dynamic eye-region models",
                "head scan geometry",
                "head poses",
                "gaze directions",
                "illumination conditions",
                "shape variations"
            ]
        },
        "id": 419,
        "cited_by": []
    },
    {
        "title": "Multi-Scale Learning for Low-Resolution Person Re-Identification",
        "authors": [
            "Xiang Li",
            "Wei-Shi Zheng",
            "Xiaojuan Wang",
            "Tao Xiang",
            "Shaogang Gong"
        ],
        "abstract": "In real world person re-identification (re-id), images of people captured at very different resolutions from different locations need be matched. Existing re-id models typically normalise all person images to the same size. However, a low-resolution (LR) image contains much less information about a person, and direct image scaling and simple size normalisation as done in conventional re-id methods cannot compensate for the loss of information. To solve this LR person re-id problem, we propose a novel joint multi-scale learning framework, termed joint multi-scale discriminant component analysis (JUDEA). The key component of this framework is a heterogeneous class mean discrepancy (HCMD) criterion for cross-scale image domain alignment, which is optimised simultaneously with discriminant modelling across multiple scales in the joint learning framework. Our experiments show that the proposed JUDEA framework outperforms existing representative re-id methods as well as other related LR visual matching models applied for the LR person re-id problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410786",
        "reference_list": [
            {
                "year": "2013",
                "id": 445
            },
            {
                "year": "2007",
                "id": 179
            },
            {
                "year": "2013",
                "id": 315
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 10,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Feature extraction",
                "Cameras",
                "Face",
                "Training",
                "Measurement",
                "Adaptation models"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image resolution",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiscale learning",
                "low-resolution person re-identification",
                "image matching",
                "person images",
                "low-resolution image",
                "LR image",
                "image scaling",
                "size normalisation",
                "LR person re-id problem",
                "joint multiscale discriminant component analysis",
                "JUDEA",
                "heterogeneous class mean discrepancy criterion",
                "HCMD criterion",
                "cross-scale image domain alignment",
                "discriminant modelling",
                "joint learning"
            ]
        },
        "id": 420,
        "cited_by": [
            {
                "year": "2017",
                "id": 51
            },
            {
                "year": "2017",
                "id": 339
            },
            {
                "year": "2017",
                "id": 565
            },
            {
                "year": "2017",
                "id": 567
            }
        ]
    },
    {
        "title": "Learning to Transfer: Transferring Latent Task Structures and Its Application to Person-Specific Facial Action Unit Detection",
        "authors": [
            "Timur Almaev",
            "Brais Martinez",
            "Michel Valstar"
        ],
        "abstract": "In this article we explore the problem of constructing person-specific models for the detection of facial Action Units (AUs), addressing the problem from the point of view of Transfer Learning and Multi-Task Learning. Our starting point is the fact that some expressions, such as smiles, are very easily elicited, annotated, and automatically detected, while others are much harder to elicit and to annotate. We thus consider a novel problem: all AU models for the target subject are to be learnt using person-specific annotated data for a reference AU (AU12 in our case), and no data or little data regarding the target AU. In order to design such a model, we propose a novel Multi-Task Learning and the associated Transfer Learning framework, in which we consider both relations across subjects and AUs. That is to say, we consider a tensor structure among the tasks. Our approach hinges on learning the latent relations among tasks using one single reference AU, and then transferring these latent relations to other AUs. We show that we are able to effectively make use of the annotated data for AU12 when learning other person-specific AU models, even in the absence of data for the target task. Finally, we show the excellent performance of our method when small amounts of annotated data for the target tasks are made available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410787",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Gold",
                "Face recognition",
                "Training",
                "Data models",
                "Facial muscles",
                "Encoding"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning (artificial intelligence)",
                "tensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "latent task structures",
                "person-specific facial action unit detection",
                "multitask learning",
                "person-specific annotated data",
                "AU12",
                "associated transfer learning framework",
                "tensor structure",
                "latent relations"
            ]
        },
        "id": 421,
        "cited_by": []
    },
    {
        "title": "Pairwise Conditional Random Forests for Facial Expression Recognition",
        "authors": [
            "Arnaud Dapogny",
            "Kevin Bailly",
            "S\u00e9verine Dubuisson"
        ],
        "abstract": "Facial expression can be seen as the dynamic variation of one's appearance over time. Successful recognition thus involves finding representations of high-dimensional spatiotemporal patterns that can be generalized to unseen facial morphologies and variations of the expression dynamics. In this paper, we propose to learn Random Forests from heterogeneous derivative features (e.g. facial fiducial point movements or texture variations) upon pairs of images. Those forests are conditioned on the expression label of the first frame to reduce the variability of the ongoing expression transitions. When testing on a specific frame of a video, pairs are created between this frame and the previous ones. Predictions for each previous frame are used to draw trees from Pairwise Conditional Random Forests (PCRF) whose pairwise outputs are averaged over time to produce robust estimates. As such, PCRF appears as a natural extension of Random Forests to learn spatio-temporal patterns, that leads to significant improvements over standard Random Forests as well as state-of-the-art approaches on several facial expression benchmarks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410788",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 11,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Vegetation",
                "Training",
                "Radio frequency",
                "Histograms",
                "Robustness",
                "Impurities",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "emotion recognition",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pairwise conditional random forest",
                "facial expression recognition",
                "high-dimensional spatio-temporal pattern",
                "unseen facial morphology",
                "unseen facial variation",
                "ongoing expression transition",
                "PCRF",
                "spatio-temporal pattern"
            ]
        },
        "id": 422,
        "cited_by": []
    },
    {
        "title": "Multi-conditional Latent Variable Model for Joint Facial Action Unit Detection",
        "authors": [
            "Stefanos Eleftheriadis",
            "Ognjen Rudovic",
            "Maja Pantic"
        ],
        "abstract": "We propose a novel multi-conditional latent variable model for simultaneous facial feature fusion and detection of facial action units. In our approach we exploit the structure-discovery capabilities of generative models such as Gaussian processes, and the discriminative power of classifiers such as logistic function. This leads to superior performance compared to existing classifiers for the target task that exploit either the discriminative or generative property, but not both. The model learning is performed via an efficient, newly proposed Bayesian learning strategy based on Monte Carlo sampling. Consequently, the learned model is robust to data overfitting, regardless of the number of both input features and jointly estimated facial action units. Extensive qualitative and quantitative experimental evaluations are performed on three publicly available datasets (CK+, Shoulder-pain and DISFA). We show that the proposed model outperforms the state-of-the-art methods for the target task on (i) feature fusion, and (ii) multiple facial action unit detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410789",
        "reference_list": [
            {
                "year": "2013",
                "id": 412
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 10,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Gold",
                "Feature extraction",
                "Facial features",
                "Bayes methods",
                "Computational modeling",
                "Robustness",
                "Logistics"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "face recognition",
                "feature extraction",
                "image classification",
                "image fusion",
                "learning (artificial intelligence)",
                "Monte Carlo methods",
                "sampling methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "latent variable model",
                "facial action unit detection",
                "facial feature fusion",
                "structure-discovery capability",
                "generative model",
                "image classifier",
                "model learning",
                "Bayesian learning strategy",
                "Monte Carlo sampling"
            ]
        },
        "id": 423,
        "cited_by": [
            {
                "year": "2017",
                "id": 416
            },
            {
                "year": "2017",
                "id": 528
            }
        ]
    },
    {
        "title": "Leveraging Datasets with Varying Annotations for Face Alignment via Deep Regression Network",
        "authors": [
            "Jie Zhang",
            "Meina Kan",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "abstract": "Facial landmark detection, as a vital topic in computer vision, has been studied for many decades and lots of datasets have been collected for evaluation. These datasets usually have different annotations, e.g., 68-landmark markup for LFPW dataset, while 74-landmark markup for GTAV dataset. Intuitively, it is meaningful to fuse all the datasets to predict a union of all types of landmarks from multiple datasets (i.e., transfer the annotations of each dataset to all other datasets), but this problem is nontrivial due to the distribution discrepancy between datasets and incomplete annotations of all types for each dataset. In this work, we propose a deep regression network coupled with sparse shape regression (DRN-SSR) to predict the union of all types of landmarks by leveraging datasets with varying annotations, each dataset with one type of annotation. Specifically, the deep regression network intends to predict the union of all landmarks, and the sparse shape regression attempts to approximate those undefined landmarks on each dataset so as to guide the learning of the deep regression network for face alignment. Extensive experiments on two challenging datasets, IBUG and GLF, demonstrate that our method can effectively leverage the multiple datasets with different annotations to predict the union of all types of landmarks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410790",
        "reference_list": [
            {
                "year": "2013",
                "id": 73
            },
            {
                "year": "2013",
                "id": 73
            },
            {
                "year": "2013",
                "id": 127
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 4,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Face",
                "Silicon",
                "Robustness",
                "Predictive models",
                "Correlation",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "neural nets",
                "object detection",
                "regression analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "GLF dataset",
                "IBUG dataset",
                "face alignment",
                "DRN-SSR",
                "sparse shape regression",
                "GTAV dataset",
                "LFPW dataset",
                "computer vision",
                "facial landmark detection",
                "deep regression network",
                "face alignment annotation"
            ]
        },
        "id": 424,
        "cited_by": [
            {
                "year": "2017",
                "id": 106
            },
            {
                "year": "2017",
                "id": 397
            }
        ]
    },
    {
        "title": "A Spatio-Temporal Appearance Representation for Video-Based Pedestrian Re-Identification",
        "authors": [
            "Kan Liu",
            "Bingpeng Ma",
            "Wei Zhang",
            "Rui Huang"
        ],
        "abstract": "Pedestrian re-identification is a difficult problem due to the large variations in a person's appearance caused by different poses and viewpoints, illumination changes, and occlusions. Spatial alignment is commonly used to address these issues by treating the appearance of different body parts independently. However, a body part can also appear differently during different phases of an action. In this paper we consider the temporal alignment problem, in addition to the spatial one, and propose a new approach that takes the video of a walking person as input and builds a spatio-temporal appearance representation for pedestrian re-identification. Particularly, given a video sequence we exploit the periodicity exhibited by a walking person to generate a spatio-temporal body-action model, which consists of a series of body-action units corresponding to certain action primitives of certain body parts. Fisher vectors are learned and extracted from individual body-action units and concatenated into the final representation of the walking person. Unlike previous spatio-temporal features that only take into account local dynamic appearance information, our representation aligns the spatio-temporal appearance of a pedestrian globally. Extensive experiments on public datasets show the effectiveness of our approach compared with the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410791",
        "reference_list": [
            {
                "year": "2013",
                "id": 393
            },
            {
                "year": "2013",
                "id": 315
            }
        ],
        "citation": {
            "ieee": 32,
            "other": 19,
            "total": 51
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Legged locomotion",
                "Video sequences",
                "Measurement",
                "Image color analysis",
                "Adaptation models",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "gait analysis",
                "image representation",
                "object tracking",
                "pedestrians",
                "spatiotemporal phenomena",
                "traffic engineering computing",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatiotemporal appearance representation",
                "video-based pedestrian reidentification",
                "person appearance",
                "illumination changes",
                "occlusions",
                "spatial alignment problem",
                "temporal alignment problem",
                "walking person video",
                "pedestrian reidentification",
                "video sequence",
                "spatio-temporal body-action model",
                "body-action units",
                "Fisher vectors",
                "spatiotemporal features",
                "local dynamic appearance information"
            ]
        },
        "id": 425,
        "cited_by": [
            {
                "year": "2017",
                "id": 256
            },
            {
                "year": "2017",
                "id": 540
            },
            {
                "year": "2017",
                "id": 567
            }
        ]
    },
    {
        "title": "Two Birds, One Stone: Jointly Learning Binary Code for Large-Scale Face Image Retrieval and Attributes Prediction",
        "authors": [
            "Yan Li",
            "Ruiping Wang",
            "Haomiao Liu",
            "Huajie Jiang",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "abstract": "We address the challenging large-scale content-based face image retrieval problem, intended as searching images based on the presence of specific subject, given one face image of him/her. To this end, one natural demand is a supervised binary code learning method. While the learned codes might be discriminating, people often have a further expectation that whether some semantic message (e.g., visual attributes) can be read from the human-incomprehensible codes. For this purpose, we propose a novel binary code learning framework by jointly encoding identity discriminability and a number of facial attributes into unified binary code. In this way, the learned binary codes can be applied to not only fine-grained face image retrieval, but also facial attributes prediction, which is the very innovation of this work, just like killing two birds with one stone. To evaluate the effectiveness of the proposed method, extensive experiments are conducted on a new purified large-scale web celebrity database, named CFW 60K, with abundant manual identity and attributes annotation, and experimental results exhibit the superiority of our method over state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410792",
        "reference_list": [
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2009",
                "id": 46
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2013",
                "id": 378
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 5,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Binary codes",
                "Face",
                "Image retrieval",
                "Semantics",
                "Visualization",
                "Birds"
            ],
            "INSPEC: Controlled Indexing": [
                "binary codes",
                "content-based retrieval",
                "face recognition",
                "image retrieval",
                "Internet",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "binary code learning",
                "large-scale content-based face image retrieval problem",
                "supervised binary code learning method",
                "human-incomprehensible code",
                "fine-grained face image retrieval",
                "large-scale Web celebrity database",
                "CFW 60K"
            ]
        },
        "id": 426,
        "cited_by": []
    },
    {
        "title": "An Accurate Iris Segmentation Framework Under Relaxed Imaging Constraints Using Total Variation Model",
        "authors": [
            "Zijing Zhao",
            "Ajay Kumar"
        ],
        "abstract": "This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410793",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 14,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Iris recognition",
                "Image segmentation",
                "Lighting",
                "Imaging",
                "Noise measurement",
                "Robustness",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image forensics",
                "image segmentation",
                "image texture",
                "iris recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "iris segmentation framework",
                "relaxed imaging constraints",
                "total variation model",
                "automatic iris region segmentation",
                "near-infrared illumination",
                "total-variation based formulation",
                "l1 norm regularization",
                "noisy texture pixel suppression",
                "limbic boundaries",
                "publicly available databases",
                "iris recognition performance"
            ]
        },
        "id": 427,
        "cited_by": [
            {
                "year": "2017",
                "id": 401
            }
        ]
    },
    {
        "title": "Discriminative Pose-Free Descriptors for Face and Object Matching",
        "authors": [
            "Soubhik Sanyal",
            "Sivaram Prasad Mudunuri",
            "Soma Biswas"
        ],
        "abstract": "Pose invariant matching is a very important and challenging problem with various applications like recognizing faces in uncontrolled scenarios, matching objects taken from different view points, etc. In this paper, we propose a discriminative pose-free descriptor (DPFD) which can be used to match faces/objects across pose variations. Training examples at very few representative poses are used to generate virtual intermediate pose subspaces. An image or image region is then represented by a feature set obtained by projecting it on all these subspaces and a discriminative transform is applied on this feature set to make it suitable for classification tasks. Finally, this discriminative feature set is represented by a single feature vector, termed as DPFD. The DPFD of images taken from different viewpoints can be directly compared for matching. Extensive experiments on recognizing faces across pose, pose and resolution on the Multi-PIE and Surveillance Cameras Face datasets and comparisons with state-of-the-art approaches show the effectiveness of the proposed approach. Experiments on matching general objects across viewpoints show the generalizability of the proposed approach beyond faces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410794",
        "reference_list": [
            {
                "year": "2011",
                "id": 118
            },
            {
                "year": "2011",
                "id": 286
            },
            {
                "year": "2013",
                "id": 311
            },
            {
                "year": "2013",
                "id": 301
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Training",
                "Image resolution",
                "Transforms",
                "Face recognition",
                "Probes",
                "Training data"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "image classification",
                "image matching",
                "image representation",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative pose-free descriptors",
                "face matching",
                "object matching",
                "pose invariant matching",
                "DPFD",
                "representative poses",
                "virtual intermediate pose subspaces",
                "image region representation",
                "subspaces projection",
                "discriminative transform",
                "classification tasks",
                "discriminative feature set representation",
                "single feature vector",
                "face recognition",
                "multiPIE",
                "surveillance cameras face datasets"
            ]
        },
        "id": 428,
        "cited_by": []
    },
    {
        "title": "Bi-Shifting Auto-Encoder for Unsupervised Domain Adaptation",
        "authors": [
            "Meina Kan",
            "Shiguang Shan",
            "Xilin Chen"
        ],
        "abstract": "In many real-world applications, the domain of model learning (referred as source domain) is usually inconsistent with or even different from the domain of testing (referred as target domain), which makes the learnt model degenerate in target domain, i.e., the test domain. To alleviate the discrepancy between source and target domains, we propose a domain adaptation method, named as Bi-shifting Auto-Encoder network (BAE). The proposed BAE attempts to shift source domain samples to target domain, and also shift the target domain samples to source domain. The non-linear transformation of BAE ensures the feasibility of shifting between domains, and the distribution consistency between the shifted domain and the desirable domain is constrained by sparse reconstruction between them. As a result, the shifted source domain is supervised and follows similar distribution as target domain. Therefore, any supervised method can be applied on the shifted source domain to train a classifier for classification in target domain. The proposed method is evaluated on three domain adaptation scenarios of face recognition, i.e., domain adaptation across view angle, ethnicity, and imaging sensor, and the promising results demonstrate that our proposed BAE can shift samples between domains and thus effectively deal with the domain discrepancy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410795",
        "reference_list": [
            {
                "year": "2011",
                "id": 126
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 6,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Decoding",
                "Manifolds",
                "Testing",
                "Image reconstruction",
                "Adaptation models",
                "Imaging",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "image reconstruction",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bi-shifting auto-encoder network",
                "unsupervised domain adaptation",
                "BAE nonlinear transformation",
                "sparse reconstruction",
                "shifted source domain",
                "classification",
                "face recognition",
                "auto-encoder neural network"
            ]
        },
        "id": 429,
        "cited_by": [
            {
                "year": "2017",
                "id": 377
            }
        ]
    },
    {
        "title": "Regressive Tree Structured Model for Facial Landmark Localization",
        "authors": [
            "Gee-Sern Hsu",
            "Kai-Hsiang Chang",
            "Shih-Chieh Huang"
        ],
        "abstract": "Although the Tree Structured Model (TSM) is proven effective for solving face detection, pose estimation and landmark localization in an unified model, its sluggish run time makes it unfavorable in practical applications, especially when dealing with cases of multiple faces. We propose the Regressive Tree Structure Model (RTSM) to improve the run-time speed and localization accuracy. The RTSM is composed of two component TSMs, the coarse TSM (c-TSM) and the refined TSM (r-TSM), and a Bilateral Support Vector Regressor (BSVR). The c-TSM is built on the low-resolution octaves of samples so that it provides coarse but fast face detection. The r-TSM is built on the mid-resolution octaves so that it can locate the landmarks on the face candidates given by the c-TSM and improve precision. The r-TSM based landmarks are used in the forward BSVR as references to locate the dense set of landmarks, which are then used in the backward BSVR to relocate the landmarks with large localization errors. The forward and backward regression goes on iteratively until convergence. The performance of the RTSM is validated on three benchmark databases, the Multi-PIE, LFPW and AFW, and compared with the latest TSM to demonstrate its efficacy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410796",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 2,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Training",
                "Face detection",
                "Regression tree analysis",
                "Computational modeling",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "regression analysis",
                "trees (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "AFW",
                "LFPW",
                "multiPIE",
                "benchmark databases",
                "localization errors",
                "backward BSVR",
                "forward BSVR",
                "face candidates",
                "midresolution octaves",
                "bilateral support vector regressor",
                "r-TSM",
                "refined TSM",
                "c-TSM",
                "coarse TSM",
                "localization accuracy",
                "run-time speed",
                "RTSM",
                "Regressive Tree Structure Model",
                "sluggish run time",
                "pose estimation",
                "face detection",
                "facial landmark localization",
                "regressive tree structured model"
            ]
        },
        "id": 430,
        "cited_by": []
    },
    {
        "title": "Person Recognition in Personal Photo Collections",
        "authors": [
            "Seong Joon Oh",
            "Rodrigo Benenson",
            "Mario Fritz",
            "Bernt Schiele"
        ],
        "abstract": "Recognising persons in everyday photos presents major challenges (occluded faces, different clothing, locations, etc.) for machine vision. We propose a convnet based person recognition system on which we provide an in-depth analysis of informativeness of different body cues, impact of training data, and the common failure modes of the system. In addition, we discuss the limitations of existing benchmarks and propose more challenging ones. Our method is simple and is built on open source and open data, yet it improves the state of the art results on a large dataset of social media photos (PIPA).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410797",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            },
            {
                "year": "2013",
                "id": 400
            },
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2013",
                "id": 261
            },
            {
                "year": "2013",
                "id": 315
            },
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Head",
                "Face recognition",
                "Training",
                "Context",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "personal photo collections",
                "everyday photos",
                "machine vision",
                "convnet based person recognition system",
                "body cues",
                "training data",
                "open source",
                "open data",
                "social media photos",
                "PIPA"
            ]
        },
        "id": 431,
        "cited_by": [
            {
                "year": "2017",
                "id": 155
            }
        ]
    },
    {
        "title": "Robust Statistical Face Frontalization",
        "authors": [
            "Christos Sagonas",
            "Yannis Panagakis",
            "Stefanos Zafeiriou",
            "Maja Pantic"
        ],
        "abstract": "Recently, it has been shown that excellent results can be achieved in both facial landmark localization and pose-invariant face recognition. These breakthroughs are attributed to the efforts of the community to manually annotate facial images in many different poses and to collect 3D facial data. In this paper, we propose a novel method for joint frontal view reconstruction and landmark localization using a small set of frontal images only. By observing that the frontal facial image is the one having the minimum rank of all different poses, an appropriate model which is able to jointly recover the frontalized version of the face as well as the facial landmarks is devised. To this end, a suitable optimization problem, involving the minimization of the nuclear norm and the matrix l1 norm is solved. The proposed method is assessed in frontal face reconstruction, face landmark localization, pose-invariant face recognition, and face verification in unconstrained conditions. The relevant experiments have been conducted on 8 databases. The experimental results demonstrate the effectiveness of the proposed method in comparison to the state-of-the-art methods for the target problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410798",
        "reference_list": [
            {
                "year": "2011",
                "id": 59
            },
            {
                "year": "2005",
                "id": 101
            },
            {
                "year": "2013",
                "id": 14
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 11,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Three-dimensional displays",
                "Shape",
                "Optimization",
                "Face recognition",
                "Image reconstruction",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image reconstruction",
                "matrix algebra",
                "minimisation",
                "pose estimation",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust statistical face frontalization",
                "facial landmark localization",
                "pose-invariant face recognition",
                "facial images annotation",
                "3D facial data",
                "frontal view reconstruction",
                "frontal facial image",
                "optimization problem",
                "minimization",
                "nuclear norm",
                "matrix l1 norm",
                "frontal face reconstruction",
                "face verification",
                "unconstrained conditions"
            ]
        },
        "id": 432,
        "cited_by": [
            {
                "year": "2017",
                "id": 257
            },
            {
                "year": "2017",
                "id": 420
            }
        ]
    },
    {
        "title": "PIEFA: Personalized Incremental and Ensemble Face Alignment",
        "authors": [
            "Xi Peng",
            "Shaoting Zhang",
            "Yu Yang",
            "Dimitris N. Metaxas"
        ],
        "abstract": "Face alignment, especially on real-time or large-scale sequential images, is a challenging task with broad applications. Both generic and joint alignment approaches have been proposed with varying degrees of success. However, many generic methods are heavily sensitive to initializations and usually rely on offline-trained static models, which limit their performance on sequential images with extensive variations. On the other hand, joint methods are restricted to offline applications, since they require all frames to conduct batch alignment. To address these limitations, we propose to exploit incremental learning for personalized ensemble alignment. We sample multiple initial shapes to achieve image congealing within one frame, which enables us to incrementally conduct ensemble alignment by group-sparse regularized rank minimization. At the same time, personalized modeling is obtained by subspace adaptation under the same incremental framework, while correction strategy is used to alleviate model drifting. Experimental results on multiple controlled and in-the-wild databases demonstrate the superior performance of our approach compared with state-of-the-arts in terms of fitting accuracy and efficiency.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410799",
        "reference_list": [
            {
                "year": "2013",
                "id": 71
            },
            {
                "year": "2007",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 3,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Shape",
                "Robustness",
                "Adaptation models",
                "Optimization",
                "Real-time systems",
                "Lighting"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image sequences",
                "learning (artificial intelligence)",
                "minimisation",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model drifting",
                "correction strategy",
                "subspace adaptation",
                "personalized modeling",
                "group-sparse regularized rank minimization",
                "image congealing",
                "multiple initial shapes",
                "incremental learning",
                "batch alignment",
                "joint alignment approaches",
                "generic alignment approaches",
                "large-scale sequential images",
                "real-time sequential images",
                "personalized incremental and ensemble face alignment",
                "PIEFA"
            ]
        },
        "id": 433,
        "cited_by": [
            {
                "year": "2017",
                "id": 185
            },
            {
                "year": "2017",
                "id": 399
            }
        ]
    },
    {
        "title": "Understanding Everyday Hands in Action from RGB-D Images",
        "authors": [
            "Gr\u00e9gory Rogez",
            "James S. Supancic",
            "Deva Ramanan"
        ],
        "abstract": "We analyze functional manipulations of handheld objects, formalizing the problem as one of fine-grained grasp classification. To do so, we make use of a recently developed fine-grained taxonomy of human-object grasps. We introduce a large dataset of 12000 RGB-D images covering 71 everyday grasps in natural interactions. Our dataset is different from past work (typically addressed from a robotics perspective) in terms of its scale, diversity, and combination of RGB and depth data. From a computer-vision perspective, our dataset allows for exploration of contact and force prediction (crucial concepts in functional grasp analysis) from perceptual cues. We present extensive experimental results with state-of-the-art baselines, illustrating the role of segmentation, object context, and 3D-understanding in functional grasp analysis. We demonstrate a near 2X improvement over prior work and a naive deep baseline, while pointing out important directions for improvement.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410800",
        "reference_list": [
            {
                "year": "2009",
                "id": 175
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 9,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Taxonomy",
                "Force",
                "Three-dimensional displays",
                "Solid modeling",
                "Robots",
                "Kinematics",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "gesture recognition",
                "image colour analysis",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "RGB-D image",
                "functional manipulation",
                "handheld object",
                "fine-grained grasp classification",
                "fine-grained taxonomy",
                "human-object grasp",
                "natural interaction",
                "computer-vision perspective",
                "force prediction",
                "functional grasp analysis",
                "image segmentation",
                "object context",
                "3D-understanding"
            ]
        },
        "id": 434,
        "cited_by": [
            {
                "year": "2017",
                "id": 329
            },
            {
                "year": "2017",
                "id": 396
            }
        ]
    },
    {
        "title": "Example-Based Modeling of Facial Texture from Deficient Data",
        "authors": [
            "Arnaud Dessein",
            "William A. P. Smith",
            "Richard C. Wilson",
            "Edwin R. Hancock"
        ],
        "abstract": "We present an approach to modeling ear-to-ear, high-quality texture from one or more partial views of a face with possibly poor resolution and noise. Our approach is example-based in that we reconstruct texture with patches from a database composed of previously seen faces. A 3D morphable model is used to establish shape correspondence between the observed data across views and training faces. The database is built on the mesh surface by segmenting it into uniform overlapping patches. Texture patches are selected by belief propagation so as to be consistent with neighbors and with observations in an appropriate image formation model. We also develop a variant that is insensitive to light and camera parameters, and incorporate soft symmetry constraints. We obtain textures of higher quality for degraded views as small as 10 pixels wide, than a standard model fitted to non-degraded data. We further show applications to super-resolution where we substantially improve quality compared to a state-of-the-art algorithm, and to texture completion where we fill in missing regions and remove facial clutter in a photorealistic manner.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410801",
        "reference_list": [
            {
                "year": "2007",
                "id": 158
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Three-dimensional displays",
                "Solid modeling",
                "Image resolution",
                "Data models",
                "Shape",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "face recognition",
                "image reconstruction",
                "image resolution",
                "image texture",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "example-based modeling",
                "facial texture",
                "ear-to-ear high-quality texture modeling",
                "3D morphable model",
                "shape correspondence",
                "mesh surface",
                "belief propagation",
                "image formation model",
                "camera parameters",
                "soft symmetry constraints",
                "super-resolution",
                "facial clutter"
            ]
        },
        "id": 435,
        "cited_by": []
    },
    {
        "title": "Learning to Predict Saliency on Face Images",
        "authors": [
            "Mai Xu",
            "Yun Ren",
            "Zulin Wang"
        ],
        "abstract": "This paper proposes a novel method, which learns to detect saliency of face images. To be more specific, we obtain a database of eye tracking over extensive face images, via conducting an eye tracking experiment. With analysis on eye tracking database, we verify that the fixations tend to cluster around facial features, when viewing images with large faces. For modeling attention on faces and facial features, the proposed method learns the Gaussian mixture model (GMM) distribution from the fixations of eye tracking data as the top-down features for saliency detection of face images. Then, in our method, the top-down features (i.e., face and facial features) upon the the learnt GMM are linearly combined with the conventional bottom-up features (i.e., color, intensity, and orientation), for saliency detection. In the linear combination, we argue that the weights corresponding to top-down feature channels depend on the face size in images, and the relationship between the weights and face size is thus investigated via learning from the training eye tracking data. Finally, experimental results show that our learning-based method is able to advance state-of-the-art saliency prediction for face images. The corresponding database and code are available online: www.ee.buaa.edu.cn/xumfiles/saliency_detection.html.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410802",
        "reference_list": [
            {
                "year": "2009",
                "id": 271
            },
            {
                "year": "2009",
                "id": 132
            },
            {
                "year": "2013",
                "id": 19
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 3,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Face",
                "Visualization",
                "Facial features",
                "Gaze tracking",
                "Feature extraction",
                "Visual databases"
            ],
            "INSPEC: Controlled Indexing": [
                "eye",
                "face recognition",
                "feature extraction",
                "Gaussian distribution",
                "Gaussian processes",
                "image colour analysis",
                "learning (artificial intelligence)",
                "mixture models",
                "object detection",
                "object tracking",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "saliency prediction learning",
                "face images",
                "eye tracking database",
                "facial features",
                "Gaussian mixture model distribution",
                "GMM distribution",
                "eye tracking data fixations",
                "saliency detection",
                "color bottom-up feature",
                "intensity bottom-up feature",
                "orientation bottom-up feature"
            ]
        },
        "id": 436,
        "cited_by": []
    },
    {
        "title": "Group Membership Prediction",
        "authors": [
            "Ziming Zhang",
            "Yuting Chen",
            "Venkatesh Saligrama"
        ],
        "abstract": "The group membership prediction (GMP) problem involves predicting whether or not a collection of instances share a certain semantic property. For instance, in kinship verification given a collection of images, the goal is to predict whether or not they share a familial relationship. In this context we propose a novel probability model and introduce latent view-specific and view-shared random variables to jointly account for the view-specific appearance and cross-view similarities among data instances. Our model posits that data from each view is independent conditioned on the shared variables. This postulate leads to a parametric probability model that decomposes group membership likelihood into a tensor product of data-independent parameters and data-dependent factors. We propose learning the data-independent parameters in a discriminative way with bilinear classifiers, and test our prediction algorithm on challenging visual recognition tasks such as multi-camera person re-identification and kinship verification. On most benchmark datasets, our method can significantly outperform the current state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410803",
        "reference_list": [
            {
                "year": "2015",
                "id": 465
            },
            {
                "year": "2013",
                "id": 315
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 7,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Tensile stress",
                "Context",
                "Data models",
                "Kernel",
                "Semantics",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image recognition",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "group membership prediction problem",
                "GMP problem",
                "probability model",
                "familial relationship",
                "parametric probability model",
                "bilinear classifiers",
                "visual recognition tasks",
                "kinship verification"
            ]
        },
        "id": 437,
        "cited_by": [
            {
                "year": "2017",
                "id": 258
            },
            {
                "year": "2017",
                "id": 497
            },
            {
                "year": "2015",
                "id": 465
            }
        ]
    },
    {
        "title": "Extraction of Virtual Baselines from Distorted Document Images Using Curvilinear Projection",
        "authors": [
            "Gaofeng Meng",
            "Zuming Huang",
            "Yonghong Song",
            "Shiming Xiang",
            "Chunhong Pan"
        ],
        "abstract": "The baselines of a document page are a set of virtual horizontal and parallel lines, to which the printed contents of document, e.g., text lines, tables or inserted photos, are aligned. Accurate baseline extraction is of great importance in the geometric correction of curved document images. In this paper, we propose an efficient method for accurate extraction of these virtual visual cues from a curved document image. Our method comes from two basic observations that the baselines of documents do not intersect with each other and that within a narrow strip, the baselines can be well approximated by linear segments. Based upon these observations, we propose a curvilinear projection based method and model the estimation of curved baselines as a constrained sequential optimization problem. A dynamic programming algorithm is then developed to efficiently solve the problem. The proposed method can extract the complete baselines through each pixel of document images in a high accuracy. It is also scripts insensitive and highly robust to image noises, non-textual objects, image resolutions and image quality degradation like blurring and non-uniform illumination. Extensive experiments on a number of captured document images demonstrate the effectiveness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410804",
        "reference_list": [
            {
                "year": "2003",
                "id": 31
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Strips",
                "Image segmentation",
                "Radon",
                "Transforms",
                "Optimization",
                "Layout",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "dynamic programming",
                "feature extraction",
                "geometry",
                "image resolution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "captured document images",
                "image quality degradation",
                "image resolutions",
                "nontextual objects",
                "image noises",
                "dynamic programming algorithm",
                "constrained sequential optimization problem",
                "curvilinear projection based method",
                "virtual visual cues",
                "curved document images",
                "geometric correction",
                "parallel lines",
                "virtual horizontal lines",
                "document page baselines",
                "distorted document images",
                "virtual baseline extraction"
            ]
        },
        "id": 438,
        "cited_by": [
            {
                "year": "2017",
                "id": 519
            }
        ]
    },
    {
        "title": "Robust RGB-D Odometry Using Point and Line Features",
        "authors": [
            "Yan Lu",
            "Dezhen Song"
        ],
        "abstract": "Lighting variation and uneven feature distribution are main challenges for indoor RGB-D visual odometry where color information is often combined with depth information. To meet the challenges, we fuse point and line features to form a robust odometry algorithm. Line features are abundant indoors and less sensitive to lighting change than points. We extract 3D points and lines from RGB-D data, analyze their measurement uncertainties, and compute camera motion using maximum likelihood estimation. We prove that fusing points and lines produces smaller motion estimate uncertainty than using either feature type alone. In experiments we compare our method with state-of-the-art methods including a keypoint-based approach and a dense visual odometry algorithm. Our method outperforms the counterparts under both constant and varying lighting conditions. Specifically, our method achieves an average translational error that is 34.9% smaller than the counterparts, when tested using public datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410805",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 0,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Visualization",
                "Uncertainty",
                "Lighting",
                "Cameras",
                "Robustness",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "feature extraction",
                "image colour analysis",
                "image motion analysis",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "indoor RGB-D visual odometry",
                "point feature",
                "line feature",
                "color information",
                "camera motion",
                "maximum likelihood estimation",
                "keypoint-based approach"
            ]
        },
        "id": 439,
        "cited_by": []
    },
    {
        "title": "Learning a Discriminative Model for the Perception of Realism in Composite Images",
        "authors": [
            "Jun-Yan Zhu",
            "Philipp Kr\u00e4henb\u00fchl",
            "Eli Shechtman",
            "Alexei A. Efros"
        ],
        "abstract": "What makes an image appear realistic? In this work, we are answering this question from a data-driven perspective by learning the perception of visual realism directly from large amounts of data. In particular, we train a Convolutional Neural Network (CNN) model that distinguishes natural photographs from automatically generated composite images. The model learns to predict visual realism of a scene in terms of color, lighting and texture compatibility, without any human annotations pertaining to it. Our model outperforms previous works that rely on hand-crafted heuristics, for the task of classifying realistic vs. unrealistic photos. Furthermore, we apply our learned model to compute optimal parameters of a compositing method, to maximize the visual realism score predicted by our CNN model. We demonstrate its advantage against existing methods via a human perception study.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410806",
        "reference_list": [
            {
                "year": "2007",
                "id": 267
            },
            {
                "year": "2011",
                "id": 60
            }
        ],
        "citation": {
            "ieee": 11,
            "other": 7,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Image color analysis",
                "Image segmentation",
                "Proposals",
                "Predictive models",
                "Shape",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "learning (artificial intelligence)",
                "neural nets",
                "prediction theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "discriminative model",
                "realism perception",
                "composite images",
                "convolutional neural network training",
                "CNN model",
                "natural photographs",
                "visual realism prediction",
                "unrealistic photo classification",
                "human perception"
            ]
        },
        "id": 440,
        "cited_by": []
    },
    {
        "title": "What Makes Tom Hanks Look Like Tom Hanks",
        "authors": [
            "Supasorn Suwajanakorn",
            "Steven M. Seitz",
            "Ira Kemelmacher-Shlizerman"
        ],
        "abstract": "We reconstruct a controllable model of a person from a large photo collection that captures his or her persona, i.e., physical appearance and behavior. The ability to operate on unstructured photo collections enables modeling a huge number of people, including celebrities and other well photographed people without requiring them to be scanned. Moreover, we show the ability to drive or puppeteer the captured person B using any other video of a different person A. In this scenario, B acts out the role of person A, but retains his/her own personality and character. Our system is based on a novel combination of 3D face reconstruction, tracking, alignment, and multi-texture modeling, applied to the puppeteering problem. We demonstrate convincing results on a large variety of celebrities derived from Internet imagery and video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410807",
        "reference_list": [
            {
                "year": "2011",
                "id": 221
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 18,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Shape",
                "Face",
                "Videos",
                "Solid modeling",
                "Image reconstruction",
                "Deformable models"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image reconstruction",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "controllable person model reconstruction",
                "photo collection",
                "unstructured photo collections",
                "B acts",
                "3D face reconstruction",
                "face tracking",
                "face alignment",
                "multitexture modeling",
                "Internet imagery",
                "Internet video"
            ]
        },
        "id": 441,
        "cited_by": [
            {
                "year": "2017",
                "id": 387
            },
            {
                "year": "2017",
                "id": 391
            }
        ]
    },
    {
        "title": "Wide-Area Image Geolocalization with Aerial Reference Imagery",
        "authors": [
            "Scott Workman",
            "Richard Souvenir",
            "Nathan Jacobs"
        ],
        "abstract": "We propose to use deep convolutional neural networks to address the problem of cross-view image geolocalization, in which the geolocation of a ground-level query image is estimated by matching to georeferenced aerial images. We use state-of-the-art feature representations for ground-level images and introduce a cross-view training approach for learning a joint semantic feature representation for aerial images. We also propose a network architecture that fuses features extracted from aerial images at multiple spatial scales. To support training these networks, we introduce a massive database that contains pairs of aerial and ground-level images from across the United States. Our methods significantly out-perform the state of the art on two benchmark datasets. We also show, qualitatively, that the proposed feature representations are discriminative at both local and continental spatial scales.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410808",
        "reference_list": [
            {
                "year": "2007",
                "id": 155
            }
        ],
        "citation": {
            "ieee": 30,
            "other": 8,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Training",
                "Geology",
                "Semantics",
                "Databases",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "geographic information systems",
                "geophysical image processing",
                "image matching",
                "image retrieval",
                "learning (artificial intelligence)",
                "neural net architecture",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "wide-area image geolocalization",
                "aerial reference imagery",
                "deep convolutional neural networks",
                "cross-view image geolocalization",
                "ground-level query image",
                "georeferenced aerial image matching",
                "state-of-the-art feature representations",
                "cross-view training",
                "joint semantic feature representation",
                "network architecture",
                "feature extraction",
                "feature fusion",
                "image database",
                "United States",
                "local spatial scales",
                "continental spatial scales"
            ]
        },
        "id": 442,
        "cited_by": [
            {
                "year": "2017",
                "id": 276
            },
            {
                "year": "2017",
                "id": 283
            },
            {
                "year": "2017",
                "id": 586
            }
        ]
    },
    {
        "title": "Personalized Age Progression with Aging Dictionary",
        "authors": [
            "Xiangbo Shu",
            "Jinhui Tang",
            "Hanjiang Lai",
            "Luoqi Liu",
            "Shuicheng Yan"
        ],
        "abstract": "In this paper, we aim to automatically render aging faces in a personalized way. Basically, a set of age-group specific dictionaries are learned, where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups, and a linear combination of these patterns expresses a particular personalized aging process. Moreover, two factors are taken into consideration in the dictionary learning process. First, beyond the aging dictionaries, each subject may have extra personalized facial characteristics, e.g. mole, which are invariant in the aging process. Second, it is challenging or even impossible to collect faces of all age groups for a particular subject, yet much easier and more practical to get face pairs from neighboring age groups. Thus a personality-aware coupled reconstruction loss is utilized to learn the dictionaries based on face pairs from neighboring age groups. Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts in term of personalized aging progression, as well as the performance gain for cross-age face verification by synthesizing aging faces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410809",
        "reference_list": [
            {
                "year": "2013",
                "id": 358
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 11,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Aging",
                "Face",
                "Dictionaries",
                "Data models",
                "Databases",
                "Radio frequency",
                "Mercury (metals)"
            ],
            "INSPEC: Controlled Indexing": [
                "age issues",
                "face recognition",
                "image reconstruction",
                "learning (artificial intelligence)",
                "rendering (computer graphics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "personalized age progression",
                "aging dictionary",
                "automatic aging face rendering",
                "age-group specific dictionaries",
                "dictionary learning process",
                "personalized facial characteristics",
                "personality-aware coupled reconstruction loss",
                "performance gain",
                "cross-age face verification",
                "aging face synthesis"
            ]
        },
        "id": 443,
        "cited_by": [
            {
                "year": "2017",
                "id": 393
            }
        ]
    },
    {
        "title": "FaceDirector: Continuous Control of Facial Performance in Video",
        "authors": [
            "Charles Malleson",
            "Jean-Charles Bazin",
            "Oliver Wang",
            "Derek Bradley",
            "Thabo Beeler",
            "Adrian Hilton",
            "Alexander Sorkine-Hornung"
        ],
        "abstract": "We present a method to continuously blend between multiple facial performances of an actor, which can contain different facial expressions or emotional states. As an example, given sad and angry video takes of a scene, our method empowers the movie director to specify arbitrary weighted combinations and smooth transitions between the two takes in post-production. Our contributions include (1) a robust nonlinear audio-visual synchronization technique that exploits complementary properties of audio and visual cues to automatically determine robust, dense spatiotemporal correspondences between takes, and (2) a seamless facial blending approach that provides the director full control to interpolate timing, facial expression, and local appearance, in order to generate novel performances after filming. In contrast to most previous works, our approach operates entirely in image space, avoiding the need of 3D facial reconstruction. We demonstrate that our method can synthesize visually believable performances with applications in emotion transition, performance correction, and timing control.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410810",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 5,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Synchronization",
                "Face",
                "Robustness",
                "Three-dimensional displays",
                "Interpolation"
            ],
            "INSPEC: Controlled Indexing": [
                "emotion recognition",
                "face recognition",
                "synchronisation",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "FaceDirector",
                "continuous control",
                "multiple-facial performance",
                "facial expression",
                "emotional state",
                "arbitrary weighted combination",
                "robust nonlinear audio-visual synchronization technique",
                "visual cue",
                "audio cue",
                "robust spatiotemporal correspondence",
                "dense spatiotemporal correspondence",
                "seamless facial blending approach",
                "local appearance",
                "image space",
                "emotion transition",
                "performance correction",
                "timing control"
            ]
        },
        "id": 444,
        "cited_by": [
            {
                "year": "2017",
                "id": 570
            }
        ]
    },
    {
        "title": "Synthesizing Illumination Mosaics from Internet Photo-Collections",
        "authors": [
            "Dinghuang Ji",
            "Enrique Dunn",
            "Jan-Michael Frahm"
        ],
        "abstract": "We propose a framework for the automatic creation of time-lapse mosaics of a given scene. We achieve this by leveraging the illumination variations captured in Internet photo-collections. In order to depict and characterize the illumination spectrum of a scene, our method relies on building discrete representations of the image appearance space through connectivity graphs defined over a pairwise image distance function. The smooth appearance transitions are found as the shortest path in the similarity graph among images, and robust image alignment is achieved by leveraging scene semantics, multi-view geometry, and image warping techniques. The attained results present an insightful and compact visualization of the scene illuminations captured in crowd-sourced imagery.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410811",
        "reference_list": [
            {
                "year": "2013",
                "id": 121
            },
            {
                "year": "2005",
                "id": 84
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Image color analysis",
                "Image sequences",
                "Internet",
                "Image segmentation",
                "Feature extraction",
                "Histograms"
            ],
            "INSPEC: Controlled Indexing": [
                "data visualisation",
                "graph theory",
                "image representation",
                "image segmentation",
                "lighting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene semantics",
                "multiview geometry",
                "image warping echnique",
                "compact visualization",
                "scene illumination",
                "crowd-sourced imagery",
                "robust image alignment",
                "similarity graph",
                "shortest path",
                "appearance transition",
                "pairwise image distance function",
                "connectivity graphs",
                "image appearance space",
                "discrete representation",
                "illumination variation",
                "time-lapse mosaic",
                "Internet photo-collection",
                "illumination mosaic"
            ]
        },
        "id": 445,
        "cited_by": []
    },
    {
        "title": "Hot or Not: Exploring Correlations between Appearance and Temperature",
        "authors": [
            "Daniel Glasner",
            "Pascal Fua",
            "Todd Zickler",
            "Lihi Zelnik-Manor"
        ],
        "abstract": "In this paper we explore interactions between the appearance of an outdoor scene and the ambient temperature. By studying statistical correlations between image sequences from outdoor cameras and temperature measurements we identify two interesting interactions. First, semantically meaningful regions such as foliage and reflective oriented surfaces are often highly indicative of the temperature. Second, small camera motions are correlated with the temperature in some scenes. We propose simple scene-specific temperature prediction algorithms which can be used to turn a camera into a crude temperature sensor. We find that for this task, simple features such as local pixel intensities outperform sophisticated, global features such as from a semantically-trained convolutional neural network.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410812",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Temperature measurement",
                "Correlation",
                "Cameras",
                "Meteorology",
                "Temperature sensors",
                "Image color analysis",
                "Image sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "neural nets",
                "temperature measurement",
                "temperature sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "convolutional neural network",
                "local pixel intensity",
                "crude temperature sensor",
                "scene-specific temperature prediction algorithm",
                "reflective oriented surface",
                "foliage",
                "temperature measurement",
                "outdoor camera",
                "image sequence",
                "statistical correlation",
                "ambient temperature",
                "outdoor scene",
                "appearance"
            ]
        },
        "id": 446,
        "cited_by": [
            {
                "year": "2017",
                "id": 283
            }
        ]
    },
    {
        "title": "SPM-BP: Sped-Up PatchMatch Belief Propagation for Continuous MRFs",
        "authors": [
            "Yu Li",
            "Dongbo Min",
            "Michael S. Brown",
            "Minh N. Do",
            "Jiangbo Lu"
        ],
        "abstract": "Markov random fields are widely used to model many computer vision problems that can be cast in an energy minimization framework composed of unary and pairwise potentials. While computationally tractable discrete optimizers such as Graph Cuts and belief propagation (BP) exist for multi-label discrete problems, they still face prohibitively high computational challenges when the labels reside in a huge or very densely sampled space. Integrating key ideas from PatchMatch of effective particle propagation and resampling, PatchMatch belief propagation (PMBP) has been demonstrated to have good performance in addressing continuous labeling problems and runs orders of magnitude faster than Particle BP (PBP). However, the quality of the PMBP solution is tightly coupled with the local window size, over which the raw data cost is aggregated to mitigate ambiguity in the data constraint. This dependency heavily influences the overall complexity, increasing linearly with the window size. This paper proposes a novel algorithm called sped-up PMBP (SPM-BP) to tackle this critical computational bottleneck and speeds up PMBP by 50-100 times. The crux of SPM-BP is on unifying efficient filter-based cost aggregation and message passing with PatchMatch-based particle generation in a highly effective way. Though simple in its formulation, SPM-BP achieves superior performance for sub-pixel accurate stereo and optical-flow on benchmark datasets when compared with more complex and task-specific approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410813",
        "reference_list": [
            {
                "year": "2013",
                "id": 294
            },
            {
                "year": "2013",
                "id": 214
            },
            {
                "year": "2011",
                "id": 205
            },
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 27,
            "other": 8,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Labeling",
                "Belief propagation",
                "Complexity theory",
                "Computer vision",
                "Message passing",
                "Proposals",
                "Adaptive optics"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "computer vision",
                "graph theory",
                "image filtering",
                "image sequences",
                "Markov processes",
                "message passing",
                "minimisation",
                "random processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SPM-BP",
                "sped-up PatchMatch belief propagation",
                "continuous MRFs",
                "Markov random fields",
                "computer vision problems",
                "energy minimization framework",
                "computationally tractable discrete optimizers",
                "graph cuts",
                "multilabel discrete problems",
                "filter-based cost aggregation",
                "message passing",
                "PatchMatch-based particle generation",
                "optical-flow"
            ]
        },
        "id": 447,
        "cited_by": [
            {
                "year": "2017",
                "id": 32
            },
            {
                "year": "2017",
                "id": 123
            },
            {
                "year": "2017",
                "id": 475
            }
        ]
    },
    {
        "title": "Flow Fields: Dense Correspondence Fields for Highly Accurate Large Displacement Optical Flow Estimation",
        "authors": [
            "Christian Bailer",
            "Bertram Taetz",
            "Didier Stricker"
        ],
        "abstract": "Modern large displacement optical flow algorithms usually use an initialization by either sparse descriptor matching techniques or dense approximate nearest neighbor fields. While the latter have the advantage of being dense, they have the major disadvantage of being very outlier prone as they are not designed to find the optical flow, but the visually most similar correspondence. In this paper we present a dense correspondence field approach that is much less outlier prone and thus much better suited for optical flow estimation than approximate nearest neighbor fields. Our approach is conceptually novel as it does not require explicit regularization, smoothing (like median filtering) or a new data term, but solely our novel purely data based search strategy that finds most inliers (even for small objects), while it effectively avoids finding outliers. Moreover, we present novel enhancements for outlier filtering. We show that our approach is better suited for large displacement optical flow estimation than state-of-the-art descriptor matching techniques. We do so by initializing EpicFlow (so far the best method on MPI-Sintel) with our Flow Fields instead of their originally used state-of-the-art descriptor matching technique. We significantly outperform the original EpicFlow on MPI-Sintel, KITTI and Middlebury.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410814",
        "reference_list": [
            {
                "year": "2013",
                "id": 23
            },
            {
                "year": "2007",
                "id": 94
            },
            {
                "year": "2011",
                "id": 203
            },
            {
                "year": "2009",
                "id": 213
            },
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 67,
            "other": 24,
            "total": 91
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical imaging",
                "Adaptive optics",
                "Estimation",
                "Computer vision",
                "Image motion analysis",
                "Data structures",
                "Boolean functions"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image filtering",
                "image matching",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "flow fields",
                "dense correspondence fields",
                "displacement optical flow estimation",
                "sparse descriptor matching techniques",
                "dense approximate nearest neighbor fields",
                "dense correspondence field approach",
                "outlier prone",
                "data based search strategy",
                "inliers",
                "outlier filtering",
                "EpicFlow",
                "MPI-Sintel"
            ]
        },
        "id": 448,
        "cited_by": [
            {
                "year": "2017",
                "id": 27
            },
            {
                "year": "2017",
                "id": 32
            },
            {
                "year": "2017",
                "id": 123
            },
            {
                "year": "2017",
                "id": 233
            },
            {
                "year": "2017",
                "id": 270
            },
            {
                "year": "2017",
                "id": 343
            },
            {
                "year": "2017",
                "id": 467
            },
            {
                "year": "2017",
                "id": 488
            }
        ]
    },
    {
        "title": "Dense Semantic Correspondence Where Every Pixel is a Classifier",
        "authors": [
            "Hilton Bristow",
            "Jack Valmadre",
            "Simon Lucey"
        ],
        "abstract": "Determining dense semantic correspondences across objects and scenes is a difficult problem that underpins many higher-level computer vision algorithms. Unlike canonical dense correspondence problems which consider images that are spatially or temporally adjacent, semantic correspondence is characterized by images that share similar high-level structures whose exact appearance and geometry may differ. Motivated by object recognition literature and recent work on rapidly estimating linear classifiers, we treat semantic correspondence as a constrained detection problem, where an exemplar LDA classifier is learned for each pixel. LDA classifiers have two distinct benefits: (i) they exhibit higher average precision than similarity metrics typically used in correspondence problems, and (ii) unlike exemplar SVM, can output globally interpretable posterior probabilities without calibration, whilst also being significantly faster to train. We pose the correspondence problem as a graphical model, where the unary potentials are computed via convolution with the set of exemplar classifiers, and the joint potentials enforce smoothly varying correspondence assignment.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410815",
        "reference_list": [
            {
                "year": "2009",
                "id": 250
            },
            {
                "year": "2013",
                "id": 344
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2005",
                "id": 5
            },
            {
                "year": "2009",
                "id": 18
            },
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 17,
            "other": 4,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Detectors",
                "Support vector machines",
                "Measurement",
                "Training",
                "Robustness",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "object recognition",
                "probability",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense semantic correspondence",
                "higher-level computer vision algorithm",
                "canonical dense correspondence problem",
                "spatially adjacent",
                "temporally adjacent",
                "high-level structure",
                "object recognition literature",
                "linear classifier",
                "constrained detection problem",
                "exemplar LDA classifier",
                "similarity metric",
                "exemplar SVM",
                "globally interpretable posterior probability",
                "graphical model",
                "unary potential",
                "exemplar classifier",
                "correspondence assignment"
            ]
        },
        "id": 449,
        "cited_by": [
            {
                "year": "2017",
                "id": 193
            },
            {
                "year": "2017",
                "id": 475
            },
            {
                "year": "2017",
                "id": 496
            }
        ]
    },
    {
        "title": "Multi-image Matching via Fast Alternating Minimization",
        "authors": [
            "Xiaowei Zhou",
            "Menglong Zhu",
            "Kostas Daniilidis"
        ],
        "abstract": "In this paper we propose a global optimization-based approach to jointly matching a set of images. The estimated correspondences simultaneously maximize pairwise feature affinities and cycle consistency across multiple images. Unlike previous convex methods relying on semidefinite programming, we formulate the problem as a low-rank matrix recovery problem and show that the desired semidefiniteness of a solution can be spontaneously fulfilled. The low-rank formulation enables us to derive a fast alternating minimization algorithm in order to handle practical problems with thousands of features. Both simulation and real experiments demonstrate that the proposed algorithm can achieve a competitive performance with an order of magnitude speedup compared to the state-of-the-art algorithm. In the end, we demonstrate the applicability of the proposed method to match the images of different object instances and as a result the potential to reconstruct category-specific object models from those images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410816",
        "reference_list": [
            {
                "year": "2013",
                "id": 310
            },
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2005",
                "id": 193
            },
            {
                "year": "2013",
                "id": 172
            },
            {
                "year": "2013",
                "id": 205
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 8,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Minimization",
                "Optimization",
                "Image matching",
                "Computer vision",
                "Image reconstruction",
                "Computational modeling",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "image matching",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiimage matching",
                "fast alternating minimization",
                "global optimization-based approach",
                "pairwise feature affinity",
                "cycle consistency",
                "convex method",
                "semidefinite programming",
                "low-rank matrix recovery problem",
                "low-rank formulation",
                "alternating minimization algorithm",
                "competitive performance",
                "magnitude speedup",
                "category-specific object model"
            ]
        },
        "id": 450,
        "cited_by": [
            {
                "year": "2017",
                "id": 427
            },
            {
                "year": "2017",
                "id": 479
            }
        ]
    },
    {
        "title": "Differential Recurrent Neural Networks for Action Recognition",
        "authors": [
            "Vivek Veeriah",
            "Naifan Zhuang",
            "Guo-Jun Qi"
        ],
        "abstract": "The long short-term memory (LSTM) neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences. It has the potential to model any time-series or sequential data, where the current hidden state has to be considered in the context of the past hidden states. This property makes LSTM an ideal choice to learn the complex dynamics of various actions. Unfortunately, the conventional LSTMs do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns, when they gate the information that ought to be memorized through time. To address this problem, we propose a differential gating scheme for the LSTM neural network, which emphasizes on the change in information gain caused by the salient motions between the successive frames. This change in information gain is quantified by Derivative of States (DoS), and thus the proposed LSTM model is termed as differential Recurrent Neural Network (dRNN). We demonstrate the effectiveness of the proposed model by automatically recognizing actions from the real-world 2D and 3D human action datasets. Our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410817",
        "reference_list": [
            {
                "year": "2007",
                "id": 147
            }
        ],
        "citation": {
            "ieee": 82,
            "other": 49,
            "total": 131
        },
        "keywords": {
            "IEEE Keywords": [
                "Logic gates",
                "Computer architecture",
                "Recurrent neural networks",
                "Microprocessors",
                "Three-dimensional displays",
                "Dynamics",
                "Integrated circuit modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image recognition",
                "image sequences",
                "learning (artificial intelligence)",
                "recurrent neural nets",
                "spatiotemporal phenomena",
                "time series"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "differential recurrent neural networks",
                "action recognition",
                "long short-term memory neural network",
                "LSTM neural network",
                "complex sequential information processing",
                "long input sequences",
                "sequential time-series data",
                "complex dynamics learning",
                "spatiotemporal dynamics",
                "salient motion patterns",
                "differential gating",
                "information gain",
                "derivative of states",
                "DoS",
                "dRNN",
                "2D human action datasets",
                "3D human action datasets",
                "complex time-series representation learning"
            ]
        },
        "id": 451,
        "cited_by": [
            {
                "year": "2017",
                "id": 151
            },
            {
                "year": "2017",
                "id": 303
            },
            {
                "year": "2017",
                "id": 611
            }
        ]
    },
    {
        "title": "Similarity Gaussian Process Latent Variable Model for Multi-modal Data Analysis",
        "authors": [
            "Guoli Song",
            "Shuhui Wang",
            "Qingming Huang",
            "Qi Tian"
        ],
        "abstract": "Data from real applications involve multiple modalities representing content with the same semantics and deliver rich information from complementary aspects. However, relations among heterogeneous modalities are simply treated as observation-to-fit by existing work, and the parameterized cross-modal mapping functions lack flexibility in directly adapting to the content divergence and semantic complicacy of multi-modal data. In this paper, we build our work based on Gaussian process latent variable model (GPLVM) to learn the non-linear non-parametric mapping functions and transform heterogeneous data into a shared latent space. We propose multi-modal Similarity Gaussian Process latent variable model (m-SimGP), which learns the nonlinear mapping functions between the intra-modal similarities and latent representation. We further propose multi-modal regularized similarity GPLVM (m-RSimGP) by encouraging similar/dissimilar points to be similar/dissimilar in the output space. The overall objective functions are solved by simple and scalable gradient decent techniques. The proposed models are robust to content divergence and high-dimensionality in multi-modal representation. They can be applied to various tasks to discover the non-linear correlations and obtain the comparable low-dimensional representation for heterogeneous modalities. On two widely used real-world datasets, we outperform previous approaches for cross-modal content retrieval and cross-modal classification.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410818",
        "reference_list": [
            {
                "year": "2011",
                "id": 306
            },
            {
                "year": "2005",
                "id": 52
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Gaussian processes",
                "Data models",
                "Semantics",
                "Analytical models",
                "Data analysis",
                "Kernel"
            ],
            "INSPEC: Controlled Indexing": [
                "data analysis",
                "data structures",
                "Gaussian processes",
                "learning (artificial intelligence)",
                "semantic networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "similarity Gaussian process latent variable model",
                "GPLVM",
                "multimodal data analysis",
                "content divergence",
                "semantic complicacy",
                "nonlinear mapping function learning",
                "shared latent space",
                "intramodal similarity",
                "latent representation",
                "nonlinear correlation"
            ]
        },
        "id": 452,
        "cited_by": [
            {
                "year": "2017",
                "id": 528
            }
        ]
    },
    {
        "title": "Learning Ensembles of Potential Functions for Structured Prediction with Latent Variables",
        "authors": [
            "Hossein Hajimirsadeghi",
            "Greg Mori"
        ],
        "abstract": "Many visual recognition tasks involve modeling variables which are structurally related. Hidden conditional random fields (HCRFs) are a powerful class of models for encoding structure in weakly supervised training examples. This paper presents HCRF-Boost, a novel and general framework for learning HCRFs in functional space. An algorithm is proposed to learn the potential functions of an HCRF as a combination of abstract nonlinear feature functions, expressed by regression models. Consequently, the resulting latent structured model is not restricted to traditional log-linear potential functions or any explicit parameterization. Further, functional optimization helps to avoid direct interactions with the possibly large parameter space of nonlinear models and improves efficiency. As a result, a complex and flexible ensemble method is achieved for structured prediction which can be successfully used in a variety of applications. We validate the effectiveness of this method on tasks such as group activity recognition, human action recognition, and multi-instance learning of video events.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410819",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Boosting",
                "Computational modeling",
                "Prediction algorithms",
                "Kernel",
                "Visualization",
                "Support vector machines"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "learning (artificial intelligence)",
                "regression analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video events",
                "multi-instance learning",
                "human action recognition",
                "group activity recognition",
                "regression models",
                "HCRF-Boost",
                "supervised training",
                "hidden conditional random fields",
                "visual recognition",
                "latent variables",
                "structured prediction",
                "potential functions",
                "learning ensembles"
            ]
        },
        "id": 453,
        "cited_by": []
    },
    {
        "title": "Simultaneous Deep Transfer Across Domains and Tasks",
        "authors": [
            "Eric Tzeng",
            "Judy Hoffman",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "abstract": "Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simultaneously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adaptation method offers empirical performance which exceeds previously published results on two standard benchmark visual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410820",
        "reference_list": [
            {
                "year": "2011",
                "id": 286
            },
            {
                "year": "2013",
                "id": 369
            }
        ],
        "citation": {
            "ieee": 127,
            "other": 73,
            "total": 200
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Visualization",
                "Adaptation models",
                "Standards",
                "Correlation",
                "Semantics",
                "Robots"
            ],
            "INSPEC: Controlled Indexing": [
                "neural nets",
                "pattern classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "classification",
                "semisupervised adaptation setting",
                "visual domain adaptation tasks",
                "soft label distribution matching loss",
                "domain transfer",
                "domain invariance",
                "fine-tuning deep models",
                "generic supervised deep CNN model",
                "simultaneous deep transfer"
            ]
        },
        "id": 454,
        "cited_by": [
            {
                "year": "2017",
                "id": 41
            },
            {
                "year": "2017",
                "id": 78
            },
            {
                "year": "2017",
                "id": 136
            },
            {
                "year": "2017",
                "id": 141
            },
            {
                "year": "2017",
                "id": 213
            },
            {
                "year": "2017",
                "id": 358
            },
            {
                "year": "2017",
                "id": 532
            },
            {
                "year": "2017",
                "id": 599
            }
        ]
    },
    {
        "title": "Low Dimensional Explicit Feature Maps",
        "authors": [
            "Ondrej Chum"
        ],
        "abstract": "Approximating non-linear kernels by finite-dimensional feature maps is a popular approach for speeding up training and evaluation of support vector machines or to encode information into efficient match kernels. We propose a novel method of data independent construction of low dimensional feature maps. The problem is cast as a linear program which jointly considers competing objectives: the quality of the approximation and the dimensionality of the feature map. For both shift-invariant and homogeneous kernels the proposed method achieves a better approximations at the same dimensionality or comparable approximations at lower dimensionality of the feature map compared with state-of-the-art methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410821",
        "reference_list": [
            {
                "year": "2009",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Optimization",
                "Harmonic analysis",
                "Solids",
                "Support vector machines",
                "Measurement uncertainty",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "computer vision",
                "linear programming",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "low dimensional explicit feature maps",
                "nonlinear kernel approximation",
                "finite-dimensional feature maps",
                "support vector machines",
                "linear program",
                "shift-invariant kernels",
                "homogeneous kernels"
            ]
        },
        "id": 455,
        "cited_by": []
    },
    {
        "title": "Unsupervised Learning of Spatiotemporally Coherent Metrics",
        "authors": [
            "Ross Goroshin",
            "Joan Bruna",
            "Jonathan Tompson",
            "David Eigen",
            "Yann LeCun"
        ],
        "abstract": "Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity priors. We establish a connection between slow feature learning and metric learning. Using this connection we define \"temporal coherence\" -- a criterion which can be used to set hyper-parameters in a principled and automated manner. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410822",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 12,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Feature extraction",
                "Measurement",
                "Dictionaries",
                "Unsupervised learning",
                "Training",
                "Convolution",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "convolution",
                "unsupervised learning",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatiotemporally coherent metrics",
                "detection algorithms",
                "classification algorithms",
                "deep convolutional networks",
                "labeled data",
                "unsupervised feature learning",
                "video data",
                "video frames",
                "convolutional pooling auto-encoder",
                "metric learning",
                "hyperparameters",
                "transfer learning"
            ]
        },
        "id": 456,
        "cited_by": [
            {
                "year": "2017",
                "id": 298
            },
            {
                "year": "2017",
                "id": 451
            }
        ]
    },
    {
        "title": "Multi-label Cross-Modal Retrieval",
        "authors": [
            "Viresh Ranjan",
            "Nikhil Rasiwasia",
            "C. V. Jawahar"
        ],
        "abstract": "In this work, we address the problem of cross-modal retrieval in presence of multi-label annotations. In particular, we introduce multi-label Canonical Correlation Analysis (ml-CCA), an extension of CCA, for learning shared subspaces taking into account high level semantic information in the form of multi-label annotations. Unlike CCA, ml-CCA does not rely on explicit pairing between modalities, instead it uses the multi-label information to establish correspondences. This results in a discriminative subspace which is better suited for cross-modal retrieval tasks. We also present Fast ml-CCA, a computationally efficient version of ml-CCA, which is able to handle large scale datasets. We show the efficacy of our approach by conducting extensive cross-modal retrieval experiments on three standard benchmark datasets. The results show that the proposed approach achieves state of the art retrieval performance on the three datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410823",
        "reference_list": [
            {
                "year": "2013",
                "id": 260
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 28,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Correlation",
                "Semantics",
                "Multimedia communication",
                "Benchmark testing",
                "Computer vision",
                "Standards",
                "Portable computers"
            ],
            "INSPEC: Controlled Indexing": [
                "correlation methods",
                "image processing",
                "image retrieval",
                "learning (artificial intelligence)",
                "semantic networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multilabel cross-modal retrieval",
                "multilabel annotation",
                "multilabel canonical correlation analysis",
                "ml-CCA",
                "subspace learning",
                "semantic information"
            ]
        },
        "id": 457,
        "cited_by": [
            {
                "year": "2017",
                "id": 49
            },
            {
                "year": "2017",
                "id": 432
            }
        ]
    },
    {
        "title": "Improving Ferns Ensembles by Sparsifying and Quantising Posterior Probabilities",
        "authors": [
            "Antonio L. Rodriguez",
            "Vitor Sequeira"
        ],
        "abstract": "Ferns ensembles offer an accurate and efficient multiclass non-linear classification, commonly at the expense of consuming a large amount of memory. We introduce a two-fold contribution that produces large reductions in their memory consumption. First, an efficient L0 regularised cost optimisation finds a sparse representation of the posterior probabilities in the ensemble by discarding elements with zero contribution to valid responses in the training samples. As a by-product this can produce a prediction accuracy gain that, if required, can be traded for further reductions in memory size and prediction time. Secondly, posterior probabilities are quantised and stored in a memory-friendly sparse data structure. We reported a minimum of 75% memory reduction for different types of classification problems using generative and discriminative ferns ensembles, without increasing prediction time or classification error. For image patch recognition our proposal produced a 90% memory reduction, and improved in several percentage points the prediction accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410824",
        "reference_list": [
            {
                "year": "2007",
                "id": 226
            },
            {
                "year": "2009",
                "id": 45
            },
            {
                "year": "2013",
                "id": 323
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Memory management",
                "Vegetation",
                "Image recognition",
                "Support vector machines",
                "Optimization",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "data structures",
                "image classification",
                "image recognition",
                "image representation",
                "probability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ferns ensembles",
                "posterior probability quantisation",
                "posterior probability sparsification",
                "multiclass nonlinear classification",
                "memory consumption",
                "L0 regularised cost optimisation",
                "sparse representation",
                "memory-friendly sparse data structure",
                "image patch recognition",
                "memory reduction"
            ]
        },
        "id": 458,
        "cited_by": []
    },
    {
        "title": "Beyond Gauss: Image-Set Matching on the Riemannian Manifold of PDFs",
        "authors": [
            "Mehrtash Harandi",
            "Mathieu Salzmann",
            "Mahsa Baktashmotlagh"
        ],
        "abstract": "State-of-the-art image-set matching techniques typically implicitly model each image-set with a Gaussian distribution. Here, we propose to go beyond these representations and model image-sets as probability distribution functions (PDFs) using kernel density estimators. To compare and match image-sets, we exploit Csiszar f-divergences, which bear strong connections to the geodesic distance defined on the space of PDFs, i.e., the statistical manifold. Furthermore, we introduce valid positive definite kernels on the statistical manifolds, which let us make use of more powerful classification schemes to match image-sets. Finally, we introduce a supervised dimensionality reduction technique that learns a latent space where f-divergences reflect the class labels of the data. Our experiments on diverse problems, such as video-based face recognition and dynamic texture classification, evidence the benefits of our approach over the state-of-the-art image-set matching methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410825",
        "reference_list": [
            {
                "year": "2013",
                "id": 41
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 6,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Kernel",
                "Measurement",
                "Gaussian distribution",
                "Covariance matrices",
                "Probability distribution",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "Gaussian distribution",
                "image classification",
                "image matching",
                "image representation",
                "statistical analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Riemannian manifold",
                "PDFs",
                "image-set matching technique",
                "Gaussian distribution",
                "probability distribution function",
                "kernel density estimator",
                "Csiszar f-divergence",
                "geodesic distance",
                "statistical manifold",
                "powerful classification scheme",
                "supervised dimensionality reduction technique",
                "image-set matching method"
            ]
        },
        "id": 459,
        "cited_by": []
    },
    {
        "title": "Unsupervised Domain Adaptation with Imbalanced Cross-Domain Data",
        "authors": [
            "Tzu Ming Harry Hsu",
            "Wei Yu Chen",
            "Cheng-An Hou",
            "Yao-Hung Hubert Tsai",
            "Yi-Ren Yeh",
            "Yu-Chiang Frank Wang"
        ],
        "abstract": "We address a challenging unsupervised domain adaptation problem with imbalanced cross-domain data. For standard unsupervised domain adaptation, one typically obtains labeled data in the source domain and only observes unlabeled data in the target domain. However, most existing works do not consider the scenarios in which either the label numbers across domains are different, or the data in the source and/or target domains might be collected from multiple datasets. To address the aforementioned settings of imbalanced cross-domain data, we propose Closest Common Space Learning (CCSL) for associating such data with the capability of preserving label and structural information within and across domains. Experiments on multiple cross-domain visual classification tasks confirm that our method performs favorably against state-of-the-art approaches, especially when imbalanced cross-domain data are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410826",
        "reference_list": [
            {
                "year": "2013",
                "id": 95
            },
            {
                "year": "2013",
                "id": 369
            },
            {
                "year": "2013",
                "id": 311
            },
            {
                "year": "2013",
                "id": 274
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 3,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Sensors",
                "Dictionaries",
                "Target recognition",
                "Manifolds",
                "Conferences",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern classification",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised domain adaptation problem",
                "imbalanced cross-domain data",
                "closest common space learning",
                "CCSL",
                "multiple cross-domain visual classification tasks",
                "preserving label",
                "structural information"
            ]
        },
        "id": 460,
        "cited_by": [
            {
                "year": "2017",
                "id": 377
            }
        ]
    },
    {
        "title": "Secrets of Matrix Factorization: Approximations, Numerics, Manifold Optimization and Random Restarts",
        "authors": [
            "Je Hyeong Hong",
            "Andrew Fitzgibbon"
        ],
        "abstract": "Matrix factorization (or low-rank matrix completion) with missing data is a key computation in many computer vision and machine learning tasks, and is also related to a broader class of nonlinear optimization problems such as bundle adjustment. The problem has received much attention recently, with renewed interest in variable-projection approaches, yielding dramatic improvements in reliability and speed. However, on a wide class of problems, no one approach dominates, and because the various approaches have been derived in a multitude of different ways, it has been difficult to unify them. This paper provides a unified derivation of a number of recent approaches, so that similarities and differences are easily observed. We also present a simple meta-algorithm which wraps any existing algorithm, yielding 100% success rate on many standard datasets. Given 100% success, the focus of evaluation must turn to speed, as 100% success is trivially achieved if we do not care about speed. Again our unification allows a number of generic improvements applicable to all members of the family to be isolated, yielding a unified algorithm that outperforms our re-implementation of existing algorithms, which in some cases already outperform the original authors' publicly available codes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410827",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Algorithm design and analysis",
                "Jacobian matrices",
                "Benchmark testing",
                "Convergence",
                "Approximation algorithms",
                "Optimization",
                "Reliability"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "learning (artificial intelligence)",
                "matrix decomposition",
                "optimisation",
                "reliability"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "matrix factorization",
                "computer vision",
                "machine learning",
                "nonlinear optimization problem",
                "variable-projection approach",
                "reliability",
                "success rate"
            ]
        },
        "id": 461,
        "cited_by": []
    },
    {
        "title": "Geometry-Aware Deep Transform",
        "authors": [
            "Jiaji Huang",
            "Qiang Qiu",
            "Robert Calderbank",
            "Guillermo Sapiro"
        ],
        "abstract": "Many recent efforts have been devoted to designing sophisticated deep learning structures, obtaining revolutionary results on benchmark datasets. The success of these deep learning methods mostly relies on an enormous volume of labeled training samples to learn a huge number of parameters in a network; therefore, understanding the generalization ability of a learned deep network cannot be overlooked, especially when restricted to a small training set, which is the case for many applications. In this paper, we propose a novel deep learning objective formulation that unifies both the classification and metric learning criteria. We then introduce a geometry-aware deep transform to enable a non-linear discriminative and robust feature transform, which shows competitive performance on small training sets for both synthetic and real-world data. We further support the proposed framework with a formal (K, \u03f5)-robustness analysis.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410828",
        "reference_list": [
            {
                "year": "2013",
                "id": 300
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Measurement",
                "Transforms",
                "Testing",
                "Robustness",
                "Machine learning",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "generalisation (artificial intelligence)",
                "geometry",
                "image classification",
                "learning (artificial intelligence)",
                "transforms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometry-aware deep transform",
                "deep learning structures",
                "deep learning methods",
                "generalization ability",
                "deep learning objective formulation",
                "metric learning criteria",
                "nonlinear discriminative transform",
                "robust feature transform",
                "formal (K,\u03f5)-robustness analysis"
            ]
        },
        "id": 462,
        "cited_by": []
    },
    {
        "title": "Learning Binary Codes for Maximum Inner Product Search",
        "authors": [
            "Fumin Shen",
            "Wei Liu",
            "Shaoting Zhang",
            "Yang Yang",
            "Heng Tao Shen"
        ],
        "abstract": "Binary coding or hashing techniques are recognized to accomplish efficient near neighbor search, and have thus attracted broad interests in the recent vision and learning studies. However, such studies have rarely been dedicated to Maximum Inner Product Search (MIPS), which plays a critical role in various vision applications. In this paper, we investigate learning binary codes to exclusively handle the MIPS problem. Inspired by the latest advance in asymmetric hashing schemes, we propose an asymmetric binary code learning framework based on inner product fitting. Specifically, two sets of coding functions are learned such that the inner products between their generated binary codes can reveal the inner products between original data vectors. We also propose an alternative simpler objective which maximizes the correlations between the inner products of the produced binary codes and raw data vectors. In both objectives, the binary codes and coding functions are simultaneously learned without continuous relaxations, which is the key to achieving high-quality binary codes. We evaluate the proposed method, dubbed Asymmetric Inner-product Binary Coding (AIBC), relying on the two objectives on several large-scale image datasets. Both of them are superior to the state-of-the-art binary coding and hashing methods in performing MIPS tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410829",
        "reference_list": [
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2015",
                "id": 123
            },
            {
                "year": "2013",
                "id": 378
            }
        ],
        "citation": {
            "ieee": 33,
            "other": 26,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Binary codes",
                "Encoding",
                "Optimization",
                "Image coding",
                "Correlation",
                "Kernel",
                "Learning systems"
            ],
            "INSPEC: Controlled Indexing": [
                "binary codes",
                "correlation methods",
                "image coding",
                "learning (artificial intelligence)",
                "search problems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "maximum inner product search",
                "MIPS problem",
                "asymmetric hashing schemes",
                "asymmetric binary code learning framework",
                "inner product fitting",
                "coding functions",
                "correlation maximization",
                "high-quality binary codes",
                "dubbed asymmetric inner-product binary coding",
                "AIBC",
                "large-scale image datasets"
            ]
        },
        "id": 463,
        "cited_by": [
            {
                "year": "2015",
                "id": 123
            }
        ]
    },
    {
        "title": "ML-MG: Multi-label Learning with Missing Labels Using a Mixed Graph",
        "authors": [
            "Baoyuan Wu",
            "Siwei Lyu",
            "Bernard Ghanem"
        ],
        "abstract": "This work focuses on the problem of multi-label learning with missing labels (MLML), which aims to label each test instance with multiple class labels given training instances that have an incomplete/partial set of these labels (i.e. some of their labels are missing). To handle missing labels, we propose a unified model of label dependencies by constructing a mixed graph, which jointly incorporates (i) instance-level similarity and class co-occurrence as undirected edges and (ii) semantic label hierarchy as directed edges. Unlike most MLML methods, We formulate this learning problem transductively as a convex quadratic matrix optimization problem that encourages training label consistency and encodes both types of label dependencies (i.e. undirected and directed edges) using quadratic terms and hard linear constraints. The alternating direction method of multipliers (ADMM) can be used to exactly and efficiently solve this problem. To evaluate our proposed method, we consider two popular applications (image and video annotation), where the label hierarchy can be derived from Wordnet. Experimental results show that our method achieves a significant improvement over state-of-the-art methods in performance and robustness to missing labels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410830",
        "reference_list": [
            {
                "year": "2009",
                "id": 39
            },
            {
                "year": "2013",
                "id": 356
            },
            {
                "year": "2013",
                "id": 35
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 9,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Training",
                "Computer vision",
                "Image edge detection",
                "Horses",
                "Games"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "edge detection",
                "graph theory",
                "learning (artificial intelligence)",
                "matrix algebra",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Wordnet",
                "video annotation",
                "image annotation",
                "ADMM",
                "alternating direction method of multiplier",
                "hard linear constraint",
                "label consistency",
                "convex quadratic matrix optimization problem",
                "learning problem",
                "MLML method",
                "semantic label hierarchy",
                "undirected edge",
                "class co-occurrence",
                "instance-level similarity",
                "label dependency",
                "training instance",
                "multiple class label",
                "multilabel learning with missing label",
                "mixed graph",
                "ML-MG"
            ]
        },
        "id": 464,
        "cited_by": [
            {
                "year": "2017",
                "id": 49
            },
            {
                "year": "2017",
                "id": 201
            }
        ]
    },
    {
        "title": "Zero-Shot Learning via Semantic Similarity Embedding",
        "authors": [
            "Ziming Zhang",
            "Venkatesh Saligrama"
        ],
        "abstract": "In this paper we consider a version of the zero-shot learning problem where seen class source and target domain data are provided. The goal during test-time is to accurately predict the class label of an unseen target domain instance based on revealed source domain side information (e.g. attributes) for unseen classes. Our method is based on viewing each source or target data as a mixture of seen class proportions and we postulate that the mixture patterns have to be similar if the two instances belong to the same unseen class. This perspective leads us to learning source/target embedding functions that map an arbitrary source/target domain data into a same semantic space where similarity can be readily measured. We develop a max-margin framework to learn these similarity functions and jointly optimize parameters by means of cross validation. Our test results are compelling, leading to significant improvement in terms of accuracy on most benchmark datasets for zero-shot recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410831",
        "reference_list": [
            {
                "year": "2011",
                "id": 155
            },
            {
                "year": "2013",
                "id": 264
            },
            {
                "year": "2015",
                "id": 437
            }
        ],
        "citation": {
            "ieee": 79,
            "other": 37,
            "total": 116
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Histograms",
                "Optimization",
                "Training data",
                "Training",
                "Benchmark testing",
                "Image color analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "learning (artificial intelligence)",
                "optimisation",
                "prediction theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic similarity embedding",
                "zero-shot learning problem",
                "seen class source",
                "target domain data",
                "class label prediction",
                "unseen target domain instance",
                "source domain side information",
                "unseen classes",
                "seen class proportions",
                "mixture patterns",
                "source/target embedding functions",
                "semantic space",
                "similarity measure",
                "max-margin framework",
                "similarity functions",
                "parameters optimization",
                "cross validation",
                "zero-shot recognition"
            ]
        },
        "id": 465,
        "cited_by": [
            {
                "year": "2017",
                "id": 129
            },
            {
                "year": "2017",
                "id": 318
            },
            {
                "year": "2017",
                "id": 366
            },
            {
                "year": "2017",
                "id": 376
            },
            {
                "year": "2017",
                "id": 443
            },
            {
                "year": "2015",
                "id": 437
            }
        ]
    },
    {
        "title": "Bayesian Model Adaptation for Crowd Counts",
        "authors": [
            "Bo Liu",
            "Nuno Vasconcelos"
        ],
        "abstract": "The problem of transfer learning is considered in the domain of crowd counting. A solution based on Bayesian model adaptation of Gaussian processes is proposed. This is shown to produce intuitive model updates, which are tractable, and lead to an adapted model (predictive distribution) that accounts for all information in both training and adaptation data. The new adaptation procedure achieves significant gains over previous approaches, based on multi-task learning, while requiring much less computation to deploy. This makes it particularly suited for the problem of expanding the capacity of crowd counting camera networks. A large video dataset for the evaluation of adaptation approaches to crowd counting is also introduced. This contains a number of adaptation tasks, involving information transfer across video collected by 1) a single camera under different scene conditions (different times of the day) and 2) video collected from different cameras. Evaluation of the proposed model adaptation procedure in this dataset shows good performance in realistic operating conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410832",
        "reference_list": [
            {
                "year": "2013",
                "id": 281
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 4,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Adaptation models",
                "Computational modeling",
                "Cameras",
                "Data models",
                "Bayes methods",
                "Kernel",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "computer vision",
                "Gaussian processes",
                "learning (artificial intelligence)",
                "video cameras",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian model adaptation",
                "transfer learning",
                "Gaussian process",
                "model update",
                "adapted model",
                "predictive distribution",
                "adaptation procedure",
                "multitask learning",
                "crowd counting camera network",
                "video dataset",
                "adaptation approach",
                "information transfer",
                "scene condition",
                "realistic operating condition"
            ]
        },
        "id": 466,
        "cited_by": [
            {
                "year": "2017",
                "id": 541
            }
        ]
    },
    {
        "title": "An NMF Perspective on Binary Hashing",
        "authors": [
            "Lopamudra Mukherjee",
            "Sathya N. Ravi",
            "Vamsi K. Ithapu",
            "Tyler Holmes",
            "Vikas Singh"
        ],
        "abstract": "The pervasiveness of massive data repositories has led to much interest in efficient methods for indexing, search, and retrieval. For image data, a rapidly developing body of work for these applications shows impressive performance with methods that broadly fall under the umbrella term of Binary Hashing. Given a distance matrix, a binary hashing algorithm solves for a binary code for the given set of examples, whose Hamming distance nicely approximates the original distances. The formulation is non-convex -- so existing solutions adopt spectral relaxations or perform coordinate descent (or quantization) on a surrogate objective that is numerically more tractable. In this paper, we first derive an Augmented Lagrangian approach to optimize the standard binary Hashing objective (i.e.,maintain fidelity with a given distance matrix). With appropriate step sizes, we find that this scheme already yields results that match or substantially outperform state of the art methods on most benchmarks used in the literature. Then, to allow the model to scale to large datasets, we obtain an interesting reformulation of the binary hashing objective as a non negative matrix factorization. Later, this leads to a simple multiplicative updates algorithm -- whose parallelization properties are exploited to obtain a fast GPU based implementation. We give a probabilistic analysis of our initialization scheme and present a range of experiments to show that the method is simple to implement and competes favorably with available methods (both for optimization and generalization).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410833",
        "reference_list": [
            {
                "year": "2009",
                "id": 274
            },
            {
                "year": "2013",
                "id": 318
            },
            {
                "year": "2003",
                "id": 99
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Hamming distance",
                "Binary codes",
                "Computer vision",
                "Optimization",
                "Benchmark testing",
                "Graphics processing units",
                "Semantics"
            ],
            "INSPEC: Controlled Indexing": [
                "binary codes",
                "database indexing",
                "file organisation",
                "graphics processing units",
                "Hamming codes",
                "matrix decomposition",
                "probability",
                "search problems",
                "spectral analysis",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "NMF perspective",
                "data repository pervasiveness",
                "image data",
                "distance matrix",
                "binary hashing algorithm",
                "binary code",
                "Hamming distance",
                "nonconvex formulation",
                "spectral relaxations",
                "augmented Lagrangian approach",
                "nonnegative matrix factorization",
                "multiplicative update algorithm",
                "GPU based implementation"
            ]
        },
        "id": 467,
        "cited_by": []
    },
    {
        "title": "Multi-view Domain Generalization for Visual Recognition",
        "authors": [
            "Li Niu",
            "Wen Li",
            "Dong Xu"
        ],
        "abstract": "In this paper, we propose a new multi-view domain generalization (MVDG) approach for visual recognition, in which we aim to use the source domain samples with multiple types of features (i.e., multi-view features) to learn robust classifiers that can generalize well to any unseen target domain. Considering the recent works show the domain generalization capability can be enhanced by fusing multiple SVM classifiers, we build upon exemplar SVMs to learn a set of SVM classifiers by using one positive sample and all negative samples in the source domain each time. When the source domain samples come from multiple latent domains, we expect the weight vectors of exemplar SVM classifiers can be organized into multiple hidden clusters. To exploit such cluster structure, we organize the weight vectors learnt on each view as a weight matrix and seek the low-rank representation by reconstructing this weight matrix using itself as the dictionary. To enforce the consistency of inherent cluster structures discovered from the weight matrices learnt on different views, we introduce a new regularizer to minimize the mismatch between any two representation matrices on different views. We also develop an efficient alternating optimization algorithm and further extend our MVDG approach for domain adaptation by exploiting the manifold structure of unlabeled target domain samples. Comprehensive experiments for visual recognition clearly demonstrate the effectiveness of our approaches for domain generalization and domain adaptation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410834",
        "reference_list": [
            {
                "year": "2013",
                "id": 95
            },
            {
                "year": "2013",
                "id": 369
            },
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2011",
                "id": 11
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Support vector machines",
                "Training data",
                "Visualization",
                "Testing",
                "Optimization",
                "Linear matrix inequalities"
            ],
            "INSPEC: Controlled Indexing": [
                "generalisation (artificial intelligence)",
                "image classification",
                "image fusion",
                "image representation",
                "matrix algebra",
                "optimisation",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview domain generalization",
                "visual recognition",
                "MVDG",
                "SVM classifier fusion",
                "multiple latent domains",
                "weight vectors",
                "exemplar SVM classifiers",
                "cluster structure",
                "weight matrix",
                "low-rank representation",
                "representation matrices",
                "alternating optimization algorithm",
                "domain adaptation"
            ]
        },
        "id": 468,
        "cited_by": [
            {
                "year": "2017",
                "id": 599
            }
        ]
    },
    {
        "title": "Infinite Feature Selection",
        "authors": [
            "Giorgio Roffo",
            "Simone Melzi",
            "Marco Cristani"
        ],
        "abstract": "Filter-based feature selection has become crucial in many classification settings, especially object recognition, recently faced with feature learning strategies that originate thousands of cues. In this paper, we propose a feature selection method exploiting the convergence properties of power series of matrices, and introducing the concept of infinite feature selection (Inf-FS). Considering a selection of features as a path among feature distributions and letting these paths tend to an infinite number permits the investigation of the importance (relevance and redundancy) of a feature when injected into an arbitrary set of cues. Ranking the importance individuates candidate features, which turn out to be effective from a classification point of view, as proved by a thoroughly experimental section. The Inf-FS has been tested on thirteen diverse benchmarks, comparing against filters, embedded methods, and wrappers, in all the cases we achieve top performances, notably on the classification tasks of PASCAL VOC 2007-2012.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410835",
        "reference_list": [],
        "citation": {
            "ieee": 34,
            "other": 36,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Convergence",
                "Standards",
                "Object recognition",
                "Benchmark testing",
                "Feature extraction",
                "Redundancy",
                "Joining processes"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "image filtering",
                "matrix algebra",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "infinite feature selection",
                "filter-based feature selection",
                "classification setting",
                "object recognition",
                "feature learning strategy",
                "matrices",
                "Inf-FS"
            ]
        },
        "id": 469,
        "cited_by": [
            {
                "year": "2017",
                "id": 146
            }
        ]
    },
    {
        "title": "Semi-Supervised Zero-Shot Classification with Label Representation Learning",
        "authors": [
            "Xin Li",
            "Yuhong Guo",
            "Dale Schuurmans"
        ],
        "abstract": "Given the challenge of gathering labeled training data, zero-shot classification, which transfers information from observed classes to recognize unseen classes, has become increasingly popular in the computer vision community. Most existing zero-shot learning methods require a user to first provide a set of semantic visual attributes for each class as side information before applying a two-step prediction procedure that introduces an intermediate attribute prediction problem. In this paper, we propose a novel zero-shot classification approach that automatically learns label embeddings from the input data in a semi-supervised large-margin learning framework. The proposed framework jointly considers multi-class classification over all classes (observed and unseen) and tackles the target prediction problem directly without introducing intermediate prediction problems. It also has the capacity to incorporate semantic label information from different sources when available. To evaluate the proposed approach, we conduct experiments on standard zero-shot data sets. The empirical results show the proposed approach outperforms existing state-of-the-art zero-shot learning methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410836",
        "reference_list": [
            {
                "year": "2013",
                "id": 322
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 16,
            "total": 35
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Training",
                "Learning systems",
                "Data models",
                "Adaptation models",
                "Predictive models",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image classification",
                "image representation",
                "learning (artificial intelligence)",
                "prediction theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiclass classification",
                "semisupervised large-margin learning framework",
                "intermediate attribute prediction problem",
                "two-step prediction procedure",
                "semantic visual attributes",
                "zero-shot learning methods",
                "computer vision community",
                "label representation learning",
                "semisupervised zero-shot classification"
            ]
        },
        "id": 470,
        "cited_by": [
            {
                "year": "2017",
                "id": 443
            }
        ]
    },
    {
        "title": "A Supervised Low-Rank Method for Learning Invariant Subspaces",
        "authors": [
            "Farzad Siyahjani",
            "Ranya Almohsen",
            "Sinan Sabri",
            "Gianfranco Doretto"
        ],
        "abstract": "Sparse representation and low-rank matrix decomposition approaches have been successfully applied to several computer vision problems. They build a generative representation of the data, which often requires complex training as well as testing to be robust against data variations induced by nuisance factors. We introduce the invariant components, a discriminative representation invariant to nuisance factors, because it spans subspaces orthogonal to the space where nuisance factors are defined. This allows developing a framework based on geometry that ensures a uniform inter-class separation, and a very efficient and robust classification based on simple nearest neighbor. In addition, we show how the approach is equivalent to a local metric learning, where the local metrics (one for each class) are learned jointly, rather than independently, thus avoiding the risk of overfitting without the need for additional regularization. We evaluated the approach for face recognition with highly corrupted training and testing data, obtaining very promising results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410837",
        "reference_list": [
            {
                "year": "2011",
                "id": 310
            },
            {
                "year": "2009",
                "id": 63
            },
            {
                "year": "2013",
                "id": 31
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Sparse matrices",
                "Matrix decomposition",
                "Training",
                "Measurement",
                "Training data",
                "Testing",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "face recognition",
                "image classification",
                "image representation",
                "learning (artificial intelligence)",
                "matrix decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "supervised low-rank method",
                "invariant subspace learning",
                "sparse representation",
                "low-rank matrix decomposition approaches",
                "computer vision problems",
                "uniform interclass separation",
                "robust classification",
                "simple nearest neighbor approach",
                "local metric learning",
                "face recognition"
            ]
        },
        "id": 471,
        "cited_by": []
    },
    {
        "title": "Recursive Fr\u00e9chet Mean Computation on the Grassmannian and Its Applications to Computer Vision",
        "authors": [
            "Rudrasis Chakraborty",
            "Baba C. Vemuri"
        ],
        "abstract": "In the past decade, Grassmann manifolds (Grassmannian) have been commonly used in mathematical formulations of many Computer Vision tasks. Averaging points on a Grassmann manifold is a very common operation in many applications including but not limited to, tracking, action recognition, video-face recognition, face recognition, etc. Computing the intrinsic/Fr\u00e9chet mean (FM) of a set of points on the Grassmann can be cast as finding the global optimum (if it exists) of the sum of squared geodesic distances cost function. A common approach to solve this problem involves the use of the gradient descent method. An alternative way to compute the FM is to develop a recursive/inductive definition that does not involve optimizing the aforementioned cost function. In this paper, we propose one such computationally efficient algorithm called the Grassmann inductive Fr\u00e9chet mean estimator (GiFME). In developing the recursive solution to find the FM of the given set of points, GiFME exploits the fact that there is a closed form solution to find the FM of two points on the Grassmann. In the limit as the number of samples tends to infinity, we prove that GiFME converges to the FM (this is called the weak consistency result on the Grassmann manifold). Further, for the finite sample case, in the limit as the number of sample paths (trials) goes to infinity, we show that GiFME converges to the finite sample FM. Moreover, we present a bound on the geodesic distance between the estimate from GiFME and the true FM. We present several experiments on synthetic and real data sets to demonstrate the performance of GiFME in comparison to the gradient descent based (batch mode) technique. Our goal in these applications is to demonstrate the computational advantage and achieve comparable accuracy to the state-of-the-art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410838",
        "reference_list": [
            {
                "year": "2013",
                "id": 223
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Frequency modulation",
                "Manifolds",
                "Measurement",
                "Computer vision",
                "Face recognition",
                "Convergence",
                "Cost function"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "gradient methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "recursive Fr\u00e9chet mean computation",
                "Grassmannian",
                "Grassmann manifolds",
                "mathematical formulations",
                "computer vision tasks",
                "squared geodesic distances cost function",
                "gradient descent method",
                "recursive definition",
                "inductive definition",
                "Grassmann inductive Fr\u00e9chet mean estimator",
                "GiFME"
            ]
        },
        "id": 472,
        "cited_by": []
    },
    {
        "title": "Multi-view Subspace Clustering",
        "authors": [
            "Hongchang Gao",
            "Feiping Nie",
            "Xuelong Li",
            "Heng Huang"
        ],
        "abstract": "For many computer vision applications, the data sets distribute on certain low-dimensional subspaces. Subspace clustering is to find such underlying subspaces and cluster the data points correctly. In this paper, we propose a novel multi-view subspace clustering method. The proposed method performs clustering on the subspace representation of each view simultaneously. Meanwhile, we propose to use a common cluster structure to guarantee the consistence among different views. In addition, an efficient algorithm is proposed to solve the problem. Experiments on four benchmark data sets have been performed to validate our proposed method. The promising results demonstrate the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410839",
        "reference_list": [
            {
                "year": "2007",
                "id": 14
            },
            {
                "year": "2001",
                "id": 184
            },
            {
                "year": "2005",
                "id": 97
            }
        ],
        "citation": {
            "ieee": 28,
            "other": 22,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Clustering methods",
                "Optimization",
                "Computer vision",
                "Clustering algorithms",
                "Computer science",
                "Image color analysis",
                "Benchmark testing"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image representation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview subspace clustering",
                "computer vision applications",
                "low-dimensional subspaces",
                "subspace representation"
            ]
        },
        "id": 473,
        "cited_by": [
            {
                "year": "2017",
                "id": 602
            }
        ]
    },
    {
        "title": "Predicting Deep Zero-Shot Convolutional Neural Networks Using Textual Descriptions",
        "authors": [
            "Jimmy Lei Ba",
            "Kevin Swersky",
            "Sanja Fidler",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo-attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end using the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410840",
        "reference_list": [
            {
                "year": "2013",
                "id": 322
            },
            {
                "year": "2011",
                "id": 126
            },
            {
                "year": "2011",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 36,
            "other": 19,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Encyclopedias",
                "Training",
                "Neural networks",
                "Visualization",
                "Electronic publishing",
                "Internet"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image texture",
                "learning (artificial intelligence)",
                "neural nets"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "deep zero-shot convolutional neural networks",
                "textual description",
                "zero-shot learning",
                "visual category",
                "semantic attribute",
                "Wikipedia articles",
                "text features",
                "CNN",
                "Caltech-UCSD bird dataset",
                "flower dataset",
                "ROC curve",
                "receiver operating characteristic curve",
                "precision-recall curve"
            ]
        },
        "id": 474,
        "cited_by": [
            {
                "year": "2017",
                "id": 63
            },
            {
                "year": "2017",
                "id": 129
            },
            {
                "year": "2017",
                "id": 211
            },
            {
                "year": "2017",
                "id": 318
            },
            {
                "year": "2017",
                "id": 366
            },
            {
                "year": "2017",
                "id": 443
            }
        ]
    },
    {
        "title": "Structured Feature Selection",
        "authors": [
            "Tian Gao",
            "Ziheng Wang",
            "Qiang Ji"
        ],
        "abstract": "Feature dimensionality reduction has been widely used in various computer vision tasks. We explore feature selection as the dimensionality reduction technique and propose to use a structured approach, based on the Markov Blanket (MB), to select features. We first introduce a new MB discovery algorithm, Simultaneous Markov Blanket (STMB) discovery, that improves the efficiency of state-of-the-art algorithms. Then we theoretically justify three advantages of structured feature selection over traditional feature selection methods. Specifically, we show that the Markov Blanket is the minimum feature set that retains the maximal mutual information and also gives the lowest Bayes classification error. Then we apply structured feature selection to two applications: 1) We introduce a new method that enables STMB to scale up and show the competitive performance of our algorithms on large-scale image classification tasks. 2) We propose a method for structured feature selection to handle hierarchical features and show the proposed method can lead to big performance gain in facial expression and action unit (AU) recognition tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410841",
        "reference_list": [
            {
                "year": "2011",
                "id": 103
            },
            {
                "year": "2013",
                "id": 18
            },
            {
                "year": "2009",
                "id": 3
            },
            {
                "year": "2013",
                "id": 260
            },
            {
                "year": "2013",
                "id": 412
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Markov processes",
                "Mutual information",
                "Computer vision",
                "Face recognition",
                "Image recognition",
                "Approximation algorithms",
                "Standards"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "computer vision",
                "face recognition",
                "feature selection",
                "image classification",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structured feature selection",
                "computer vision task",
                "dimensionality reduction technique",
                "Markov blanket",
                "simultaneous Markov blanket discovery",
                "Bayes classification error",
                "large-scale image classification task",
                "facial expression",
                "action unit recognition"
            ]
        },
        "id": 475,
        "cited_by": []
    },
    {
        "title": "Conditional High-Order Boltzmann Machine: A Supervised Learning Model for Relation Learning",
        "authors": [
            "Yan Huang",
            "Wei Wang",
            "Liang Wang"
        ],
        "abstract": "Relation learning is a fundamental operation in many computer vision tasks. Recently, high-order Boltzmann machine and its variants have exhibited the great power of modelling various data relation. However, most of them are unsupervised learning models which are not very discriminative and thus cannot server as a standalone solution to relation learning tasks. In this paper, we explore supervised learning algorithms and propose a new model named Conditional High-order Boltzmann Machine (CHBM), which can be directly used as a bilinear classifier to assign similarity scores for pairwise images. Then, to better deal with complex data relation, we propose a gated version of CHBM which untangles factors of variation by exploiting a set of latent variables to gate classification. We perform four-order tensor factorization for parameter reduction, and present two efficient supervised learning algorithms from the perspectives of being generative and discriminative, respectively. The experimental results of image transformation visualization, binary-way classification and face verification demonstrate that, by performing supervised learning, our models can greatly improve the performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410842",
        "reference_list": [
            {
                "year": "2013",
                "id": 244
            },
            {
                "year": "2013",
                "id": 188
            },
            {
                "year": "2013",
                "id": 300
            },
            {
                "year": "2009",
                "id": 63
            }
        ],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Logic gates",
                "Computational modeling",
                "Tensile stress",
                "Data models",
                "Supervised learning",
                "Face",
                "Measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "Boltzmann machines",
                "computer vision",
                "face recognition",
                "image classification",
                "learning (artificial intelligence)",
                "matrix decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "conditional high-order Boltzmann machine",
                "supervised learning",
                "relation learning",
                "computer vision",
                "CHBM",
                "bilinear classifier",
                "pairwise image",
                "four-order tensor factorization",
                "parameter reduction",
                "image transformation visualization",
                "binary-way classification",
                "face verification"
            ]
        },
        "id": 476,
        "cited_by": []
    },
    {
        "title": "Learning Image and User Features for Recommendation in Social Networks",
        "authors": [
            "Xue Geng",
            "Hanwang Zhang",
            "Jingwen Bian",
            "Tat-Seng Chua"
        ],
        "abstract": "Good representations of data do help in many machine learning tasks such as recommendation. It is often a great challenge for traditional recommender systems to learn representative features of both users and images in large social networks, in particular, social curation networks, which are characterized as the extremely sparse links between users and images, and the extremely diverse visual contents of images. To address the challenges, we propose a novel deep model which learns the unified feature representations for both users and images. This is done by transforming the heterogeneous user-image networks into homogeneous low-dimensional representations, which facilitate a recommender to trivially recommend images to users by feature similarity. We also develop a fast online algorithm that can be easily scaled up to large networks in an asynchronously parallel way. We conduct extensive experiments on a representative subset of Pinterest, containing 1,456,540 images and 1,000,000 users. Results of image recommendation experiments demonstrate that our feature learning approach significantly outperforms other state-of-the-art recommendation methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410843",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 12,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Social network services",
                "Sparse matrices",
                "Collaboration",
                "Computer vision",
                "Recommender systems",
                "Machine learning",
                "Matrix decomposition"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "learning (artificial intelligence)",
                "recommender systems",
                "social networking (online)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "recommendation method",
                "feature learning approach",
                "image recommendation experiment",
                "online algorithm",
                "feature similarity",
                "homogeneous low-dimensional representation",
                "heterogeneous user-image network",
                "feature representation",
                "visual content",
                "sparse link",
                "social curation network",
                "representative feature",
                "recommender system",
                "machine learning task",
                "social network recommendation",
                "user feature",
                "learning image"
            ]
        },
        "id": 477,
        "cited_by": []
    },
    {
        "title": "Dual-Feature Warping-Based Motion Model Estimation",
        "authors": [
            "Shiwei Li",
            "Lu Yuan",
            "Jian Sun",
            "Long Quan"
        ],
        "abstract": "To break down the geometry assumptions of traditional motion models (e.g., homography, affine), warping-based motion model recently becomes very popular and is adopted in many latest applications (e.g., image stitching, video stabilization). With high degrees of freedom, the accuracy of model heavily relies on data-terms (keypoint correspondences). In some low-texture environments (e.g., indoor) where keypoint feature is insufficient or unreliable, the warping model is often erroneously estimated. In this paper we propose a simple and effective approach by considering both keypoint and line segment correspondences as data-term. Line segment is a prominent feature in artificial environments and it can supply sufficient geometrical and structural information of scenes, which not only helps guild to a correct warp in low-texture condition, but also prevents the undesired distortion induced by warping. The combination aims to complement each other and benefit for a wider range of scenes. Our method is general and can be ported to many existing applications. Experiments demonstrate that using dual-feature yields more robust and accurate result especially for those low-texture images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410844",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation",
                "Transforms",
                "Mathematical model",
                "Feature extraction",
                "Motion segmentation",
                "Euclidean distance"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "estimation theory",
                "image motion analysis",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "line segment correspondence",
                "keypoint correspondence",
                "low-texture image",
                "warping-based motion model",
                "geometry assumption",
                "motion model estimation",
                "dual-feature warping"
            ]
        },
        "id": 478,
        "cited_by": []
    },
    {
        "title": "An Adaptive Data Representation for Robust Point-Set Registration and Merging",
        "authors": [
            "Dylan Campbell",
            "Lars Petersson"
        ],
        "abstract": "This paper presents a framework for rigid point-set registration and merging using a robust continuous data representation. Our point-set representation is constructed by training a one-class support vector machine with a Gaussian radial basis function kernel and subsequently approximating the output function with a Gaussian mixture model. We leverage the representation's sparse parametrisation and robustness to noise, outliers and occlusions in an efficient registration algorithm that minimises the L2 distance between our support vector -- parametrised Gaussian mixtures. In contrast, existing techniques, such as Iterative Closest Point and Gaussian mixture approaches, manifest a narrower region of convergence and are less robust to occlusions and missing data, as demonstrated in the evaluation on a range of 2D and 3D datasets. Finally, we present a novel algorithm, GMMerge, that parsimoniously and equitably merges aligned mixture models, allowing the framework to be used for reconstruction and mapping.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410845",
        "reference_list": [
            {
                "year": "2007",
                "id": 237
            },
            {
                "year": "2003",
                "id": 61
            },
            {
                "year": "2013",
                "id": 181
            }
        ],
        "citation": {
            "ieee": 10,
            "other": 4,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Support vector machines",
                "Robustness",
                "Kernel",
                "Merging",
                "Training",
                "Gaussian mixture model"
            ],
            "INSPEC: Controlled Indexing": [
                "convergence",
                "data structures",
                "Gaussian processes",
                "mixture models",
                "radial basis function networks",
                "support vector machines"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "adaptive data representation",
                "rigid point-set registration",
                "robust continuous data representation",
                "point-set representation",
                "one-class support vector machine",
                "SVM",
                "Gaussian radial basis function kernel",
                "sparse parametrisation",
                "parametrised Gaussian mixtures",
                "iterative closest point approaches",
                "convergence",
                "GMMerge"
            ]
        },
        "id": 479,
        "cited_by": []
    },
    {
        "title": "Local Subspace Collaborative Tracking",
        "authors": [
            "Lin Ma",
            "Xiaoqin Zhang",
            "Weiming Hu",
            "Junliang Xing",
            "Jiwen Lu",
            "Jie Zhou"
        ],
        "abstract": "Subspace models have been widely used for appearance based object tracking. Most existing subspace based trackers employ a linear subspace to represent object appearances, which are not accurate enough to model large variations of objects. To address this, this paper presents a local subspace collaborative tracking method for robust visual tracking, where multiple linear and nonlinear subspaces are learned to better model the nonlinear relationship of object appearances. First, we retain a set of key samples and compute a set of local subspaces for each key sample. Then, we construct a hyper sphere to represent the local nonlinear subspace for each key sample. The hyper sphere of one key sample passes the local key samples and also is tangent to the local linear subspace of the specific key sample. In this way, we are able to represent the nonlinear distribution of the key samples and also approximate the local linear subspace near the specific key sample, so that local distributions of the samples can be represented more accurately. Experimental results on challenging video sequences demonstrate the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410846",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 81
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 3,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Visualization",
                "Collaboration",
                "Robustness",
                "Principal component analysis",
                "Adaptation models",
                "Object tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image sequences",
                "learning (artificial intelligence)",
                "object tracking",
                "statistical distributions",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local subspace collaborative tracking",
                "subspace model",
                "visual object tracking",
                "subspace learning",
                "nonlinear distribution",
                "local linear subspace approximation",
                "video sequence"
            ]
        },
        "id": 480,
        "cited_by": []
    },
    {
        "title": "Learning Spatially Regularized Correlation Filters for Visual Tracking",
        "authors": [
            "Martin Danelljan",
            "Gustav H\u00e4ger",
            "Fahad Shahbaz Khan",
            "Michael Felsberg"
        ],
        "abstract": "Robust and accurate visual tracking is one of the most challenging computer vision problems. Due to the inherent lack of training data, a robust approach for constructing a target appearance model is crucial. Recently, discriminatively learned correlation filters (DCF) have been successfully applied to address this problem for tracking. These methods utilize a periodic assumption of the training samples to efficiently learn a classifier on all patches in the target neighborhood. However, the periodic assumption also introduces unwanted boundary effects, which severely degrade the quality of the tracking model. We propose Spatially Regularized Discriminative Correlation Filters (SRDCF) for tracking. A spatial regularization component is introduced in the learning to penalize correlation filter coefficients depending on their spatial location. Our SRDCF formulation allows the correlation filters to be learned on a significantly larger set of negative training samples, without corrupting the positive samples. We further propose an optimization strategy, based on the iterative Gauss-Seidel method, for efficient online learning of our SRDCF. Experiments are performed on four benchmark datasets: OTB-2013, ALOV++, OTB-2015, and VOT2014. Our approach achieves state-of-the-art results on all four datasets. On OTB-2013 and OTB-2015, we obtain an absolute gain of 8.0% and 8.2% respectively, in mean overlap precision, compared to the best existing trackers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410847",
        "reference_list": [
            {
                "year": "2013",
                "id": 383
            },
            {
                "year": "2011",
                "id": 33
            },
            {
                "year": "2013",
                "id": 344
            }
        ],
        "citation": {
            "ieee": 291,
            "other": 177,
            "total": 468
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Correlation",
                "Standards",
                "Target tracking",
                "Discrete Fourier transforms",
                "Visualization",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "filtering theory",
                "iterative methods",
                "learning (artificial intelligence)",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning spatially regularized correlation filter",
                "robust visual tracking",
                "accurate visual tracking",
                "computer vision problem",
                "training data",
                "target appearance model",
                "learned correlation filter",
                "periodic assumption",
                "unwanted boundary effect",
                "tracking model",
                "spatially regularized discriminative correlation filter",
                "spatial regularization component",
                "correlation filter coefficient",
                "spatial location",
                "SRDCF formulation",
                "optimization strategy",
                "iterative Gauss-Seidel method",
                "online learning",
                "OTB-2013",
                "ALOV++",
                "OTB-2015",
                "VOT2014",
                "mean overlap precision"
            ]
        },
        "id": 481,
        "cited_by": [
            {
                "year": "2017",
                "id": 11
            },
            {
                "year": "2017",
                "id": 33
            },
            {
                "year": "2017",
                "id": 118
            },
            {
                "year": "2017",
                "id": 119
            },
            {
                "year": "2017",
                "id": 186
            },
            {
                "year": "2017",
                "id": 269
            },
            {
                "year": "2017",
                "id": 350
            },
            {
                "year": "2017",
                "id": 508
            },
            {
                "year": "2017",
                "id": 575
            }
        ]
    },
    {
        "title": "SpeDo: 6 DOF Ego-Motion Sensor Using Speckle Defocus Imaging",
        "authors": [
            "Kensei Jo",
            "Mohit Gupta",
            "Shree K. Nayar"
        ],
        "abstract": "Sensors that measure their motion with respect to the surrounding environment (ego-motion sensors) can be broadly classified into two categories. First is inertial sensors such as accelerometers. In order to estimate position and velocity, these sensors integrate the measured acceleration, which often results in accumulation of large errors over time. Second, camera-based approaches such as SLAM that can measure position directly, but their performance depends on the surrounding scene's properties. These approaches cannot function reliably if the scene has low frequency textures or small depth variations. We present a novel ego-motion sensor called SpeDo that addresses these fundamental limitations. SpeDo is based on using coherent light sources and cameras with large defocus. Coherent light, on interacting with a scene, creates a high frequency interferometric pattern in the captured images, called speckle. We develop a theoretical model for speckle flow (motion of speckle as a function of sensor motion), and show that it is quasi-invariant to surrounding scene's properties. As a result, SpeDo can measure ego-motion (not derivative of motion) simply by estimating optical flow at a few image locations. We have built a low-cost and compact hardware prototype of SpeDo and demonstrated high precision 6 DOF ego-motion estimation for complex trajectories in scenarios where the scene properties are challenging (e.g., repeating or no texture) as well as unknown.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410848",
        "reference_list": [
            {
                "year": "2013",
                "id": 67
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Speckle",
                "Cameras",
                "Robot sensing systems",
                "Motion measurement",
                "Light sources",
                "Optical variables measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "accelerometers",
                "cameras",
                "image capture",
                "image motion analysis",
                "image sensors",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "SpeDo",
                "DOF",
                "ego-motion sensor",
                "speckle defocus imaging",
                "inertial sensor",
                "accelerometer",
                "position estimation",
                "velocity estimation",
                "camera-based approach",
                "SLAM",
                "position measurement",
                "scene property",
                "coherent light source",
                "high frequency interferometric pattern",
                "image capture",
                "optical flow",
                "ego-motion estimation"
            ]
        },
        "id": 482,
        "cited_by": []
    },
    {
        "title": "Unsupervised Trajectory Clustering via Adaptive Multi-kernel-Based Shrinkage",
        "authors": [
            "Hongteng Xu",
            "Yang Zhou",
            "Weiyao Lin",
            "Hongyuan Zha"
        ],
        "abstract": "This paper proposes a shrinkage-based framework for unsupervised trajectory clustering. Facing to the challenges of trajectory clustering, e.g., large variations within a cluster and ambiguities across clusters, we first introduce an adaptive multi-kernel-based estimation process to estimate the 'shrunk' positions and speeds of trajectories' points. This kernel-based estimation effectively leverages both multiple structural information within a trajectory and the local motion patterns across multiple trajectories, such that the discrimination of the shrunk point can be properly increased. We further introduce a speed-regularized optimization process, which utilizes the estimated speeds to regularize the optimal shrunk points, so as to guarantee the smoothness and the discriminative pattern of the final shrunk trajectory. Using our approach, the variations among similar trajectories can be reduced while the boundaries between different clusters are enlarged. Experimental results demonstrate that our approach is superior to the state-of-art approaches on both clustering accuracy and robustness. Besides, additional experiments further reveal the effectiveness of our approach when applied to trajectory analysis applications such as anomaly detection and route analysis.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410849",
        "reference_list": [
            {
                "year": "2011",
                "id": 147
            },
            {
                "year": "2013",
                "id": 385
            },
            {
                "year": "2013",
                "id": 443
            },
            {
                "year": "2013",
                "id": 275
            }
        ],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Kernel",
                "Hidden Markov models",
                "Estimation",
                "Manifolds",
                "Clustering algorithms",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "estimation theory",
                "image motion analysis",
                "optimisation",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised trajectory clustering",
                "adaptive multikernel-based shrinkage",
                "kernel-based estimation",
                "motion pattern",
                "shrunk point",
                "speed-regularized optimization"
            ]
        },
        "id": 483,
        "cited_by": []
    },
    {
        "title": "TRIC-track: Tracking by Regression with Incrementally Learned Cascades",
        "authors": [
            "Xiaomeng Wang",
            "Michel Valstar",
            "Brais Martinez",
            "Muhammad Haris Khan",
            "Tony Pridmore"
        ],
        "abstract": "This paper proposes a novel approach to part-based tracking by replacing local matching of an appearance model by direct prediction of the displacement between local image patches and part locations. We propose to use cascaded regression with incremental learning to track generic objects without any prior knowledge of an object's structure or appearance. We exploit the spatial constraints between parts by implicitly learning the shape and deformation parameters of the object in an online fashion. We integrate a multiple temporal scale motion model to initialise our cascaded regression search close to the target and to allow it to cope with occlusions. Experimental results show that our tracker ranks first on the CVPR 2013 Benchmark.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410850",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 10,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Shape",
                "Adaptation models",
                "Predictive models",
                "Visualization",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "learning (artificial intelligence)",
                "object tracking",
                "regression analysis",
                "shape recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "TRIC-track",
                "cascaded regression",
                "incremental learning",
                "part-based tracking",
                "local matching",
                "appearance model",
                "object tracking",
                "spatial constraint",
                "shape parameter",
                "deformation parameter",
                "multiple temporal scale motion model"
            ]
        },
        "id": 484,
        "cited_by": []
    },
    {
        "title": "Recurrent Network Models for Human Dynamics",
        "authors": [
            "Katerina Fragkiadaki",
            "Sergey Levine",
            "Panna Felsen",
            "Jitendra Malik"
        ],
        "abstract": "We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and prediction of human body pose in videos and motion capture. The ERD model is a recurrent neural network that incorporates nonlinear encoder and decoder networks before and after recurrent layers. We test instantiations of ERD architectures in the tasks of motion capture (mocap) generation, body pose labeling and body pose forecasting in videos. Our model handles mocap training data across multiple subjects and activity domains, and synthesizes novel motions while avoiding drifting for long periods of time. For human pose labeling, ERD outperforms a per frame body part detector by resolving left-right body part confusions. For video pose forecasting, ERD predicts body joint displacements across a temporal horizon of 400ms and outperforms a first order motion model based on optical flow. ERDs extend previous Long Short Term Memory (LSTM) models in the literature to jointly learn representations and their dynamics. Our experiments show such representation learning is crucial for both labeling and prediction in space-time. We find this is a distinguishing feature between the spatio-temporal visual domain in comparison to 1D text, speech or handwriting, where straightforward hard coded representations have shown excellent results when directly combined with recurrent units [31].",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410851",
        "reference_list": [
            {
                "year": "2011",
                "id": 78
            },
            {
                "year": "2011",
                "id": 334
            },
            {
                "year": "2009",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 48,
            "other": 39,
            "total": 87
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Hidden Markov models",
                "Predictive models",
                "Forecasting",
                "Labeling",
                "Visualization",
                "Decoding"
            ],
            "INSPEC: Controlled Indexing": [
                "data handling",
                "decoding",
                "image motion analysis",
                "image sequences",
                "learning (artificial intelligence)",
                "pose estimation",
                "recurrent neural nets",
                "spatiotemporal phenomena",
                "video coding"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "recurrent network models",
                "human dynamics",
                "encoder-recurrent-decoder model",
                "ERD model",
                "human body pose prediction",
                "human body pose recognition",
                "recurrent neural network",
                "nonlinear encoder-decoder networks",
                "ERD architectures",
                "motion capture generation",
                "mocap generation",
                "body pose labeling",
                "body pose forecasting",
                "mocap training data handling",
                "motion synthesis",
                "human pose labeling",
                "body part detector",
                "video pose forecasting",
                "temporal horizon",
                "first order motion model",
                "optical flow",
                "long short term memory models",
                "LSTM models",
                "representation learning",
                "space-time labeling",
                "spatiotemporal visual domain"
            ]
        },
        "id": 485,
        "cited_by": []
    },
    {
        "title": "Contour Flow: Middle-Level Motion Estimation by Combining Motion Segmentation and Contour Alignment",
        "authors": [
            "Huijun Di",
            "Qingxuan Shi",
            "Feng Lv",
            "Ming Qin",
            "Yao Lu"
        ],
        "abstract": "Our goal is to estimate contour flow (the contour pairs with consistent point correspondence) from inconsistent contours extracted independently in two video frames. We formulate the contour flow estimation locally as a motion segmentation problem where motion patterns grouped from optical flow field are exploited for local correspondence measurement. To solve local ambiguities, contour flow estimation is further formulated globally as a contour alignment problem. We propose a novel two-staged strategy to obtain global consistent point correspondence under various contour transitions such as splitting, merging and branching. The goal of the first stage is to obtain possible accurate contour-to-contour alignments, and the second stage aims to make a consistent fusion of many partial alignments. Such a strategy can properly balance the accuracy and the consistency, which enables a middle-level motion representation to be constructed by just concatenating frame-by-frame contour flow estimation. Experiments prove the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410852",
        "reference_list": [
            {
                "year": "2013",
                "id": 413
            },
            {
                "year": "2007",
                "id": 197
            },
            {
                "year": "2011",
                "id": 253
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Motion segmentation",
                "Image motion analysis",
                "Estimation",
                "Image edge detection",
                "Optical imaging",
                "Motion measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image segmentation",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "middle-level motion estimation",
                "motion segmentation",
                "contour alignment",
                "video frames",
                "contour flow estimation",
                "motion patterns",
                "optical flow field",
                "global consistent point correspondence",
                "contour transitions",
                "splitting",
                "merging",
                "branching",
                "contour-to-contour alignments",
                "middle-level motion representation",
                "frame-by-frame contour flow estimation"
            ]
        },
        "id": 486,
        "cited_by": []
    },
    {
        "title": "FollowMe: Efficient Online Min-Cost Flow Tracking with Bounded Memory and Computation",
        "authors": [
            "Philip Lenz",
            "Andreas Geiger",
            "Raquel Urtasun"
        ],
        "abstract": "One of the most popular approaches to multi-target tracking is tracking-by-detection. Current min-cost flow algorithms which solve the data association problem optimally have three main drawbacks: they are computationally expensive, they assume that the whole video is given as a batch, and they scale badly in memory and computation with the length of the video sequence. In this paper, we address each of these issues, resulting in a computationally and memory-bounded solution. First, we introduce a dynamic version of the successive shortest-path algorithm which solves the data association problem optimally while reusing computation, resulting in faster inference than standard solvers. Second, we address the optimal solution to the data association problem when dealing with an incoming stream of data (i.e., online setting). Finally, we present our main contribution which is an approximate online solution with bounded memory and computation which is capable of handling videos of arbitrary length while performing tracking in real time. We demonstrate the effectiveness of our algorithms on the KITTI and PETS2009 benchmarks and show state-of-the-art performance, while being significantly faster than existing solvers.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410853",
        "reference_list": [
            {
                "year": "2013",
                "id": 22
            },
            {
                "year": "2013",
                "id": 420
            },
            {
                "year": "2013",
                "id": 287
            },
            {
                "year": "2013",
                "id": 2
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 8,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Trajectory",
                "Heuristic algorithms",
                "Optimization",
                "Target tracking",
                "Joining processes",
                "Image edge detection",
                "Benchmark testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "sensor fusion",
                "target tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "FollowMe",
                "video sequence",
                "multitarget tracking",
                "data association problem",
                "successive shortest-path algorithm",
                "online min-cost flow tracking"
            ]
        },
        "id": 487,
        "cited_by": [
            {
                "year": "2017",
                "id": 268
            }
        ]
    },
    {
        "title": "Learning to Divide and Conquer for Online Multi-target Tracking",
        "authors": [
            "Francesco Solera",
            "Simone Calderara",
            "Rita Cucchiara"
        ],
        "abstract": "Online Multiple Target Tracking (MTT) is often addressed within the tracking-by-detection paradigm. Detections are previously extracted independently in each frame and then objects trajectories are built by maximizing specifically designed coherence functions. Nevertheless, ambiguities arise in presence of occlusions or detection errors. In this paper we claim that the ambiguities in tracking could be solved by a selective use of the features, by working with more reliable features if possible and exploiting a deeper representation of the target only if necessary. To this end, we propose an online divide and conquer tracker for static camera scenes, which partitions the assignment problem in local subproblems and solves them by selectively choosing and combining the best features. The complete framework is cast as a structural learning task that unifies these phases and learns tracker parameters from examples. Experiments on two different datasets highlights a significant improvement of tracking performances (MOTA +10%) over the state of the art.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410854",
        "reference_list": [
            {
                "year": "2013",
                "id": 287
            }
        ],
        "citation": {
            "ieee": 21,
            "other": 8,
            "total": 29
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Trajectory",
                "Computational modeling",
                "Feature extraction",
                "Coherence",
                "Correlation"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "divide and conquer methods",
                "feature selection",
                "image representation",
                "learning (artificial intelligence)",
                "object detection",
                "target tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "online multitarget tracking",
                "MTT",
                "tracking-by-detection paradigm",
                "objects trajectories",
                "coherence functions",
                "occlusions",
                "detection errors",
                "target representation",
                "online divide and conquer tracker",
                "static camera scenes",
                "assignment problem",
                "feature selection",
                "structural learning task",
                "tracker parameters"
            ]
        },
        "id": 488,
        "cited_by": []
    },
    {
        "title": "Minimizing Human Effort in Interactive Tracking by Incremental Learning of Model Parameters",
        "authors": [
            "Arridhana Ciptadi",
            "James M. Rehg"
        ],
        "abstract": "We address the problem of minimizing human effort in interactive tracking by learning sequence-specific model parameters. Determining the optimal model parameters for each sequence is a critical problem in tracking. We demonstrate that by using the optimal model parameters for each sequence we can achieve high precision tracking results with significantly less effort. We leverage the sequential nature of interactive tracking to formulate an efficient method for learning model parameters through a maximum margin framework. By using our method we are able to save ~60 -- 90% of human effort to achieve high precision on two datasets: the VIRAT dataset and an Infant-Mother Interaction dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410855",
        "reference_list": [
            {
                "year": "2007",
                "id": 110
            },
            {
                "year": "2009",
                "id": 186
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cost function",
                "Trajectory",
                "Interpolation",
                "Histograms",
                "Computational modeling",
                "Object tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interactive tracking",
                "human effort minimization",
                "incremental learning",
                "optimal model parameters",
                "learning model parameters",
                "VIRAT dataset",
                "infant-mother interaction dataset"
            ]
        },
        "id": 489,
        "cited_by": [
            {
                "year": "2017",
                "id": 30
            }
        ]
    },
    {
        "title": "A Novel Representation of Parts for Accurate 3D Object Detection and Tracking in Monocular Images",
        "authors": [
            "Alberto Crivellaro",
            "Mahdi Rad",
            "Yannick Verdie",
            "Kwang Moo Yi",
            "Pascal Fua",
            "Vincent Lepetit"
        ],
        "abstract": "We present a method that estimates in real-time and under challenging conditions the 3D pose of a known object. Our method relies only on grayscale images since depth cameras fail on metallic objects, it can handle poorly textured objects, and cluttered, changing environments, the pose it predicts degrades gracefully in presence of large occlusions. As a result, by contrast with the state-of-the-art, our method is suitable for practical Augmented Reality applications even in industrial environments. To be robust to occlusions, we first learn to detect some parts of the target object. Our key idea is to then predict the 3D pose of each part in the form of the 2D projections of a few control points. The advantages of this representation is three-fold: We can predict the 3D pose of the object even when only one part is visible, when several parts are visible, we can combine them easily to compute a better pose of the object, the 3D pose we obtain is usually very accurate, even when only few parts are visible.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410856",
        "reference_list": [
            {
                "year": "2007",
                "id": 287
            },
            {
                "year": "2011",
                "id": 295
            },
            {
                "year": "2011",
                "id": 124
            },
            {
                "year": "2013",
                "id": 255
            },
            {
                "year": "2005",
                "id": 196
            },
            {
                "year": "2013",
                "id": 217
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 8,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cameras",
                "Robustness",
                "Training",
                "Object detection",
                "Augmented reality",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "augmented reality",
                "object detection",
                "object tracking",
                "pose estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object detection",
                "object tracking",
                "monocular images",
                "grayscale image",
                "augmented reality",
                "occlusions",
                "3D pose estimation"
            ]
        },
        "id": 490,
        "cited_by": [
            {
                "year": "2017",
                "id": 403
            },
            {
                "year": "2017",
                "id": 406
            }
        ]
    },
    {
        "title": "Linearization to Nonlinear Learning for Visual Tracking",
        "authors": [
            "Bo Ma",
            "Hongwei Hu",
            "Jianbing Shen",
            "Yuping Zhang",
            "Fatih Porikli"
        ],
        "abstract": "Due to unavoidable appearance variations caused by occlusion, deformation, and other factors, classifiers for visual tracking are nonlinear as a necessity. Building on the theory of globally linear approximations to nonlinear functions, we introduce an elegant method that jointly learns a nonlinear classifier and a visual dictionary for tracking objects in a semi-supervised sparse coding fashion. This establishes an obvious distinction from conventional sparse coding based discriminative tracking algorithms that usually maintain two-stage learning strategies, i.e., learning a dictionary in an unsupervised way then followed by training a classifier. However, the treating dictionary learning and classifier training as separate stages may not produce both descriptive and discriminative models for objects. By contrast, our method is capable of constructing a dictionary that not only fully reflects the intrinsic manifold structure of the data, but also possesses discriminative power. This paper presents an optimization method to obtain such an optimal dictionary, associated sparse coding, and a classifier in an iterative process. Our experiments on a benchmark show our tracker attains outstanding performance compared with the state-of-the-art algorithms.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410857",
        "reference_list": [
            {
                "year": "2011",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 4,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Encoding",
                "Visualization",
                "Target tracking",
                "Manifolds",
                "Linear programming",
                "Optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "approximation theory",
                "image classification",
                "object tracking",
                "optimisation",
                "unsupervised learning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "linear approximation",
                "nonlinear classifier learning",
                "visual object tracking",
                "visual dictionary",
                "unsupervised learning",
                "optimization method"
            ]
        },
        "id": 491,
        "cited_by": []
    },
    {
        "title": "Self-Occlusions and Disocclusions in Causal Video Object Segmentation",
        "authors": [
            "Yanchao Yang",
            "Ganesh Sundaramoorthi",
            "Stefano Soatto"
        ],
        "abstract": "We propose a method to detect disocclusion in video sequences of three-dimensional scenes and to partition the disoccluded regions into objects, defined by coherent deformation corresponding to surfaces in the scene. Our method infers deformation fields that are piecewise smooth by construction without the need for an explicit regularizer and the associated choice of weight. It then partitions the disoccluded region and groups its components with objects by leveraging on the complementarity of motion and appearance cues: Where appearance changes within an object, motion can usually be reliably inferred and used for grouping. Where appearance is close to constant, it can be used for grouping directly. We integrate both cues in an energy minimization framework, incorporate prior assumptions explicitly into the energy, and propose a numerical scheme.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410858",
        "reference_list": [
            {
                "year": "2013",
                "id": 20
            },
            {
                "year": "2011",
                "id": 253
            },
            {
                "year": "2013",
                "id": 273
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Three-dimensional displays",
                "Tracking",
                "Computer vision",
                "Reliability",
                "Optical imaging",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image segmentation",
                "image sequences",
                "smoothing methods",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "self-occlusion",
                "causal video object segmentation",
                "disocclusion detection",
                "video sequences",
                "three-dimensional scenes",
                "coherent deformation",
                "piecewise smoothing",
                "energy minimization framework",
                "motion cues",
                "appearance cues"
            ]
        },
        "id": 492,
        "cited_by": [
            {
                "year": "2017",
                "id": 379
            }
        ]
    },
    {
        "title": "Large Displacement 3D Scene Flow with Occlusion Reasoning",
        "authors": [
            "Andrei Zanfir",
            "Cristian Sminchisescu"
        ],
        "abstract": "The emergence of modern, affordable and accurate RGB-D sensors increases the need for single view approaches to estimate 3-dimensional motion, also known as scene flow. In this paper we propose a coarse-to-fine, dense, correspondence-based scene flow formulation that relies on explicit geometric reasoning to account for the effects of large displacements and to model occlusion. Our methodology enforces local motion rigidity at the level of the 3d point cloud without explicitly smoothing the parameters of adjacent neighborhoods. By integrating all geometric and photometric components in a single, consistent, occlusion-aware energy model, defined over overlapping, image-adaptive neighborhoods, our method can process fast motions and large occlusions areas, as present in challenging datasets like the MPI Sintel Flow Dataset, recently augmented with depth information. By explicitly modeling large displacements and occlusion, we can handle difficult sequences which cannot be currently processed by state of the art scene flow methods. We also show that by integrating depth information into the model, we can obtain correspondence fields with improved spatial support and sharper boundaries compared to the state of the art, large-displacement optical flow methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410859",
        "reference_list": [
            {
                "year": "2011",
                "id": 291
            },
            {
                "year": "2007",
                "id": 160
            },
            {
                "year": "2013",
                "id": 214
            },
            {
                "year": "2011",
                "id": 163
            },
            {
                "year": "2013",
                "id": 171
            },
            {
                "year": "2013",
                "id": 172
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Cognition",
                "Optical imaging",
                "Cameras",
                "Optical sensors",
                "Adaptive optics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "image colour analysis",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "displacement 3D scene flow",
                "occlusion reasoning",
                "RGB-D sensor",
                "3-dimensional motion",
                "correspondence-based scene flow formulation",
                "geometric reasoning",
                "local motion rigidity",
                "3D point cloud",
                "parameter smoothing",
                "adjacent neighborhood",
                "geometric component",
                "photometric component",
                "occlusion-aware energy model",
                "defined over overlapping",
                "image-adaptive neighborhood",
                "MPI sintel flow dataset",
                "2depth information",
                "scene flow method",
                "spatial support",
                "sharper boundary",
                "large-displacement optical flow method"
            ]
        },
        "id": 493,
        "cited_by": []
    },
    {
        "title": "Co-Interest Person Detection from Multiple Wearable Camera Videos",
        "authors": [
            "Yuewei Lin",
            "Kareem Abdelfatah",
            "Youjie Zhou",
            "Xiaochuan Fan",
            "Hongkai Yu",
            "Hui Qian",
            "Song Wang"
        ],
        "abstract": "Wearable cameras, such as Google Glass and Go Pro, enable video data collection over larger areas and from different views. In this paper, we tackle a new problem of locating the co-interest person (CIP), i.e., the one who draws attention from most camera wearers, from temporally synchronized videos taken by multiple wearable cameras. Our basic idea is to exploit the motion patterns of people and use them to correlate the persons across different videos, instead of performing appearance-based matching as in traditional video co-segmentation/localization. This way, we can identify CIP even if a group of people with similar appearance are present in the view. More specifically, we detect a set of persons on each frame as the candidates of the CIP and then build a Conditional Random Field (CRF) model to select the one with consistent motion patterns in different videos and high spacial-temporal consistency in each video. We collect three sets of wearable-camera videos for testing the proposed algorithm. All the involved people have similar appearances in the collected videos and the experiments demonstrate the effectiveness of the proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410860",
        "reference_list": [
            {
                "year": "2013",
                "id": 162
            },
            {
                "year": "2013",
                "id": 278
            },
            {
                "year": "2011",
                "id": 21
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Cameras",
                "Synchronization",
                "Three-dimensional displays",
                "Feature extraction",
                "Tracking",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image motion analysis",
                "image segmentation",
                "object detection",
                "random processes",
                "synchronisation",
                "video cameras",
                "wearable computers"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "co-interest person detection",
                "multiple wearable camera video",
                "Google Glass",
                "Go Pro",
                "video data collection",
                "CIP",
                "camera wearer",
                "synchronized video",
                "motion pattern",
                "appearance-based matching",
                "video cosegmentation",
                "video localization",
                "conditional random field model",
                "CRF model",
                "spacial-temporal consistency"
            ]
        },
        "id": 494,
        "cited_by": [
            {
                "year": "2017",
                "id": 301
            }
        ]
    },
    {
        "title": "Sparse Dynamic 3D Reconstruction from Unsynchronized Videos",
        "authors": [
            "Enliang Zheng",
            "Dinghuang Ji",
            "Enrique Dunn",
            "Jan-Michael Frahm"
        ],
        "abstract": "We target the sparse 3D reconstruction of dynamic objects observed by multiple unsynchronized video cameras with unknown temporal overlap. To this end, we develop a framework to recover the unknown structure without sequencing information across video sequences. Our proposed compressed sensing framework poses the estimation of 3D structure as the problem of dictionary learning. Moreover, we define our dictionary as the temporally varying 3D structure, while we define local sequencing information in terms of the sparse coefficients describing a locally linear 3D structural interpolation. Our formulation optimizes a biconvex cost function that leverages a compressed sensing formulation and enforces both structural dependency coherence across video streams, as well as motion smoothness across estimates from common video sources. Experimental results demonstrate the effectiveness of our approach in both synthetic data and captured imagery.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410861",
        "reference_list": [
            {
                "year": "2013",
                "id": 121
            },
            {
                "year": "2011",
                "id": 25
            }
        ],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Videos",
                "Cameras",
                "Sequential analysis",
                "Image reconstruction",
                "Trajectory",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "compressed sensing",
                "image reconstruction",
                "image sequences",
                "motion estimation",
                "sparse matrices",
                "video cameras",
                "video signal processing",
                "video streaming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sparse dynamic 3D reconstruction",
                "unsynchronized videos",
                "dynamic objects",
                "unsynchronized video cameras",
                "temporal overlap",
                "sequencing information",
                "video sequences",
                "compressed sensing framework",
                "3D structure estimation",
                "dictionary learning",
                "local sequencing information",
                "sparse coefficients",
                "locally linear 3D structural interpolation",
                "biconvex cost function",
                "structural dependency coherence",
                "video streams",
                "motion smoothness",
                "video sources",
                "captured imagery",
                "synthetic data"
            ]
        },
        "id": 495,
        "cited_by": []
    },
    {
        "title": "Category-Blind Human Action Recognition: A Practical Recognition System",
        "authors": [
            "Wenbo Li",
            "Longyin Wen",
            "Mooi Choo Chuah",
            "Siwei Lyu"
        ],
        "abstract": "Existing human action recognition systems for 3D sequences obtained from the depth camera are designed to cope with only one action category, either single-person action or two-person interaction, and are difficult to be extended to scenarios where both action categories co-exist. In this paper, we propose the category-blind human recognition method (CHARM) which can recognize a human action without making assumptions of the action category. In our CHARM approach, we represent a human action (either a single-person action or a two-person interaction) class using a co-occurrence of motion primitives. Subsequently, we classify an action instance based on matching its motion primitive co-occurrence patterns to each class representation. The matching task is formulated as maximum clique problems. We conduct extensive evaluations of CHARM using three datasets for single-person actions, two-person interactions, and their mixtures. Experimental results show that CHARM performs favorably when compared with several state-of-the-art single-person action and two-person interaction based methods without making explicit assumptions of action category.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410862",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 6,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Visualization",
                "Feature extraction",
                "Reliability",
                "Thigh",
                "Streaming media",
                "Real-time systems"
            ],
            "INSPEC: Controlled Indexing": [
                "pattern matching",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "category-blind human action recognition",
                "CHARM approach",
                "practical recognition system",
                "3D sequences",
                "depth camera",
                "single-person action",
                "two-person interaction",
                "action instance classification",
                "motion primitive cooccurrence pattern matching",
                "maximum clique problems"
            ]
        },
        "id": 496,
        "cited_by": [
            {
                "year": "2017",
                "id": 151
            }
        ]
    },
    {
        "title": "Temporal Subspace Clustering for Human Motion Segmentation",
        "authors": [
            "Sheng Li",
            "Kang Li",
            "Yun Fu"
        ],
        "abstract": "Subspace clustering is an effective technique for segmenting data drawn from multiple subspaces. However, for time series data (e.g., human motion), exploiting temporal information is still a challenging problem. We propose a novel temporal subspace clustering (TSC) approach in this paper. We improve the subspace clustering technique from two aspects. First, a temporal Laplacian regularization is designed, which encodes the sequential relationships in time series data. Second, to obtain expressive codings, we learn a non-negative dictionary from data. An efficient optimization algorithm is presented to jointly learn the representation codings and dictionary. After constructing an affinity graph using the codings, multiple temporal segments can be grouped via spectral clustering. Experimental results on three action and gesture datasets demonstrate the effectiveness of our approach. In particular, TSC significantly improves the clustering accuracy, compared to the state-of-the-art subspace clustering methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410863",
        "reference_list": [
            {
                "year": "2007",
                "id": 122
            },
            {
                "year": "2013",
                "id": 442
            }
        ],
        "citation": {
            "ieee": 14,
            "other": 10,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Encoding",
                "Laplace equations",
                "Time series analysis",
                "Clustering methods",
                "Computer vision",
                "Motion segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "graph theory",
                "image coding",
                "image motion analysis",
                "image representation",
                "image segmentation",
                "Laplace equations",
                "optimisation",
                "pattern clustering",
                "time series"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporal subspace clustering",
                "human motion segmentation",
                "time series data",
                "temporal information",
                "TSC approach",
                "temporal Laplacian regularization",
                "nonnegative dictionary",
                "optimization algorithm",
                "representation codings",
                "affinity graph",
                "spectral clustering",
                "gesture datasets",
                "action datasets",
                "clustering accuracy improvement"
            ]
        },
        "id": 497,
        "cited_by": []
    },
    {
        "title": "Weakly-Supervised Alignment of Video with Text",
        "authors": [
            "P. Bojanowski",
            "R. Lajugie",
            "E. Grave",
            "F. Bach",
            "I. Laptev",
            "J. Ponce",
            "C. Schmid"
        ],
        "abstract": "Suppose that we are given a set of videos, along with natural language descriptions in the form of multiple sentences (e.g., manual annotations, movie scripts, sport summaries etc.), and that these sentences appear in the same temporal order as their visual counterparts. We propose in this paper a method for aligning the two modalities, i.e., automatically providing a time (frame) stamp for every sentence. Given vectorial features for both video and text, this can be cast as a temporal assignment problem, with an implicit linear mapping between the two feature modalities. We formulate this problem as an integer quadratic program, and solve its continuous convex relaxation using an efficient conditional gradient algorithm. Several rounding procedures are proposed to construct the final integer solution. After demonstrating significant improvements over the state of the art on the related task of aligning video with symbolic labels [7], we evaluate our method on a challenging dataset of videos with associated textual descriptions [37], and explore bag-of-words and continuous representations for text.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410864",
        "reference_list": [
            {
                "year": "2013",
                "id": 168
            },
            {
                "year": "2013",
                "id": 284
            },
            {
                "year": "2013",
                "id": 54
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 8,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Data models",
                "Hidden Markov models",
                "Natural languages",
                "Manuals",
                "Speech",
                "Optimization methods",
                "Minimization"
            ],
            "INSPEC: Controlled Indexing": [
                "gradient methods",
                "integer programming",
                "quadratic programming",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "weakly-supervised alignment",
                "video with text",
                "vectorial feature",
                "temporal assignment problem",
                "implicit linear mapping",
                "integer quadratic program",
                "continuous convex relaxation",
                "conditional gradient algorithm",
                "symbolic label"
            ]
        },
        "id": 498,
        "cited_by": [
            {
                "year": "2017",
                "id": 224
            },
            {
                "year": "2017",
                "id": 552
            },
            {
                "year": "2017",
                "id": 553
            }
        ]
    },
    {
        "title": "Learning Temporal Embeddings for Complex Video Analysis",
        "authors": [
            "Vignesh Ramanathan",
            "Kevin Tang",
            "Greg Mori",
            "Li Fei-Fei"
        ],
        "abstract": "In this paper, we propose to learn temporal embeddings of video frames for complex video analysis. Large quantities of unlabeled video data can be easily obtained from the Internet. These videos possess the implicit weak label that they are sequences of temporally and semantically coherent images. We leverage this information to learn temporal embeddings for video frames by associating frames with the temporal context that they appear in. To do this, we propose a scheme for incorporating temporal context based on past and future frames in videos, and compare this to other contextual representations. In addition, we show how data augmentation using multi-resolution samples and hard negatives helps to significantly improve the quality of the learned embeddings. We evaluate various design decisions for learning temporal embeddings, and show that our embeddings can improve performance for multiple video tasks such as retrieval, classification, and temporal order recovery in unconstrained Internet video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410865",
        "reference_list": [
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2013",
                "id": 112
            }
        ],
        "citation": {
            "ieee": 16,
            "other": 10,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Context",
                "Semantics",
                "Visualization",
                "Context modeling",
                "Coherence",
                "Computer vision",
                "Internet"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporal embedding",
                "complex video analysis",
                "data augmentation",
                "multiresolution sample"
            ]
        },
        "id": 499,
        "cited_by": [
            {
                "year": "2017",
                "id": 384
            }
        ]
    },
    {
        "title": "Unsupervised Semantic Parsing of Video Collections",
        "authors": [
            "Ozan Sener",
            "Amir R. Zamir",
            "Silvio Savarese",
            "Ashutosh Saxena"
        ],
        "abstract": "Human communication typically has an underlying structure. This is reflected in the fact that in many user generated videos, a starting point, ending, and certain objective steps between these two can be identified. In this paper, we propose a method for parsing a video into such semantic steps in an unsupervised way. The proposed method is capable of providing a semantic \"storyline\" of the video composed of its objective steps. We accomplish this utilizing both visual and language cues in a joint generative model. The proposed method can also provide a textual description for each of identified semantic steps and video segments. We evaluate this method on a large number of complex YouTube videos and show results of unprecedented quality for this new and impactful problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410866",
        "reference_list": [
            {
                "year": "2009",
                "id": 191
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2011",
                "id": 253
            },
            {
                "year": "2009",
                "id": 204
            }
        ],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Proposals",
                "Semantics",
                "YouTube",
                "Atomic measurements",
                "Conferences",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "natural language processing",
                "text analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised semantic parsing",
                "language cues",
                "visual cues",
                "joint generative model",
                "textual description",
                "YouTube videos"
            ]
        },
        "id": 500,
        "cited_by": [
            {
                "year": "2017",
                "id": 224
            },
            {
                "year": "2017",
                "id": 608
            }
        ]
    },
    {
        "title": "Learning Spatiotemporal Features with 3D Convolutional Networks",
        "authors": [
            "Du Tran",
            "Lubomir Bourdev",
            "Rob Fergus",
            "Lorenzo Torresani",
            "Manohar Paluri"
        ],
        "abstract": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410867",
        "reference_list": [
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 697,
            "other": 396,
            "total": 1093
        },
        "keywords": {
            "IEEE Keywords": [
                "Three-dimensional displays",
                "Convolution",
                "Kernel",
                "Feature extraction",
                "Solid modeling",
                "Streaming media",
                "Training"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image classification",
                "learning (artificial intelligence)",
                "neural nets",
                "spatiotemporal phenomena",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatiotemporal feature learning",
                "3D convolutional networks",
                "deep 3-dimensional convolutional networks",
                "3D ConvNets",
                "large scale supervised video dataset",
                "homogeneous architecture",
                "convolution kernels",
                "C3D features",
                "convolutional 3D",
                "linear classifier",
                "UCF101 dataset"
            ]
        },
        "id": 501,
        "cited_by": [
            {
                "year": "2017",
                "id": 10
            },
            {
                "year": "2017",
                "id": 29
            },
            {
                "year": "2017",
                "id": 42
            },
            {
                "year": "2017",
                "id": 72
            },
            {
                "year": "2017",
                "id": 76
            },
            {
                "year": "2017",
                "id": 147
            },
            {
                "year": "2017",
                "id": 149
            },
            {
                "year": "2017",
                "id": 152
            },
            {
                "year": "2017",
                "id": 224
            },
            {
                "year": "2017",
                "id": 226
            },
            {
                "year": "2017",
                "id": 227
            },
            {
                "year": "2017",
                "id": 229
            },
            {
                "year": "2017",
                "id": 298
            },
            {
                "year": "2017",
                "id": 302
            },
            {
                "year": "2017",
                "id": 306
            },
            {
                "year": "2017",
                "id": 307
            },
            {
                "year": "2017",
                "id": 322
            },
            {
                "year": "2017",
                "id": 323
            },
            {
                "year": "2017",
                "id": 371
            },
            {
                "year": "2017",
                "id": 381
            },
            {
                "year": "2017",
                "id": 382
            },
            {
                "year": "2017",
                "id": 383
            },
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2017",
                "id": 396
            },
            {
                "year": "2017",
                "id": 440
            },
            {
                "year": "2017",
                "id": 463
            },
            {
                "year": "2017",
                "id": 542
            },
            {
                "year": "2017",
                "id": 553
            },
            {
                "year": "2017",
                "id": 580
            },
            {
                "year": "2017",
                "id": 590
            },
            {
                "year": "2017",
                "id": 606
            },
            {
                "year": "2017",
                "id": 607
            },
            {
                "year": "2017",
                "id": 610
            },
            {
                "year": "2017",
                "id": 612
            }
        ]
    },
    {
        "title": "Temporal Perception and Prediction in Ego-Centric Video",
        "authors": [
            "Yipin Zhou",
            "Tamara L. Berg"
        ],
        "abstract": "Given a video of an activity, can we predict what will happen next? In this paper we explore two simple tasks related to temporal prediction in egocentric videos of everyday activities. We provide both human experiments to understand how well people can perform on these tasks and computational models for prediction. Experiments indicate that humans and computers can do well on temporal prediction and that personalization to a particular individual or environment provides significantly increased performance. Developing methods for temporal prediction could have far reaching benefits for robots or intelligent agents to anticipate what a person will do, before they do it.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410868",
        "reference_list": [
            {
                "year": "2013",
                "id": 121
            },
            {
                "year": "2011",
                "id": 51
            },
            {
                "year": "2013",
                "id": 401
            },
            {
                "year": "2011",
                "id": 61
            },
            {
                "year": "2011",
                "id": 131
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Context",
                "Predictive models",
                "Visualization",
                "Computers",
                "Footwear",
                "Data collection",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "prediction theory",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "temporal perception",
                "temporal prediction",
                "egocentric video",
                "computational model",
                "intelligent agent"
            ]
        },
        "id": 502,
        "cited_by": [
            {
                "year": "2017",
                "id": 304
            },
            {
                "year": "2017",
                "id": 352
            }
        ]
    },
    {
        "title": "Describing Videos by Exploiting Temporal Structure",
        "authors": [
            "Li Yao",
            "Atousa Torabi",
            "Kyunghyun Cho",
            "Nicolas Ballas",
            "Christopher Pal",
            "Hugo Larochelle",
            "Aaron Courville"
        ],
        "abstract": "Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410869",
        "reference_list": [
            {
                "year": "2013",
                "id": 54
            }
        ],
        "citation": {
            "ieee": 143,
            "other": 108,
            "total": 251
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Decoding",
                "Natural languages",
                "Computer vision",
                "Recurrent neural networks",
                "Convolutional codes"
            ],
            "INSPEC: Controlled Indexing": [
                "image motion analysis",
                "image representation",
                "image segmentation",
                "natural language processing",
                "recurrent neural nets",
                "text analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "recurrent neural networks",
                "image description",
                "video description",
                "static images",
                "dynamic temporal structure modeling",
                "natural language description model",
                "local temporal structure",
                "global temporal structure",
                "spatial temporal 3D convolutional neural network",
                "3D CNN representation",
                "temporal dynamics",
                "video action recognition",
                "human motion",
                "human behavior",
                "temporal attention mechanism",
                "local temporal modeling",
                "temporal segments",
                "text-generating RNN",
                "BLEU metrics",
                "METEOR metrics",
                "Youtube2Text dataset"
            ]
        },
        "id": 503,
        "cited_by": [
            {
                "year": "2017",
                "id": 5
            },
            {
                "year": "2017",
                "id": 10
            },
            {
                "year": "2017",
                "id": 42
            },
            {
                "year": "2017",
                "id": 54
            },
            {
                "year": "2017",
                "id": 73
            },
            {
                "year": "2017",
                "id": 130
            },
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2017",
                "id": 354
            },
            {
                "year": "2017",
                "id": 384
            },
            {
                "year": "2017",
                "id": 385
            },
            {
                "year": "2017",
                "id": 440
            },
            {
                "year": "2017",
                "id": 462
            }
        ]
    },
    {
        "title": "Person Re-Identification with Discriminatively Trained Viewpoint Invariant Dictionaries",
        "authors": [
            "Srikrishna Karanam",
            "Yang Li",
            "Richard J. Radke"
        ],
        "abstract": "This paper introduces a new approach to address the person re-identification problem in cameras with non-overlapping fields of view. Unlike previous approaches that learn Mahalanobis-like distance metrics in some transformed feature space, we propose to learn a dictionary that is capable of discriminatively and sparsely encoding features representing different people. Our approach directly addresses two key challenges in person re-identification: viewpoint variations and discriminability. First, to tackle viewpoint and associated appearance changes, we learn a single dictionary to represent both gallery and probe images in the training phase. We then discriminatively train the dictionary by enforcing explicit constraints on the associated sparse representations of the feature vectors. In the testing phase, we re-identify a probe image by simply determining the gallery image that has the closest sparse representation to that of the probe image in the Euclidean sense. Extensive performance evaluations on three publicly available multi-shot re-identification datasets demonstrate the advantages of our algorithm over several state-of-the-art dictionary learning, temporal sequence matching, and spatial appearance and metric learning based techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410870",
        "reference_list": [
            {
                "year": "2011",
                "id": 68
            }
        ],
        "citation": {
            "ieee": 51,
            "other": 16,
            "total": 67
        },
        "keywords": {
            "IEEE Keywords": [
                "Dictionaries",
                "Cameras",
                "Probes",
                "Measurement",
                "Optimization",
                "Feature extraction",
                "Encoding"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "metric learning based techniques",
                "temporal sequence matching",
                "dictionary learning",
                "publicly available multishot reidentification datasets",
                "probe image",
                "feature vectors",
                "viewpoint variations",
                "Mahalanobis-like distance metrics",
                "discriminatively trained viewpoint invariant dictionaries",
                "person reidentification"
            ]
        },
        "id": 504,
        "cited_by": [
            {
                "year": "2017",
                "id": 256
            },
            {
                "year": "2017",
                "id": 497
            },
            {
                "year": "2017",
                "id": 540
            },
            {
                "year": "2017",
                "id": 567
            }
        ]
    },
    {
        "title": "Storyline Representation of Egocentric Videos with an Applications to Story-Based Search",
        "authors": [
            "Bo Xiong",
            "Gunhee Kim",
            "Leonid Sigal"
        ],
        "abstract": "Egocentric videos are a valuable source of information as a daily log of our lives. However, large fraction of egocentric video content is typically irrelevant and boring to re-watch. It is an agonizing task, for example, to manually search for the moment when your daughter first met Mickey Mouse from hours-long egocentric videos taken at Disneyland. Although many summarization methods have been successfully proposed to create concise representations of videos, in practice, the value of the subshots to users may change according to their immediate preference/mood, thus summaries with fixed criteria may not fully satisfy users' various search intents. To address this, we propose a storyline representation that expresses an egocentric video as a set of jointly inferred, through MRF inference, story elements comprising of actors, locations, supporting objects and events, depicted on a timeline. We construct such a storyline with very limited annotation data (a list of map locations and weak knowledge of what events may be possible at each location), by bootstrapping the process with data obtained through focused Web image and video searches. Our representation promotes story-based search with queries in the form of AND-OR graphs, which span any subset of story elements and their spatio-temporal composition. We show effectiveness of our approach on a set of unconstrained YouTube egocentric videos of visits to Disneyland.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410871",
        "reference_list": [
            {
                "year": "2013",
                "id": 284
            },
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2011",
                "id": 84
            },
            {
                "year": "2003",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 9,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Search problems",
                "YouTube",
                "Training",
                "Semantics",
                "Visualization",
                "TV"
            ],
            "INSPEC: Controlled Indexing": [
                "graph theory",
                "inference mechanisms",
                "Markov processes",
                "statistical analysis",
                "video retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "storyline representation",
                "story-based search",
                "egocentric video content",
                "Mickey Mouse",
                "summarization methods",
                "concise video representations",
                "MRF inference",
                "annotation data",
                "process bootstrapping",
                "focused Web image searches",
                "video searches",
                "AND-OR graphs",
                "spatio-temporal composition",
                "YouTube egocentric videos",
                "Disneyland visits"
            ]
        },
        "id": 505,
        "cited_by": [
            {
                "year": "2017",
                "id": 385
            }
        ]
    },
    {
        "title": "Sequence to Sequence -- Video to Text",
        "authors": [
            "Subhashini Venugopalan",
            "Marcus Rohrbach",
            "Jeffrey Donahue",
            "Raymond Mooney",
            "Trevor Darrell",
            "Kate Saenko"
        ],
        "abstract": "Real-world videos often have complex dynamics, methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410872",
        "reference_list": [
            {
                "year": "2013",
                "id": 338
            },
            {
                "year": "2013",
                "id": 54
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 171,
            "other": 107,
            "total": 278
        },
        "keywords": {
            "IEEE Keywords": [
                "Decoding",
                "Encoding",
                "Feature extraction",
                "Visualization",
                "Recurrent neural networks",
                "Optical imaging",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "learning (artificial intelligence)",
                "recurrent neural nets",
                "text analysis",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-world videos",
                "open-domain video descriptions",
                "temporal structure",
                "variable length input",
                "variable length output",
                "frame sequence",
                "word sequence",
                "end-to-end sequence-to-sequence model",
                "video captions",
                "recurrent neural networks",
                "image caption generation",
                "LSTM model",
                "video-sentence pairs",
                "video frame sequence",
                "video clip",
                "temporal structure learning",
                "language model",
                "visual features",
                "YouTube videos",
                "movie description dataset",
                "M-VAD dataset",
                "MPII-MD dataset",
                "sequence-to-sequence video-to-text approach",
                "S2VT approach"
            ]
        },
        "id": 506,
        "cited_by": [
            {
                "year": "2017",
                "id": 5
            },
            {
                "year": "2017",
                "id": 54
            },
            {
                "year": "2017",
                "id": 73
            },
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2017",
                "id": 311
            },
            {
                "year": "2017",
                "id": 430
            },
            {
                "year": "2017",
                "id": 462
            },
            {
                "year": "2017",
                "id": 608
            },
            {
                "year": "2015",
                "id": 0
            }
        ]
    },
    {
        "title": "Context Aware Active Learning of Activity Recognition Models",
        "authors": [
            "Mahmudul Hasan",
            "Amit K. Roy-Chowdhury"
        ],
        "abstract": "Activity recognition in video has recently benefited from the use of the context e.g., inter-relationships among the activities and objects. However, these approaches require data to be labeled and entirely available at the outset. In contrast, we formulate a continuous learning framework for context aware activity recognition from unlabeled video data which has two distinct advantages over most existing methods. First, we propose a novel active learning technique which not only exploits the informativeness of the individual activity instances but also utilizes their contextual information during the query selection process, this leads to significant reduction in expensive manual annotation effort. Second, the learned models can be adapted online as more data is available. We formulate a conditional random field (CRF) model that encodes the context and devise an information theoretic approach that utilizes entropy and mutual information of the nodes to compute the set of most informative query instances, which need to be labeled by a human. These labels are combined with graphical inference techniques for incrementally updating the model as new videos come in. Experiments on four challenging datasets demonstrate that our framework achieves superior performance with significantly less amount of manual labeling.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410873",
        "reference_list": [
            {
                "year": "2009",
                "id": 129
            },
            {
                "year": "2011",
                "id": 61
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 6,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Context",
                "Streaming media",
                "Context modeling",
                "Visualization",
                "Computational modeling",
                "Mathematical model",
                "Adaptation models"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "learning (artificial intelligence)",
                "object detection",
                "ubiquitous computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "context aware active learning",
                "activity recognition models",
                "continuous learning framework",
                "context aware activity recognition",
                "video data",
                "active learning technique",
                "contextual information",
                "query selection process",
                "conditional random field",
                "CRF model",
                "information theoretic approach",
                "informative query instances",
                "graphical inference techniques"
            ]
        },
        "id": 507,
        "cited_by": [
            {
                "year": "2017",
                "id": 600
            },
            {
                "year": "2017",
                "id": 606
            }
        ]
    },
    {
        "title": "Action Recognition by Hierarchical Mid-Level Action Elements",
        "authors": [
            "Tian Lan",
            "Yuke Zhu",
            "Amir Roshan Zamir",
            "Silvio Savarese"
        ],
        "abstract": "Realistic videos of human actions exhibit rich spatiotemporal structures at multiple levels of granularity: an action can always be decomposed into multiple finer-grained elements in both space and time. To capture this intuition, we propose to represent videos by a hierarchy of mid-level action elements (MAEs), where each MAE corresponds to an action-related spatiotemporal segment in the video. We introduce an unsupervised method to generate this representation from videos. Our method is capable of distinguishing action-related segments from background segments and representing actions at multiple spatiotemporal resolutions. Given a set of spatiotemporal segments generated from the training data, we introduce a discriminative clustering algorithm that automatically discovers MAEs at multiple levels of granularity. We develop structured models that capture a rich set of spatial, temporal and hierarchical relations among the segments, where the action label and multiple levels of MAE labels are jointly inferred. The proposed model achieves state-of-the-art performance in multiple action recognition benchmarks. Moreover, we demonstrate the effectiveness of our model in real-world applications such as action recognition in large-scale untrimmed videos and action parsing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410874",
        "reference_list": [
            {
                "year": "2005",
                "id": 182
            },
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2009",
                "id": 191
            },
            {
                "year": "2003",
                "id": 96
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2007",
                "id": 265
            },
            {
                "year": "2013",
                "id": 342
            },
            {
                "year": "2013",
                "id": 226
            },
            {
                "year": "2009",
                "id": 204
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 15,
            "other": 9,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Spatiotemporal phenomena",
                "Videos",
                "Proposals",
                "Training",
                "Semantics",
                "Manganese",
                "Distance measurement"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation",
                "object recognition",
                "pattern clustering",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "action recognition",
                "MAE label",
                "action label",
                "hierarchical relation",
                "temporal relation",
                "spatial relation",
                "structured models",
                "discriminative clustering algorithm",
                "training data",
                "spatiotemporal resolution",
                "action representation",
                "background segments",
                "unsupervised method",
                "video",
                "action-related spatiotemporal segment",
                "spatiotemporal structures",
                "human action",
                "hierarchical mid-level action elements"
            ]
        },
        "id": 508,
        "cited_by": [
            {
                "year": "2017",
                "id": 209
            },
            {
                "year": "2017",
                "id": 461
            }
        ]
    },
    {
        "title": "Selecting Relevant Web Trained Concepts for Automated Event Retrieval",
        "authors": [
            "Bharat Singh",
            "Xintong Han",
            "Zhe Wu",
            "Vlad I. Morariu",
            "Larry S. Davis"
        ],
        "abstract": "Complex event retrieval is a challenging research problem, especially when no training videos are available. An alternative to collecting training videos is to train a large semantic concept bank a priori. Given a text description of an event, event retrieval is performed by selecting concepts linguistically related to the event description and fusing the concept responses on unseen videos. However, defining an exhaustive concept lexicon and pre-training it requires vast computational resources. Therefore, recent approaches automate concept discovery and training by leveraging large amounts of weakly annotated web data. Compact visually salient concepts are automatically obtained by the use of concept pairs or, more generally, n-grams. However, not all visually salient n-grams are necessarily useful for an event query -- some combinations of concepts may be visually compact but irrelevant -- and this drastically affects performance. We propose an event retrieval algorithm that constructs pairs of automatically discovered concepts and then prunes those concepts that are unlikely to be helpful for retrieval. Pruning depends both on the query and on the specific video instance being evaluated. Our approach also addresses calibration and domain adaptation issues that arise when applying concept detectors to unseen videos. We demonstrate large improvements over other vision based systems on the TRECVID MED 13 dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410875",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 7,
            "other": 5,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Videos",
                "Detectors",
                "Visualization",
                "Training",
                "Calibration",
                "Tires",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "Internet",
                "query processing",
                "text detection",
                "video retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "relevant Web trained concept",
                "automated event retrieval",
                "complex event retrieval",
                "training video",
                "text description",
                "event description",
                "exhaustive concept lexicon",
                "computational resource",
                "concept discovery",
                "Web data",
                "visually salient n-gram",
                "event query",
                "event retrieval algorithm",
                "video instance",
                "domain adaptation",
                "concept detector",
                "unseen video",
                "vision based system",
                "TRECVID MED 13 dataset"
            ]
        },
        "id": 509,
        "cited_by": [
            {
                "year": "2017",
                "id": 153
            },
            {
                "year": "2017",
                "id": 384
            }
        ]
    },
    {
        "title": "Beyond Covariance: Feature Representation with Nonlinear Kernel Matrices",
        "authors": [
            "Lei Wang",
            "Jianjia Zhang",
            "Luping Zhou",
            "Chang Tang",
            "Wanqing Li"
        ],
        "abstract": "Covariance matrix has recently received increasing attention in computer vision by leveraging Riemannian geometry of symmetric positive-definite (SPD) matrices. Originally proposed as a region descriptor, it has now been used as a generic representation in various recognition tasks. However, covariance matrix has shortcomings such as being prone to be singular, limited capability in modeling complicated feature relationship, and having a fixed form of representation. This paper argues that more appropriate SPD-matrix-based representations shall be explored to achieve better recognition. It proposes an open framework to use the kernel matrix over feature dimensions as a generic representation and discusses its properties and advantages. The proposed framework significantly elevates covariance representation to the unlimited opportunities provided by this new representation. Experimental study shows that this representation consistently outperforms its covariance counterpart on various visual recognition tasks. In particular, it achieves significant improvement on skeleton-based human action recognition, demonstrating the state-of-the-art performance over both the covariance and the existing non-covariance representations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410876",
        "reference_list": [
            {
                "year": "2013",
                "id": 343
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 7,
            "total": 26
        },
        "keywords": {
            "IEEE Keywords": [
                "Covariance matrices",
                "Kernel",
                "Feature extraction",
                "Symmetric matrices",
                "Measurement",
                "Training",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image representation",
                "matrix algebra",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "feature representation",
                "nonlinear kernel matrices",
                "symmetric positive-definite matrices",
                "SPD-matrix-based representations",
                "feature dimensions",
                "covariance representation",
                "visual recognition tasks",
                "skeleton-based human action recognition"
            ]
        },
        "id": 510,
        "cited_by": [
            {
                "year": "2017",
                "id": 62
            },
            {
                "year": "2017",
                "id": 448
            }
        ]
    },
    {
        "title": "Multiresolution Hierarchy Co-Clustering for Semantic Segmentation in Sequences with Small Variations",
        "authors": [
            "David Varas",
            "M\u00f3nica Alfaro",
            "Ferran Marques"
        ],
        "abstract": "This paper presents a co-clustering technique that, given a collection of images and their hierarchies, clusters nodes from these hierarchies to obtain a coherent multiresolution representation of the image collection. We formalize the co-clustering as Quadratic Semi-Assignment Problem and solve it with a linear programming relaxation approach that makes effective use of information from hierarchies. Initially, we address the problem of generating an optimal, coherent partition per image and, afterwards, we extend this method to a multiresolution framework. Finally, we particularize this framework to an iterative multiresolution video segmentation algorithm in sequences with small variations. We evaluate the algorithm on the Video Occlusion/Object Boundary Detection Dataset, showing that it produces state-of-the-art results in these scenarios.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410877",
        "reference_list": [
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2013",
                "id": 279
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image resolution",
                "Merging",
                "Optimization",
                "Streaming media",
                "Semantics",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image resolution",
                "image segmentation",
                "image sequences",
                "linear programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiresolution hierarchy coclustering",
                "semantic segmentation",
                "image collection coherent multiresolution representation",
                "quadratic semiassignment problem",
                "linear programming relaxation approach",
                "iterative multiresolution video segmentation algorithm",
                "video occlusion",
                "object boundary detection dataset"
            ]
        },
        "id": 511,
        "cited_by": []
    },
    {
        "title": "Objects2action: Classifying and Localizing Actions without Any Video Example",
        "authors": [
            "Mihir Jain",
            "Jan C. van Gemert",
            "Thomas Mensink",
            "Cees G. M. Snoek"
        ],
        "abstract": "The goal of this paper is to recognize actions in video without the need for examples. Different from traditional zero-shot approaches we do not demand the design and specification of attribute classifiers and class-to-attribute mappings to allow for transfer from seen classes to unseen classes. Our key contribution is objects2action, a semantic word embedding that is spanned by a skip-gram model of thousands of object categories. Action labels are assigned to an object encoding of unseen video based on a convex combination of action and object affinities. Our semantic embedding has three main characteristics to accommodate for the specifics of actions. First, we propose a mechanism to exploit multiple-word descriptions of actions and objects. Second, we incorporate the automated selection of the most responsive objects per action. And finally, we demonstrate how to extend our zero-shot approach to the spatio-temporal localization of actions in video. Experiments on four action datasets demonstrate the potential of our approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410878",
        "reference_list": [
            {
                "year": "2013",
                "id": 322
            },
            {
                "year": "2013",
                "id": 338
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2011",
                "id": 254
            },
            {
                "year": "2011",
                "id": 63
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 17,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Semantics",
                "Image recognition",
                "Encoding",
                "Neural networks",
                "Training",
                "Visualization",
                "Computational modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "convex programming",
                "gesture recognition",
                "image classification",
                "object detection",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spatio-temporal localization",
                "automated selection",
                "multiple-word description",
                "semantic embedding",
                "object affinity",
                "convex combination",
                "object encoding",
                "action label",
                "object category",
                "skip-gram model",
                "semantic word embedding",
                "class-to-attribute mapping",
                "attribute classifier",
                "zero-shot approach",
                "action recognition",
                "video example",
                "action localization",
                "action classification",
                "objects2action"
            ]
        },
        "id": 512,
        "cited_by": [
            {
                "year": "2017",
                "id": 129
            },
            {
                "year": "2017",
                "id": 384
            },
            {
                "year": "2017",
                "id": 466
            }
        ]
    },
    {
        "title": "Human Action Recognition Using Factorized Spatio-Temporal Convolutional Networks",
        "authors": [
            "Lin Sun",
            "Kui Jia",
            "Dit-Yan Yeung",
            "Bertram E. Shi"
        ],
        "abstract": "Human actions in video sequences are three-dimensional (3D) spatio-temporal signals characterizing both the visual appearance and motion dynamics of the involved humans and objects. Inspired by the success of convolutional neural networks (CNN) for image classification, recent attempts have been made to learn 3D CNNs for recognizing human actions in videos. However, partly due to the high complexity of training 3D convolution kernels and the need for large quantities of training videos, only limited success has been reported. This has triggered us to investigate in this paper a new deep architecture which can handle 3D signals more effectively. Specifically, we propose factorized spatio-temporal convolutional networks (FstCN) that factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels in the lower layers (called spatial convolutional layers), followed by learning 1D temporal kernels in the upper layers (called temporal convolutional layers). We introduce a novel transformation and permutation operator to make factorization in FstCN possible. Moreover, to address the issue of sequence alignment, we propose an effective training and inference strategy based on sampling multiple video clips from a given action video sequence. We have tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51). Without using auxiliary training videos to boost the performance, FstCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410879",
        "reference_list": [
            {
                "year": "2007",
                "id": 147
            },
            {
                "year": "2011",
                "id": 325
            },
            {
                "year": "2013",
                "id": 443
            }
        ],
        "citation": {
            "ieee": 83,
            "other": 53,
            "total": 136
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Convolution",
                "Three-dimensional displays",
                "Video sequences",
                "Training",
                "Visualization",
                "Computer architecture"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image motion analysis",
                "image sequences",
                "learning (artificial intelligence)",
                "neural nets",
                "object recognition",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human action recognition",
                "factorized spatio-temporal convolutional network",
                "video sequences",
                "3D spatio-temporal signal",
                "visual appearance",
                "motion dynamics",
                "convolutional neural network",
                "CNN",
                "image classification",
                "FstCN",
                "3D convolution kernel learning",
                "spatial convolutional layer",
                "temporal convolutional layer",
                "inference strategy"
            ]
        },
        "id": 513,
        "cited_by": [
            {
                "year": "2017",
                "id": 42
            },
            {
                "year": "2017",
                "id": 74
            },
            {
                "year": "2017",
                "id": 226
            },
            {
                "year": "2017",
                "id": 392
            },
            {
                "year": "2017",
                "id": 580
            },
            {
                "year": "2017",
                "id": 610
            }
        ]
    },
    {
        "title": "Bayesian Non-parametric Inference for Manifold Based MoCap Representation",
        "authors": [
            "Fabrizio Natola",
            "Valsamis Ntouskos",
            "Marta Sanzari",
            "Fiora Pirri"
        ],
        "abstract": "We propose a novel approach to human action recognition, with motion capture data (MoCap), based on grouping sub-body parts. By representing configurations of actions as manifolds, joint positions are mapped on a subspace via principal geodesic analysis. The reduced space is still highly informative and allows for classification based on a non-parametric Bayesian approach, generating behaviors for each sub-body part. Having partitioned the set of joints, poses relative to a sub-body part are exchangeable, given a specified prior and can elicit, in principle, infinite behaviors. The generation of these behaviors is specified by a Dirichlet process mixture. We show with several experiments that the recognition gives very promising results, outperforming methods requiring temporal alignment.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410880",
        "reference_list": [
            {
                "year": "2011",
                "id": 72
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Manifolds",
                "Skeleton",
                "Three-dimensional displays",
                "Algebra",
                "Bayes methods",
                "Nickel",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "belief networks",
                "image motion analysis",
                "image representation",
                "inference mechanisms",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian nonparametric inference",
                "manifold based MoCap representation",
                "motion capture data",
                "human action recognition",
                "subbody parts grouping",
                "principal geodesic analysis",
                "reduced space",
                "infinite behaviors",
                "Dirichlet process mixture",
                "temporal alignment"
            ]
        },
        "id": 514,
        "cited_by": []
    },
    {
        "title": "Semantic Video Entity Linking Based on Visual Content and Metadata",
        "authors": [
            "Yuncheng Li",
            "Xitong Yang",
            "Jiebo Luo"
        ],
        "abstract": "Video entity linking, which connects online videos to the related entities in a semantic knowledge base, can enable a wide variety of video based applications including video retrieval and video recommendation. Most existing systems for video entity linking rely on video metadata. In this paper, we propose to exploit video visual content to improve video entity linking. In the proposed framework, videos are first linked to entity candidates using a text-based method. Next, the entity candidates are verified and reranked according to visual content. In order to properly handle large variations in visual content matching, we propose to use Multiple Instance Metric Learning to learn a \"set to sequence\" metric for this specific matching problem. To evaluate the proposed framework, we collect and annotate 1912 videos crawled from the YouTube open API. Experiment results have shown consistent gains by the proposed framework over several strong baselines.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410881",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 4,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Joining processes",
                "Measurement",
                "Visualization",
                "Semantics",
                "Encyclopedias",
                "Electronic publishing"
            ],
            "INSPEC: Controlled Indexing": [
                "knowledge based systems",
                "learning (artificial intelligence)",
                "meta data",
                "recommender systems",
                "semantic Web",
                "social networking (online)",
                "video retrieval"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "semantic video entity linking",
                "video visual content",
                "online videos",
                "semantic knowledge base",
                "video retrieval",
                "video recommendation",
                "video metadata",
                "text-based method",
                "visual content matching",
                "multiple instance metric learning",
                "set to sequence metric",
                "YouTube open API"
            ]
        },
        "id": 515,
        "cited_by": []
    },
    {
        "title": "Love Thy Neighbors: Image Annotation by Exploiting Image Metadata",
        "authors": [
            "Justin Johnson",
            "Lamberto Ballan",
            "Li Fei-Fei"
        ],
        "abstract": "Some images that are difficult to recognize on their own may become more clear in the context of a neighborhood of related images with similar social-network metadata. We build on this intuition to improve multilabel image annotation. Our model uses image metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities, then uses a deep neural network to blend visual information from the image and its neighbors. Prior work typically models image metadata parametrically, in contrast, our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing. We perform comprehensive experiments on the NUS-WIDE dataset, where we show that our model outperforms state-of-the-art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410882",
        "reference_list": [
            {
                "year": "2013",
                "id": 175
            },
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2009",
                "id": 39
            },
            {
                "year": "2009",
                "id": 251
            },
            {
                "year": "2011",
                "id": 77
            },
            {
                "year": "2009",
                "id": 54
            }
        ],
        "citation": {
            "ieee": 19,
            "other": 23,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Metadata",
                "Visualization",
                "Training",
                "Vocabulary",
                "Computational modeling",
                "Adaptation models",
                "Social network services"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "image retrieval",
                "meta data",
                "neural nets",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image recognition",
                "images neighborhood",
                "social-network metadata",
                "multilabel image annotation",
                "image metadata",
                "Jaccard similarities",
                "deep neural network",
                "visual information",
                "nonparametric treatment",
                "NUS-WIDE dataset"
            ]
        },
        "id": 516,
        "cited_by": []
    },
    {
        "title": "Unsupervised Extraction of Video Highlights via Robust Recurrent Auto-Encoders",
        "authors": [
            "Huan Yang",
            "Baoyuan Wang",
            "Stephen Lin",
            "David Wipf",
            "Minyi Guo",
            "Baining Guo"
        ],
        "abstract": "With the growing popularity of short-form video sharing platforms such as Instagram and Vine, there has been an increasing need for techniques that automatically extract highlights from video. Whereas prior works have approached this problem with heuristic rules or supervised learning, we present an unsupervised learning approach that takes advantage of the abundance of user-edited videos on social media websites such as YouTube. Based on the idea that the most significant sub-events within a video class are commonly present among edited videos while less interesting ones appear less frequently, we identify the significant sub-events via a robust recurrent auto-encoder trained on a collection of user-edited videos queried for each particular class of interest. The auto-encoder is trained using a proposed shrinking exponential loss function that makes it robust to noise in the web-crawled training data, and is configured with bidirectional long short term memory (LSTM) [5] cells to better model the temporal structure of highlight segments. Different from supervised techniques, our method can infer highlights using only a set of downloaded edited videos, without also needing their pre-edited counterparts which are rarely available online. Extensive experiments indicate the promise of our proposed solution in this challenging unsupervised setting.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410883",
        "reference_list": [],
        "citation": {
            "ieee": 25,
            "other": 17,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Training",
                "Feature extraction",
                "Supervised learning",
                "Robustness",
                "Training data",
                "Pipelines",
                "Data models"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "social networking (online)",
                "video coding"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unsupervised extraction",
                "video highlight",
                "robust recurrent auto-encoder",
                "short-form video sharing platform",
                "Instagram",
                "Vine",
                "heuristic rule",
                "unsupervised learning approach",
                "user-edited video",
                "social media Web site",
                "YouTube",
                "video class",
                "shrinking exponential loss function",
                "Web-crawled training data",
                "bidirectional long short term memory cell",
                "LSTM cell",
                "unsupervised setting"
            ]
        },
        "id": 517,
        "cited_by": [
            {
                "year": "2017",
                "id": 73
            },
            {
                "year": "2017",
                "id": 385
            },
            {
                "year": "2017",
                "id": 608
            }
        ]
    },
    {
        "title": "Learning Visual Clothing Style with Heterogeneous Dyadic Co-Occurrences",
        "authors": [
            "Andreas Veit",
            "Balazs Kovacs",
            "Sean Bell",
            "Julian McAuley",
            "Kavita Bala",
            "Serge Belongie"
        ],
        "abstract": "With the rapid proliferation of smart mobile devices, users now take millions of photos every day. These include large numbers of clothing and accessory images. We would like to answer questions like 'What outfit goes well with this pair of shoes?' To answer these types of questions, one has to go beyond learning visual similarity and learn a visual notion of compatibility across categories. In this paper, we propose a novel learning framework to help answer these types of questions. The main idea of this framework is to learn a feature transformation from images of items into a latent space that expresses compatibility. For the feature transformation, we use a Siamese Convolutional Neural Network (CNN) architecture, where training examples are pairs of items that are either compatible or incompatible. We model compatibility based on co-occurrence in large-scale user behavior data, in particular co-purchase data from Amazon.com. To learn cross-category fit, we introduce a strategic method to sample training data, where pairs of items are heterogeneous dyads, i.e., the two elements of a pair belong to different high-level categories. While this approach is applicable to a wide variety of settings, we focus on the representative problem of learning compatible clothing style. Our results indicate that the proposed framework is capable of learning semantic information about visual style and is able to generate outfits of clothes, with items from different categories, that go well together.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410884",
        "reference_list": [],
        "citation": {
            "ieee": 32,
            "other": 26,
            "total": 58
        },
        "keywords": {
            "IEEE Keywords": [
                "Visualization",
                "Measurement",
                "Training",
                "Footwear",
                "Training data",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "clothing",
                "feature extraction",
                "image matching",
                "learning (artificial intelligence)",
                "neural net architecture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual clothing style",
                "heterogeneous dyadic co-occurrences",
                "smart mobile devices",
                "clothing images",
                "accessory images",
                "visual similarity",
                "learning framework",
                "feature transformation",
                "latent space",
                "Siamese convolutional neural network architecture",
                "CNN architecture",
                "large-scale user behavior data",
                "co-purchase data",
                "Amazon.com",
                "cross-category fit",
                "high-level categories",
                "clothing style compatibility"
            ]
        },
        "id": 518,
        "cited_by": [
            {
                "year": "2017",
                "id": 40
            },
            {
                "year": "2017",
                "id": 441
            }
        ]
    },
    {
        "title": "Text Flow: A Unified Text Detection System in Natural Scene Images",
        "authors": [
            "Shangxuan Tian",
            "Yifeng Pan",
            "Chang Huang",
            "Shijian Lu",
            "Kai Yu",
            "Chew Lim Tan"
        ],
        "abstract": "The prevalent scene text detection approach follows four sequential steps comprising character candidate detection, false character candidate removal, text line extraction, and text line verification. However, errors occur and accumulate throughout each of these sequential steps which often lead to low detection performance. To address these issues, we propose a unified scene text detection system, namely Text Flow, by utilizing the minimum cost (min-cost) flow network model. With character candidates detected by cascade boosting, the min-cost flow network model integrates the last three sequential steps into a single process which solves the error accumulation problem at both character level and text line level effectively. The proposed technique has been tested on three public datasets, i.e, ICDAR2011 dataset, ICDAR2013 dataset and a multilingual dataset and it outperforms the state-of-the-art methods on all three datasets with much higher recall and F-score. The good performance on the multilingual dataset shows that the proposed technique can be used for the detection of texts in different languages.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410885",
        "reference_list": [
            {
                "year": "2013",
                "id": 97
            },
            {
                "year": "2013",
                "id": 154
            },
            {
                "year": "2011",
                "id": 184
            }
        ],
        "citation": {
            "ieee": 46,
            "other": 27,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Boosting",
                "Feature extraction",
                "Image edge detection",
                "Computational efficiency",
                "Joining processes",
                "Layout",
                "Stability analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "text analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "unified text detection system",
                "text flow",
                "natural scene images",
                "scene text detection",
                "false character candidate removal",
                "character candidate detection",
                "text line extraction",
                "text line verification",
                "cascade boosting"
            ]
        },
        "id": 519,
        "cited_by": [
            {
                "year": "2017",
                "id": 77
            },
            {
                "year": "2017",
                "id": 156
            },
            {
                "year": "2017",
                "id": 321
            },
            {
                "year": "2017",
                "id": 519
            },
            {
                "year": "2017",
                "id": 525
            },
            {
                "year": "2017",
                "id": 550
            }
        ]
    },
    {
        "title": "Uncovering Interactions and Interactors: Joint Estimation of Head, Body Orientation and F-Formations from Surveillance Videos",
        "authors": [
            "Elisa Ricci",
            "Jagannadan Varadarajan",
            "Ramanathan Subramanian",
            "Samuel Rota Bul\u00f2",
            "Narendra Ahuja",
            "Oswald Lanz"
        ],
        "abstract": "We present a novel approach for jointly estimating targets' head, body orientations and conversational groups called F-formations from a distant social scene (e.g., a cocktail party captured by surveillance cameras). Differing from related works that have (i) coupled head and body pose learning by exploiting the limited range of orientations that the two can jointly take, or (ii) determined F-formations based on the mutual head (but not body) orientations of interactors, we present a unified framework to jointly infer both (i) and (ii). Apart from exploiting spatial and orientation relationships, we also integrate cues pertaining to temporal consistency and occlusions, which are beneficial while handling low-resolution data under surveillance settings. Efficacy of the joint inference framework reflects via increased head, body pose and F-formation estimation accuracy over the state-of-the-art, as confirmed by extensive experiments on two social datasets.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410886",
        "reference_list": [
            {
                "year": "2011",
                "id": 298
            },
            {
                "year": "2013",
                "id": 187
            }
        ],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Head",
                "Videos",
                "Surveillance",
                "Foot",
                "Target tracking"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image resolution",
                "learning (artificial intelligence)",
                "pose estimation",
                "video surveillance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interaction uncovering",
                "head-body orientation estimation",
                "F-formations",
                "surveillance videos",
                "distant social scene",
                "cocktail party",
                "surveillance cameras",
                "coupled head body pose learning",
                "temporal consistency",
                "occlusions",
                "low-resolution data handling",
                "joint inference framework"
            ]
        },
        "id": 520,
        "cited_by": []
    },
    {
        "title": "Generating Notifications for Missing Actions: Don't Forget to Turn the Lights Off!",
        "authors": [
            "Bilge Soran",
            "Ali Farhadi",
            "Linda Shapiro"
        ],
        "abstract": "We all have experienced forgetting habitual actions among our daily activities. For example, we probably have forgotten to turn the lights off before leaving a room or turn the stove off after cooking. In this paper, we propose a solution to the problem of issuing notifications on actions that may be missed. This involves learning about interdependencies between actions and being able to predict an ongoing action while segmenting the input video stream. In order to show a proof of concept, we collected a new egocentric dataset, in which people wear a camera while making lattes. We show promising results on the extremely challenging task of issuing correct and timely reminders. We also show that our model reliably segments the actions, while predicting the ongoing one when only a few frames from the beginning of the action are observed. The overall prediction accuracy is 46.2% when only 10 frames of an action are seen (2/3 of a sec). Moreover, the overall recognition and segmentation accuracy is shown to be 72.7% when the whole activity sequence is observed. Finally, the online prediction and segmentation accuracy is 68.3% when the prediction is made at every time step.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410887",
        "reference_list": [
            {
                "year": "2011",
                "id": 98
            },
            {
                "year": "2011",
                "id": 51
            },
            {
                "year": "2011",
                "id": 131
            },
            {
                "year": "2013",
                "id": 343
            }
        ],
        "citation": {
            "ieee": 6,
            "other": 8,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Dairy products",
                "Cameras",
                "Hidden Markov models",
                "Predictive models",
                "Robot vision systems",
                "Streaming media",
                "Detectors"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "computer vision",
                "image segmentation",
                "learning (artificial intelligence)",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "input video stream segmentation",
                "egocentric dataset",
                "camera",
                "overall prediction accuracy",
                "learning"
            ]
        },
        "id": 521,
        "cited_by": [
            {
                "year": "2017",
                "id": 316
            }
        ]
    },
    {
        "title": "Partial Person Re-Identification",
        "authors": [
            "Wei-Shi Zheng",
            "Xiang Li",
            "Tao Xiang",
            "Shengcai Liao",
            "Jianhuang Lai",
            "Shaogang Gong"
        ],
        "abstract": "We address a new partial person re-identification (re-id) problem, where only a partial observation of a person is available for matching across different non-overlapping camera views. This differs significantly from the conventional person re-id setting where it is assumed that the full body of a person is detected and aligned. To solve this more challenging and realistic re-id problem without the implicit assumption of manual body-parts alignment, we propose a matching framework consisting of 1) a local patch-level matching model based on a novel sparse representation classification formulation with explicit patch ambiguity modelling, and 2) a global part-based matching model providing complementary spatial layout information. Our framework is evaluated on a new partial person re-id dataset as well as two existing datasets modified to include partial person images. The results show that the proposed method outperforms significantly existing re-id methods as well as other partial visual matching methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410888",
        "reference_list": [
            {
                "year": "2013",
                "id": 55
            },
            {
                "year": "2007",
                "id": 179
            },
            {
                "year": "2013",
                "id": 74
            },
            {
                "year": "2013",
                "id": 393
            },
            {
                "year": "2013",
                "id": 315
            }
        ],
        "citation": {
            "ieee": 31,
            "other": 16,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational modeling",
                "Probes",
                "Face recognition",
                "Dictionaries",
                "Cameras",
                "Clothing",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image matching",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "partial person re-identification",
                "sparse representation classification formulation",
                "global part-based matching model",
                "patch ambiguity modelling"
            ]
        },
        "id": 522,
        "cited_by": [
            {
                "year": "2017",
                "id": 117
            },
            {
                "year": "2017",
                "id": 258
            },
            {
                "year": "2017",
                "id": 339
            },
            {
                "year": "2017",
                "id": 565
            }
        ]
    },
    {
        "title": "Shape Interaction Matrix Revisited and Robustified: Efficient Subspace Clustering with Corrupted and Incomplete Data",
        "authors": [
            "Pan Ji",
            "Mathieu Salzmann",
            "Hongdong Li"
        ],
        "abstract": "The Shape Interaction Matrix (SIM) is one of the earliest approaches to performing subspace clustering (i.e., separating points drawn from a union of subspaces). In this paper, we revisit the SIM and reveal its connections to several recent subspace clustering methods. Our analysis lets us derive a simple, yet effective algorithm to robustify the SIM and make it applicable to realistic scenarios where the data is corrupted by noise. We justify our method by intuitive examples and the matrix perturbation theory. We then show how this approach can be extended to handle missing data, thus yielding an efficient and general subspace clustering algorithm. We demonstrate the benefits of our approach over state-of-the-art subspace clustering methods on several challenging motion segmentation and face clustering problems, where the data includes corruptions and missing measurements.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410889",
        "reference_list": [
            {
                "year": "2001",
                "id": 184
            },
            {
                "year": "2009",
                "id": 86
            }
        ],
        "citation": {
            "ieee": 8,
            "other": 1,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Computer vision",
                "Shape",
                "Motion segmentation",
                "Trajectory",
                "Clustering algorithms",
                "Clustering methods"
            ],
            "INSPEC: Controlled Indexing": [
                "data handling",
                "matrix algebra",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape interaction matrix",
                "incomplete data",
                "SIM",
                "subspace clustering methods",
                "matrix perturbation theory",
                "missing data handling",
                "subspace clustering algorithm",
                "motion segmentation",
                "face clustering problems",
                "missing measurements"
            ]
        },
        "id": 523,
        "cited_by": [
            {
                "year": "2017",
                "id": 490
            }
        ]
    },
    {
        "title": "Multiple Hypothesis Tracking Revisited",
        "authors": [
            "Chanho Kim",
            "Fuxin Li",
            "Arridhana Ciptadi",
            "James M. Rehg"
        ],
        "abstract": "This paper revisits the classical multiple hypotheses tracking (MHT) algorithm in a tracking-by-detection framework. The success of MHT largely depends on the ability to maintain a small list of potential hypotheses, which can be facilitated with the accurate object detectors that are currently available. We demonstrate that a classical MHT implementation from the 90's can come surprisingly close to the performance of state-of-the-art methods on standard benchmark datasets. In order to further utilize the strength of MHT in exploiting higher-order information, we introduce a method for training online appearance models for each track hypothesis. We show that appearance models can be learned efficiently via a regularized least squares framework, requiring only a few extra operations for each hypothesis branch. We obtain state-of-the-art results on popular tracking-by-detection datasets such as PETS and the recent MOT challenge.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410890",
        "reference_list": [
            {
                "year": "2013",
                "id": 273
            },
            {
                "year": "2013",
                "id": 362
            },
            {
                "year": "2011",
                "id": 17
            }
        ],
        "citation": {
            "ieee": 79,
            "other": 40,
            "total": 119
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Radar tracking",
                "Standards",
                "Computational modeling",
                "Visualization",
                "Space exploration"
            ],
            "INSPEC: Controlled Indexing": [
                "least squares approximations",
                "object detection",
                "object tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple hypotheses tracking algorithm",
                "MHT algorithm",
                "tracking-by-detection framework",
                "object detectors",
                "online appearance model training",
                "regularized least squares framework"
            ]
        },
        "id": 524,
        "cited_by": [
            {
                "year": "2017",
                "id": 31
            },
            {
                "year": "2017",
                "id": 268
            },
            {
                "year": "2017",
                "id": 508
            }
        ]
    },
    {
        "title": "Learning to Track: Online Multi-object Tracking by Decision Making",
        "authors": [
            "Yu Xiang",
            "Alexandre Alahi",
            "Silvio Savarese"
        ],
        "abstract": "Online Multi-Object Tracking (MOT) has wide applications in time-critical video analysis scenarios, such as robot navigation and autonomous driving. In tracking-by-detection, a major challenge of online MOT is how to robustly associate noisy object detections on a new video frame with previously tracked objects. In this work, we formulate the online MOT problem as decision making in Markov Decision Processes (MDPs), where the lifetime of an object is modeled with a MDP. Learning a similarity function for data association is equivalent to learning a policy for the MDP, and the policy learning is approached in a reinforcement learning fashion which benefits from both advantages of offline-learning and online-learning for data association. Moreover, our framework can naturally handle the birth/death and appearance/disappearance of targets by treating them as state transitions in the MDP while leveraging existing online single object tracking methods. We conduct experiments on the MOT Benchmark [24] to verify the effectiveness of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/7410891",
        "reference_list": [
            {
                "year": "2013",
                "id": 287
            },
            {
                "year": "2011",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 116,
            "other": 38,
            "total": 154
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Object detection",
                "Decision making",
                "Markov processes",
                "Object tracking",
                "Training",
                "Learning (artificial intelligence)"
            ],
            "INSPEC: Controlled Indexing": [
                "decision making",
                "learning (artificial intelligence)",
                "Markov processes",
                "object tracking",
                "sensor fusion",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "learning-to-track",
                "online multiobject tracking",
                "decision making",
                "time-critical video analysis scenarios",
                "robot navigation",
                "autonomous driving",
                "tracking-by-detection",
                "video frame",
                "noisy object detections",
                "online MOT problem",
                "Markov decision processes",
                "MDP",
                "data association",
                "policy learning",
                "reinforcement learning fashion",
                "birth handling",
                "death handling",
                "target appearance handling",
                "target disappearance handling",
                "MOT Benchmark",
                "similarity function",
                "offline-learning"
            ]
        },
        "id": 525,
        "cited_by": [
            {
                "year": "2017",
                "id": 31
            },
            {
                "year": "2017",
                "id": 33
            },
            {
                "year": "2017",
                "id": 247
            },
            {
                "year": "2017",
                "id": 508
            }
        ]
    }
]