[
    {
        "title": "High dynamic range panoramic imaging",
        "authors": [
            "M. Aggarwal",
            "N. Ahuja"
        ],
        "abstract": "Most imaging sensors have a limited dynamic range and hence can satisfactorily respond to only a part of illumination levels present in a scene. This is particularly disadvantageous for omnidirectional and panoramic cameras since larger fields of view have larger brightness ranges. We propose a simple modification to existing high resolution omnidirectional/panoramic cameras in which the process of increasing the dynamic range is coupled with the process of increasing the field of view. This is achieved by placing a graded transparency (mask) in front of the sensor which allows every scene point to be imaged under multiple exposure settings as the camera pans, a process anyway required to capture large fields of view at high resolution. The sequence of images are then mosaiced to construct a high resolution, high dynamic range panoramic/omnidirectional image. Our method is robust to alignment errors between the mask and the sensor grid and does not require the mask to be placed on the sensing surface. We have designed a panoramic camera with the proposed modifications and have discussed various theoretical and practical issues encountered in obtaining a robust design. We show with an example of high resolution, high dynamic range panoramic image obtained from the camera we designed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937492",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 4,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Dynamic range",
                "Cameras",
                "Image resolution",
                "Layout",
                "Image sensors",
                "Brightness",
                "Lenses",
                "Robustness",
                "High-resolution imaging",
                "Charge coupled devices"
            ],
            "INSPEC: Controlled Indexing": [
                "cameras",
                "image sensors",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "panoramic imaging",
                "imaging sensors",
                "panoramic cameras",
                "high resolution"
            ]
        },
        "id": 0,
        "cited_by": [
            {
                "year": "2007",
                "id": 317
            },
            {
                "year": "2003",
                "id": 152
            },
            {
                "year": "2003",
                "id": 153
            }
        ]
    },
    {
        "title": "A variational model for filling-in gray level and color images",
        "authors": [
            "C. Ballester",
            "V. Caselles",
            "J. Verdera",
            "M. Bertalmio",
            "G. Sapiro"
        ],
        "abstract": "A variational approach for filling-in regions of missing data in gray-level and color images is introduced in this paper. The approach is based on joint interpolation of the image gray-levels and gradient/isophores directions, smoothly extending in an automatic fashion the isophote lines into the holes of missing data. This interpolation is computed solving the variational problem via its gradient descent flow, which lends to a set of coupled second order partial differential equations, one for the gray-levels and one for the gradient orientations. The process underlying this approach can be considered as an interpretation of the Gestaltist's principle of good continuation. No limitations are imposed on the topology of the holes, and all regions of missing data can be simultaneously processed, even if they are surrounded by completely different structures. Applications of this technique include the restoration of old photographs and removal of superimposed text like dates, subtitles, or publicity. Examples of these applications are given.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937493",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 9,
            "total": 22
        },
        "keywords": {
            "IEEE Keywords": [
                "Color",
                "Level set",
                "Interpolation",
                "Image restoration",
                "Photometry",
                "Partial differential equations",
                "Topology",
                "Digital images",
                "Layout",
                "Painting"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration",
                "variational techniques",
                "partial differential equations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "variational model",
                "filling-in",
                "color images",
                "gray level images",
                "joint interpolation",
                "variational problem",
                "gradient descent flow",
                "partial differential equations"
            ]
        },
        "id": 1,
        "cited_by": []
    },
    {
        "title": "Generalized mosaicing",
        "authors": [
            "Y.Y. Schechner",
            "S.K. Nayar"
        ],
        "abstract": "We present an approach that significantly enhances the capabilities of traditional image mosaicing. The key observation is that as a camera moves, it senses each scene point multiple times. We rigidly attach to the camera an optical filter with spatially varying properties, so that multiple measurements are obtained for each scene point under different optical settings. Fusing the data captured in the multiple images yields an image mosaic that includes additional information about the scene. This information can come in the form of extended dynamic range, high spectral quality, or enhancements to other dimensions of imaging. We refer to this approach as generalized mosaicing. The approach was tested using a filter with spatially varying transmittance and a standard 8-bit black/white video camera, to achieve image mosaicing with dynamic range comparable to imaging with a 16-bit camera. In another experiment, we attached a spatially varying spectral filter to the same camera to obtain mosaics that represent the spectral distribution (rather than the usual RGB measurements) of each scene point. We also discuss how generalized mosaicing can be used to explore other imaging dimensions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937494",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 5,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical filters",
                "Layout",
                "Cameras",
                "Optical imaging",
                "Optical sensors",
                "Dynamic range",
                "Spatial resolution",
                "Computer science",
                "Testing",
                "Video sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image mosaicing",
                "optical settings",
                "multiple images",
                "imaging"
            ]
        },
        "id": 2,
        "cited_by": [
            {
                "year": "2015",
                "id": 382
            },
            {
                "year": "2003",
                "id": 153
            }
        ]
    },
    {
        "title": "The space of all stereo images",
        "authors": [
            "S.M. Seitz"
        ],
        "abstract": "A theory of stereo image formation is presented that enables a complete classification of all possible stereo views, including non-perspective varieties. Towards this end, the notion of epipolar geometry is generalized to apply to multiperspective images. It is shown that any stereo pair must consist of rays lying on one of three varieties of quadric surfaces. A unified representation is developed to model all classes of stereo views, based on the concept of a quadric view. The benefits include a unified treatment of projection and triangulation operations for all stereo views. The framework is applied to derive new types of stereo image representations with unusual and useful properties.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937495",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 16,
            "total": 44
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Surface treatment",
                "Image representation",
                "Layout",
                "Image sensors",
                "Sensor phenomena and characterization",
                "Particle measurements",
                "Computer science",
                "Graphics",
                "Stereo image processing"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo images",
                "stereo image formation",
                "stereo views",
                "epipolar geometry",
                "image representations",
                "projection",
                "triangulation",
                "quadric view"
            ]
        },
        "id": 3,
        "cited_by": [
            {
                "year": "2013",
                "id": 60
            },
            {
                "year": "2009",
                "id": 155
            },
            {
                "year": "2003",
                "id": 130
            },
            {
                "year": "2003",
                "id": 152
            }
        ]
    },
    {
        "title": "Optimal motion estimation from multiview normalized epipolar constraint",
        "authors": [
            "R. Vidal",
            "Y. Ma",
            "S. Hsu",
            "S. Sastry"
        ],
        "abstract": "In this paper, we study the structure from motion problem as a constrained nonlinear least squares problem which minimizes the so called reprojection error subject to all constraints among multiple images. By converting this constrained optimization problem to an unconstrained one, we obtain a multiview version of the normalized epipolar constraint of two views. Such a multiview normalized epipolar constraint serves as a statistically optimal objective function for motion (and structure) estimation. Since such a function is defined naturally on a product of Stiefel manifolds, we show how to use geometric optimization techniques to minimize it. We present experimental results on real images to evaluate the proposed algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937496",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Constraint optimization",
                "Least squares approximation",
                "Cameras",
                "Computer errors",
                "Image converters",
                "Computer vision",
                "Clouds",
                "Layout",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "least squares approximations",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion estimation",
                "multiview",
                "epipolar constraint",
                "constrained nonlinear least squares problem",
                "reprojection error",
                "constrained optimization",
                "geometric optimization"
            ]
        },
        "id": 4,
        "cited_by": []
    },
    {
        "title": "Linear multi view reconstruction and camera recovery",
        "authors": [
            "C. Rother",
            "S. Carlsson"
        ],
        "abstract": "This paper presents a linear algorithm for the simultaneous computation of 3D points and camera positions from multiple perspective views, based on having four points on a reference plane visible in all views. The reconstruction and camera recovery is achieved, in a single step, by finding the null-space of a matrix using singular value decomposition. Unlike factorization algorithms, the presented algorithm does not require all points to be visible in all views. By simultaneously reconstructing points and views the numerically stabilizing effect of having wide spread cameras with large mutual baselines is exploited. Experimental results are presented for both finite and infinite reference planes. An especially interesting application of this method is the reconstruction of architectural scenes with the reference plane taken as the plane at infinity which is visible via three orthogonal vanishing points. This is demonstrated by reconstructing the outside and inside (courtyard) of a building on the basis of 35 views in one single SVD.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937497",
        "reference_list": [
            {
                "year": "2001",
                "id": 56
            }
        ],
        "citation": {
            "ieee": 20,
            "other": 8,
            "total": 28
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Computer vision",
                "Layout",
                "Laboratories",
                "H infinity control",
                "Inverse problems",
                "Iterative algorithms",
                "Iterative methods",
                "Reconstruction algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "singular value decomposition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multi view reconstruction",
                "camera recovery",
                "3D points",
                "camera positions",
                "multiple perspective views",
                "reconstruction",
                "architectural scenes",
                "reference plane"
            ]
        },
        "id": 5,
        "cited_by": [
            {
                "year": "2003",
                "id": 158
            },
            {
                "year": "2001",
                "id": 56
            }
        ]
    },
    {
        "title": "Segmentation with pairwise attraction and repulsion",
        "authors": [
            "S.X. Yu",
            "J. Shi"
        ],
        "abstract": "We propose a method of image segmentation by integrating pairwise attraction and directional repulsion derived from local grouping and figure-ground cues. These two kinds of pairwise relationships are encoded in the real and imaginary parts of an Hermitian graph weight matrix, through which we can directly generalize the normalized cuts criterion. With bi-graph constructions, this method can be readily extended to handle nondirectional repulsion that captures dissimilarity. We demonstrate the use of repulsion in image segmentation with relative depth cues, which allows segmentation and figure-ground segregation to be computed simultaneously. As a general mechanism to represent the dual measures of attraction and repulsion, this method can also be employed to solve other constraint satisfaction and optimization problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937498",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 7,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Cognitive robotics",
                "Gratings",
                "Constraint optimization",
                "Cognition",
                "Visual perception",
                "Visual system",
                "Data mining",
                "Computer errors",
                "Bayesian methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "constraint theory"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "pairwise attraction",
                "directional repulsion",
                "local grouping",
                "figure-ground cues",
                "bi-graph constructions",
                "dissimilarity",
                "constraint satisfaction"
            ]
        },
        "id": 6,
        "cited_by": [
            {
                "year": "2007",
                "id": 3
            }
        ]
    },
    {
        "title": "Stereoscopic segmentation",
        "authors": [
            "A.J. Yezzi",
            "S. Soatto"
        ],
        "abstract": "We cast the problem of multiframe stereo reconstruction of a smooth shape as the global region segmentation of a collection of images of the scene. Dually, the problem of segmenting multiple calibrated images of an object becomes that of estimating the solid shape that gives rise to such images. We assume that the radiance has smooth statistics. This assumption covers Lambertian scenes with smooth or constant albedo as well as fine homogeneous textures, which are known challenges to stereo algorithms based on local correspondence. We pose the segmentation problem within a variational framework, and use fast level set methods to approximate the optimal solution numerically. Our algorithm does nor work in the presence of strong textures, where traditional reconstruction algorithms do. It enjoys significant robustness to noise under the assumptions it is designed for.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937499",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 10,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Shape",
                "Layout",
                "Stereo image processing",
                "Image reconstruction",
                "Solids",
                "Statistics",
                "Level set",
                "Reconstruction algorithms",
                "Noise robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image segmentation",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiframe stereo reconstruction",
                "global region segmentation",
                "stereo algorithms",
                "local correspondence"
            ]
        },
        "id": 7,
        "cited_by": [
            {
                "year": "2009",
                "id": 84
            },
            {
                "year": "2003",
                "id": 87
            }
        ]
    },
    {
        "title": "Gradient vector flow fast geodesic active contours",
        "authors": [
            "N. Paragios",
            "O. Mellina-Gottardo",
            "V. Ramesh"
        ],
        "abstract": "This paper proposes a new front propagation flow for boundary extraction. The proposed framework is inspired by the geodesic active contour model and leads to a paradigm that is relatively free from the initial curve position. Towards this end, it makes use of a recently introduced external boundary force, the gradient vector field that refers to a spatial diffusion of the boundary information. According to the proposed flow, the traditional boundary attraction term is replaced with a new force that guides the propagation to the object boundaries from both sides. This new geometric flow is implemented using a level set approach, thereby allowing dealing naturally with topological changes and important shape deformations. Moreover the level set motion equations are implemented using a recently introduced numerical approximation scheme, the Additive Operator Splitting Schema (AOS) which has a fast convergence rate and stable behavior. Encouraging experimental results are provided using real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937500",
        "reference_list": [],
        "citation": {
            "ieee": 25,
            "other": 11,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Active contours",
                "Deformable models",
                "Computer vision",
                "Level set",
                "Data visualization",
                "Educational institutions",
                "Shape",
                "Equations",
                "Image converters",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "edge detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geodesic active contours",
                "boundary extraction",
                "geometric flow",
                "convergence rate",
                "Additive Operator Splitting Schema"
            ]
        },
        "id": 8,
        "cited_by": []
    },
    {
        "title": "Blind removal of image non-linearities",
        "authors": [
            "H. Faird",
            "A.C. Popescu"
        ],
        "abstract": "This paper presents a technique for blindly removing image non-linearities in the absence of any calibration information or explicit knowledge of the imaging device. The basic approach exploits the fact that a non-linearity introduces specific higher-order correlations in the frequency domain (beyond second-order). These correlations can be detected using tools from polyspectral analysis. The non-linearities can then be estimated and removed by simply minimizing these correlations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937501",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Lenses",
                "Layout",
                "Speech analysis",
                "Frequency domain analysis",
                "Optical imaging",
                "Image processing",
                "Higher order statistics",
                "Computer science",
                "Educational institutions"
            ],
            "INSPEC: Controlled Indexing": [
                "image enhancement"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image non-linearities",
                "higher-order correlations",
                "frequency domain",
                "polyspectral analysis"
            ]
        },
        "id": 9,
        "cited_by": []
    },
    {
        "title": "A new imaging model",
        "authors": [
            "M. Aggarwal",
            "N. Ahuja"
        ],
        "abstract": "This paper has been prompted by observations of some anomalies in the performance of the standard imaging models (pin-hole, thin-lens and Gaussian thicklens), in the context of composing omnifocus images and estimating depth maps from a sequence of images. A closer examination of the models revealed that they assume a position of the aperture that conflicts with the designs of many available lenses. We have shown in this paper that the imaging geometry and photometric properties of an image are significantly influenced by the position of the aperture. This is confirmed by the discrepancies between observed mappings and those predicted by the models. We have therefore concluded that the current imaging models do not adequately represent practical imaging systems. We have proposed a new imaging model which overcomes these deficiencies and have given the associated mappings. The impact of this model on some common imaging scenarios is described, along with experimental verification of the better performance of the model on three real lenses.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937502",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Lenses",
                "Apertures",
                "Solid modeling",
                "Layout",
                "Photometry",
                "Image sensors",
                "Context modeling",
                "Predictive models",
                "Position measurement",
                "Optical design"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "imaging model",
                "standard imaging models",
                "Gaussian thicklens",
                "omnifocus images",
                "depth maps estimation",
                "images sequences",
                "imaging geometry",
                "photometric properties",
                "imaging scenarios"
            ]
        },
        "id": 10,
        "cited_by": []
    },
    {
        "title": "Precise sub-pixel estimation on area-based matching",
        "authors": [
            "M. Shimizu",
            "M. Okutomi"
        ],
        "abstract": "Area-based matching is a common procedure in various fields such as image-based measurements, stereo image processing, and fluidics. Sub-pixel estimation using parabola fitting over three points with their similarity measures is also a common method to increase the resolution of matching. However, few investigations or studies concerning the characteristics of this estimation have been reported. In this paper we have analyzed the sub-pixel estimation error by using an approximate image function and three kinds of similarity measures for matching. The results illustrate some inherently problematic phenomena such as so called \"pixel-locking\". In addition to this, we propose a new algorithm to greatly reduce sub-pixel estimation error. This method is independent from the similarity measure and quite simple to implement. The advantage of our novel method is confirmed through experiments using three different types of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937503",
        "reference_list": [],
        "citation": {
            "ieee": 31,
            "other": 15,
            "total": 46
        },
        "keywords": {
            "IEEE Keywords": [
                "Estimation error",
                "Optical imaging",
                "Area measurement",
                "Fluidics",
                "Histograms",
                "Pixel",
                "Transfer functions",
                "Apertures",
                "Information science",
                "Image processing"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image sequences",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "precise sub-pixel estimation",
                "area-based matching",
                "image-based measurements",
                "stereo image processing",
                "fluidics",
                "parabola fitting",
                "similarity measures",
                "approximate image function",
                "inherently problematic phenomena",
                "sub-pixel estimation error",
                "similarity measure"
            ]
        },
        "id": 11,
        "cited_by": [
            {
                "year": "2007",
                "id": 381
            },
            {
                "year": "2005",
                "id": 71
            }
        ]
    },
    {
        "title": "Region segmentation via deformable model-guided split and merge",
        "authors": [
            "L. Liu",
            "S. Sclaroff"
        ],
        "abstract": "An improved method for deformable shape-based image segmentation is described. Image regions are merged together and/or split apart, based on their agreement with an a priori distribution on the global deformation parameters for a shape template. Perceptually-motivated criteria are used to determine where/how to split regions, based on the local shape properties of the region group's bounding contour. A globally consistent interpretation is determined in part by the minimum description length principle. Experiments show that model-guided split and merge yields a significant improvement in segmention over a method that uses merging alone.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937504",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 2,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Shape",
                "Image segmentation",
                "Humans",
                "Merging",
                "Image retrieval",
                "Content based retrieval",
                "Computer science",
                "Object detection",
                "Indexing"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "merging",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "region segmentation",
                "deformable model-guided split and merge",
                "deformable shape-based image segmentation",
                "image regions",
                "a priori distribution",
                "global deformation parameters",
                "shape template",
                "local shape properties",
                "bounding contour",
                "minimum description length principle"
            ]
        },
        "id": 12,
        "cited_by": []
    },
    {
        "title": "Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images",
        "authors": [
            "Y.Y. Boykov",
            "M.-P. Jolly"
        ],
        "abstract": "In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as \"object\" or \"background\" to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both \"object\" and \"background\" segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937505",
        "reference_list": [],
        "citation": {
            "ieee": 1026,
            "other": 448,
            "total": 1474
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Cost function",
                "Visualization",
                "Educational institutions",
                "Topology",
                "Biomedical imaging",
                "Region 5",
                "Level set"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "computational geometry",
                "interactive systems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "interactive graph cuts",
                "optimal boundary & region segmentation",
                "N-D images",
                "interactive segmentation",
                "N-dimensional images",
                "hard constraints",
                "soft constraints",
                "globally optimal segmentation",
                "medical image segmentation",
                "Gestalt example",
                "max-flow algorithm"
            ]
        },
        "id": 13,
        "cited_by": [
            {
                "year": "2017",
                "id": 285
            },
            {
                "year": "2017",
                "id": 287
            },
            {
                "year": "2017",
                "id": 343
            },
            {
                "year": "2017",
                "id": 518
            },
            {
                "year": "2015",
                "id": 173
            },
            {
                "year": "2015",
                "id": 181
            },
            {
                "year": "2015",
                "id": 183
            },
            {
                "year": "2015",
                "id": 190
            },
            {
                "year": "2015",
                "id": 191
            },
            {
                "year": "2015",
                "id": 192
            },
            {
                "year": "2015",
                "id": 197
            },
            {
                "year": "2015",
                "id": 202
            },
            {
                "year": "2015",
                "id": 329
            },
            {
                "year": "2013",
                "id": 103
            },
            {
                "year": "2013",
                "id": 220
            },
            {
                "year": "2013",
                "id": 230
            },
            {
                "year": "2013",
                "id": 263
            },
            {
                "year": "2011",
                "id": 13
            },
            {
                "year": "2011",
                "id": 46
            },
            {
                "year": "2011",
                "id": 145
            },
            {
                "year": "2011",
                "id": 211
            },
            {
                "year": "2011",
                "id": 278
            },
            {
                "year": "2011",
                "id": 328
            },
            {
                "year": "2009",
                "id": 2
            },
            {
                "year": "2009",
                "id": 17
            },
            {
                "year": "2009",
                "id": 35
            },
            {
                "year": "2009",
                "id": 36
            },
            {
                "year": "2009",
                "id": 59
            },
            {
                "year": "2009",
                "id": 65
            },
            {
                "year": "2009",
                "id": 66
            },
            {
                "year": "2009",
                "id": 84
            },
            {
                "year": "2009",
                "id": 92
            },
            {
                "year": "2009",
                "id": 93
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2009",
                "id": 96
            },
            {
                "year": "2009",
                "id": 99
            },
            {
                "year": "2009",
                "id": 101
            },
            {
                "year": "2009",
                "id": 108
            },
            {
                "year": "2007",
                "id": 68
            },
            {
                "year": "2007",
                "id": 92
            },
            {
                "year": "2007",
                "id": 118
            },
            {
                "year": "2007",
                "id": 119
            },
            {
                "year": "2007",
                "id": 277
            },
            {
                "year": "2007",
                "id": 283
            },
            {
                "year": "2005",
                "id": 33
            },
            {
                "year": "2005",
                "id": 72
            },
            {
                "year": "2005",
                "id": 120
            },
            {
                "year": "2005",
                "id": 151
            },
            {
                "year": "2003",
                "id": 3
            }
        ]
    },
    {
        "title": "Simultaneous estimation of super-resolved intensity and depth maps from low resolution defocused observations of a scene",
        "authors": [
            "D. Rajan",
            "S. Chaudhuri"
        ],
        "abstract": "This paper presents a novel technique to simultaneously estimate the depth map and the focused image of a scene, both at a super-resolution, from its defocused observations. Given a sequence of low resolution, blurred and noisy observations of a static scene, the problem is to generate a dense depth map at a resolution higher than one that can be generated from the observations as well as to estimate the true focused, super-resolved image. Both the depth and the intensity maps are modeled as separate Markov random fields (MRF) and a maximum a posteriori estimation method is used to recover the high resolution fields. Since there is no relative motion between the scene and the camera, as is the case with most of the super-resolution and structure recovery techniques, we do away with the correspondence problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937506",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 1,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Layout",
                "Spatial resolution",
                "Image sensors",
                "Focusing",
                "Sensor arrays",
                "Degradation",
                "Lenses",
                "Apertures",
                "Noise generators"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "image sequences",
                "computer vision",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "simultaneous estimation",
                "super-resolved intensity",
                "depth maps",
                "low resolution defocused observations",
                "dense depth map",
                "super-resolved image",
                "Markov random fields",
                "maximum a posteriori estimation method",
                "structure recovery"
            ]
        },
        "id": 14,
        "cited_by": []
    },
    {
        "title": "Efficient dense depth estimation from dense multiperspective panoramas",
        "authors": [
            "Yin Li",
            "Chi-Keung Tang",
            "Heung-Yeung Shum"
        ],
        "abstract": "In this paper we study how to compute a dense depth map with panoramic field of view (e.g., 360 degrees) from multi-perspective panoramas. A dense sequence of multiperspective panoramas is used for better accuracy and reduced ambiguity by taking advantage of significant data redundancy. To speed up the reconstruction, we derive an approximate epipolar plane image that is associated with the planar sweeping camera setup, and use one-dimensional window for efficient matching. To address the aperture problem introduced by one-dimensional window matching, we keep a set of possible depth candidates from matching scores. These candidates are then passed to a novel two-pass tensor voting scheme to select the optimal depth. By propagating the continuity and uniqueness constraints non-iteratively in the voting process, our method produces high-quality reconstruction results even when significant occlusion is present. Experiments on challenging synthetic and real scenes demonstrate the effectiveness and efficacy of our method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937507",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 0,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Iterative algorithms",
                "Voting",
                "Layout",
                "Stereo image processing",
                "Councils",
                "Geometry",
                "Computer science",
                "Tensile stress",
                "Navigation"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "motion estimation",
                "computer graphics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "dense depth estimation",
                "dense multiperspective panoramas",
                "dense depth map",
                "panoramic field of view",
                "dense sequence",
                "data redundancy",
                "approximate epipolar plane image",
                "planar sweeping camera setup",
                "one-dimensional window",
                "aperture problem",
                "one-dimensional window matching",
                "depth candidates",
                "two-pass tensor voting scheme",
                "uniqueness constraints",
                "real scenes"
            ]
        },
        "id": 15,
        "cited_by": []
    },
    {
        "title": "Accurate catadioptric calibration for real-time pose estimation in room-size environments",
        "authors": [
            "D.G. Aliaga"
        ],
        "abstract": "Omnidirectional video cameras are becoming increasingly popular in computer vision. One family of these cameras uses a catadioptric system with a paraboloidal mirror and an orthographic lens to produce an omnidirectional image with a single center-of-projection. In this paper, we develop a novel calibration model that we combine with a beacon-based pose estimation algorithm. Our approach relaxes the assumption of an ideal paraboloidal catadioptric system and achieves an order of magnitude improvement in pose estimation accuracy compared to calibration with an ideal camera model. Our complete standalone system, placed on a radio-controlled motorized cart, moves in a room-size environment, capturing high-resolution frames to disk and recovering camera pose with an average error of 0.56% in a region 15 feet in diameter.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937508",
        "reference_list": [],
        "citation": {
            "ieee": 29,
            "other": 12,
            "total": 41
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Mirrors",
                "Cameras",
                "Lenses",
                "Radio control",
                "Computer vision",
                "Surges",
                "Image reconstruction",
                "Navigation",
                "Apertures"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "calibration",
                "video cameras",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Accurate catadioptric calibration",
                "real-time pose estimation",
                "room-size environments",
                "omnidirectional video cameras",
                "computer vision",
                "paraboloidal mirror",
                "orthographic lens",
                "center-of-projection",
                "calibration model",
                "beacon-based pose estimation algorithm",
                "pose estimation",
                "ideal camera model"
            ]
        },
        "id": 16,
        "cited_by": [
            {
                "year": "2005",
                "id": 200
            },
            {
                "year": "2005",
                "id": 201
            },
            {
                "year": "2003",
                "id": 101
            },
            {
                "year": "2003",
                "id": 176
            }
        ]
    },
    {
        "title": "Omni-rig: linear self-recalibration of a rig with varying internal and external parameters",
        "authors": [
            "A. Zomet",
            "L. Wolf",
            "A. Shashua"
        ],
        "abstract": "We describe the principles of building a moving vision platform (a Rig) that once calibrated can thereon self-adjust to changes in its internal configuration and maintain an Euclidean representation of the 3D world using only projective measurements. We term this calibration paradigm \"Omni-Rig\". We assume that after calibration the cameras may change critical elements of their configuration, including internal parameters and centers of projection. Theoretically we show that knowing only the rotations between a set of cameras is sufficient for Euclidean calibration even with varying internal parameters and unknown translations. No other information of the world is required.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937509",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 5,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Computer science",
                "Maintenance engineering",
                "Image recognition",
                "Computer graphics",
                "Machinery",
                "Nonlinear equations"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "omni-rig",
                "linear self-recalibration",
                "external parameters",
                "internal parameters",
                "moving vision platform",
                "Euclidean representation",
                "3D world"
            ]
        },
        "id": 17,
        "cited_by": []
    },
    {
        "title": "Camera calibration and 3D reconstruction from single images using parallelepipeds",
        "authors": [
            "M. Wilczkowiak",
            "E. Boyer",
            "P. Sturm"
        ],
        "abstract": "In this paper parallelepipeds and their use in camera calibration and 3D reconstruction processes are studied. Parallelepipeds naturally characterize rigidity constraints present in a scene, such as parallelism and orthogonality. A subclass of parallelepipeds-the cuboids-has been frequently used over the past to partially calibrate cameras. However, the full potential of parallelepipeds, in camera calibration as well as in scene reconstruction, has never been clearly established. We propose a new framework for the use of parallelepipeds which is based on an extensive study of this potential. In particular, we exhibit the complete duality that exists between the intrinsic metric characteristics of a parallelepiped and the intrinsic parameters of a camera. Our framework allows to fully exploit parallelepipeds and thus overcomes several limitations of calibration approaches based on cuboids. To illustrate this framework, we present an original and very efficient interactive method for 3D reconstruction from single images. This method allows to quickly build a scene model from a single uncalibrated image.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937510",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 16,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Image reconstruction",
                "Layout",
                "Application software",
                "Parallel processing",
                "Computer vision",
                "Parameter estimation",
                "Transmission line matrix methods",
                "Augmented reality"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image reconstruction",
                "interactive systems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera calibration",
                "3D reconstruction",
                "single images",
                "parallelepipeds",
                "rigidity constraints",
                "cuboids",
                "scene reconstruction",
                "interactive method",
                "scene model"
            ]
        },
        "id": 18,
        "cited_by": [
            {
                "year": "2007",
                "id": 302
            },
            {
                "year": "2003",
                "id": 20
            }
        ]
    },
    {
        "title": "Flux maximizing geometric flows",
        "authors": [
            "A. Vasilevskiy",
            "K. Siddiqi"
        ],
        "abstract": "Several geometric active contour models have been proposed for segmentation in computer vision. The essential idea is to evolve a curve (in 2D) or a surface (in 3D) under constraints from image forces so that it clings to features of interest in an intensity image. Recent variations on this theme take into account properties of enclosed regions and allow for multiple curves or surfaces to be simultaneously represented. However, it is not clear how to apply these techniques to images of low contrast elongated structures, such as those of blood vessels. To address this problem we derive the gradient flow which maximizes the rate of increase of flux of an auxiliary vector field through a curve or surface. The calculation leads to a simple and elegant interpretation which is essentially parameter free. We illustrate its advantages with level-set based segmentations of 2D and 3D MRA images of blood vessels.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937511",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 7,
            "total": 19
        },
        "keywords": {
            "IEEE Keywords": [
                "Blood vessels",
                "Biomedical imaging",
                "Image segmentation",
                "Active contours",
                "Computer vision",
                "Shape",
                "Computer science",
                "Machine intelligence",
                "Blood flow",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "blood vessels",
                "image segmentation",
                "biomedical MRI"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "flux maximizing geometric flows",
                "geometric active contour models",
                "segmentation",
                "computer vision",
                "enclosed regions",
                "multiple curves",
                "gradient flow",
                "auxiliary vector field",
                "MRA images",
                "blood vessels"
            ]
        },
        "id": 19,
        "cited_by": []
    },
    {
        "title": "Dimensional analysis of image motion",
        "authors": [
            "M.S. Langer",
            "R. Mann"
        ],
        "abstract": "Studies of image motion typically address motion categories on a case-by-case basis. Examples include a moving point, a moving contour, or a 2D optical flow field. The typical assumption made in these studies is that there is a unique velocity at each moving point in the image. In this paper we relax this assumption. We introduce a broader set of motion categories in which the set of motions at a moving point can be 0-D, 1-D, or 2-D. We consider one new motion category in detail, which we call optical snow. This motion category occurs, for example, when an observer translates relative to a massively cluttered scene. Examples include the motion seen by an observer moving through bushes, or falling snow seen by a stationary observer. Optical snow is characterized by a 1-D set of velocities at each moving point and as such, it cannot be analyzed using a classical computational method such as optical flow. We introduce a technique for analyzing optical snow which is based on a bow tie signature of the motion in the frequency domain. We demonstrate the effectiveness of the technique using both synthetic and real image sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937512",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image analysis",
                "Image motion analysis",
                "Motion analysis",
                "Snow",
                "Computer science",
                "Pixel",
                "Image edge detection",
                "Layout",
                "Optical computing",
                "Frequency domain analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image motion",
                "motion categories",
                "optical snow",
                "image sequences"
            ]
        },
        "id": 20,
        "cited_by": []
    },
    {
        "title": "Multiple motion scene reconstruction from uncalibrated views",
        "authors": [
            "M. Han",
            "T. Kanade"
        ],
        "abstract": "We describe a reconstruction method of multiple motion scenes, which are the scenes containing multiple moving objects, from uncalibrated views. Assuming that the objects are moving with constant velocities, the method recovers the scene structure, the trajectories of the moving objects, the camera motion and the camera intrinsic parameters (except skews) simultaneously. The number of the moving objects is automatically detected without prior motion segmentation. The method is based on a unified geometrical representation of the static scene and the moving objects. It first performs a projective reconstruction using a bilinear factorization algorithm, and then converts the projective solution to a Euclidean one by enforcing metric constraints. Experimental results on synthetic and real images are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937513",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 11,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Shape",
                "Image reconstruction",
                "Reconstruction algorithms",
                "Image sequences",
                "Robot vision systems",
                "Object detection",
                "Motion detection",
                "Motion segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "motion estimation",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene reconstruction",
                "uncalibrated views",
                "multiple motion scenes",
                "reconstruction method",
                "camera motion",
                "projective reconstruction",
                "bilinear factorization"
            ]
        },
        "id": 21,
        "cited_by": []
    },
    {
        "title": "Very high accuracy velocity estimation using orientation tensors, parametric motion, and simultaneous segmentation of the motion field",
        "authors": [
            "G. Farneback"
        ],
        "abstract": "In a previous paper, the author presented a new velocity estimation algorithm, using orientation tensors and parametric motion models to provide both fast and accurate results. One of the tradeoffs between accuracy and speed was that no attempts were made to obtain regions of coherent motion when estimating the parametric models. In this paper we show how this can be improved by doing a simultaneous segmentation of the motion field. The resulting algorithm is slower than the previous one, but more accurate. This is shown by evaluation on the well-known Yosemite sequence, where already the previous algorithm showed an accuracy which was substantially better than for earlier published methods. This result has now been improved further.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937514",
        "reference_list": [],
        "citation": {
            "ieee": 37,
            "other": 15,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Tensile stress",
                "Image segmentation",
                "Filtering",
                "Stacking",
                "Image sequences",
                "Spatiotemporal phenomena",
                "Computer vision",
                "Laboratories",
                "Parametric statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "velocity estimation",
                "orientation tensors",
                "parametric motion",
                "simultaneous segmentation",
                "motion field",
                "Farneback (2000)"
            ]
        },
        "id": 22,
        "cited_by": [
            {
                "year": "2015",
                "id": 338
            },
            {
                "year": "2003",
                "id": 24
            },
            {
                "year": "2003",
                "id": 116
            }
        ]
    },
    {
        "title": "Estimation and interpretation of discontinuities in optical flow fields",
        "authors": [
            "M. Middendorf",
            "H.-H. Nagel"
        ],
        "abstract": "A systematic categorization of an adaptively determined local estimate for an OF-vector allows to detect non-local 'walls of discontinuities' which establish an essentially closed boundary around images of objects whose motion differs from that of foreground and background. The estimation process has been refined to the point where occasionally observable-initially counter-intuitive-discontinuity blobs inside a region corresponding to the image of a moving object found an acceptable explanation. Segmentation results obtained on this basis for different real-world image sequences will be used to illustrate the approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937515",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 8,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Image segmentation",
                "Image sequences",
                "Layout",
                "Workstations",
                "Optical recording",
                "Motion estimation",
                "Object detection",
                "Motion detection",
                "Surveillance"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "optical flow fields",
                "systematic categorization",
                "walls of discontinuities",
                "moving object",
                "segmentation",
                "image sequences"
            ]
        },
        "id": 23,
        "cited_by": []
    },
    {
        "title": "Ambiguous configurations for the 1D structure and motion problem",
        "authors": [
            "F. Kahl",
            "K. Astrom"
        ],
        "abstract": "In this paper we investigate, determine and classify the critical configurations for solving structure and motion problems for 1D retina vision. We give a complete categorization of all ambiguous configurations for a 1D perspective camera irrespective of the number of points and views. Both calibrated and uncalibrated cameras are considered. Several examples and illustrations are provided to explain the results and to provide geometrical insight.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937516",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Retina",
                "Vehicles",
                "Motion estimation",
                "Switches",
                "Computer vision",
                "Shape",
                "Inverse problems",
                "Councils",
                "Navigation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "motion estimation",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "1D retina vision",
                "structure",
                "motion",
                "ambiguous configurations",
                "1D perspective camera"
            ]
        },
        "id": 24,
        "cited_by": []
    },
    {
        "title": "Shadow carving",
        "authors": [
            "S. Savarese",
            "H. Rushmeier",
            "F. Bernardini",
            "P. Perona"
        ],
        "abstract": "The shape of an object may be estimated by observing the shadows on its surface. We present a method that is robust with respect to a conservative classification of shadow regions. Assuming that a conservative estimate of the object shape is available, we analyze images of the object illuminated with known point light sources taken from known camera locations. We adjust our surface estimate using the shadow regions to produce a refinement that is still a conservative estimate. A proof of correctness is provided. No assumptions about the object topology are made, although any tangent plane discontinuities over the object's surface are supposed to be detectable. An implementation and some experimental results are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937517",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 3,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Light sources",
                "Robustness",
                "Cameras",
                "Image analysis",
                "Topology",
                "Object detection",
                "Surface texture",
                "Optical collimators",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shadow carving",
                "classification",
                "shadow regions",
                "surface estimate",
                "point light sources",
                "camera locations"
            ]
        },
        "id": 25,
        "cited_by": []
    },
    {
        "title": "A linear dual-space approach to 3D surface reconstruction from occluding contours using algebraic surfaces",
        "authors": [
            "K. Kang",
            "J.-P. Tarel",
            "R. Fishman",
            "D. Cooper"
        ],
        "abstract": "We present a linear approach to the 3D reconstruction problem from occluding contours using algebraic surfaces. The problem of noise and missing data in the occluding contours extracted from the images leads us to this approach. Our approach is based first on the intensive use of the duality property between 3D points and tangent planes, and second on the algebraic representation of 3D surfaces by implicit polynomials of degree 2 and higher.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937518",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Cameras",
                "Surface treatment",
                "Surface fitting",
                "Data mining",
                "Polynomials",
                "Noise measurement",
                "Noise robustness",
                "Bayesian methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D surface reconstruction",
                "occluding contours",
                "algebraic surfaces",
                "3D reconstruction"
            ]
        },
        "id": 26,
        "cited_by": []
    },
    {
        "title": "Stochastic road shape estimation",
        "authors": [
            "B. Southall",
            "C.J. Taylor"
        ],
        "abstract": "We describe a new system for estimating road shape ahead of a vehicle for the purpose of driver assistance. The method utilises a single on board colour camera, together with inertial and velocity information, to estimate both the position of the host car with respect to the lane it is following and also the width and curvature of the lane ahead at distances of up to 80 metres. The system's image processing extracts a variety of different styles of lane markings from road imagery, and is able to compensate for a range of lighting conditions. Road shape and car position are estimated using a particle filter. The system, which runs at 10.5 frames per second, has been applied with some success to several hours' worth of data captured from highways under varying imaging conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937519",
        "reference_list": [],
        "citation": {
            "ieee": 85,
            "other": 20,
            "total": 105
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Shape",
                "Road vehicles",
                "Cameras",
                "Vehicle driving",
                "Data mining",
                "Road transportation",
                "Radar imaging",
                "Radar tracking",
                "Radar detection"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "traffic engineering computing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "road shape estimation",
                "driver assistance",
                "on board colour camera",
                "image processing",
                "lane markings",
                "road imagery"
            ]
        },
        "id": 27,
        "cited_by": []
    },
    {
        "title": "Motion estimation from disparity images",
        "authors": [
            "D. Demirdjian",
            "T. Darrell"
        ],
        "abstract": "A new method for 3D rigid motion estimation from stereo is proposed in this paper. The appealing feature of this method is that it directly uses the disparity images obtained from stereo matching. We assume that the stereo rig has parallel cameras and show, in that case, the geometric and topological properties of the disparity images. Then we introduce a rigid transformation (called d-motion) that maps two disparity images of a rigidly moving object. We show how it is related to the Euclidean rigid motion and a motion estimation algorithm is derived. We show with experiments that our approach is simple and more accurate than standard approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937520",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 5,
            "total": 33
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Stereo vision",
                "Image reconstruction",
                "Computer vision",
                "Stereo image processing",
                "Cameras",
                "Layout",
                "Artificial intelligence",
                "Laboratories",
                "Motion detection"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "stereo image processing",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion estimation",
                "stereo",
                "stereo matching",
                "disparity images",
                "parallel cameras",
                "Euclidean rigid motion",
                "motion estimation algorithm",
                "rigid transformation",
                "3D rigid motion estimation"
            ]
        },
        "id": 28,
        "cited_by": []
    },
    {
        "title": "Separation of multiple objects in motion images by clustering",
        "authors": [
            "K. Inoue",
            "K. Urahama"
        ],
        "abstract": "A method is presented for estimating the number of rigid objects moving independently and separating the feature points tracked on a motion image into individual objects by clustering. In the method, feature points are firstly mapped into a low dimensional space suitable for grouping them into each object. In this low dimensional space, clusters are extracted sequentially by a graph spectral method. The number of clusters i.e. objects can be estimated on the basis of the variation in the cohesiveness of extracted clusters. We show by numerical experiments that the present method is robust to moderate measurement noises and distortion by perspective projection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937521",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Transmission line matrix methods",
                "Noise robustness",
                "Distortion measurement",
                "Noise measurement",
                "Multi-stage noise shaping",
                "Motion estimation",
                "Data mining",
                "Image sequences",
                "Shape measurement",
                "Coordinate measuring machines"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "image sequences",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion images",
                "clustering",
                "multiple objects",
                "motion image",
                "factorization method",
                "image sequence"
            ]
        },
        "id": 29,
        "cited_by": []
    },
    {
        "title": "New perspectives on geometric reflection theory from rough surfaces",
        "authors": [
            "A.J. Lundberg",
            "L.B. Wolff",
            "D.A. Socolinsky"
        ],
        "abstract": "Exact wave theories of specular reflectance from rough surfaces are computationally intractable thus motivating the practical need for geometric reflectance models which treat only the geometric ray nature of light reflection. The cornerstone of geometric reflectance modeling from rough surfaces in computer vision and computer graphics over the past two decades has been the Torrance-Sparrow model. This model has worked well as an intuitive description of rough surfaces as a collection of planar Fresnel reflectors called microfacets together with the concept of geometric attenuation for light which is obscured during reflection under an assumed rough surface geometry. Experimental data and analysis show that the current conceptualization of how specularly reflected light rays geometrically interact with rough surfaces needs to be seriously revised. The Torrance-Sparrow model while in qualitative agreement with specular reflection from rough surfaces is seen to be quantitatively inaccurate. Furthermore there are conceptual inconsistencies upon which derivation of this reflectance model is based. We show how significant quantitative improvement can be achieved for a geometric reflectance model by making some fundamental revisions to notions of microfacet probability distributions and geometric attenuation. Work is currently undergoing, to relate physical surface reconstructions from Atomic Force Microscope data to reflectance data from these same surfaces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937522",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 3,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Rough surfaces",
                "Surface roughness",
                "Reflectivity",
                "Solid modeling",
                "Optical reflection",
                "Surface treatment",
                "Fresnel reflection",
                "Surface reconstruction",
                "Atomic force microscopy",
                "Surface waves"
            ],
            "INSPEC: Controlled Indexing": [
                "rough surfaces",
                "computer vision",
                "computer graphics",
                "surface reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometric reflection theory",
                "rough surfaces",
                "exact wave theories",
                "specular reflectance",
                "geometric ray nature",
                "light reflection",
                "computer vision",
                "computer graphics",
                "Torrance-Sparrow model",
                "planar Fresnel reflectors",
                "microfacets",
                "geometric attenuation",
                "qualitative agreement",
                "geometric reflectance model"
            ]
        },
        "id": 30,
        "cited_by": []
    },
    {
        "title": "Illumination insensitive eigenspaces",
        "authors": [
            "H. Bischof",
            "H. Wildenauer",
            "A. Leonardis"
        ],
        "abstract": "Variations in illumination can have a dramatic effect on the appearance of an object in an image. In this paper we propose how to deal with illumination variations in eigenspace methods. We demonstrate that the eigenimages obtained by a training set under a single illumination condition (ambient light) can be used for recognition of objects taken under different illumination conditions. The major idea is to incorporate a set of gradient based filter banks into the eigenspace recognition framework. This can be achieved since the eigenimage coefficients are invariant for linearly filtered images (input and eigenimages). To achieve further illumination insensitivity we devised a robust procedure for coefficient recovery. The proposed approach has been extensively evaluated on a set of 2160 images and the results were compared to other approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937523",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 7,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Nonlinear filters",
                "Principal component analysis",
                "Image coding",
                "Object recognition",
                "Filtering",
                "Filter bank",
                "Image recognition",
                "Information science",
                "Image sampling"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "illumination insensitive eigenspaces",
                "illumination variations",
                "eigenimages",
                "objects recognition",
                "gradient based filter banks",
                "eigenspace recognition framework",
                "linearly filtered images",
                "robust procedure"
            ]
        },
        "id": 31,
        "cited_by": [
            {
                "year": "2003",
                "id": 35
            }
        ]
    },
    {
        "title": "Color constancy using KL-divergence",
        "authors": [
            "C. Rosenberg",
            "M. Hebert",
            "S. Thrun"
        ],
        "abstract": "Color is a useful feature for machine vision tasks. However its effectiveness is often limited by the fact that the measured pixel values in a scene are influenced by both object surface reflectance properties and incident illumination. Color constancy algorithms attempt to compute color features which are invariant of the incident illumination by estimating the parameters of the global scene illumination and factoring out its effect. A number of recently developed algorithms utilize statistical methods to estimate the maximum likelihood values of the illumination parameters. This paper details the use of KL-divergence as a means of selecting estimated illumination parameter values. We provide experimental results demonstrating the usefulness of the KL-divergence technique for accurately estimating the global illumination parameters of real world images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937524",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Cameras",
                "Layout",
                "Image sensors",
                "Sensor phenomena and characterization",
                "Pixel",
                "Reflectivity",
                "Machine vision",
                "Parameter estimation",
                "Surface waves"
            ],
            "INSPEC: Controlled Indexing": [
                "statistical analysis",
                "computer vision",
                "visual databases",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "color constancy",
                "KL-divergence",
                "machine vision",
                "measured pixel values",
                "object surface reflectance",
                "incident illumination",
                "color features",
                "global scene illumination",
                "statistical methods",
                "maximum likelihood values",
                "real world images"
            ]
        },
        "id": 32,
        "cited_by": [
            {
                "year": "2013",
                "id": 237
            },
            {
                "year": "2009",
                "id": 25
            }
        ]
    },
    {
        "title": "Smarter presentations: exploiting homography in camera-projector systems",
        "authors": [
            "R. Sukthankar",
            "R.G. Stockton",
            "M.D. Mullin"
        ],
        "abstract": "Standard presentation systems consisting of a laptop connected to a projector suffer from two problems: (1) the projected image appears distorted (keystoned) unless the projector is precisely aligned to the projection screen; (2) the speaker is forced to interact with the computer rather than the audience. This paper shows how the addition of an uncalibrated camera, aimed at the screen, solves both problems. Although the locations, orientations and optical parameters of the camera and projector are unknown, the projector-camera system calibrates itself by exploiting the homography between the projected slide and the camera image. Significant improvements are possible over passively calibrating systems since the projector actively manipulates the environment by placing feature points into the scene. For instance, using a low-resolution (160/spl times/120) camera, we can achieve an accuracy of /spl plusmn/3 pixels in a 1024/spl times/768 presentation slide. The camera-projector system infers models for the projector-to-camera and projector-to-screen mappings in order to provide two major benefits.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937525",
        "reference_list": [],
        "citation": {
            "ieee": 83,
            "other": 31,
            "total": 114
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical distortion",
                "Cameras",
                "Portable computers",
                "Mice",
                "Robots",
                "Layout",
                "Geometry",
                "Hardware",
                "Java",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "technical presentation",
                "business graphics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "smarter presentations",
                "homography",
                "camera-projector systems",
                "orientations",
                "optical parameters",
                "projector-camera system"
            ]
        },
        "id": 33,
        "cited_by": [
            {
                "year": "2001",
                "id": 215
            }
        ]
    },
    {
        "title": "3D object recognition using shape similiarity-based aspect graph",
        "authors": [
            "C.M. Cyr",
            "B.B. Kimia"
        ],
        "abstract": "We present an aspect-graph approach to 3D object recognition where the definition of an aspect is motivated by its role in the subsequent recognition step. Specifically, we measure the similarity between two views by a 2D shape metric of similarity measuring the distance between the projected segmented shapes of the 3D object. This endows the viewing sphere with a metric which is used to group similar views into aspects, and to represent each aspect by a prototype. The same shape similarity metric is then used to rate the similarity between unknown views of unknown objects and stored prototypes to identify the object and its pose. The performance of this approach on a database of 18 objects each viewed in five degree increments along the ground viewing plane is demonstrated.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937526",
        "reference_list": [
            {
                "year": "2001",
                "id": 102
            }
        ],
        "citation": {
            "ieee": 45,
            "other": 14,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Prototypes",
                "Shape measurement",
                "Principal component analysis",
                "Visual databases",
                "Humans",
                "Visual system",
                "Solid modeling",
                "Deformable models",
                "Psychology"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "computer graphics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object recognition",
                "shape similarity-based aspect graph",
                "aspect-graph approach",
                "2D shape metric",
                "segmented shapes",
                "3D object",
                "viewing sphere",
                "shape similarity metric",
                "ground viewing plane"
            ]
        },
        "id": 34,
        "cited_by": [
            {
                "year": "2011",
                "id": 3
            }
        ]
    },
    {
        "title": "Invariant mixture recognition in hyperspectral images",
        "authors": [
            "Pei-Hsiu Suen",
            "G. Healey"
        ],
        "abstract": "We present an algorithm for identifying linear mixtures of a specified set of materials in 0.4-2.5 /spl mu/m airborne imaging spectrometer data. The algorithm is invariant to the illumination and atmospheric conditions and the relative amounts of the specified materials within a pixel. Only the spectral reflectance functions for the specified materials are required by the algorithm. Invariance over illumination and atmosphere conditions is achieved by incorporating a physical model for scene variability in the constrained optimization formulation. The algorithm also computes estimates of the amounts of the specified materials in identified mixtures. We demonstrate the effectiveness of the algorithm using real and synthetic HYDICE imagery acquired over a range of conditions and altitudes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937527",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Image recognition",
                "Hyperspectral imaging",
                "Atmospheric modeling",
                "Reflectivity",
                "Hyperspectral sensors",
                "Pixel",
                "Layout",
                "Lighting",
                "Image analysis",
                "Atmospheric measurements"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "optimisation",
                "spectrometers"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "invariant mixture recognition",
                "hyperspectral images",
                "linear mixtures",
                "airborne imaging spectrometer data",
                "atmospheric conditions",
                "physical model",
                "constrained optimization",
                "HYDICE imagery"
            ]
        },
        "id": 35,
        "cited_by": []
    },
    {
        "title": "Combining single view recognition and multiple view stereo for architectural scenes",
        "authors": [
            "A.R. Dick",
            "P.H.S. Torr",
            "S.J. Ruffle",
            "R. Cipolla"
        ],
        "abstract": "This paper describes a structure from motion and recognition paradigm for generating 3D models from 2D sets of images. In particular we consider the domain of architectural photographs. A model based approach is adopted with the architectural model built from a \"Lego kit\" of parameterised parts. The approach taken is different from traditional stereo or shape from X approaches in that identification of the parameterised components (such as windows, doors, buttresses etc) from one image is combined with parallax information in order to generate the 3D model. This model based approach has two main benefits: first, it allows the inference of shape and texture where the evidence from the images is weak; and second, it recovers not only geometry and texture but also an interpretation of the model, which can be used for automatic enhancement techniques such as the application of reflective textures to windows.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937528",
        "reference_list": [],
        "citation": {
            "ieee": 23,
            "other": 11,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Shape",
                "Geometry",
                "Architecture",
                "Costs",
                "Markov random fields",
                "Constraint optimization",
                "Statistics",
                "Data mining",
                "Books"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single view recognition",
                "multiple view stereo",
                "architectural scenes",
                "structure from motion",
                "3D models",
                "2D sets",
                "model based approach",
                "Lego kit"
            ]
        },
        "id": 36,
        "cited_by": [
            {
                "year": "2003",
                "id": 132
            }
        ]
    },
    {
        "title": "Self-supervised learning for object recognition based on kernel discriminant-EM algorithm",
        "authors": [
            "Y. Wu",
            "T.S. Huang",
            "K. Toyama"
        ],
        "abstract": "It is often tedious and expensive to label large training data sets for learning-based object recognition systems. This problem could be alleviated by self-supervised learning techniques, which take a hybrid of labeled and unlabeled training data to learn classifiers. Discriminant-EM (D-EM) proposed a framework for such tasks and current D-EM algorithm employed linear discriminant analysis. However, the algorithm is limited by its dependence on linear transformations. This paper extends the linear D-EM to nonlinear kernel algorithm, Kernel D-EM, based on kernel multiple discriminant analysis (KMDA). KMDA provides better ability to simplify the probabilistic structures of data distributions in a discrimination space. We propose two novel data-sampling schemes for efficient training of kernel discriminants. Experimental results show that classifiers using KMDA learning compare with SVM performance on standard benchmark tests, and that Kernel D-EM outperforms a variety of supervised and semi-supervised learning algorithms for a hand-gesture recognition task and fingertip tracking task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937529",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 7,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Kernel",
                "Performance analysis",
                "Linear discriminant analysis",
                "Face recognition",
                "Sampling methods",
                "Training data",
                "Ear"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "self-supervised learning",
                "object recognition",
                "kernel discriminant-EM algorithm",
                "training data sets",
                "linear discriminant analysis",
                "kernel multiple discriminant analysis",
                "data-sampling schemes",
                "learning algorithms"
            ]
        },
        "id": 37,
        "cited_by": []
    },
    {
        "title": "Learning inhomogeneous Gibbs model of faces by minimax entropy",
        "authors": [
            "Ce Liu",
            "Song Chun Zhu",
            "Heung-Yeung Shum"
        ],
        "abstract": "In this paper we propose a novel inhomogeneous Gibbs model by the minimax entropy principle, and apply it to face modeling. The maximum entropy principle generalizes the statistical properties of the observed samples and results in the Gibbs distribution, while the minimum entropy principle makes the learnt distribution close to the observed one. To capture the fine details of a face, an inhomogeneous Gibbs model is derived to learn the local statistics of facial feature paints. To alleviate the high dimensionality problem of face models, we propose to learn the distribution in a subspace reduced by principal component analysis or PCA. We demonstrate that our model effectively captures important and subtle non-Gaussian face patterns and efficiently generates good face models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937530",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 0,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Minimax techniques",
                "Entropy",
                "Face detection",
                "Principal component analysis",
                "Statistical distributions",
                "Deformable models",
                "Face recognition",
                "Computer vision",
                "Humans",
                "History"
            ],
            "INSPEC: Controlled Indexing": [
                "principal component analysis",
                "computer vision",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "inhomogeneous Gibbs model learning",
                "minimax entropy",
                "face modeling",
                "statistical properties",
                "principal component analysis"
            ]
        },
        "id": 38,
        "cited_by": [
            {
                "year": "2007",
                "id": 177
            }
        ]
    },
    {
        "title": "Stochastic processes in vision: from Langevin to Beltrami",
        "authors": [
            "N.A. Sochen"
        ],
        "abstract": "Diffusion processes which are widely used in low level vision are presented as a result of an underlying stochastic process. The short-time non-linear diffusion is interpreted as a Fokker-Planck equation which governs the evolution in time of a probability distribution for a Brownian motion on a Riemannian surface. The non linearity of the diffusion has a direct relation to the geometry of the surface. A short time kernel to the diffusion as well as generalizations are found.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937531",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Nonlinear equations",
                "Diffusion processes",
                "Kernel",
                "Laplace equations",
                "Image analysis",
                "Mathematics",
                "Probability distribution",
                "Linearity",
                "Information geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "stochastic processes",
                "Brownian motion",
                "probability",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stochastic processes",
                "diffusion processes",
                "low level vision",
                "Fokker-Planck equation",
                "probability distribution",
                "Brownian motion",
                "Riemannian surface"
            ]
        },
        "id": 39,
        "cited_by": []
    },
    {
        "title": "Topology free hidden Markov models: application to background modeling",
        "authors": [
            "B. Stenger",
            "V. Ramesh",
            "N. Paragios",
            "F. Coetzee",
            "J.M. Buhmann"
        ],
        "abstract": "Hidden Markov models (HMMs) are increasingly being used in computer vision for applications such as: gesture analysis, action recognition from video, and illumination modeling. Their use involves an off-line learning step that is used as a basis for on-line decision making (i.e. a stationarity assumption on the model parameters). But, real-world applications are often non-stationary in nature. This leads to the need for a dynamic mechanism to learn and update the model topology as well as its parameters. This paper presents a new framework for HMM topology and parameter estimation in an online, dynamic fashion. The topology and parameter estimation is posed as a model selection problem with an MDL prior. Online modifications to the topology are made possible by incorporating a state splitting criterion. To demonstrate the potential of the algorithm, the background modeling problem is considered. Theoretical validation and real experiments are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937532",
        "reference_list": [],
        "citation": {
            "ieee": 64,
            "other": 26,
            "total": 90
        },
        "keywords": {
            "IEEE Keywords": [
                "Hidden Markov models",
                "Topology",
                "Application software",
                "Parameter estimation",
                "Signal processing algorithms",
                "State estimation",
                "Visualization",
                "Computer science",
                "Computer vision",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "hidden Markov models",
                "computer vision",
                "parameter estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "topology free hidden Markov models",
                "background modeling",
                "computer vision",
                "gesture analysis",
                "action recognition",
                "illumination modeling",
                "off-line learning step",
                "parameter estimation",
                "model selection problem",
                "state splitting criterion"
            ]
        },
        "id": 40,
        "cited_by": [
            {
                "year": "2007",
                "id": 77
            },
            {
                "year": "2003",
                "id": 170
            }
        ]
    },
    {
        "title": "What value covariance information in estimating vision parameters?",
        "authors": [
            "M.J. Brooks",
            "W. Chojnacki",
            "D. Gawley",
            "A. van den Hengel"
        ],
        "abstract": "Many parameter estimation methods used in computer vision are able to utilise covariance information describing the uncertainty of data measurements. This paper considers the value of this information to the estimation process when applied to measured image point locations. Covariance matrices are first described and a procedure is then outlined whereby covariances may be associated with image features located via a measurement process. An empirical study is made of the conditions under which covariance information enables generation of improved parameter estimates. Also explored is the extent to which the noise should be anisotropic and inhomogeneous if improvements are to be obtained over covariance-free methods. Critical in this is the devising of synthetic experiments under which noise conditions can be precisely controlled. Given that covariance information is, in itself, subject to estimation error tests are also undertaken to determine the impact of imprecise covariance information upon the quality of parameter estimates. Finally, an experiment is carried out to assess the value of covariances in estimating the fundamental matrix from real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937533",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 6,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Parameter estimation",
                "Covariance matrix",
                "Pollution measurement",
                "Gaussian distribution",
                "Anisotropic magnetoresistance",
                "Measurement errors",
                "Computer science",
                "Measurement uncertainty",
                "Estimation error",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "parameter estimation",
                "computer vision",
                "covariance matrices",
                "Gaussian distribution"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "covariance information",
                "vision parameters estimation",
                "computer vision",
                "image features",
                "measurement process",
                "real images"
            ]
        },
        "id": 41,
        "cited_by": []
    },
    {
        "title": "Model-based initialisation of vehicle tracking: dependency on illumination",
        "authors": [
            "H. Leuck",
            "H.-H. Nagel"
        ],
        "abstract": "Although a model-based vehicle tracking approach offers the promise to be more reliable than a purely data-driven one, based on the additional knowledge brought to bear during the tracking phase, a suitable initialisation of the tracking phase still presents considerable problems. Part of these difficulties are related to the appropriate choice of assumptions concerning the prevailing illumination of the recorded scene. We present an approach to automatically detect elongated shadow-casting structures in the scene and exploit these structures to automatically distinguish between directed and diffuse illumination. Extended experiments with real-world traffic scenes illustrate the principal practicality of this approach, but simultaneously reveal unexpected difficulties.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937534",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Layout",
                "Vehicle detection",
                "Road vehicles",
                "Traffic control",
                "Image sequences",
                "Vehicle driving",
                "Machine vision",
                "Time varying systems",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based vehicle tracking approach",
                "elongated shadow-casting structures",
                "real-world traffic scenes"
            ]
        },
        "id": 42,
        "cited_by": []
    },
    {
        "title": "Reducing drift in parametric motion tracking",
        "authors": [
            "A. Rahimi",
            "L.-P. Morency",
            "T. Darrell"
        ],
        "abstract": "We develop a class of differential motion trackers that automatically stabilize when in finite domains. Most differential trackers compute motion only relative to one previous frame, accumulating errors indefinitely. We estimate pose changes between a set of past frames, and develop a probabilistic framework for integrating those estimates. We use an approximation to the posterior distribution of pose changes as an uncertainty model for parametric motion in order to help arbitrate the use of multiple base frames. We demonstrate this framework on a simple 2D translational tracker and a 3D, 6-degree of freedom tracker.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937535",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 4,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Tracking",
                "Motion estimation",
                "Maximum likelihood estimation",
                "Video sequences",
                "Artificial intelligence",
                "Parameter estimation",
                "Fuses",
                "Layout",
                "Measurement uncertainty",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "drift reduction",
                "parametric motion tracking",
                "differential motion trackers",
                "finite domains",
                "pose changes",
                "probabilistic framework",
                "posterior distribution",
                "uncertainty model",
                "parametric motion",
                "multiple base frames",
                "2D translational tracker",
                "6-degree of freedom tracker"
            ]
        },
        "id": 43,
        "cited_by": []
    },
    {
        "title": "Guiding random particles by deterministic search",
        "authors": [
            "J. Sullivan",
            "J. Rittscher"
        ],
        "abstract": "Among the algorithms developed towards the goal of robust and efficient tracking, two approaches which stand out due to their success are those based on particle filtering and variational approaches. The Bayesian approach led to the development of the particle filter, which performs a random search guided by a stochastic motion model. On the other hand, localising an object can be based on minimising a cost function. This minimum can be found using variational methods. The search paradigms differ in these two methods. One is stochastic and model-driven while the other is deterministic and data-driven. This paper presents a new algorithm to incorporate the strengths of both approaches into one consistent framework. To allow this fusion a smooth, wide likelihood function is constructed, based on a sum-of-squares distance measure and an appropriate sampling scheme is introduced. Based on low-level information this scheme automatically mixes the two methods of search and adapts the computational demands of the algorithm to the difficulty of the problem at hand. The ability to effectively track complex motions without the need for finely tuned motion models is demonstrated.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937536",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 12,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Particle filters",
                "Robustness",
                "Particle tracking",
                "Cost function",
                "Sampling methods",
                "Bayesian methods",
                "Motion estimation",
                "Fuses",
                "Image sequences"
            ],
            "INSPEC: Controlled Indexing": [
                "stochastic processes",
                "deterministic algorithms",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "random particles guiding",
                "deterministic search",
                "particle filtering",
                "variational approaches",
                "stochastic motion model",
                "sum-of-squares distance measure",
                "sampling scheme"
            ]
        },
        "id": 44,
        "cited_by": []
    },
    {
        "title": "Human tracking in multiple cameras",
        "authors": [
            "S. Khan",
            "O. Javed",
            "Z. Rasheed",
            "M. Shah"
        ],
        "abstract": "Multiple cameras are needed to cover large environments for monitoring activity. To track people successfully in multiple perspective imagery, one needs to establish correspondence between objects captured in multiple cameras. We present a system for tracking people in multiple uncalibrated cameras. The system is able to discover spatial relationships between the camera fields of view and use this information to correspond between different perspective views of the same person. We employ the novel approach of finding the limits of field of view (FOV) of a camera as visible in the other cameras. Using this information, when a person is seen in one camera, we are able to predict all the other cameras in which this person will be visible. Moreover, we apply the FOV constraint to disambiguate between possible candidates of correspondence. We present results on sequences of up to three cameras with multiple people. The proposed approach is very fast compared to camera calibration based approaches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937537",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 20,
            "total": 55
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Cameras",
                "Calibration",
                "Computer vision",
                "Computerized monitoring",
                "Surveillance",
                "Computer science",
                "Sensor fusion",
                "Application software",
                "Feeds"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "calibration",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human tracking",
                "multiple cameras",
                "multiple perspective imagery",
                "spatial relationships",
                "field of view",
                "camera calibration"
            ]
        },
        "id": 45,
        "cited_by": [
            {
                "year": "2003",
                "id": 125
            }
        ]
    },
    {
        "title": "Selecting objects with freehand sketches",
        "authors": [
            "Kar-Han Tan",
            "N. Ahuja"
        ],
        "abstract": "We present in this paper the design of an interactive tool for selecting objects using simple freehand sketches. The objective is to extract object boundaries precisely while requiring little skill and time from the user. The tool proposed achieves this objective by integrating user input and image computation in a two-phase algorithm. In the first phase, the input sketch is used along with a coarse global segmentation of the image to derive an initial selection and a triangulation of the region around the boundary. The triangles are used to formulate subproblems of local finer-grained segmentation and selection. Each of the subproblems is processed independently in the second phase, where a linear approximation of the local boundary as well as a local, finer-grained segmentation are computed. The approximate boundary is then used with the local segmentation to compute a final selection, represented with an alpha channel to fully capture diffused object boundaries. Experimental results show that the tool allows very simple sketches to be used to select objects with complex boundaries. Therefore, the tool has immediate applications in graphics systems for image editing, manipulation, synthesis, retrieval and processing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937538",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Graphics",
                "Handheld computers",
                "Cranes",
                "Computer science",
                "Linear approximation",
                "Image retrieval",
                "Content based retrieval",
                "Digital images",
                "TV broadcasting"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "computer vision",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "objects selection",
                "freehand sketches",
                "interactive tool",
                "object boundaries",
                "user input",
                "image computation",
                "two-phase algorithm",
                "coarse global segmentation",
                "finer-grained segmentation",
                "linear approximation",
                "image editing"
            ]
        },
        "id": 46,
        "cited_by": []
    },
    {
        "title": "Parallel-perspective stereo mosaics",
        "authors": [
            "Z. Zhu",
            "E.M. Riseman",
            "A.R. Hanson"
        ],
        "abstract": "In this paper we present a novel method for automatically and efficiently generating stereoscopic mosaics by seamless registration of optical data collected by a video camera mounted on an airborne platform that undergoes dominant translational motion. There are four critical points discussed in this paper: (1) Using a parallel-perspective representation, a pair of geometrically registered stereo mosaics can be constructed before we explicitly recover any 3D information under rather general motion. (2) A PRISM (parallel ray interpolation for stereo mosaicing) technique is proposed to make stereo mosaics seamless in the presence of motion parallax and for rather arbitrary scenes. A fast PRISM algorithm is presented and issues on stitching point selection and occlusion handling are discussed. (3) The epipolar geometry of parallel-perspective stereo mosaics generated under constrained 6 DOF motion is formulated, which shows optimal baselines, easy search for correspondence and constant depth resolution. (4) The proposed methods for the generation of stereo mosaics and then the reconstruction of a 3D map are efficient in both computation and storage. Experimental results on long video sequences are given.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937539",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 3,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Robot vision systems",
                "Computational geometry",
                "Stereo image processing",
                "Computer science",
                "Geometrical optics",
                "Interpolation",
                "Layout",
                "Image reconstruction",
                "Concurrent computing"
            ],
            "INSPEC: Controlled Indexing": [
                "interpolation",
                "computer vision",
                "image registration",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "parallel-perspective stereo mosaics",
                "stereoscopic mosaics",
                "seamless registration",
                "optical data",
                "video camera",
                "dominant translational motion",
                "parallel-perspective representation",
                "geometrically registered stereo mosaics",
                "PRISM",
                "parallel ray interpolation",
                "stereo mosaicing",
                "stereo mosaics",
                "motion parallax",
                "PRISM algorithm",
                "occlusion handling",
                "epipolar geometry",
                "video sequences"
            ]
        },
        "id": 47,
        "cited_by": [
            {
                "year": "2003",
                "id": 21
            },
            {
                "year": "2001",
                "id": 203
            }
        ]
    },
    {
        "title": "Video object segmentation using Eulerian region-based active contours",
        "authors": [
            "S. Jehan-Besson",
            "M. Barlaud",
            "G. Aubert"
        ],
        "abstract": "We address the problem of moving object segmentation using active contours. As far as segmentation of moving objects is concerned, region-based terms must be incorporated in the evolution equation of the active contour, in addition to classical boundary-based terms. In this paper, we propose a general framework for region-based active contours. Novel aspects of the segmentation method include a new Eulerian proof to compute the evolution equation of the active contour from the minimization of a criterion, and the introduction of functions name \"descriptors\" of the regions. In this proof, the dynamical scheme is directly introduced in the criterion before differentiation. With such a method, the case of descriptors depending on the evolution of the curve, i.e. depending upon features globally attached to the region, can readily be taken into account. The variation of these descriptors upon the evolution of the curve induces additional terms in the evolution equation of the active contour. The proof ensures the fastest decrease of the active contour towards a minimum of the criterion. Inside this theoretical framework, a set of descriptors is evaluated on real sequences for the detection of moving objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937540",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 8,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Object segmentation",
                "Active contours",
                "Equations",
                "Object detection",
                "Minimization methods",
                "Image segmentation",
                "Standards development",
                "Video compression",
                "MPEG 4 Standard",
                "Markov random fields"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object segmentation",
                "region-based active contours",
                "active contours",
                "moving objects",
                "Eulerian proof",
                "evolution equation"
            ]
        },
        "id": 48,
        "cited_by": []
    },
    {
        "title": "Robust principal component analysis for computer vision",
        "authors": [
            "F. De la Torre",
            "M.J. Black"
        ],
        "abstract": "Principal Component Analysis (PCA) has been widely used for the representation of shape, appearance and motion. One drawback of typical PCA methods is that they are least squares estimation techniques and hence fail to account for \"outliers\" which are common in realistic training sets. In computer vision applications, outliers typically occur within a sample (image) due to pixels that are corrupted by noise, alignment errors, or occlusion. We review previous approaches for making PCA robust to outliers and present a new method that uses an intra-sample outlier process to account for pixel outliers. We develop the theory of Robust Principal Component Analysis (RPCA) and describe a robust M-estimation algorithm for learning linear multi-variate representations of high dimensional data such as images. Quantitative comparisons with traditional PCA and previous robust algorithms illustrate the benefits of RPCA when outliers are present. Details of the algorithm are described and a software implementation is being made publically available.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937541",
        "reference_list": [],
        "citation": {
            "ieee": 100,
            "other": 51,
            "total": 151
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis",
                "Computer vision",
                "Noise robustness",
                "Shape",
                "Motion analysis",
                "Least squares approximation",
                "Application software",
                "Pixel",
                "Multi-stage noise shaping",
                "Computer errors"
            ],
            "INSPEC: Controlled Indexing": [
                "principal component analysis",
                "computer vision",
                "least squares approximations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "principal component analysis",
                "computer vision",
                "outliers",
                "robust M-estimation algorithm",
                "least squares estimation"
            ]
        },
        "id": 49,
        "cited_by": [
            {
                "year": "2015",
                "id": 245
            },
            {
                "year": "2013",
                "id": 18
            },
            {
                "year": "2009",
                "id": 75
            },
            {
                "year": "2007",
                "id": 256
            },
            {
                "year": "2003",
                "id": 5
            },
            {
                "year": "2003",
                "id": 170
            },
            {
                "year": "2003",
                "id": 193
            },
            {
                "year": "2003",
                "id": 195
            }
        ]
    },
    {
        "title": "Visual learning by integrating descriptive and generative methods",
        "authors": [
            "Gheng-En Guo",
            "Song-Chun Zhu",
            "Yingnian Wu"
        ],
        "abstract": "This paper presents a mathematical framework for visual learning that integrates two popular statistical learning paradigms in the literature: (I). Descriptive learning, such as Markov random fields and minimax entropy learning, and (II). Generative learning, such as PCA, ICA, TCA, image coding and HMM. We apply this integrated learning framework to texton modeling, and we assume that an observed texture image is generated by multiple layers of hidden stochastic \"texton processes\" with each texton being a window function, like a mini-template or a wavelet, under affine transformations. The spatial arrangements of the textons are characterized by minimax entropy models. The texton processes generate images by occlusion or linear addition. Thus given a raw input image, the learning framework achieves four goals: (i). Computing the appearance of the textons. (ii) Inferring the hidden stochastic texton processes. (iii). Learning Gibbs models for each texton process and (iv). Verifying the learnt textons and Gibbs models through random sampling and texture synthesis. The integrated framework subsumes the minimax entropy learning paradigm and creates a richer class of probability models for visual patterns, which are suited for middle level vision representations. Furthermore we show that the integration of description and generative methods yields a natural and general framework of visual learning. We demonstrate the proposed framework and algorithms on many real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937542",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Minimax techniques",
                "Entropy",
                "Hidden Markov models",
                "Image generation",
                "Stochastic processes",
                "Statistical learning",
                "Markov random fields",
                "Principal component analysis",
                "Independent component analysis",
                "Image coding"
            ],
            "INSPEC: Controlled Indexing": [
                "image coding",
                "learning (artificial intelligence)",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual learning",
                "statistical learning",
                "descriptive learning",
                "generative learning",
                "Markov random fields",
                "minimax entropy learning",
                "texton modeling",
                "texture image",
                "learning framework",
                "vision representations"
            ]
        },
        "id": 50,
        "cited_by": []
    },
    {
        "title": "3D hand pose reconstruction using specialized mappings",
        "authors": [
            "R. Rosales",
            "V. Athitsos",
            "L. Sigal",
            "S. Sclaroff"
        ],
        "abstract": "A system for recovering 3D hand pose from monocular color sequences is proposed. The system employs a non-linear supervised learning framework, the specialized mappings architecture (SMA), to map image features to likely 3D hand poses. The SMA's fundamental components are a set of specialized forward mapping functions, and a single feedback matching function. The forward functions are estimated directly from training data, which in our case are examples of hand joint configurations and their corresponding visual features. The joint angle data in the training set is obtained via a CyberGlove, a glove with 22 sensors that monitor the angular motions of the palm and fingers. In training, the visual features are generated using a computer graphics module that renders the hand from arbitrary viewpoints given the 22 joint angles. The viewpoint is encoded by two real values, therefore 24 real values represent a hand pose. We test our system both on synthetic sequences and on sequences taken with a color camera. The system automatically detects and tracks both bands of the user, calculates the appropriate features, and estimates the 3D hand joint angles and viewpoint from those features. Results are encouraging given the complexity of the task.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937543",
        "reference_list": [],
        "citation": {
            "ieee": 64,
            "other": 11,
            "total": 75
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Supervised learning",
                "Feedback",
                "Training data",
                "Data gloves",
                "Computerized monitoring",
                "Fingers",
                "Computer graphics",
                "Rendering (computer graphics)",
                "System testing"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "image reconstruction",
                "data gloves",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D hand pose",
                "monocular color sequences",
                "supervised learning",
                "feedback matching function",
                "forward mapping functions",
                "visual features",
                "viewpoint"
            ]
        },
        "id": 51,
        "cited_by": [
            {
                "year": "2011",
                "id": 265
            },
            {
                "year": "2007",
                "id": 190
            },
            {
                "year": "2003",
                "id": 145
            }
        ]
    },
    {
        "title": "A probabilistic framework for space carving",
        "authors": [
            "A. Broadhurst",
            "T.W. Drummond",
            "R. Cipolla"
        ],
        "abstract": "This paper introduces a new probabilistic framework for Space Carving. In this framework each voxel is assigned a probability, which is computed by comparing the likelihoods for the voxel existing and not existing. This new framework avoids many of the difficulties associated with the original Space Carving algorithm. Specifically, it does not need a global threshold parameter, and it guarantees that no holes will be carved in the model. This paper also proposes that a voxel-based thick texture is a realistic and efficient representation for scenes which contain dominant planes. The algorithm is tested using both real and synthetic data, and both qualitative and quantitative results are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937544",
        "reference_list": [],
        "citation": {
            "ieee": 76,
            "other": 26,
            "total": 102
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image reconstruction",
                "Shape",
                "Semiconductor device modeling",
                "Testing",
                "Cameras",
                "Image matching",
                "Level set",
                "Image generation"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic framework",
                "Space Carving",
                "representation",
                "thick texture",
                "dominant planes",
                "reconstruction",
                "recover",
                "static scene",
                "representations"
            ]
        },
        "id": 52,
        "cited_by": [
            {
                "year": "2011",
                "id": 257
            },
            {
                "year": "2009",
                "id": 219
            },
            {
                "year": "2009",
                "id": 234
            },
            {
                "year": "2007",
                "id": 53
            },
            {
                "year": "2005",
                "id": 89
            },
            {
                "year": "2005",
                "id": 161
            },
            {
                "year": "2005",
                "id": 213
            },
            {
                "year": "2003",
                "id": 76
            },
            {
                "year": "2003",
                "id": 174
            }
        ]
    },
    {
        "title": "Articulated soft objects for video-based body modeling",
        "authors": [
            "R. Plankers",
            "P. Fua"
        ],
        "abstract": "We develop a framework for 3-D shape and motion recovery of articulated deformable objects. We propose a formalism that incorporates the use of implicit surfaces into earlier robotics approaches that were designed to handle articulated structures. We demonstrate its effectiveness for human body modeling from video sequences. Our method is both robust and generic. It could easily be applied to other shape and motion recovery problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937545",
        "reference_list": [],
        "citation": {
            "ieee": 34,
            "other": 16,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Biological system modeling",
                "Shape",
                "Humans",
                "Video sequences",
                "Robustness",
                "Robots",
                "Skeleton",
                "Skin",
                "Solid modeling",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "motion estimation",
                "computer animation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video-based body modeling",
                "articulated deformable objects",
                "3-D shape",
                "motion recovery",
                "human body modeling"
            ]
        },
        "id": 53,
        "cited_by": [
            {
                "year": "2003",
                "id": 120
            }
        ]
    },
    {
        "title": "A novel modeling algorithm for shape recovery of unknown topology",
        "authors": [
            "Y. Duan",
            "H. Qin"
        ],
        "abstract": "This paper presents a novel modeling algorithm that is capable of simultaneously recovering correct shape geometry as well as its unknown topology from arbitrarily complicated datasets. Our algorithm starts from a simple seed model (of genus zero) that can be arbitrarily initiated by users within any dataset. The deformable behavior of our model is governed by a locally defined objective function associated with each vertex of the model. Through the numerical computation of function optimization, our algorithm can adaptively subdivide the model geometry, automatically detect self-collision of the model, properly modify its topology (because of the occurrence of self-collision), continuously evolve the model towards the object boundary, and reduce fitting error and improve fitting quality via global subdivision. Commonly used mesh optimization techniques are employed throughout the geometric deformation and topological variation in order to ensure the model both locally smooth and globally well conditioned. We have applied our algorithm to various real/synthetic range data as well as volumetric image data in order to empirically verify and validate its usefulness. Based on our experiments, the new modeling algorithm proves to be very powerful and extremely valuable for shape recovery in computer vision, reverse engineering in computer graphics, and iso-surface extraction in visualization.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937546",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Topology",
                "Solid modeling",
                "Deformable models",
                "Computational geometry",
                "Object detection",
                "Computer vision",
                "Reverse engineering",
                "Computer graphics",
                "Data mining"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image reconstruction",
                "computer graphics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape recovery",
                "unknown topology",
                "modeling algorithm",
                "recovering correct shape geometry",
                "complicated datasets",
                "model geometry",
                "function optimization",
                "mesh optimization"
            ]
        },
        "id": 54,
        "cited_by": []
    },
    {
        "title": "On projection matrices P/sup k//spl rarr/P/sup 2/, k=3,...,6, and their applications in computer vision",
        "authors": [
            "L. Wolf",
            "A. Shashua"
        ],
        "abstract": "Projection matrices from projective spaces P/sup 3/ to P/sup 2/ have long been used in multiple-view geometry to model the perspective projection created by the pin-hole camera. In this work we introduce higher-dimensional mappings P/sup k//spl rarr/P/sup 2/, k=4,5,6 for the representation of various applications in which the world we view is no longer rigid. We also describe the multi-view constraints from these new projection matrices and methods for extracting the (non-rigid) structure and motion for each application.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937547",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Tensile stress",
                "Layout",
                "Application software",
                "Transmission line matrix methods",
                "Computer vision",
                "Integrated circuit modeling",
                "Computer science",
                "Electronic mail",
                "Computational geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "matrix algebra",
                "feature extraction",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "projection matrices",
                "computer vision",
                "projective spaces",
                "multiple-view geometry",
                "higher-dimensional mappings",
                "multi-view constraints",
                "structure",
                "motion"
            ]
        },
        "id": 55,
        "cited_by": []
    },
    {
        "title": "Plane-based projective reconstruction",
        "authors": [
            "R. Kaucic",
            "R. Hartley",
            "N. Dano"
        ],
        "abstract": "A linear method for computing a projective reconstruction from a large number of images is presented and then evaluated. The method uses planar homographies between views to linearize the resecting of the cameras. Constraints based on the fundamental matrix, trifocus tensor or quadrifocal tensor are used to derive relationship between the position vectors of all the cameras at once. The resulting set of equations are solved using a SVD. The algorithm is computationally efficient as it is linear in the number of matched points used. A key feature of the algorithm is that all of the images are processed simultaneously, as in the Sturm-Triggs factorization method, but it differs in not requiring that all points be visible in all views. An additional advantage is that it works with any mixture of line and point correspondence through the constraints these impose on the multilinear tensors. Experiments on both synthetic and real data confirm the method's utility.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937548",
        "reference_list": [
            {
                "year": "2001",
                "id": 5
            }
        ],
        "citation": {
            "ieee": 13,
            "other": 8,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Cameras",
                "Image reconstruction",
                "Transmission line matrix methods",
                "Equations",
                "Research and development",
                "Vectors",
                "Gold",
                "Reconstruction algorithms",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "projective reconstruction",
                "planar homographies",
                "fundamental matrix",
                "trifocus tensor",
                "quadrifocal tensor"
            ]
        },
        "id": 56,
        "cited_by": [
            {
                "year": "2003",
                "id": 158
            },
            {
                "year": "2001",
                "id": 5
            }
        ]
    },
    {
        "title": "A characterization of inherent stereo ambiguities",
        "authors": [
            "S. Baker",
            "T. Sim",
            "T. Kanade"
        ],
        "abstract": "The complete set of measurements that could ever be used by a stereo algorithm is the plenoptic function or light-field. We give a concise characterization of when the light-field of a Lambertian scene uniquely determines its shape, and, conversely, when stereo is inherently ambiguous. We show that stereo computed from the complete light-field is ambiguous if and only if the scene is radiating light of a constant intensity (and color) over an extended region.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937549",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 6,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Shape measurement",
                "Optical polarization",
                "Particle measurements",
                "Robots",
                "Tires",
                "Color",
                "Cameras",
                "Writing"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "inherent stereo ambiguities",
                "plenoptic function",
                "light-field",
                "Lambertian scene",
                "constant intensity",
                "stereo ambiguities"
            ]
        },
        "id": 57,
        "cited_by": [
            {
                "year": "2007",
                "id": 162
            }
        ]
    },
    {
        "title": "The variable bandwidth mean shift and data-driven scale selection",
        "authors": [
            "D. Comaniciu",
            "V. Ramesh",
            "P. Meer"
        ],
        "abstract": "We present two solutions for the scale selection problem in computer vision. The first one is completely nonparametric and is based on the the adaptive estimation of the normalized density gradient. Employing the sample point estimator, we define the Variable Bandwidth Mean Shift, prove its convergence, and show its superiority over the fixed bandwidth procedure. The second technique has a semiparametric nature and imposes a local structure on the data to extract reliable scale information. The local scale of the underlying density is taken as the bandwidth which maximizes the magnitude of the normalized mean shift vector. Both estimators provide practical tools for autonomous image and quasi real-time video analysis and several examples are shown to illustrate their effectiveness.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937550",
        "reference_list": [],
        "citation": {
            "ieee": 124,
            "other": 60,
            "total": 184
        },
        "keywords": {
            "IEEE Keywords": [
                "Bandwidth",
                "Kernel",
                "Computer vision",
                "Image analysis",
                "Laplace equations",
                "Visualization",
                "Educational institutions",
                "Adaptive estimation",
                "Data mining",
                "Image segmentation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "adaptive estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale selection problem",
                "computer vision",
                "adaptive estimation",
                "Variable Bandwidth Mean Shift",
                "semiparametric nature",
                "scale information",
                "normalized mean shift vector",
                "video analysis"
            ]
        },
        "id": 58,
        "cited_by": [
            {
                "year": "2013",
                "id": 434
            },
            {
                "year": "2009",
                "id": 61
            },
            {
                "year": "2009",
                "id": 107
            },
            {
                "year": "2005",
                "id": 178
            },
            {
                "year": "2003",
                "id": 0
            },
            {
                "year": "2003",
                "id": 60
            }
        ]
    },
    {
        "title": "Fast algorithm for nearest neighbor search based on a lower bound tree",
        "authors": [
            "Yong-Sheng Chen",
            "Yi-Ping Hung",
            "Chiou-Shann Fuh"
        ],
        "abstract": "This paper presents a novel algorithm for fast nearest neighbor search. At the preprocessing stage, the proposed algorithm constructs a lower bound tree by agglomeratively clustering the sample points in the database. Calculation of the distance between the query and the sample points can be avoided if the lower bound of the distance is already larger than the minimum distance. The search process can thus be accelerated because the computational cost of the lower bound which can be calculated by using the internal node of the lower bound tree, is less than that of the distance. To reduce the number of the lower bounds actually calculated the winner-update search strategy is used for traversing the tree. Moreover, the query and the sample points can be transformed for further efficiency improvement. Our experiments show that the proposed algorithm can greatly speed up the nearest neighbor search process. When applying to the real database used in Nayar's object recognition system, the proposed algorithm is about one thousand times faster than the exhaustive search.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937551",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Nearest neighbor searches",
                "Clustering algorithms",
                "Information science",
                "Computer science",
                "Acceleration",
                "Computational efficiency",
                "Image databases",
                "Object recognition",
                "Pattern recognition",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "tree searching",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nearest neighbor search",
                "lower bound tree",
                "preprocessing",
                "clustering",
                "winner-update search strategy",
                "object recognition"
            ]
        },
        "id": 59,
        "cited_by": []
    },
    {
        "title": "Matching shapes",
        "authors": [
            "S. Belongie",
            "J. Malik",
            "J. Puzicha"
        ],
        "abstract": "We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by (1) solving for correspondences between points on the two shapes, (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. Dis-similarity between two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework. Results are presented for silhouettes, trademarks, handwritten digits and the COIL dataset.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937552",
        "reference_list": [],
        "citation": {
            "ieee": 93,
            "other": 66,
            "total": 159
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape measurement",
                "Electric variables measurement",
                "Trademarks",
                "Brightness",
                "Humans",
                "Heart",
                "Eyes",
                "Collaboration",
                "Face detection",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "measuring similarity",
                "correspondences",
                "correspondence problem",
                "shape context",
                "thin-plate splines",
                "matching errors",
                "nearest-neighbor classification"
            ]
        },
        "id": 60,
        "cited_by": [
            {
                "year": "2009",
                "id": 3
            },
            {
                "year": "2005",
                "id": 159
            },
            {
                "year": "2005",
                "id": 192
            },
            {
                "year": "2003",
                "id": 2
            },
            {
                "year": "2003",
                "id": 35
            }
        ]
    },
    {
        "title": "Error analysis of pure rotation-based self-calibration",
        "authors": [
            "Lei Wang",
            "Sing Bing Kang",
            "Heung-Yeung Shum",
            "Guangyou Xu"
        ],
        "abstract": "Self-calibration using pure rotation is a well-known technique, and has been shown to be a reliable means for recovering intrinsic camera parameters. However, in practice, it is virtually impossible to ensure that the camera motion for this type of self-calibration is a pure rotation. In this paper we present an error analysis of recovered intrinsic camera parameters due to the presence of translation. We derived closed-form error expressions for a single pair of images with non-degenerate motion; for multiple rotations, for which there are no closed-form solutions, analysis was done through repeated experiments. Among others, we show that translation-independent solutions do exist under certain practical conditions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937553",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Error analysis",
                "Cameras",
                "Calibration",
                "Layout",
                "Motion analysis",
                "Closed-form solution",
                "Independent component analysis",
                "Image analysis",
                "Image motion analysis",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "error analysis",
                "calibration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "self-calibration",
                "rotation-based",
                "error analysis",
                "intrinsic camera parameters"
            ]
        },
        "id": 61,
        "cited_by": []
    },
    {
        "title": "On cosine-fourth and vignetting effects in real lenses",
        "authors": [
            "M. Aggarwal",
            "H. Hua",
            "N. Ahuja"
        ],
        "abstract": "This paper has been prompted by observations of disparities between the observed fall-off in irradiance for off-axis points and that accounted for by the cosine-fourth and vignetting effects. A closer examination of the image formation process for real lenses revealed that even in the absence of vignetting a point light source does not uniformly illuminate the aperture, an effect known as pupil aberration. For example, we found the variation for a 16 mm lens to be as large as 31% for a field angle of 10/spl deg/. In this paper, we critically evaluate the roles of cosine-fourth and vignetting effects and demonstrate the significance of the pupil aberration on the fall-off in irradiance away from image center. The pupil aberration effect strongly depends on the aperture size and shape and this dependence has been demonstrated through two sets of experiments with three real lenses. The effect of pupil aberration is thus a third important cause of fall in irradiance away from the image center in addition to the familiar cosine-fourth and vignetting effects, that must be taken into account in applications that rely heavily on photometric variation such as shape from shading and mosaicing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937554",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 8,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Lenses",
                "Apertures",
                "Photometry",
                "Cameras",
                "Shape",
                "Layout",
                "Brightness",
                "Light sources",
                "Predictive models",
                "Image generation"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cosine-fourth",
                "vignetting effects",
                "irradiance",
                "off-axis points",
                "image formation",
                "pupil aberration",
                "photometric variation",
                "shading",
                "mosaicing"
            ]
        },
        "id": 62,
        "cited_by": []
    },
    {
        "title": "Statistical calibration of CCD imaging process",
        "authors": [
            "Y. Tsin",
            "V. Ramesh",
            "T. Kanade"
        ],
        "abstract": "Charge-Coupled Device (CCD) cameras are widely used imaging sensors in computer vision systems. Many photometric algorithms, such as shape from shading, color constancy and photometric stereo, implicitly assume that the image intensity is proportional to scene radiance. The actual image measurements deviate significantly from this assumption since the transformation from scene radiance to image intensity is non-linear and is a function of various factors including: noise sources in the CCD sensor, as well as various transformations occurring in the camera including: white balancing, gamma correction and automatic gain control. This paper illustrates how careful modeling of the error sources and the various processing steps enable us to accurately estimate the \"response function\", the inverse mapping from image measurements to scene radiance for a given camera exposure setting. It is shown that the estimation algorithm outperforms the calibration procedures known to us in terms of reduced bias and variance. Further, we demonstrate how the error modelling helps us to obtain uncertainty estimates of the camera irradiance value. The power of this uncertainty modeling is illustrated by a vision task involving High Dynamic Range image generation followed by change detection. Change can be detected reliably even in situation where the two images (the reference scene image and the current image) are taken several hours apart.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937555",
        "reference_list": [],
        "citation": {
            "ieee": 82,
            "other": 22,
            "total": 104
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Charge coupled devices",
                "Charge-coupled image sensors",
                "Layout",
                "Photometry",
                "Cameras",
                "Sensor systems",
                "Computer vision",
                "Shape",
                "Colored noise"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "computer vision",
                "CCD image sensors"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "CCD imaging",
                "imaging sensors",
                "computer vision",
                "photometric algorithms",
                "shape from shading",
                "color constancy",
                "photometric stereo",
                "error sources",
                "inverse mapping",
                "estimation algorithm",
                "calibration procedures",
                "error modelling",
                "uncertainty estimates",
                "uncertainty modeling"
            ]
        },
        "id": 63,
        "cited_by": []
    },
    {
        "title": "Depth from defocus in presence of partial self occlusion",
        "authors": [
            "S.S. Bhasin",
            "S. Chaudhuri"
        ],
        "abstract": "Contrary to the normal belief we show that self occlusion is present in any real aperture image and we present a method on how we can take care of the occlusion while recovering the depth using the defocus as the cue. The space-variant blur is modeled as an MRF and the MAP estimates are obtained for both the depth map and the everywhere focused intensity image. The blur kernel is adjusted in the regions where occlusion is present, particularly at the regions of discontinuities in the scene. The performance of the proposed algorithm is tested over synthetic data and the estimates are found to be better than the earlier schemes where such subtle effects were ignored.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937556",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 3,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Design for disassembly",
                "Layout",
                "Apertures",
                "Kernel",
                "Deconvolution",
                "Frequency domain analysis",
                "Filtering",
                "Focusing",
                "Frequency estimation",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "image restoration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "self occlusion",
                "depth from defocus",
                "recovering",
                "MRF",
                "MAP estimates",
                "space-variant blur"
            ]
        },
        "id": 64,
        "cited_by": [
            {
                "year": "2017",
                "id": 169
            },
            {
                "year": "2007",
                "id": 59
            }
        ]
    },
    {
        "title": "Affine calibration from moving objects",
        "authors": [
            "R.A. Manning",
            "C.R. Dyer"
        ],
        "abstract": "This paper introduces a novel linear algorithm for determining the affine calibration between two camera views of a dynamic scene. The affine calibration is computed directly from the fundamental matrices associated with various moving objects in the scene, as well as from the fundamental matrix for the static background if the cameras are at different locations. A minimum of two fundamental matrices are required, but any number of additional fundamental matrices can be incorporated into the linear system to improve the stability of the computation. The technique is demonstrated on both real and synthetic data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937557",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Layout",
                "Cameras",
                "Linear systems",
                "Stability",
                "Information resources",
                "Algorithm design and analysis",
                "H infinity control",
                "Machine vision",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image reconstruction",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "moving objects",
                "affine calibration",
                "fundamental matrices",
                "stability",
                "dynamic scene",
                "camera views"
            ]
        },
        "id": 65,
        "cited_by": []
    },
    {
        "title": "Segmentation of the left ventricle in cardiac MR images",
        "authors": [
            "M.-P. Jolly",
            "N. Duta",
            "G. Funka-Lea"
        ],
        "abstract": "This paper describes a segmentation technique to automatically extract the myocardium in 4D cardiac MR images for quantitative cardiac analysis and the diagnosis of patients. Three different modules are presented. The automatic localization algorithm is able to approximately locate the left ventricle in an image using a maximum discrimination technique. Then, the local deformation algorithm can deform active contours so that they align to the edges in the image to produce the desired outlining of the myocardium. Finally, the global localization algorithm is able to propagate segmented contours from one image in the data set to all the others. We have experimented with the proposed method on a large number of patients and present some examples to show the strengths and pitfalls of our algorithm.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937558",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 8,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Myocardium",
                "Blood",
                "Image analysis",
                "Cardiovascular diseases",
                "Heart",
                "Cardiac disease",
                "Magnetic resonance imaging",
                "Visualization",
                "Active contours"
            ],
            "INSPEC: Controlled Indexing": [
                "medical image processing",
                "image segmentation",
                "feature extraction",
                "cardiology",
                "biomedical MRI"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "cardiac MR images",
                "segmentation technique",
                "myocardium",
                "4D cardiac MR images",
                "quantitative cardiac analysis",
                "active contours",
                "outlining",
                "segmented contours"
            ]
        },
        "id": 66,
        "cited_by": []
    },
    {
        "title": "Region extraction from multiple images",
        "authors": [
            "H. Ishikawa",
            "I.H. Jermyn"
        ],
        "abstract": "We present a method for region identification in multiple images. A set of regions in different images and the correspondences on their boundaries can be thought of as a boundary in the multi-dimensional space formed by the product of the individual image domains. We minimize an energy functional on the space of such boundaries, thereby identifying simultaneously both the optimal regions in each image and the optimal correspondences on their boundaries. We use a ratio form for the energy functional, thus enabling the global minimization of the energy functional using a polynomial time graph algorithm, among other desirable properties. We choose a simple form for this energy that favours boundaries that lie on high intensity gradients in each image, while encouraging correspondences between boundaries in different images that match intensity values. The latter tendency is weighted by a novel heuristic energy that encourages the boundaries to lie on disparity or optical flow discontinuities, although no dense optical flow or disparity map is computed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937559",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 4,
            "total": 11
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Active contours",
                "Data mining",
                "Minimization methods",
                "Polynomials",
                "Optical computing",
                "Personnel",
                "Image motion analysis",
                "Object detection"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple images",
                "region identification",
                "energy functional",
                "region extraction",
                "optical flow",
                "disparity map",
                "correspondences"
            ]
        },
        "id": 67,
        "cited_by": [
            {
                "year": "2007",
                "id": 131
            },
            {
                "year": "2007",
                "id": 132
            }
        ]
    },
    {
        "title": "Image segmentation with minimum mean cut",
        "authors": [
            "S. Wang",
            "J.M. Siskind"
        ],
        "abstract": "We introduce a new graph-theoretic approach to image segmentation based on minimizing a novel class of 'mean cut' cost functions. Minimizing these cost functions corresponds to finding a cut with minimum mean edge weight in a connected planar graph. This approach has several advantages over prior approaches to image segmentation. First, it allows cuts with both open and closed boundaries. Second, it guarantees that the partitions are connected. Third, the cost function does not introduce an explicit bias, such as a preference for large-area foregrounds, smooth or short boundaries, or similar-weight partitions. This lack of bias allows it to produce segmentations that are better aligned with image edges, even in the presence of long thin regions. Finally, the global minimum of this cost function is largely insensitive to the precise choice of edge-weight function. In particular, we show that the global minimum is invariant under a linear transformation of the edge weights and thus insensitive to image contrast. Building on algorithms by Ahuja et al. (1993), we present a polynomial-time algorithm for finding a global minimum of the mean-cut cost function and illustrate the results of applying that algorithm to several synthetic and real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937560",
        "reference_list": [],
        "citation": {
            "ieee": 16,
            "other": 8,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Cost function",
                "Pixel",
                "Partitioning algorithms",
                "Space exploration",
                "Polynomials",
                "National electric code",
                "Active contours",
                "Constraint optimization"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "computational geometry",
                "optimisation",
                "polynomials"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image segmentation",
                "minimum mean cut",
                "graph-theoretic approach",
                "mean cut cost functions",
                "connected planar graph",
                "cost function",
                "global minimum",
                "edge-weight function",
                "polynomial-time algorithm"
            ]
        },
        "id": 68,
        "cited_by": []
    },
    {
        "title": "Indexing based on scale invariant interest points",
        "authors": [
            "K. Mikolajczyk",
            "C. Schmid"
        ],
        "abstract": "This paper presents a new method for detecting scale invariant interest points. The method is based on two recent results on scale space: (1) Interest points can be adapted to scale and give repeatable results (geometrically stable). (2) Local extrema over scale of normalized derivatives indicate the presence of characteristic local structures. Our method first computes a multi-scale representation for the Harris interest point detector. We then select points at which a local measure (the Laplacian) is maximal over scales. This allows a selection of distinctive points for which the characteristic scale is known. These points are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. For indexing, the image is characterized by a set of scale invariant points; the scale associated with each point allows the computation of a scale invariant descriptor. Our descriptors are, in addition, invariant to image rotation, of affine illumination changes and robust to small perspective deformations. Experimental results for indexing show an excellent performance up to a scale factor of 4 for a database with more than 5000 images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937561",
        "reference_list": [],
        "citation": {
            "ieee": 306,
            "other": 180,
            "total": 486
        },
        "keywords": {
            "IEEE Keywords": [
                "Indexing",
                "Robustness",
                "Detectors",
                "Laplace equations",
                "Lighting",
                "Image databases",
                "Layout",
                "Filters"
            ],
            "INSPEC: Controlled Indexing": [
                "database indexing",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scale invariant interest points",
                "indexing",
                "normalized derivatives",
                "multi-scale representation",
                "Harris interest point detector",
                "scale invariant points",
                "affine illumination changes"
            ]
        },
        "id": 69,
        "cited_by": [
            {
                "year": "2015",
                "id": 65
            },
            {
                "year": "2011",
                "id": 45
            },
            {
                "year": "2009",
                "id": 77
            },
            {
                "year": "2009",
                "id": 161
            },
            {
                "year": "2009",
                "id": 295
            },
            {
                "year": "2007",
                "id": 5
            },
            {
                "year": "2007",
                "id": 54
            },
            {
                "year": "2007",
                "id": 239
            },
            {
                "year": "2005",
                "id": 155
            },
            {
                "year": "2005",
                "id": 190
            },
            {
                "year": "2005",
                "id": 237
            },
            {
                "year": "2003",
                "id": 57
            },
            {
                "year": "2003",
                "id": 84
            },
            {
                "year": "2003",
                "id": 86
            }
        ]
    },
    {
        "title": "A global matching framework for stereo computation",
        "authors": [
            "H. Tao",
            "H.S. Sawhney",
            "R. Kumar"
        ],
        "abstract": "This paper presents a new global matching framework for stereo computation. In this framework, the second view is first predicted from the reference view using the depth information. A global match measure is then defined as the similarity function between the predicted image and the actual image. Stereo computation is converted into a search problem where the goal is to find the depth map that maximizes the global match measure. The major advantage of this framework is that the global visibility constraint is inherently enforced in the computation. This paper explores several key components of this framework including (1) three color segmentation based depth representations, (2) an incremental warping algorithm that dramatically reduces the computational complexity, and (3) scene constraints such as the smoothness constraint and the color similarity constraint. Experimental results using different types of depth representations are presented. The quality of the computed depth maps is demonstrated through image-based rendering from new viewpoints.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937562",
        "reference_list": [],
        "citation": {
            "ieee": 107,
            "other": 37,
            "total": 144
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Rendering (computer graphics)",
                "Image converters",
                "Search problems",
                "Pixel",
                "Image segmentation",
                "Computational complexity",
                "Cameras",
                "Motion measurement",
                "Search methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "stereo image processing",
                "rendering (computer graphics)",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "global matching framework",
                "stereo computation",
                "global match measure",
                "similarity function",
                "search problem",
                "color segmentation based depth representations",
                "incremental warping algorithm",
                "computational complexity",
                "image-based rendering"
            ]
        },
        "id": 70,
        "cited_by": [
            {
                "year": "2011",
                "id": 104
            },
            {
                "year": "2009",
                "id": 94
            },
            {
                "year": "2005",
                "id": 73
            },
            {
                "year": "2005",
                "id": 171
            },
            {
                "year": "2005",
                "id": 172
            },
            {
                "year": "2005",
                "id": 185
            }
        ]
    },
    {
        "title": "Stereo matching by compact windows via minimum ratio cycle",
        "authors": [
            "O. Veksler"
        ],
        "abstract": "Window size and shape selection is a difficult problem in area based stereo. We propose an algorithm which chooses an appropriate window shape by optimizing over a large class of \"compact\" windows. We call them compact because their ratio of perimeter to area tends to be small. We believe that this is the first window matching algorithm which can explicitly construct non-rectangular windows. Efficient optimization over the compact window class is achieved via the minimum ratio cycle algorithm. In practice it takes time linear in the size of the largest window in our class. Still the straightforward approach to find the optimal window for each pixel-disparity pair is too slow. We develop pruning heuristics which gave practically the same results while reducing running time from minutes to seconds. Our experiments show that unlike fixed window algorithms, our method avoids blurring disparity boundaries as well as constructs large windows in low textured areas. The algorithm has few parameters which are easy to choose, and the same parameters work well for different image pairs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937563",
        "reference_list": [],
        "citation": {
            "ieee": 24,
            "other": 8,
            "total": 32
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Pixel",
                "Uncertainty",
                "Costs",
                "National electric code",
                "Computational Intelligence Society",
                "Computational efficiency"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image matching",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo matching",
                "compact windows",
                "minimum ratio cycle",
                "window size",
                "shape selection",
                "area based stereo",
                "pruning heuristics"
            ]
        },
        "id": 71,
        "cited_by": []
    },
    {
        "title": "Cheirality in epipolar geometry",
        "authors": [
            "T. Werner",
            "T. Pajdla"
        ],
        "abstract": "The image points in two images satisfy epipolar constraint. However, not all sets of points satisfying epipolar constraint correspond to any real geometry because there can exist no cameras and scene points projecting to given image points such that all image points have positive depth. Using the cheirability theory due to Hartley and previous work an oriented projective geometry, we give necessary and sufficient conditions for an image point set to correspond to any real geometry. For images from conventional cameras, this condition is simple and given in terms of epipolar lines and epipoles. Surprising, this is not sufficient for central panoramic cameras. Apart from giving the insight to epipolar geometry, among the applications are reducing the search space and ruling out impossible matches in stereo, and ruling out impossible solutions for a fundamental matrix computed from seven points.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937564",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 4,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Computational geometry",
                "Equations",
                "Educational programs",
                "Strain control",
                "Manufacturing",
                "Computer vision",
                "TV",
                "Mirrors"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "epipolar geometry",
                "image points",
                "epipolar constraint",
                "real geometry",
                "cheirability theory",
                "necessary and sufficient conditions"
            ]
        },
        "id": 72,
        "cited_by": []
    },
    {
        "title": "Modelling faces dynamically across views and over time",
        "authors": [
            "Yongmin Li",
            "Shaogang Gong",
            "H. Liddell"
        ],
        "abstract": "A comprehensive novel multi-view dynamic face model is presented in this paper to address two challenging problems in face recognition and facial analysis: modelling faces with large pose variation and modelling faces dynamically in video sequences. The model consists of a sparse 3D shape model learnt from 2D images, a shape-and-pose-free texture model, and an affine geometrical model. Model fitting is performed by optimising (1) a global fitting criterion on the overall face appearance while it changes across views and over time, (2) a local fitting criterion on a set of landmarks, and (3) a temporal fitting criterion between successive frames in a video sequence. By temporally estimating the model parameters over a sequence input, the identity and geometrical information of a face is extracted separately. The former is crucial to face recognition and facial analysis. The latter is used to aid tracking and aligning faces. We demonstrate the results of successfully applying this model on faces with large variation of pose and expression over time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937565",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Face recognition",
                "Video sequences",
                "Active appearance model",
                "Active shape model",
                "Support vector machines",
                "Computer science",
                "Image sequence analysis",
                "Data mining",
                "Aging"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image sequences",
                "parameter estimation",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "faces modelling",
                "multi-view dynamic face model",
                "face recognition",
                "facial analysis",
                "large pose variation",
                "video sequences",
                "sparse 3D shape model",
                "shape-and-pose-free texture model",
                "affine geometrical model",
                "global fitting criterion",
                "face appearance",
                "video sequence",
                "model parameters estimation"
            ]
        },
        "id": 73,
        "cited_by": []
    },
    {
        "title": "Geometrical fundamentals of polycentric panoramas",
        "authors": [
            "F. Huang",
            "Shou Kang Wei",
            "R. Klette"
        ],
        "abstract": "This paper proposes polycentric panoramas as a general model of panoramic images. The model formalizes essential characteristics of panoramic geometry. It is able to describe a wide range of panoramic images, including those potentially of future interest, or previously introduced such as single-center, multi-perspective, or concentric panoramas. This paper presents geometrical fundamentals towards stereo applications based on sets of polycentric panoramas. We discuss the image acquisition model, epipolar geometry and a 3D reconstruction approach for this general model of polycentric panoramas. Our theorems on epipolar curve and 3D reconstruction hold for any pair of polycentric panoramas. Corollaries demonstrate that the proposed mathematical model clarifies the understanding and characterization of more specific models. Epipolar curves of special cases are illustrated on panoramic images acquired by a high resolution line-camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937566",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 8,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Layout",
                "Computer science",
                "Computational Intelligence Society",
                "Visualization",
                "Navigation",
                "Image reconstruction",
                "Cameras",
                "Stability",
                "Solid modeling"
            ],
            "INSPEC: Controlled Indexing": [
                "computational geometry",
                "computer vision",
                "stereo image processing",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "geometrical fundamentals",
                "polycentric panoramas",
                "panoramic images",
                "panoramic geometry",
                "stereo applications",
                "image acquisition model",
                "epipolar geometry",
                "3D reconstruction approach",
                "epipolar curve",
                "3D reconstruction"
            ]
        },
        "id": 74,
        "cited_by": []
    },
    {
        "title": "Automated 3D PDM construction using deformable models",
        "authors": [
            "M.R. Kaus",
            "V. Pekar",
            "C. Lorenz",
            "R. Truyen",
            "S. Lobregt",
            "J. Richolt",
            "J. Weese"
        ],
        "abstract": "In recent years several methods have been proposed for constructing statistical shape models to aid image analysis tasks by providing a-priori knowledge. Examples include principal component analysis (PCA) of manually or semi-automatically placed corresponding landmarks on the learning shapes (point distribution models, PDM), which is time consuming and subjective. However automatically establishing surface correspondences continues to be a difficult problem. This paper presents a novel method for the automated construction of 3D PDM from segmented images. Corresponding surface landmarks are established by adapting a triangulated learning shape to segmented volumetric images of the remaining shapes. The adaptation is based on a novel deformable model technique. We illustrate our approach using CT data of the vertebra and the femur. We demonstrate that our method accurately represents and predicts shapes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937567",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Shape",
                "Image segmentation",
                "Biomedical imaging",
                "Image analysis",
                "Principal component analysis",
                "Laboratories",
                "Signal processing",
                "Orthopedic surgery",
                "Hospitals"
            ],
            "INSPEC: Controlled Indexing": [
                "principal component analysis",
                "image segmentation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automated 3D PDM construction",
                "deformable models",
                "statistical shape models",
                "image analysis",
                "a-priori knowledge",
                "principal component analysis",
                "learning shapes",
                "surface correspondences",
                "segmented images",
                "surface landmarks",
                "triangulated learning shape",
                "deformable model technique"
            ]
        },
        "id": 75,
        "cited_by": []
    },
    {
        "title": "Incorporating differential constraints in a 3D reconstruction process application to stereo",
        "authors": [
            "R. Lengagne",
            "P. Fua"
        ],
        "abstract": "We propose to incorporate a priori geometric constraints in a 3-D stereo reconstruction scheme to cope with the many cases where image information alone is not sufficient to accurately recover 3-D shape. Our approach is based on the iterative deformation of a 3-D surface mesh to minimize an objective function. We show that combining anisotropic meshing with a nonquadratic approach to regularization enables us to obtain satisfactory reconstruction results using triangulations with few vertices. Structural or numerical constraints can then be added locally to the reconstruction process through a constrained optimization scheme. They improve the reconstruction results and enforce their consistency with a priori knowledge about object shape. The strong description and modeling properties of differential features make them useful tools that can be efficiently used as constraints for 3-D reconstruction.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937569",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 0,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Surface reconstruction",
                "Stereo image processing",
                "Shape",
                "Application software",
                "Anisotropic magnetoresistance",
                "Image restoration",
                "Computer graphics",
                "Laboratories",
                "Iterative methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D reconstruction",
                "stereo",
                "3-D surface mesh",
                "a priori knowledge",
                "object shape",
                "differential features",
                "3-D reconstruction"
            ]
        },
        "id": 76,
        "cited_by": []
    },
    {
        "title": "Unambiguous determination of shape from photometric stereo with unknown light sources",
        "authors": [
            "O. Drbohlav",
            "R. Sara"
        ],
        "abstract": "Photometric stereo with uncalibrated lights determines surface orientations ambiguously up to any regular transformation. If the surface reflectance model is separable with respect to the illumination and viewing directions, its inherent symmetries enable to design two previously unrecognized constraints on normals that reduce this ambiguity. The two constraints represent projections of normals onto planes perpendicular to the viewing and illumination directions, respectively. We identify the classes of transformations that leave each constraint invariant. We construct the constraints using polarization measurement under the assumption of separable reflectance model for smooth dielectrics. We verify that applying the first constraint together with the integrability constraint results in bas-relief ambiguity, while application of the second constraint on integrable normals reduces the ambiguity to convex/concave ambiguity. Importantly, the latter result is also obtained when the first and second constraints alone are combined.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937570",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 10,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Photometry",
                "Light sources",
                "Reflectivity",
                "Lighting",
                "Pixel",
                "US Department of Transportation",
                "Polarization",
                "Electric variables measurement",
                "Brightness"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "determination of shape",
                "photometric stereo",
                "uncalibrated lights",
                "surface reflectance model"
            ]
        },
        "id": 77,
        "cited_by": []
    },
    {
        "title": "Accurate optical flow in noisy image sequences",
        "authors": [
            "Hagen Spies",
            "Hanno Scharr"
        ],
        "abstract": "Optical flow estimation in noisy image sequences requires a special denoising strategy. Towards this end we introduce a new tensor-driven anisotropic diffusion scheme which is designed to enhance optical-flow-like spatio-temporal structures. This is achieved by selecting diffusivities in a special manner depending on the eigenvalues of the well known structure tensor. We illustrate how the proposed choice differs from edge- and coherence-enhancing anisotropic diffusion. Furthermore we extend a recently discovered discretization scheme for anisotropic diffusion to 3D data. An automatic stop criterion to terminate the diffusion after a suitable time is given. The performance of the introduced method is examined quantitatively using image sequences with a substantial amount of noise added.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937571",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 11,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Image motion analysis",
                "Optical noise",
                "Image sequences",
                "Optical filters",
                "Anisotropic magnetoresistance",
                "Geometrical optics",
                "Tensile stress",
                "Nonlinear optics",
                "Smoothing methods",
                "Filtering"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "eigenvalues and eigenfunctions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "accurate optical flow",
                "noisy image sequences",
                "denoising strategy",
                "tensor-driven anisotropic diffusion scheme",
                "optical-flow-like spatio-temporal structures",
                "eigenvalues",
                "coherence-enhancing anisotropic diffusion",
                "discretization scheme",
                "anisotropic diffusion"
            ]
        },
        "id": 78,
        "cited_by": []
    },
    {
        "title": "Projective structure and motion from two views of a piecewise planar scene",
        "authors": [
            "A. Bartoli",
            "P. Sturm",
            "R. Haraud"
        ],
        "abstract": "In this paper, we address the problem of structure and motion recovery from two views of a scene containing planes, i.e, sets of coplanar points. Most of the existing works do only exploit this constraint in a sub-optimal manner We propose to parameterize the structure of such scenes with planes and points on planes and derive the MLE (Maximum Likelihood Estimator) using a minimal parameterization based on 2D entities. The result is the estimation of camera motion and 3D structure in projective space, that minimizes reprojection error while satisfying the piecewise planarity. We propose a quasi-linear estimator that provides reliable initialization values for plane equations. Experimental results show that the reconstruction is of clearly superior quality compared to traditional methods based only on points, even if the scene is not perfectly piecewise planar.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937572",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 1,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Maximum likelihood estimation",
                "Cameras",
                "Equations",
                "Image reconstruction",
                "Transmission line matrix methods",
                "Geometry",
                "Europe",
                "Computer vision",
                "Motion estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "piecewise planar scene",
                "motion recovery",
                "structure recovery",
                "camera motion",
                "3D structure",
                "projective space"
            ]
        },
        "id": 79,
        "cited_by": []
    },
    {
        "title": "Determining reflectance parameters and illumination distribution from a sparse set of images for view-dependent image synthesis",
        "authors": [
            "K. Nishino",
            "Zhengyou Zhang",
            "K. Ikeuchi"
        ],
        "abstract": "A framework for photo-realistic view-dependent image synthesis of a shiny object from a sparse set of images and a geometric model is proposed. Each image is aligned with the 3D model and decomposed into two images with regards to the reflectance components based on the intensity variation of object surface points. The view-independent surface reflection (diffuse reflection) is stored as one texture map. The view-dependent reflection (specular reflection) images are used to recover the initial approximation of the illumination distribution, and then a two step numerical minimization algorithm utilizing a simplified Torrance-Sparrow reflection model is used to estimate the reflectance parameters and refine the illumination distribution. This provides a very compact representation of the data necessary to render synthetic images from arbitrary view points. We have conducted experiments with real objects to synthesize photorealistic view-dependent images within the proposed framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937573",
        "reference_list": [],
        "citation": {
            "ieee": 29,
            "other": 10,
            "total": 39
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflectivity",
                "Lighting",
                "Reflection",
                "Image generation",
                "Solid modeling",
                "Surface texture",
                "Minimization methods",
                "Approximation algorithms",
                "Parameter estimation",
                "Rendering (computer graphics)"
            ],
            "INSPEC: Controlled Indexing": [
                "realistic images",
                "rendering (computer graphics)",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "reflectance parameters",
                "illumination distribution",
                "image synthesis",
                "photo-realistic",
                "view-dependent",
                "view-dependent reflection",
                "specular reflection",
                "Torrance-Sparrow reflection model",
                "representation"
            ]
        },
        "id": 80,
        "cited_by": [
            {
                "year": "2015",
                "id": 397
            },
            {
                "year": "2009",
                "id": 74
            },
            {
                "year": "2005",
                "id": 184
            },
            {
                "year": "2003",
                "id": 106
            },
            {
                "year": "2003",
                "id": 129
            },
            {
                "year": "2003",
                "id": 197
            }
        ]
    },
    {
        "title": "Color eigenflows: statistical modeling of joint color changes",
        "authors": [
            "E.G. Miller",
            "K. Tieu"
        ],
        "abstract": "We develop a linear model of commonly observed joint color changes in images due to variation in lighting and certain non-geometric camera parameters. This is done by observing how all of the colors are mapped between two images of the same scene under various \"real-world\" lighting changes. We represent each instance of such a joint color mapping as a 3-D vector field in RGB color space. We show that the variance in these maps is well represented by a low-dimensional linear subspace of these vector fields. We dub the principal components of this space the color eigenflows. When applied to a new image, the maps define an image subspace (different for each new image) of plausible variations of the image as seen under a wide variety of naturally observed lighting conditions. We examine the ability of the eigenflows and a base image to reconstruct a second image taken under different lighting conditions, showing our technique to be superior to other methods. Setting a threshold on this reconstruction error gives a simple system for scene recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937574",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 2,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Vectors",
                "Image reconstruction",
                "Light sources",
                "Artificial intelligence",
                "Smart cameras",
                "Apertures",
                "Transducers",
                "Color",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "joint color changes",
                "images",
                "lighting",
                "camera parameters",
                "RGB color space",
                "eigenflows",
                "scene recognition",
                "reconstruction"
            ]
        },
        "id": 81,
        "cited_by": []
    },
    {
        "title": "Robust histogram construction from color invariants",
        "authors": [
            "T. Gevers"
        ],
        "abstract": "A simple and effective object recognition scheme is to represent and march images on the basis of color histograms. To obtain robustness against varying imaging circumstances (e.g. a change in illumination, object pose, and viewpoint), color histograms are constructed from color invariants. However in general, color invariants are negatively affected by sensor noise due to the instabilities of these color invariant transforms at many RGB values. To suppress the effect of noise blow-up for unstable color invariant values, in this paper color invariant histograms are computed using variable kernel density estimators. To apply variable kernel density estimation in a principled way, models are proposed for the propagation of sensor noise through color invariants. As a result the associated uncertainty is known for each color invariant value. The associated uncertainty is used to derive the parameterization of the variable kernel density estimator during histogram construction. It is empirically verified that the proposed method compares favorably to traditional color histograms for object recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937575",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 2,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Histograms",
                "Colored noise",
                "Kernel",
                "Uncertainty",
                "Object recognition",
                "Noise robustness",
                "Lighting",
                "Dentistry",
                "Optical reflection",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "histogram construction",
                "object recognition",
                "color histograms",
                "color invariant transforms",
                "associated uncertainty",
                "sensor noise",
                "variable kernel density estimation"
            ]
        },
        "id": 82,
        "cited_by": []
    },
    {
        "title": "Feature based object recognition using statistical occlusion models with one-to-one correspondence",
        "authors": [
            "Zhengrong Ying",
            "D. Castanon"
        ],
        "abstract": "In this paper we present a new Bayesian framework for partially occluded object recognition with one-to-one correspondence. We introduce two different statistical models for occlusion: One model assumes that each feature in the model can be occluded independent of whether any other features are occluded, whereas the second model uses spatially correlated occlusion to represent the extent of occlusion. Using these models, the object recognition problem reduces to finding the object hypothesis with largest generalized likelihood We develop fast algorithms for finding the optimal one-to-one correspondence between scene features and object model features to compute the generalized likelihood. We evaluate our algorithms using examples extracted from synthetic aperture radar imagery, and illustrate the performance advantages of our approach over alternative algorithms proposed by others.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937576",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Object recognition",
                "Layout",
                "Feature extraction",
                "Bayesian methods",
                "Data mining",
                "Shape",
                "Image resolution",
                "Synthetic aperture radar",
                "Probability",
                "Markov random fields"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "synthetic aperture radar",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object recognition",
                "statistical occlusion models",
                "Bayesian framework",
                "partially occluded",
                "statistical models",
                "synthetic aperture radar imagery"
            ]
        },
        "id": 83,
        "cited_by": []
    },
    {
        "title": "Classifying and solving minimal structure and motion problems with missing data",
        "authors": [
            "M. Oskarsson",
            "K. Astrom",
            "N. Overgaard"
        ],
        "abstract": "In this paper we investigate the structure and motion problem for calibrated one-dimensional projections of a two-dimensional environment. In a previous paper the structure and motion problem for all cases with non-missing data was classified and solved. Our aim is here to classify all structure and motion problems, even those with missing data, and to solve them. Although our focus here is on one-dimensional retina, the classification part works equally well for ordinary cameras, and we give some results for those as well.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937577",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Retina",
                "Navigation",
                "Equations",
                "Road vehicles",
                "Manufacturing automation",
                "Strips",
                "Councils",
                "Artificial intelligence",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "image classification",
                "motion estimation",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure and motion problem",
                "missing data",
                "classification",
                "motion recovery",
                "structure recovery",
                "image classification"
            ]
        },
        "id": 84,
        "cited_by": []
    },
    {
        "title": "Learning low dimensional invariant signature of 3-D object under varying view and illumination from 2-D appearances",
        "authors": [
            "S.Z. Li",
            "Jie Yan",
            "Xin Wen Hou",
            "Ze Yu Li",
            "Hongjiang Zhang"
        ],
        "abstract": "In this paper, we propose an invariant signature representation for appearances of 3-D object under varying view and illumination, and a method for learning the signature from multi-view appearance examples. The signature, a nonlinear feature, provides a good basis for 3-D object detection and pose estimation due to its following properties. (I) Its location in the signature feature space is a simple function of the view and is insensitive or invariant to illumination. (2) It changes continuously as the view changes, so that the object appearances at all possible views should constitute a known simple curve segment (manifold) in the feature space. (3) The coordinates of rite object appearances in the feature space are correlated in a known way according to a predefined function of the view. The first two properties provide a basis for object detection and the third for view (pose) estimation. To compute the signature representation from input, we present a nonlinear regression method for learning a nonlinear mapping from the input (e.g. image) space to the feature space. The ideas of the signature representation and the learning method are illustrated with experimental results for the object of human face. It is shown that the face object can be effectively, modeled compactly in a 10-D nonlinear feature space. The 10-D signature presents excellent insensitivity to changes in illumination for any view. The correlation of the signature coordinates is well determined by the predefined parametric function. Applications of the proposed method in face detection and pose estimation are demonstrated.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937578",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Object detection",
                "Face detection",
                "Principal component analysis",
                "Learning systems",
                "Parameter estimation",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "invariant signature",
                "3-D object",
                "invariant signature representation",
                "multi-view appearance",
                "nonlinear feature",
                "pose estimation",
                "object detection",
                "signature coordinates",
                "nonlinear regression",
                "human face"
            ]
        },
        "id": 85,
        "cited_by": []
    },
    {
        "title": "Sparse PCA. Extracting multi-scale structure from data",
        "authors": [
            "C. Chennubhotla",
            "A. Jepson"
        ],
        "abstract": "Sparse Principal Component Analysis (S-PCA) is a novel framework for learning a linear, orthonormal basis representation for structure intrinsic to an ensemble of images. S-PCA is based on the discovery that natural images exhibit structure in a low-dimensional subspace in a sparse, scale-dependent form. The S-PCA basis optimizes an objective function which trades off correlations among output coefficients for sparsity in the description of basis vector elements. This objective function is minimized by a simple, robust and highly scalable adaptation algorithm, consisting of successive planar rotations of pairs of basis vectors. The formulation of S-PCA is novel in that multi-scale representations emerge for a variety of ensembles including face images, images from outdoor scenes and a database of optical flow vectors representing a motion class.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937579",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 2,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Principal component analysis",
                "Data mining",
                "Independent component analysis",
                "Layout",
                "Higher order statistics",
                "Statistical distributions",
                "Computer science",
                "Educational institutions",
                "Robustness",
                "Image databases"
            ],
            "INSPEC: Controlled Indexing": [
                "principal component analysis",
                "image sequences",
                "feature extraction",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Sparse Principal Component Analysis",
                "S-PCA",
                "orthonormal basis representation",
                "natural images",
                "adaptation algorithm",
                "objective function",
                "multi-scale representations"
            ]
        },
        "id": 86,
        "cited_by": []
    },
    {
        "title": "Probabilistic learning and modelling of object dynamics for tracking",
        "authors": [
            "T. Tay",
            "Kah Kay Sung"
        ],
        "abstract": "The problem of tracking can be decomposed and independently addressed in two steps, namely the prediction step and the verification step. In this paper we present a new approach of addressing the prediction step that is based on modelling joint probability densities of successive states of tracked objects. This approach has the advantage that it is conceptually general such that given sufficient training data, it is capable of modelling a wide range of complex dynamics. Furthermore, we show that this conceptual prediction framework can be implemented in a tractable manner using a Gaussian mixture representation which allows predictions to be generated efficiently. We then descibe experiments that demonstrate these benefits.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937580",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Uncertainty",
                "Predictive models",
                "History",
                "Training data",
                "Dynamic range",
                "Image sequences",
                "Target tracking",
                "Shape",
                "Coherence"
            ],
            "INSPEC: Controlled Indexing": [
                "inference mechanisms",
                "image sequences",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic learning and modelling",
                "object dynamics",
                "tracking",
                "prediction step",
                "verification step",
                "probability densities",
                "tracked objects",
                "Gaussian mixture representation"
            ]
        },
        "id": 87,
        "cited_by": []
    },
    {
        "title": "A maximum likelihood framework for iterative eigendecomposition",
        "authors": [
            "A. Robles-Kelly",
            "E.R. Hancock"
        ],
        "abstract": "This paper presents an iterative maximum likelihood framework for perceptual grouping. We pose the problem of perceptual grouping as one of pairwise relational clustering. The method is quite generic and can be applied to a number of problems including region segmentation and line-linking. The task is to assign image tokens to clusters in which there is strong relational affinity between token pairs. The parameters of our model are the cluster memberships and the link weights between pairs of tokens. Commencing from a simple probability distribution for these parameters, we show how they may be estimated using an EM-like algorithm. The cluster memberships are estimated using an eigendecomposition method. Once the cluster memberships are to hand, then the updated link-weights are the expected values of their pairwise products. The new method is demonstrated on region segmentation and line-segment grouping problems where it is shown to outperform a noniterative eigenclustering method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937582",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 4,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Maximum likelihood estimation",
                "Image segmentation",
                "Clustering algorithms",
                "Probability distribution",
                "Marine vehicles",
                "Iterative methods",
                "Graph theory",
                "Optimization methods",
                "Iterative algorithms",
                "Casting"
            ],
            "INSPEC: Controlled Indexing": [
                "matrix decomposition",
                "maximum likelihood estimation",
                "probability",
                "eigenvalues and eigenfunctions",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "maximum likelihood framework",
                "iterative eigendecomposition",
                "perceptual grouping",
                "pairwise relational clustering",
                "region segmentation",
                "image tokens",
                "relational affinity",
                "cluster memberships",
                "EM-like algorithm",
                "line-segment grouping problems",
                "noniterative eigenclustering method"
            ]
        },
        "id": 88,
        "cited_by": []
    },
    {
        "title": "Stochastic rigidity: image registration for nowhere-static scenes",
        "authors": [
            "A.W. Fitzgibbon"
        ],
        "abstract": "We consider the registration of sequences of images where the observed scene is entirely non-rigid for example a camera flying over water, a panning shot of a field of sunflowers in the wind, or footage of a crowd applauding at a sports event. In these cases, it is not possible to impose the constraint that world points have similar colour in successive views, so existing registration techniques cannot be applied. Indeed the relationship between a point's colours in successive frames is essentially a random process. However by treating the sequence of images as a set of samples from a multidimensional stochastic time-series, we can learn a stochastic model (e.g. an AR model) of the random process which generated the sequence of images. With a static camera, this stochastic model can be used to extend the sequence arbitrarily in time. Driving the model with random noise results in an infinitely varying sequence of images which always looks like the short input sequence. In this way, we can create \"videotextures\" which can play forever without repetition. With a moving camera, the image generation process comprises two components-a stochastic component-generated by the videotexture, and a parametric component due to the camera motion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937584",
        "reference_list": [],
        "citation": {
            "ieee": 40,
            "other": 13,
            "total": 53
        },
        "keywords": {
            "IEEE Keywords": [
                "Stochastic processes",
                "Image registration",
                "Layout",
                "Cameras",
                "Stochastic resonance",
                "Image generation",
                "Registers",
                "Multimedia communication",
                "Broadcasting",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "image sequences",
                "random noise",
                "time series"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stochastic rigidity",
                "image registration",
                "nowhere-static scenes",
                "images sequences registration",
                "panning shot",
                "sunflowers",
                "stochastic model",
                "random process",
                "random noise",
                "videotextures",
                "videotexture"
            ]
        },
        "id": 89,
        "cited_by": [
            {
                "year": "2015",
                "id": 76
            },
            {
                "year": "2007",
                "id": 196
            },
            {
                "year": "2003",
                "id": 145
            },
            {
                "year": "2003",
                "id": 161
            }
        ]
    },
    {
        "title": "Learning local evidence for shading and reflectance",
        "authors": [
            "M. Bell",
            "E.T. Freeman"
        ],
        "abstract": "We address the important and unsolved problem of determining whether variations in image intensity are caused by changes in surface normal (shading) or reflectance (paint). A solution to this problem is necessary for machines to interpret images as people do and could have many applications. We take a learning-based approach. We generate a trainiiag set of synthetic images containing both surface normal and reflectance variations, and then label the variations at each position, scale, and orientation as to whether they are caused by shading or paint. The classification is done locally, using a feature vector of nonlinear filter responses. We fit a probability density model to the filter outputs using a mixture of factor analyzers. The resulting model indicates the probability based on local image evidence, that a pyramid coefficient at each orientation and scale is caused by shading or reflectance variations. Although the classification is done using a fixed lighting direction, we can solve for the correct lighting direction by rotating the image to the orientation, relative to the light source, that gives the most shape-like labelings. The labeling allows us to reconstruct two high passed images: one contains those parts of the input image caused by shading effects, while the other contains only those parts caused by reflectance changes. The resulting classifications compare well with human psychophysical performance on a test set of images, and show good results for test photographs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937585",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 7,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflectivity",
                "Paints",
                "Labeling",
                "Testing",
                "Surface fitting",
                "Nonlinear filters",
                "Light sources",
                "Image reconstruction",
                "Humans",
                "Psychology"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "probability",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "local evidence learning",
                "shading",
                "reflectance",
                "image intensity",
                "surface normal",
                "learning-based approach",
                "synthetic images",
                "reflectance variations",
                "feature vector",
                "nonlinear filter responses",
                "probability density model",
                "factor analyzers",
                "local image evidence",
                "shape-like labelings"
            ]
        },
        "id": 90,
        "cited_by": [
            {
                "year": "2013",
                "id": 30
            },
            {
                "year": "2009",
                "id": 300
            }
        ]
    },
    {
        "title": "Occlusion robust adaptive template tracking",
        "authors": [
            "H.T. Nguyen",
            "M. Worring",
            "R. van den Boomgaard"
        ],
        "abstract": "We propose a new method for tracking rigid objects in image sequences using template matching. A Kalman filter is used to make the template adapt to changes in object orientation or illumination. This approach is novel since the Kalman filter has been used in tracking mainly for smoothing the object trajectory. The performance of the Kalman filter is further improved by employing a robust and adaptive filtering algorithm. Special attention is paid to occlusion handling.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937587",
        "reference_list": [],
        "citation": {
            "ieee": 41,
            "other": 22,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Object detection",
                "Lighting",
                "Robust stability",
                "Intelligent systems",
                "Intelligent sensors",
                "Information systems",
                "Image sequences",
                "Kalman filters",
                "Trajectory"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image matching",
                "Kalman filters"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "occlusion robust adaptive template tracking",
                "rigid objects tracking",
                "image sequences",
                "template matching",
                "Kalman filter",
                "object orientation",
                "illumination",
                "object trajectory",
                "occlusion handling"
            ]
        },
        "id": 91,
        "cited_by": [
            {
                "year": "2005",
                "id": 194
            },
            {
                "year": "2003",
                "id": 9
            }
        ]
    },
    {
        "title": "Real-time feature tracking and outlier rejection with changes in illumination",
        "authors": [
            "Hailin Jin",
            "P. Favaro",
            "S. Soatto"
        ],
        "abstract": "We develop an efficient algorithm to track point features supported by image patches undergoing affine deformations and changes in illumination. The algorithm is based on a combined model of geometry and photometry, that is used to track features as well as to detect outliers in a hypothesis testing framework. The algorithm runs in real time on a personal computer; and is available to the public.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937588",
        "reference_list": [],
        "citation": {
            "ieee": 34,
            "other": 2,
            "total": 36
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Layout",
                "Robustness",
                "Target tracking",
                "Real time systems",
                "Application software",
                "Computer science",
                "Photometry",
                "Testing",
                "Microcomputers"
            ],
            "INSPEC: Controlled Indexing": [
                "real-time systems",
                "computer vision",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time feature tracking",
                "outlier rejection",
                "point features",
                "image patches",
                "affine deformations",
                "geometry",
                "photometry"
            ]
        },
        "id": 92,
        "cited_by": [
            {
                "year": "2007",
                "id": 106
            },
            {
                "year": "2005",
                "id": 81
            },
            {
                "year": "2005",
                "id": 99
            },
            {
                "year": "2003",
                "id": 27
            },
            {
                "year": "2003",
                "id": 80
            },
            {
                "year": "2003",
                "id": 82
            },
            {
                "year": "2003",
                "id": 157
            }
        ]
    },
    {
        "title": "Human tracking with mixtures of trees",
        "authors": [
            "S. Ioffe",
            "D. Forsyth"
        ],
        "abstract": "Tree-structured probabilistic models admit simple, fast inference. However they are not well suited to phenonena such as occlusion, where multiple components of an object may disappear simultaneously. We address this problem with mixtures of trees, and demonstrate an efficient and compact representation of this mixture, which admits simple learning and inference algorithms. We use this method to build an automated tracker for Muybridge sequences of a variety of human activities. Tracking is difficult, because the temporal dependencies rule out simple inference methods. We show how to use our model for efficient inference, using a method that employs alternate spatial and temporal inference. The result is a cracker that (a) uses a very loose motion model, and so can track many different activities at a variable frame rate and (b) is entirely, automatic.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937589",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 14,
            "total": 49
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Assembly",
                "Biological system modeling",
                "Tracking",
                "Computer science",
                "Inference algorithms",
                "Torso",
                "Object recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "inference mechanisms",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "human tracking",
                "mixtures of trees",
                "tree-structured probabilistic models",
                "inference",
                "occlusion",
                "automated tracker",
                "Muybridge sequences",
                "human activities",
                "temporal dependencies",
                "inference methods"
            ]
        },
        "id": 93,
        "cited_by": [
            {
                "year": "2003",
                "id": 45
            }
        ]
    },
    {
        "title": "Video phase-locked loops in gait recognition",
        "authors": [
            "J.E. Boyd"
        ],
        "abstract": "In the perception of gaits, the relative timing of the individual motions in a gait is critical. When events occur periodically, as they do in a gait, then relative timing is equivalent to phase. Therefore, to perceive gaits requires frequency entertainment (finding the fundamental frequency of the gait) and phase locking (finding the phase relationships among the components of the gait). In this paper we present an innovation in motion recognition rye call the video phase-locked loop (PLL). While the innovation is simple, the resulting system is uniquely well suited to recognizing gaits because it performs both frequency entrainment and phase locking. A video PLL produces a pattern of phasors (rotating phase vectors) over a region in an image that encapsulates the relative timing of motion in the images. We show how the Procrustes method of statistical shape analysis can be used to compute means over a set of phasor patterns and distances between pairs of patterns. They combine to form the basis of a gait recognition system. The means provide exemplars, while the distance measure allows matching of patterns to exemplars. We provide a set of examples that demonstrates the operation of video PLLs and the application of Procrustes shape analysis to the resulting phasor configurations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937591",
        "reference_list": [],
        "citation": {
            "ieee": 24,
            "other": 6,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Phase locked loops",
                "Timing",
                "Frequency",
                "Leg",
                "Technological innovation",
                "Shape",
                "Legged locomotion",
                "Control system synthesis",
                "Limit-cycles",
                "Motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "phase locked loops",
                "timing",
                "data compression",
                "video coding",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video phase-locked loops",
                "gait recognition",
                "phase locking",
                "frequency entrainment",
                "rotating phase vectors",
                "Procrustes method",
                "statistical shape analysis",
                "gait recognition system",
                "phasor configurations"
            ]
        },
        "id": 94,
        "cited_by": [
            {
                "year": "2003",
                "id": 189
            }
        ]
    },
    {
        "title": "Visual servoing invariant to changes in camera intrinsic parameters",
        "authors": [
            "E. Malis"
        ],
        "abstract": "This paper presents a new visual servoing scheme which is invariant to changes in camera intrinsic parameters. Current visual servoing techniques are based on the learning of a reference image with the same camera used during the servoing. The scheme proposed in the paper differs from previous techniques in that it is camera independent. With the new scheme it is possible to position a camera (with eventually varying intrinsic parameters), with respect to a non-planar object, given a \"reference image\" taken with a completely different camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937592",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 5,
            "total": 20
        },
        "keywords": {
            "IEEE Keywords": [
                "Visual servoing",
                "Cameras",
                "Robot vision systems",
                "Convergence",
                "Robot sensing systems",
                "Manipulators",
                "Feature extraction",
                "Machine vision",
                "Computer errors"
            ],
            "INSPEC: Controlled Indexing": [
                "robot vision",
                "learning (artificial intelligence)",
                "image recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual servoing invariant",
                "camera intrinsic parameters",
                "reference image",
                "learning"
            ]
        },
        "id": 95,
        "cited_by": []
    },
    {
        "title": "Car detection in low resolution aerial image",
        "authors": [
            "Tao Zhao",
            "R. Nevatia"
        ],
        "abstract": "We present a system to detect passenger cars in aerial images where cars appear as small objects. We pose this as a 3D object recognition problem to account for the variation in viewpoint and the shadow. We started from psychological tests to find important features for human detection of cars. Based on these observations, we selected the boundary of the car body, the boundary of the front windshield and the shadow as the features. Some of these features are affected by the intensity of the car and whether or not there is a shadow along it. This information is represented in the structure of the Bayesian network that we use to integrate all features. Experiments show very promising results even on some very challenging images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937593",
        "reference_list": [],
        "citation": {
            "ieee": 38,
            "other": 10,
            "total": 48
        },
        "keywords": {
            "IEEE Keywords": [
                "Image resolution",
                "Image edge detection",
                "Vehicle detection",
                "Vehicles",
                "Intelligent robots",
                "Object recognition",
                "Intelligent systems",
                "Object detection",
                "Psychology",
                "Testing"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image recognition",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "car detection",
                "low resolution aerial image",
                "passenger cars detection",
                "3D object recognition problem",
                "psychological tests",
                "human detection",
                "windshield",
                "Bayesian network"
            ]
        },
        "id": 96,
        "cited_by": [
            {
                "year": "2007",
                "id": 295
            },
            {
                "year": "2005",
                "id": 131
            },
            {
                "year": "2005",
                "id": 147
            },
            {
                "year": "2003",
                "id": 69
            }
        ]
    },
    {
        "title": "Towards real-time multi-modality 3-D medical image registration",
        "authors": [
            "T. Netsch",
            "P. Rosch",
            "A. van Muiswinkel",
            "J. Weese"
        ],
        "abstract": "Intensity value-based registration is a widely used technique for the spatial alignment of medical images. Generally, the registration transformation is determined by iteratively optimizing a similarity measure calculated from the grey values of both images. However, such algorithms may have high computational costs, especially in the case of multi-modality registration, which makes their integration into systems difficult. At present, registration based on mutual information (MI) still requires computation times of the order of several minutes. In this contribution we focus on a new similarity measure based on local correlation (LC) which is well-suited for numerical optimization. We show that LC can be formulated as a least-squares criterion which allows the use of dedicated methods. Thus, it is possible to register MR neuro perfusion time-series (128/sup 2//spl times/30 voxel, 40 images) on a moderate workstation in real-time: the registration of an image takes about 500 ms and is therefore several times faster than image acquisition time. For the registration of CT-MR images (512/sup 2//spl times/87 CT 256/sup 2//spl times/128 MR) a multiresolution framework is used. On top of the decomposition, which requires 47 s of computation time, the optimization with an algorithm based on Ml previously described in the literature takes 97 s. In contrast, the proposed approach only takes 13 s, corresponding to a speedup about a factor of 7. Furthermore, we demonstrate that the superior computational performance of LC is not gained at the expense of accuracy. In particular experiments with dual contrast MR images providing ground truth for the registration show a comparable sub-voxel accuracy of LC and MI similarity.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937595",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 9,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Biomedical imaging",
                "Image registration",
                "Mutual information",
                "Workstations",
                "Robustness",
                "Laboratories",
                "Computational efficiency",
                "Spatial resolution",
                "Image resolution",
                "Medical services"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "medical image processing",
                "real-time systems",
                "least squares approximations",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "real-time multi-modality 3-D medical image registration",
                "intensity value-based registration",
                "spatial alignment",
                "registration transformation",
                "similarity measure",
                "computational costs",
                "local correlation",
                "numerical optimization",
                "least-squares criterion",
                "MR neuro perfusion time-series",
                "CT-MR images",
                "multiresolution framework"
            ]
        },
        "id": 97,
        "cited_by": []
    },
    {},
    {
        "title": "A background model initialization algorithm for video surveillance",
        "authors": [
            "D. Gutchess",
            "M. Trajkovics",
            "E. Cohen-Solal",
            "D. Lyons",
            "A.K. Jain"
        ],
        "abstract": "Many motion detection and tracking algorithms rely on the process of background subtraction, a technique which detects changes from a model of the background scene. We present a new algorithm for the purpose of background model initialization. The algorithm takes as input a video sequence in which moving objects are present, and outputs a statistical background model describing the static parts of the scene. Multiple hypotheses of the background value at each pixel are generated by locating periods of stable intensity in the sequence. The likelihood of each hypothesis is then evaluated using optical flow information from the neighborhood around the pixel, and the most likely hypothesis is chosen to represent the background. Our results are compared with those of several standard background modeling techniques using surveillance video of humans in indoor environments.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937598",
        "reference_list": [],
        "citation": {
            "ieee": 78,
            "other": 23,
            "total": 101
        },
        "keywords": {
            "IEEE Keywords": [
                "Video surveillance",
                "Layout",
                "Motion detection",
                "Tracking",
                "Video sequences",
                "Humans",
                "Indoor environments",
                "Optical sensors",
                "Image motion analysis",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "surveillance",
                "tracking",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "background model initialization algorithm",
                "video surveillance",
                "motion detection and tracking algorithms",
                "background subtraction",
                "video sequence",
                "statistical background model",
                "optical flow information",
                "surveillance video"
            ]
        },
        "id": 99,
        "cited_by": [
            {
                "year": "2005",
                "id": 134
            },
            {
                "year": "2003",
                "id": 8
            }
        ]
    },
    {
        "title": "Sequential Monte Carlo fusion of sound and vision for speaker tracking",
        "authors": [
            "J. Vermaak",
            "M. Gangnet",
            "A. Blake",
            "P. Perez"
        ],
        "abstract": "Video telephony could be considerably enhanced by provision of a tracking system that allows freedom of movement to the speaker while maintaining a well-framed image, for transmission over limited bandwidth. Already commercial multi-microphone systems exist which track speaker direction in order to reject background noise. Stereo sound and vision are complementary modalities in that sound is good for initialisation (where vision is expensive) whereas vision is good for localisation (where sound is less precise). Using generative probabilistic models and particle filtering, we show that stereo sound and vision can indeed be fused effectively, to make a system more capable than with either modality on its own.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937600",
        "reference_list": [],
        "citation": {
            "ieee": 51,
            "other": 10,
            "total": 61
        },
        "keywords": {
            "IEEE Keywords": [
                "Monte Carlo methods",
                "Loudspeakers",
                "Microphones",
                "Cameras",
                "Telephony",
                "Tracking",
                "Fuses",
                "Signal processing",
                "Delay effects",
                "Reverberation"
            ],
            "INSPEC: Controlled Indexing": [
                "videotelephony",
                "stereo image processing",
                "computer vision",
                "Monte Carlo methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sequential Monte Carlo fusion",
                "speaker tracking",
                "video telephony",
                "well-framed image",
                "multi-microphone systems",
                "stereo sound",
                "stereo vision",
                "complementary modalities",
                "generative probabilistic models",
                "particle filtering"
            ]
        },
        "id": 100,
        "cited_by": [
            {
                "year": "2007",
                "id": 99
            }
        ]
    },
    {
        "title": "Constrained active appearance models",
        "authors": [
            "T.F. Cootes",
            "C.J. Taylor"
        ],
        "abstract": "Active Appearance Models (AAMs) have been shown to be useful for interpreting images of deformable objects. Here we place the AAM matching algorithm in a statistical framework, allowing extra constraints to be applied. This enables the models to be combined with other methods of object location. We demonstrate how user interaction can be used to guide the search and give results of experiments showing the effect of constraints on the performance of model matching.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937601",
        "reference_list": [],
        "citation": {
            "ieee": 53,
            "other": 17,
            "total": 70
        },
        "keywords": {
            "IEEE Keywords": [
                "Active appearance model",
                "Deformable models",
                "Detectors",
                "Face detection",
                "Active shape model",
                "Mathematical model",
                "Biomedical imaging",
                "Biomedical engineering",
                "Image converters",
                "Computer vision"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "constrained active appearance models",
                "deformable objects",
                "image interpretation",
                "statistical framework",
                "object location",
                "user interaction",
                "model matching",
                "performance"
            ]
        },
        "id": 101,
        "cited_by": [
            {
                "year": "2005",
                "id": 79
            }
        ]
    },
    {
        "title": "Recognition of shapes by editing shock graphs",
        "authors": [],
        "abstract": "This paper presents a novel recognition framework which is based on matching shock graphs of 2D shape outlines, where the distance between two shapes is defined to be the cost of the least action path deforming one shape to the other. Two key issues are addressed which render the implementation of this framework practical. First, the shape space is partitioned by defining an equivalence class on shapes, where two shapes with the same shock graph topology are considered equivalent. The space of deformations is then discretized based on a formal enumeration of all possible transitions, where the shock graph topology changes, by defining all deformations with the same sequence of shock graph transitions as equivalent. Second, we employ a graph edit distance algorithm that searches in the space of all possible transition sequences and finds the globally optimal sequence in polynomial time. The effectiveness of the proposed technique in the presence of a variety of visual transformations including occlusion, articulation and deformation of parts, shadow and highlights, viewpoint variation, scale, and boundary perturbations is demonstrated. Indexing into two separate databases of roughly 100 shapes results in 100% accuracy for top three matches and 99.5% for the next three matches.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937602",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Electric shock",
                "Costs",
                "Topology",
                "Polynomials",
                "Indexing",
                "Visual databases",
                "Object recognition",
                "Marine animals",
                "Image sampling"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "database indexing",
                "equivalence classes",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shapes recognition",
                "shock graphs editing",
                "2D shape outlines",
                "shape space",
                "equivalence class",
                "formal enumeration",
                "graph edit distance algorithm",
                "visual transformations",
                "occlusion",
                "articulation",
                "deformation",
                "database indexing"
            ]
        },
        "id": 102,
        "cited_by": [
            {
                "year": "2017",
                "id": 285
            },
            {
                "year": "2009",
                "id": 293
            },
            {
                "year": "2007",
                "id": 186
            },
            {
                "year": "2003",
                "id": 34
            },
            {
                "year": "2001",
                "id": 34
            }
        ]
    },
    {
        "title": "Statistical context priming for object detection",
        "authors": [
            "A. Torralba",
            "P. Sinha"
        ],
        "abstract": "There is general consensus that context can be a rich source of information about an object's identity, location and scale. However the issue of how to formalize centextual influences is still largely open. Here we introduce a simple probabilistic framework for modeling the relationship between context and object properties. We represent global context information in terms of the spatial layout of spectral components. The resulting scheme serves as an effective procedure for context driven focus of attention and scale-selection on real-world scenes. Based on a simple holistic analysis of an image, the scheme is able to accurately predict object locations and sizes.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937604",
        "reference_list": [],
        "citation": {
            "ieee": 47,
            "other": 26,
            "total": 73
        },
        "keywords": {
            "IEEE Keywords": [
                "Object detection",
                "Layout",
                "Visual system",
                "Image recognition",
                "Cognitive science",
                "Information resources",
                "Context modeling",
                "Marine vehicles",
                "Focusing",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "statistical context priming",
                "object detection",
                "object's identity",
                "probabilistic framework",
                "spatial layout",
                "spectral components"
            ]
        },
        "id": 103,
        "cited_by": [
            {
                "year": "2013",
                "id": 34
            },
            {
                "year": "2007",
                "id": 121
            },
            {
                "year": "2005",
                "id": 131
            }
        ]
    },
    {
        "title": "Caustics of catadioptric cameras",
        "authors": [
            "R. Swaminathan",
            "M.D. Grossberg",
            "S.K. Nayar"
        ],
        "abstract": "Conventional vision systems and algorithms assume the camera to have a single viewpoint. However, sensors need not always maintain a single viewpoint. For instance, an incorrectly aligned system could cause non-single viewpoints. Also, systems could be designed to specifically deviate from a single viewpoint to trade-off image characteristics such as resolution and field of view. In these cases, the locus of viewpoints forms what is called a caustic. In this paper, we present an in-depth analysis of caustics of catadioptric cameras with conic reflectors. Properties of caustics with respect to field of view and resolution are presented. Finally, we present ways to calibrate conic catadioptric systems and estimate their caustics from known camera motion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937581",
        "reference_list": [],
        "citation": {
            "ieee": 49,
            "other": 15,
            "total": 64
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Lenses",
                "Mirrors",
                "Sensor phenomena and characterization",
                "Layout",
                "Spatial resolution",
                "Computer science",
                "Machine vision",
                "Image resolution",
                "Motion estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "vision systems",
                "single viewpoint",
                "image characteristics",
                "field of view",
                "caustic",
                "catadioptric cameras",
                "conic reflectors",
                "conic catadioptric systems",
                "camera motion"
            ]
        },
        "id": 104,
        "cited_by": [
            {
                "year": "2009",
                "id": 155
            },
            {
                "year": "2009",
                "id": 234
            },
            {
                "year": "2007",
                "id": 261
            },
            {
                "year": "2007",
                "id": 317
            },
            {
                "year": "2005",
                "id": 74
            },
            {
                "year": "2001",
                "id": 117
            }
        ]
    },
    {
        "title": "Split aperture imaging for high dynamic range",
        "authors": [
            "M. Aggarwal",
            "N. Ahuja"
        ],
        "abstract": "Most imaging sensors have limited dynamic range and hence are sensitive to only a part of the illumination range present in a natural scene. The dynamic range can be improved by acquiring multiple images of the same scene under different exposure settings and then combining them. In this paper, we describe a camera design for simultaneously acquiring multiple images of the same scene under different exposure settings. The cross-section of the incoming beam from a scene point is partitioned into as many parts as the desired degree of split. This is done by splitting the aperture into multiple parts and directing the light exiting from each in a different direction using an assembly of mirrors. A sensor is placed in the path of each beam and exposure of each sensor is controlled either by appropriately setting its exposure parameter, or by splitting the incoming beam unevenly. The resulting multiple exposure images are used to construct a high dynamic range image. We have implemented a video-rate camera based on this design and the results obtained are presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937583",
        "reference_list": [],
        "citation": {
            "ieee": 13,
            "other": 4,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Apertures",
                "Dynamic range",
                "Image sensors",
                "Layout",
                "Cameras",
                "Brightness",
                "Image resolution",
                "Spatial resolution",
                "Mirrors",
                "Lenses"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "split aperture imaging",
                "high dynamic range",
                "imaging sensors",
                "scene point",
                "multiple exposure images",
                "video-rate camera"
            ]
        },
        "id": 105,
        "cited_by": []
    },
    {
        "title": "A pseudo-distance map for the segmentation-free skeletonization of gray-scale images",
        "authors": [
            "Jeong-Hun Jang",
            "Ki-Sang Hong"
        ],
        "abstract": "In this paper we introduce a new tool, called a pseudo-distance map (PDM), for extracting skeletons from grayscale images without region segmentation or edge detection. Given an edge-strength function (ESF) of a gray-scale image, the PDM is computed from the ESF using the partial differential equations we propose. The PDM can be thought of as a relaxed version of a Euclidean distance map. Therefore, its ridges correspond to the skeleton of the original gray-scale image and it provides information on the approximate width of skeletonized structures. Since the PDM is directly computed from the ESF without thresholding it, the skeletonization result is generally robust and less noisy. We tested our method using a variety of synthetic and real images. The experimental results show that our method works well on such images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937586",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 0,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Gray-scale",
                "Skeleton",
                "Image edge detection",
                "Data mining",
                "Shape",
                "Equations",
                "Robustness",
                "Euclidean distance",
                "Noise shaping"
            ],
            "INSPEC: Controlled Indexing": [
                "partial differential equations",
                "edge detection",
                "image thinning",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pseudo-distance map",
                "segmentation-free skeletonization",
                "gray-scale images",
                "skeletons extraction",
                "grayscale images",
                "edge-strength function",
                "partial differential equations",
                "Euclidean distance map"
            ]
        },
        "id": 106,
        "cited_by": []
    },
    {
        "title": "A co-inference approach to robust visual tracking",
        "authors": [
            "Ying Wu",
            "T.S. Huang"
        ],
        "abstract": "Visual tracking could be treated as a parameter estimation problem of target representation based on observations in image sequences. A richer target representations would incur better chances of successful tracking in cluttered and dynamic environments. However, the dimensionality of target's state space also increases making tracking a formidable estimation problem. In this paper, the problem of tracking and integrating multiple cues is formulated in a probabilistic framework; and represented by factorized graphical model. Structured variational analysis of such graphical model factorizes different modalities and suggests a co-inference process among these modalities. A sequential Monte Carlo algorithm is proposed to give an efficient approximation of the co-inference based on the importance sampling technique. This algorithm is implemented in real-time at around 30 Hz. Specifically, tracking both position, shape and color distribution of a target is investigated in this paper. Our extensive experiments show that the proposed algorithm performs robustly in a large variety of trucking scenarios. The approach presented in this paper has the potential to solve other sensor fusion problems.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937590",
        "reference_list": [],
        "citation": {
            "ieee": 38,
            "other": 4,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Robustness",
                "Target tracking",
                "Graphical models",
                "Monte Carlo methods",
                "Parameter estimation",
                "Image sequences",
                "State-space methods",
                "State estimation",
                "Approximation algorithms",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "parameter estimation",
                "importance sampling",
                "image sequences",
                "sensor fusion",
                "clutter",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "co-inference approach",
                "robust visual tracking",
                "parameter estimation problem",
                "target representation",
                "image sequences",
                "dynamic environments",
                "cluttered environments",
                "multiple cues",
                "probabilistic framework",
                "factorized graphical model",
                "structured variational analysis",
                "graphical model",
                "sequential Monte Carlo algorithm",
                "importance sampling",
                "sensor fusion problems"
            ]
        },
        "id": 107,
        "cited_by": [
            {
                "year": "2007",
                "id": 99
            }
        ]
    },
    {
        "title": "BraMBLe: a Bayesian multiple-blob tracker",
        "authors": [
            "M. Isard",
            "J. MacCormick"
        ],
        "abstract": "Blob trackers have become increasingly powerful in recent years largely due to the adoption of statistical appearance models which allow effective background subtraction and robust tracking of deforming foreground objects. It has been standard, however, to treat background and foreground modelling as separate processes-background subtraction is followed by blob detection and tracking-which prevents a principled computation of image likelihoods. This paper presents two theoretical advances which address this limitation and lead to a robust multiple-person tracking system suitable for single-camera real-time surveillance applications. The first innovation is a multi-blob likelihood function which assigns directly comparable likelihoods to hypotheses containing different numbers of objects. This likelihood function has a rigorous mathematical basis: it is adapted from the theory of Bayesian correlation, but uses the assumption of a static camera to create a more specific background model while retaining a unified approach to background and foreground modelling. Second we introduce a Bayesian filter for tracking multiple objects when the number of objects present is unknown and varies over time. We show how a particle filter can be used to perform joint inference on both the number of objects present and their configurations. Finally we demonstrate that our system runs comfortably in real time on a modest workstation when the number of blobs in the scene is small.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937594",
        "reference_list": [],
        "citation": {
            "ieee": 230,
            "other": 77,
            "total": 307
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Robustness",
                "Real time systems",
                "Mathematical model",
                "Deformable models",
                "Power system modeling",
                "Surveillance",
                "Technological innovation",
                "Cameras",
                "Filters"
            ],
            "INSPEC: Controlled Indexing": [
                "Bayes methods",
                "statistical analysis",
                "inference mechanisms",
                "tracking",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "BraMBLe",
                "Bayesian multiple-blob tracker",
                "statistical appearance models",
                "robust tracking",
                "foreground modelling",
                "background modelling",
                "background subtraction",
                "image likelihoods",
                "multiple-person tracking system",
                "single-camera real-time surveillance",
                "Bayesian correlation",
                "Bayesian filter"
            ]
        },
        "id": 108,
        "cited_by": [
            {
                "year": "2013",
                "id": 420
            },
            {
                "year": "2009",
                "id": 188
            },
            {
                "year": "2009",
                "id": 199
            },
            {
                "year": "2009",
                "id": 208
            },
            {
                "year": "2009",
                "id": 233
            },
            {
                "year": "2007",
                "id": 55
            },
            {
                "year": "2007",
                "id": 103
            },
            {
                "year": "2007",
                "id": 113
            },
            {
                "year": "2007",
                "id": 235
            },
            {
                "year": "2007",
                "id": 383
            },
            {
                "year": "2003",
                "id": 16
            },
            {
                "year": "2003",
                "id": 146
            }
        ]
    },
    {
        "title": "Continuous global evidence-based Bayesian modality fusion for simultaneous tracking of multiple objects",
        "authors": [
            "J. Sherrah",
            "Shaogang Gong"
        ],
        "abstract": "Robust, real-time tracking of objects from visual data requires probabilistic fusion of multiple visual cues. Previous approaches have either been ad hoc or relied on a Bayesian network with discrete spatial variables which suffers from discretisation and computational complexity problems. We present a new Bayesian modality fusion network that uses continuous domain variables. The network architecture distinguishes between cues that are necessary or unnecessary for the object's presence. Computationally expensive and inexpensive modalities are also handled differently to minimise cost. The method provides a formal, tractable and robust probabilistic method for simultaneously tracking multiple objects. While instantaneous inference is exact, approximation is required for propagation over time.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937596",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 11,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Bayesian methods",
                "Uncertainty",
                "Computer science",
                "Computational complexity",
                "Computer architecture",
                "Costs",
                "Noise robustness",
                "Trajectory",
                "Noise generators",
                "Focusing"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "sensor fusion",
                "tracking",
                "object detection",
                "belief networks"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Bayesian modality fusion",
                "simultaneous tracking",
                "multiple objects",
                "real-time tracking",
                "probabilistic fusion",
                "Bayesian modality fusion network",
                "continuous domain variables"
            ]
        },
        "id": 109,
        "cited_by": [
            {
                "year": "2007",
                "id": 99
            }
        ]
    },
    {
        "title": "Probabilistic tracking in a metric space",
        "authors": [
            "K. Toyama",
            "A. Blake"
        ],
        "abstract": "A new exemplar-based, probabilistic paradigm for visual tracking is presented. Probabilistic mechanisms are attractive because they handle fusion of information, especially temporal fusion, in a principled manner. Exemplars are selected representatives of raw training data, used here to represent probabilistic mixture distributions of object configurations. Their use avoids tedious hand-construction of object models and problems with changes of topology. Using exemplars in place of a parameterized model poses several challenges, addressed here with what we call the \"Metric Mixture\" (M/sup 2/) approach. The M/sup 2/ model has several valuable properties. Principally, it provides alternatives to standard learning algorithms by allowing the use of metrics that are not embedded in a vector space. Secondly, it uses a noise model that is learned from training data. Lastly, it eliminates any need for an assumption of probabilistic pixelwise independence. Experiments demonstrate the effectiveness of the M/sup 2/ model in two domains tracking walking people using chamfer distances on binary edge images and tracking mouth movements by means of a shuffle distance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937599",
        "reference_list": [],
        "citation": {
            "ieee": 95,
            "other": 32,
            "total": 127
        },
        "keywords": {
            "IEEE Keywords": [
                "Extraterrestrial measurements",
                "Tracking",
                "Training data",
                "Legged locomotion",
                "Filtering",
                "Pixel",
                "Topology",
                "Mouth",
                "Uncertainty",
                "Sensor fusion"
            ],
            "INSPEC: Controlled Indexing": [
                "learning (artificial intelligence)",
                "computer vision",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "probabilistic tracking",
                "metric space",
                "visual tracking",
                "exemplar-based probabilistic paradigm",
                "temporal fusion",
                "raw training data",
                "object models",
                "M/sup 2/ model",
                "learning algorithms",
                "shuffle distance",
                "noise model",
                "probabilistic pixelwise independence"
            ]
        },
        "id": 110,
        "cited_by": [
            {
                "year": "2007",
                "id": 10
            },
            {
                "year": "2005",
                "id": 60
            },
            {
                "year": "2005",
                "id": 179
            },
            {
                "year": "2003",
                "id": 4
            },
            {
                "year": "2003",
                "id": 139
            }
        ]
    },
    {
        "title": "Multi-view scene capture by surfel sampling: from video streams to non-rigid 3D motion, shape and reflectance",
        "authors": [
            "R.L. Carceroni",
            "K.N. Kutalakos"
        ],
        "abstract": "In this paper we study the problem of recovering the 3D shape, reflectance, and non-rigid motion of a dynamic 3D scene. Because these properties are completely unknown, our approach uses multiple views to build a piecewise continuous geometric and radiometric representation of the scene's trace in space-time. Basic primitive of this representation is the dynamic surfel, which (1) encodes the instantaneous local shape, reflectance, and motion of a small region in the scene, and (2) enables accurate prediction of the region's dynamic appearance under known illumination conditions. We show that complete surfel-based reconstructions can be created by repeatedly applying an algorithm called surfel sampling that combines sampling and parameter estimation to fit a single surfel to a small, bounded region of space-time. Experimental results with the Phong reflectance model and complex real scenes (clothing, skin, shiny objects) illustrate our method's ability to explain pixels and pixel variations in terms of their physical causes-shape, reflectance, motion, illumination, and visibility.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937603",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 5,
            "total": 40
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Sampling methods",
                "Streaming media",
                "Reflectivity",
                "Shape",
                "Lighting",
                "Radiometry",
                "Parameter estimation",
                "Clothing",
                "Skin"
            ],
            "INSPEC: Controlled Indexing": [
                "parameter estimation",
                "image motion analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiview scene capture",
                "surfel sampling",
                "video streams",
                "nonrigid 3D motion",
                "reflectance",
                "3D shape recovery",
                "radiometric representation",
                "dynamic surfel",
                "accurate prediction",
                "dynamic appearance",
                "Phong reflectance model"
            ]
        },
        "id": 111,
        "cited_by": [
            {
                "year": "2003",
                "id": 76
            },
            {
                "year": "2003",
                "id": 79
            },
            {
                "year": "2003",
                "id": 157
            }
        ]
    },
    {
        "title": "Deriving intrinsic images from image sequences",
        "authors": [
            "Y. Weiss"
        ],
        "abstract": "Intrinsic images are a useful midlevel description of scenes proposed by H.G. Barrow and J.M. Tenenbaum (1978). An image is de-composed into two images: a reflectance image and an illumination image. Finding such a decomposition remains a difficult problem in computer vision. We focus on a slightly, easier problem: given a sequence of T images where the reflectance is constant and the illumination changes, can we recover T illumination images and a single reflectance image? We show that this problem is still imposed and suggest approaching it as a maximum-likelihood estimation problem. Following recent work on the statistics of natural images, we use a prior that assumes that illumination images will give rise to sparse filter outputs. We show that this leads to a simple, novel algorithm for recovering reflectance images. We illustrate the algorithm's performance on real and synthetic image sequences.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937606",
        "reference_list": [],
        "citation": {
            "ieee": 213,
            "other": 73,
            "total": 286
        },
        "keywords": {
            "IEEE Keywords": [
                "Image sequences",
                "Lighting",
                "Reflectivity",
                "Layout",
                "Image segmentation",
                "Equations",
                "Computer vision",
                "Inference algorithms",
                "Computer science",
                "Maximum likelihood estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "image sequences",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "intrinsic images",
                "image sequences",
                "midlevel description of scenes",
                "reflectance image",
                "illumination image",
                "computer vision",
                "maximum-likelihood estimation"
            ]
        },
        "id": 112,
        "cited_by": [
            {
                "year": "2017",
                "id": 254
            },
            {
                "year": "2017",
                "id": 502
            },
            {
                "year": "2015",
                "id": 19
            },
            {
                "year": "2015",
                "id": 48
            },
            {
                "year": "2015",
                "id": 90
            },
            {
                "year": "2013",
                "id": 30
            },
            {
                "year": "2013",
                "id": 303
            },
            {
                "year": "2011",
                "id": 2
            },
            {
                "year": "2011",
                "id": 41
            },
            {
                "year": "2011",
                "id": 113
            },
            {
                "year": "2009",
                "id": 23
            },
            {
                "year": "2009",
                "id": 89
            },
            {
                "year": "2009",
                "id": 192
            },
            {
                "year": "2009",
                "id": 227
            },
            {
                "year": "2009",
                "id": 300
            },
            {
                "year": "2005",
                "id": 61
            },
            {
                "year": "2005",
                "id": 142
            },
            {
                "year": "2005",
                "id": 146
            },
            {
                "year": "2003",
                "id": 181
            }
        ]
    },
    {
        "title": "Alignment of non-overlapping sequences",
        "authors": [
            "Y. Caspi",
            "M. Irani"
        ],
        "abstract": "This paper shows how two image sequences that have no spatial overlap between their fields of view can be aligned both in time and in space. Such alignment is possible when the two cameras are attached closely together and are moved jointly in space. The common motion induces \"similar\" changes over time within the two sequences. This correlated temporal behavior is used to recover the spatial and temporal transformations between the two sequences. The requirement of \"coherent appearance\" in standard image alignment techniques is therefore replaced by \"coherent temporal behavior\", which is often easier to satisfy. This approach to alignment can be used not only for aligning nan-overlapping sequences, but also for handling other cases that are inherently difficult for standard image alignment techniques. We demonstrate applications of this approach to three real-world problems: (i) alignment of non-overlapping sequences for generating wide-screen movies, (ii) alignment of images (sequences) obtained at significantly different zooms, for surveillance applications, and (iii) multi-sensor image alignment for multi-sensor fusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937607",
        "reference_list": [],
        "citation": {
            "ieee": 38,
            "other": 13,
            "total": 51
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image sequences",
                "Layout",
                "Robot vision systems",
                "Computer science",
                "Motion pictures",
                "Surveillance",
                "Hardware"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "sensor fusion"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "nonoverlapping sequences alignment",
                "image sequences",
                "fields of view",
                "temporal behavior",
                "standard image alignment",
                "image alignment",
                "multi-sensor image alignment",
                "multi-sensor fusion"
            ]
        },
        "id": 113,
        "cited_by": [
            {
                "year": "2003",
                "id": 78
            },
            {
                "year": "2003",
                "id": 123
            },
            {
                "year": "2003",
                "id": 185
            }
        ]
    },
    {
        "title": "Multi-agent event recognition",
        "authors": [
            "S. Hongeng",
            "R. Nevatia"
        ],
        "abstract": "This paper presents a new approach to recognizing multiagent events observed by a static camera. To track objects robustly, knowledge about the ground plane and the events is used. An event is considered as composed of action threads, each thread being executed by a single actor. A single thread of action is recognized from the characteristics of the trajectory and moving blob of the actor using Bayesian methods. A multi-agent event is represented by a number of action threads related by temporal constraints. Multi-agent events are recognized by propagating the constraints and likelihoods of event threads in a temporal logic network.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937608",
        "reference_list": [],
        "citation": {
            "ieee": 55,
            "other": 26,
            "total": 81
        },
        "keywords": {
            "IEEE Keywords": [
                "Yarn",
                "Hidden Markov models",
                "Bayesian methods",
                "Intelligent robots",
                "Cameras",
                "Noise robustness",
                "Shape",
                "Layout",
                "Intelligent systems",
                "Intelligent agent"
            ],
            "INSPEC: Controlled Indexing": [
                "temporal logic",
                "multi-agent systems",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multi-agent event recognition",
                "multiagent events",
                "static camera",
                "ground plane",
                "action threads",
                "Bayesian methods",
                "multi-agent event",
                "temporal constraints",
                "temporal logic network"
            ]
        },
        "id": 114,
        "cited_by": [
            {
                "year": "2015",
                "id": 351
            },
            {
                "year": "2007",
                "id": 55
            },
            {
                "year": "2007",
                "id": 171
            }
        ]
    },
    {
        "title": "Biologically motivated, precise and simple calibration and reconstruction using a stereo light microscope",
        "authors": [
            "L. Eckert",
            "R.-R. Grigat"
        ],
        "abstract": "Stereoscopic calibration and reconstruction is applied to the specialized optics of a binocular monobjective stereo light microscope. Such a microscope exhibits a special kind of image distortion. Despite the difficulty of modelling the microscope, a simple calibration method as well as a fast and simple, yet precise, reconstruction algorithm is developed. Their fundamental scheme is based upon biological binocular vision. The reconstruction uses polynomial approximations up to a degree of 2 and thus has a very low computational complexity. The polynomial coefficients are identified during calibration and their number is minimal by construction. No lens data is required. Both the calibration and reconstruction algorithm are robust against a rigid motion of the microscope. Their power is proven with real data using an off-the-shelf PC.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937609",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Image reconstruction",
                "Optical distortion",
                "Reconstruction algorithms",
                "Polynomials",
                "Biomedical optical imaging",
                "Stereo image processing",
                "Optical microscopy",
                "Biological system modeling",
                "Computational complexity"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "polynomial approximation",
                "calibration",
                "computer vision",
                "stereo image processing",
                "optical microscopes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "biologically motivated calibration",
                "stereo light microscope",
                "stereoscopic calibration",
                "binocular monobjective stereo light microscope",
                "image distortion",
                "calibration method",
                "biological binocular vision",
                "polynomial approximations",
                "computational complexity",
                "polynomial coefficients",
                "off-the-shelf PC"
            ]
        },
        "id": 115,
        "cited_by": []
    },
    {
        "title": "True single view point cone mirror omni-directional catadioptric system",
        "authors": [
            "Shih-Schon Lin",
            "R. Bajcsy"
        ],
        "abstract": "Pinhole camera model is a simplified subset of geometric optics. In special cases like the image formation of the cone (a degenerate conic section) mirror in an omnidirectional view catadioptric system, there are more complex optical phenomena involved that the simple pinhole model can not explain. We show that using the full geometric optics model a true single viewpoint cone mirror omni-directional system can be built. We show how such a system is built first, and then show in detail how each optical phenomenon works together to make the system true single viewpoint. The new system requires only simple off-the-shelf components and still outperforms other single viewpoint omni-systems for many applications.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937610",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 0,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Mirrors",
                "Cameras",
                "Optical distortion",
                "Geometrical optics",
                "Solid modeling",
                "Lenses",
                "Computational Intelligence Society",
                "Shape",
                "Power system modeling",
                "Optical feedback"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "single view point cone mirror omni-directional catadioptric system",
                "pinhole camera model",
                "geometric optics",
                "image formation",
                "degenerate conic section",
                "geometric optics model"
            ]
        },
        "id": 116,
        "cited_by": []
    },
    {
        "title": "A general imaging model and a method for finding its parameters",
        "authors": [
            "M.D. Grossberg",
            "S.K. Nayar"
        ],
        "abstract": "Linear perspective projection has served as the dominant imaging model in computer vision. Recent developments in image sensing make the perspective model highly restrictive. This paper presents a general imaging model that can be used to represent an arbitrary imaging system. It is observed that all imaging systems perform a mapping from incoming scene rays to photo-sensitive elements on the image detector. This mapping can be conveniently described using a set of virtual sensing elements called raxels. Raxels include geometric, radiometric and optical properties. We present a novel calibration method that uses structured light patterns to extract the raxel parameters of an arbitrary imaging system. Experimental results for perspective as well as ion-perspective imaging systems are included.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937611",
        "reference_list": [
            {
                "year": "2001",
                "id": 104
            }
        ],
        "citation": {
            "ieee": 87,
            "other": 34,
            "total": 121
        },
        "keywords": {
            "IEEE Keywords": [
                "Optical imaging",
                "Cameras",
                "Lenses",
                "Layout",
                "Calibration",
                "Computer vision",
                "Optical distortion",
                "Optical sensors",
                "Mirrors",
                "Eyes"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "calibration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "general imaging model",
                "linear perspective projection",
                "dominant imaging model",
                "computer vision",
                "image sensing",
                "arbitrary imaging system",
                "scene rays",
                "virtual sensing elements",
                "raxels",
                "optical properties"
            ]
        },
        "id": 117,
        "cited_by": [
            {
                "year": "2013",
                "id": 118
            },
            {
                "year": "2011",
                "id": 150
            },
            {
                "year": "2011",
                "id": 293
            },
            {
                "year": "2007",
                "id": 149
            },
            {
                "year": "2007",
                "id": 154
            },
            {
                "year": "2007",
                "id": 304
            },
            {
                "year": "2005",
                "id": 15
            },
            {
                "year": "2005",
                "id": 200
            },
            {
                "year": "2003",
                "id": 152
            }
        ]
    },
    {
        "title": "Calibration with robust use of cheirality by quasi-affine reconstruction of the set of camera projection centres",
        "authors": [
            "D. Nister"
        ],
        "abstract": "A method for upgrading a projective reconstruction to metric is presented. The reconstruction is first transformed by considering cheirality so that the convex hull of the set of camera projection centres is the same as in the metric counterpart. The method then proceeds iteratively and starting from such a reconstruction is a necessary condition for many iterative calibration algorithms to converge. The results show that in practice it is also most often sufficient provided that the minimised objective function is a geometrically meaningful quantity. The method has been found extremely reliable for both large and small reconstructions in a large number of experiments on real data. When subjected to the common degeneracy of little or no rotation between the views, the method still yields a very reasonable member of the family of possible solutions. Furthermore, the method is very fast and therefore suitable for the purpose of viewing reconstructions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937612",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Robustness",
                "Cameras",
                "Image reconstruction",
                "H infinity control",
                "Layout",
                "Iterative methods",
                "Iterative algorithms",
                "Computer vision",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "projective reconstruction",
                "cheirality",
                "convex hull",
                "camera projection centres",
                "iterative calibration",
                "viewing reconstructions",
                "robust",
                "quasi-affine reconstruction",
                "calibration"
            ]
        },
        "id": 118,
        "cited_by": []
    },
    {
        "title": "Using scene constraints during the calibration procedure",
        "authors": [
            "D. Bondyfalat",
            "T. Papdopoulo",
            "B. Mourrain"
        ],
        "abstract": "This paper focuses on the problem of calibration from a single view and a map of a scene. This situation arises quite often when modelling urban scenes, e.g. for augmented reality purposes. We show how some scenes constraints can be used to achieve a calibration like procedure. An example excerpted from a sequence of pictures for which self-calibration-like techniques consistently fail illustrates some of the benefits of the approach.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937613",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 2,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Calibration",
                "Cameras",
                "Computational geometry",
                "Retina",
                "Bonding",
                "Augmented reality",
                "Image recognition",
                "Algebra",
                "Geometrical optics"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "augmented reality",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "scene constraints",
                "calibration procedure",
                "urban scenes",
                "augmented reality",
                "scenes constraints",
                "calibration"
            ]
        },
        "id": 119,
        "cited_by": []
    },
    {
        "title": "Image segmentation by data driven Markov chain Monte Carlo",
        "authors": [
            "Zhuowen Tu",
            "Song-Chun Zhu",
            "Heung-Yeung Shum"
        ],
        "abstract": "This paper presents a computational paradigm called Data Driven Markov Chain Monte Carlo (DDMCMC) for image segmentation in the Bayesian, statistical framework. The paper contributes to image segmentation in three aspects. Firstly, it designs effective and well balanced Markov Chain dynamics to explore the solution space and makes the split and merge process reversible at a middle level vision formulation. Thus it achieves globally optimal solution independent of initial segmentations. Secondly, instead of computing a single maximum a posteriori solution, it proposes a mathematical principle for computing multiple distinct solutions to incorporates intrinsic ambiguities in image segmentation. A k-adventurers algorithm is proposed for extracting distinct multiple solutions from the Markov chain sequence. Thirdly, it utilizes data-driven (bottom-up) techniques, such as clustering and edge detection, to compute importance proposal probabilities, which effectively drive the Markov chain dynamics and achieve tremendous speedup in comparison to traditional jump-diffusion method. Thus DDM-CMC paradigm provides a unifying framework where the role of existing segmentation algorithms, such as; edge detection, clustering, region growing, split-merge, SNAKEs, region competition, are revealed as either realizing Markov chain dynamics or computing importance proposal probabilities. We report some results on color and grey level image segmentation in this paper and refer to a detailed report and a web site for extensive discussion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937614",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Monte Carlo methods",
                "Stochastic processes",
                "Bayesian methods",
                "Space exploration",
                "Clustering algorithms",
                "Image edge detection",
                "Proposals",
                "Computer vision",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "edge detection",
                "Markov processes",
                "Monte Carlo methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Data Driven Markov Chain Monte Carlo",
                "image segmentation",
                "Bayesian",
                "statistical framework",
                "intrinsic ambiguities",
                "clustering",
                "edge detection",
                "DDM-CMC"
            ]
        },
        "id": 120,
        "cited_by": [
            {
                "year": "2003",
                "id": 1
            }
        ]
    },
    {
        "title": "Confidence and curvature estimation of curvilinear structures in 3-D",
        "authors": [
            "P. Bakker",
            "L.J. van Vliet",
            "P.W. Verbeek"
        ],
        "abstract": "In this paper we present a new method for estimating confidence and curvature of 3-D curvilinear structures. The gradient structure tensor (GST) models shift-invariance. The eigenstructure of the tensor allows estimation of local dimensionality, orientation, and the corresponding confidence value. Local rotational invariance, which occurs often in images, causes a lower confidence estimate. This underestimation can be corrected for by a parabolic deformation of the data, in such a way that it becomes translational invariant. We show that the optimal deformation can be found analytically and yields a local curvature estimate as a valuable by-product. We tested our new method on synthetic images and applied it to the detection of channels in 3-D seismic delta.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937616",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Geologic measurements",
                "Eigenvalues and eigenfunctions",
                "Yield estimation",
                "Robustness",
                "Testing",
                "Pattern recognition",
                "Physics",
                "Mathematical model",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "invariance"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curvilinear structures",
                "confidence",
                "curvature",
                "3-D curvilinear structures",
                "gradient structure tensor",
                "eigenstructure",
                "ocal dimensionality",
                "confidence value",
                "local rotational invariance",
                "local curvature estimate"
            ]
        },
        "id": 121,
        "cited_by": []
    },
    {
        "title": "Probabilistic framework for segmenting people under occlusion",
        "authors": [
            "A.E. Elgammal",
            "L.S. Davis"
        ],
        "abstract": "In this paper we address the problem of segmenting foreground regions corresponding to a group of people given models of their appearance that were initialized before occlusion. We present a general framework that uses maximum likelihood estimation to estimate the best arrangement for people in terms of 2D translation that yields a segmentation for the foreground region. Given the segmentation result we conduct occlusion reasoning to recover relative depth information and we show how to utilize this depth information in the same segmentation framework. We also present a more practical solution for the segmentation problem that is online to avoid searching an exponential space of hypothesis. The person model is based on segmenting the body into regions in order to spatially localize the color-features corresponding to the way people are dressed. Modeling these regions involves modeling their appearance (color distributions) as well us their spatial distribution with respect to the body. We use a non-parametric approach bused on kernel density estimation to represent the color distribution of each region and therefore we do not restrict the clothing to be of uniform color instead it can be any mixture of colors and/or patterns. We also present a method to automatically initialize these models and learn them before the occlusion.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937617",
        "reference_list": [],
        "citation": {
            "ieee": 68,
            "other": 36,
            "total": 104
        },
        "keywords": {
            "IEEE Keywords": [
                "Surveillance",
                "Target tracking",
                "Yield estimation",
                "Kernel",
                "Layout",
                "Clothing",
                "Computer vision",
                "Laboratories",
                "Educational institutions",
                "Indexing"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "image recognition",
                "maximum likelihood estimation",
                "hidden feature removal"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "segmenting foreground regions",
                "people",
                "occlusion",
                "maximum likelihood estimation",
                "segmentation",
                "occlusion reasoning",
                "recover relative depth",
                "color-features",
                "clothing"
            ]
        },
        "id": 122,
        "cited_by": [
            {
                "year": "2007",
                "id": 113
            },
            {
                "year": "2007",
                "id": 235
            }
        ]
    },
    {
        "title": "Folds and cuts: how shading flows into edges",
        "authors": [
            "P.S. Huggins",
            "S.W. Zucker"
        ],
        "abstract": "We consider the interactions between edges and intensity distributions in semi-open image neighborhoods surrounding them. Locally this amounts to a kind of figure-ground problem, and we analyze the case of smooth surface occluding one another. Techniques from differential topology permits a classification of edges based on what we call folds and cuts. Intuititively, folds arise when a surface \"folds\" out of sight, which in turn may \"cut\" another surface from view. The classification depends on tangency between an edge tangent map and a shading flow field. Examples are included.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937618",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 6,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Image edge detection",
                "Layout",
                "Geometry",
                "Computer vision",
                "Distributed computing",
                "Computed tomography",
                "Topology"
            ],
            "INSPEC: Controlled Indexing": [
                "edge detection",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shading",
                "edges",
                "intensity distributions",
                "image neighborhoods",
                "differential topology",
                "classification of edges"
            ]
        },
        "id": 123,
        "cited_by": []
    },
    {
        "title": "Feature selection from huge feature sets",
        "authors": [
            "J. Bins",
            "B.A. Draper"
        ],
        "abstract": "The number of features that can be completed over an image is, for practical purposes, limitless. Unfortunately, the number of features that can be computed and exploited by most computer vision systems is considerably less. As a result, it is important to develop techniques for selecting features from very large data sets that include many irrelevant or redundant features. This work addresses the feature selection problem by proposing a three-step algorithm. The first step uses a variation of the well known Relief algorithm to remove irrelevance; the second step clusters features using K-means to remove redundancy; and the third step is a standard combinatorial feature selection algorithm. This three-step combination is shown to be more effective than standard feature selection algorithms for large data sets with lots of irrelevant and redundant features. It is also shown to he no worse than standard techniques for data sets that do not have these properties. Finally, we show a third experiment in which a data set with 4096 features is reduced to 5% of its original size with very little information loss.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937619",
        "reference_list": [],
        "citation": {
            "ieee": 27,
            "other": 16,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Computer vision",
                "Principal component analysis",
                "Probes",
                "Computer science",
                "Supervised learning",
                "Particle measurements",
                "Size measurement",
                "Data mining",
                "Object recognition",
                "Biometrics"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "feature extraction",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "huge feature sets",
                "computer vision",
                "feature selection",
                "Relief algorithm",
                "clusters",
                "redundancy"
            ]
        },
        "id": 124,
        "cited_by": [
            {
                "year": "2017",
                "id": 146
            }
        ]
    },
    {
        "title": "Optimal method for the affine F-matrix and its uncertainty estimation in the sense of both noise and outliers",
        "authors": [
            "S. Brandt",
            "J. Heikkonen"
        ],
        "abstract": "We propose, in maximum likelihood sense, an optimal method for the affine fundamental matrix estimation in the presence of both Gaussian noise and outliers. It is based on weighting the squared residuals by the iteratively completed, residual posterior probabilities to be relevant. The proposed principle is also used for the covariance matrix estimation of the affine F-matrix where the novelty is in the fact that all data is used rather than the (erroneously) relevant classified matching points. The experiments on both synthetic and real data verify the optimality of the method in the sense of both false matches and Gaussian noise in data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937620",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Uncertainty",
                "Gaussian noise",
                "Covariance matrix",
                "Least squares approximation",
                "Laboratories",
                "Maximum likelihood estimation",
                "Stereo vision",
                "Random variables",
                "Noise robustness",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "Gaussian noise",
                "parameter estimation",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "affine F-matrix",
                "uncertainty estimation",
                "noise",
                "outliers",
                "maximum likelihood",
                "affine fundamental matrix estimation",
                "Gaussian noise",
                "covariance matrix estimation",
                "stereo vision"
            ]
        },
        "id": 125,
        "cited_by": []
    },
    {
        "title": "Affine invariant erosion of 3D shapes",
        "authors": [
            "S. Betelu",
            "G. Sapiro",
            "A. Tannenbaum"
        ],
        "abstract": "A new definition of affine invariant erosion of 3D surfaces is introduced. Instead of being based in terms of Euclidean distances, the volumes enclosed between the surface and its chords are used. The resulting erosion is insensitive to noise, and by construction, it is affine invariant. We prove some key properties about this erosion operation, and we propose a simple method to compute the erosion of implicit surfaces. We also discuss how the affine erosion can be used to define 3D affine invariant robust skeletons.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937621",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Skeleton",
                "Surface morphology",
                "Computer vision",
                "Mathematics",
                "Application software",
                "Noise robustness",
                "Filters",
                "Morphological operations",
                "Transmission line matrix methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image thinning"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "affine invariant erosion",
                "3D surfaces",
                "Euclidean distances",
                "erosion operation",
                "implicit surfaces",
                "robust skeletons"
            ]
        },
        "id": 126,
        "cited_by": []
    },
    {
        "title": "Multiple view geometry of non-planar algebraic curves",
        "authors": [
            "J.Y. Kaminski",
            "M. Fryers",
            "A. Shashua",
            "M. Teicher"
        ],
        "abstract": "We introduce a number of new results in the context of multi-view geometry from general algebraic curves. We start with the derivation of the extended Kruppa's equations which are responsible for describing the epipolar constraint of two projections of a general (non-planar) algebraic curve. As part of the derivation of those constraints we address the issue of dimension analysis and as a result establish the minimal number of algebraic curves required for a solution of the epipolar geometry as a function of their degree and genus. We then establish new results on the reconstruction of general algebraic curves from multiple views. We address three different representations of curves: (i) the regular point representation for which we show that the reconstruction from two views of a curve of degree d admits two solutions, one of degree d and the other of degree d(d-1), (ii) the dual space representation (tangents) for which we derive a lower bound for the number of views necessary for reconstruction as a function of the curve degree and genus, and (iii) a new representation (to computer vision) based on the set of lines meeting the curve which does not require any curve fitting in image space, for which we also derive lower bounds for the number of views necessary for reconstruction as a function of the curve degree alone.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937622",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 6,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Transmission line matrix methods",
                "Image reconstruction",
                "Computer science",
                "Layout",
                "Sparse matrices",
                "Mathematics",
                "Computer vision",
                "Surface reconstruction",
                "Machinery"
            ],
            "INSPEC: Controlled Indexing": [
                "curve fitting",
                "computer vision",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multiple view geometry",
                "nonplanar algebraic curves",
                "extended Kruppa's equations",
                "epipolar constraint",
                "algebraic curve",
                "dimension analysis",
                "regular point representation",
                "dual space representation",
                "lower bound",
                "computer vision",
                "curve fitting",
                "image space",
                "lower bounds",
                "curve degree"
            ]
        },
        "id": 127,
        "cited_by": [
            {
                "year": "2003",
                "id": 134
            }
        ]
    },
    {
        "title": "Do ambiguous reconstructions always give ambiguous images?",
        "authors": [
            "M. Pollefeys",
            "L. Van Gool"
        ],
        "abstract": "In many cases self-calibration is not able to yield a unique solution for the 3D reconstruction of a scene. This is due to the occurrence of critical motion sequences. If this is the case, an ambiguity is left on the reconstruction. In this paper we derive under which conditions correct novel views can be generated from ambiguous reconstructions. The problem is first approached from a theoretical point of view: It is proven that novel views are correct as long as the inclusion of the next view the sequence yields the same ambiguity on the reconstruction. The problem is therefore much related to the problem of critical motion sequences since the virtual camera can be arbitrarily moved within the smallest critical motion set that the recovered camera motion without distortions becoming visible. Based on these results a practical measure for the expected ambiguity on a novel view generated from the recovered structure and motion is derived. As an application a viewer was built that indicates if a specific novel view can be trusted or not by altering the background color.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937623",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 1,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Image reconstruction",
                "Cameras",
                "Application software",
                "Layout",
                "Computer vision",
                "Distortion measurement",
                "Collision mitigation",
                "Metrology",
                "Visualization",
                "Computer graphics"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "ambiguous reconstructions",
                "ambiguous images",
                "self-calibration",
                "3D reconstruction",
                "critical motion sequences",
                "virtual camera",
                "camera motion"
            ]
        },
        "id": 128,
        "cited_by": []
    },
    {
        "title": "Concentric mosaic(s), planar motion and 1D cameras",
        "authors": [
            "Long Quan",
            "Le Lu",
            "Heung-Yeong Shum",
            "M. Lhuillier"
        ],
        "abstract": "General SFM methods give poor results for images captured by constrained motions such as planar motion of concentric mosaics (CM). In this paper we propose new SFM algorithms for both images captured by CM and composite mosaic images from CM. We first introduce ID affine camera model for completing 1D camera models. Then we show that a 2D image captured by CM can be decoupled into two 1D images: one 1D projective and one ID affine; a composite mosaic image can by rebinned into a calibrated ID panorama projective camera. Finally we describe subspace reconstruction methods and demonstrate both in theory and experiments the advantage of the decomposition method over the general SFM methods by incorporating the constrained motion into the earliest stage of motion analysis.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937624",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Transmission line matrix methods",
                "H infinity control",
                "Geometry",
                "Image analysis",
                "Tensile stress",
                "Solid modeling",
                "Reconstruction algorithms",
                "Constraint theory",
                "Subspace constraints"
            ],
            "INSPEC: Controlled Indexing": [
                "rendering (computer graphics)",
                "image segmentation",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "planar motion",
                "concentric mosaics",
                "SFM algorithms",
                "ID affine camera model",
                "mosaic image",
                "subspace reconstruction",
                "panorama projective camera",
                "rendering"
            ]
        },
        "id": 129,
        "cited_by": []
    },
    {
        "title": "Joint feature distributions for image correspondence",
        "authors": [
            "B. Triggs"
        ],
        "abstract": "We introduce 'Joint Feature Distributions', a general statistical framework for feature based multi-image matching that explicitly models the joint probability distributions of corresponding features across several images. Conditioning on feature positions in some of the images gives well-localized distributions for their correspondents in the others, and hence tight likelihood regions for correspondence search. We apply the framework in the simplest case of Gaussian-like distributions over the direct sum (affine images) and tensor product (projective images) of the image coordinates. This produces probabilistic correspondence models that generalize the geometric multi-image matching constraints, roughly speaking by a form of model-averaging over them. These very simple methods predict accurate correspondence likelihood regions for any scene geometry including planar and near-planar scenes, without ill-conditioning or explicit model selection. Small amounts of distortion and non-rigidity are also tolerated. We develop the theory for any number of affine or projective images, explain its relationship to matching tensors, and give results for an initial implementation.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937625",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Solid modeling",
                "Layout",
                "Tensile stress",
                "Probability distribution",
                "Predictive models",
                "Training data",
                "World Wide Web",
                "Gaussian distribution",
                "Information geometry",
                "Search problems"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image correspondence",
                "multi-image matching",
                "image matching",
                "Joint Feature Distributions",
                "tight likelihood regions",
                "correspondence search",
                "affine images",
                "tensor product",
                "projective images",
                "probabilistic correspondence models",
                "multi-image matching constraints",
                "correspondence likelihood regions",
                "scene geometry"
            ]
        },
        "id": 130,
        "cited_by": [
            {
                "year": "2009",
                "id": 216
            }
        ]
    },
    {
        "title": "Shape deformation: SVM regression and application to medical image segmentation",
        "authors": [
            "Song Wang",
            "Weiyu Zhu",
            "Zhi-Pei Liang"
        ],
        "abstract": "This paper presents a novel landmark-based shape deformation method. This method effectively solves two problems inherent in landmark-based shape deformation: (a) identification of landmark points from a given input image, and (b) regularized deformation the shape of an an object defined in a template. The second problem is solved using a new constrained support vector machine (SVM) regression technique, in which a thin-plate kernel is utilized to provide non-rigid shape deformations. This method offers several advantages over existing landmark-based methods. First, it has a unique capability to detect and use multiple candidate landmark points in an input image to improve landmark detection. Second, it can handle the case of missing landmarks, which often arises in dealing with occluded images. We have applied the proposed method to extract the scalp contours from brain cryosection images with very encouraging results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937626",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Support vector machines",
                "Biomedical imaging",
                "Image segmentation",
                "Active contours",
                "Deformable models",
                "Kernel",
                "Scalp",
                "Computer vision",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "medical image processing",
                "learning automata",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "medical image segmentation",
                "SVM regression",
                "shape deformation",
                "landmark-based",
                "support vector machine",
                "regression technique",
                "thin-plate kernel",
                "landmark detection",
                "scalp contours",
                "brain cryosection images"
            ]
        },
        "id": 131,
        "cited_by": [
            {
                "year": "2005",
                "id": 69
            },
            {
                "year": "2003",
                "id": 121
            }
        ]
    },
    {
        "title": "Structure and motion from silhouettes",
        "authors": [
            "K.-Y.K. Wong",
            "R. Cipolla"
        ],
        "abstract": "This paper addresses the problem of recovering structure and motion from silhouettes. Silhouettes are projections of contour generators which are viewpoint dependent, and hence do not readily provide point correspondences for exploitation in motion estimation. Previous works have exploited correspondences induced by epipolar tangencies, and a successful solution has been developed in the special case of circular motion (turnable sequences). However, the main drawbacks are (1) new views cannot be added easily at a later time, and (2) part of the structure will always remain invisible under circular motion. In this paper we overcome the above problems by incorporating arbitrary general views and estimating the camera poses using silhouettes alone. We present a complete and practical system which produces high quality 3D models from 2D uncalibrated silhouettes. The 3D models thus obtained can be refined incrementally by adding new arbitrary views and estimating their poses. Experimental results on various objects are presented, demonstrating the quality of the reconstructions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937627",
        "reference_list": [],
        "citation": {
            "ieee": 42,
            "other": 17,
            "total": 59
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion estimation",
                "Cameras",
                "Data mining",
                "Shape",
                "Surface texture",
                "Geometry"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "recovering structure",
                "contour generators",
                "recovering motion",
                "motion estimation",
                "3D models",
                "uncalibrated silhouettes",
                "reconstructions"
            ]
        },
        "id": 132,
        "cited_by": [
            {
                "year": "2017",
                "id": 560
            },
            {
                "year": "2009",
                "id": 12
            },
            {
                "year": "2009",
                "id": 151
            },
            {
                "year": "2007",
                "id": 58
            },
            {
                "year": "2003",
                "id": 25
            }
        ]
    },
    {
        "title": "Propagation of innovative information in non-linear least-squares structure from motion",
        "authors": [
            "D. Steedly",
            "I. Essa"
        ],
        "abstract": "We present a new technique that improves upon existing structure from motion (SFM) methods. We propose a SFM algorithm that is both recursive and optimal. Our method incorporates innovative information from new frames into an existing solution without optimizing every camera pose and scene structure parameter. To do this, we incrementally optimize larger subsets of parameters until the error is minimized. These additional parameters are included in the optimization by tracing connections between points and frames. In many cases, the complexity of adding a frame is much smaller than full bundle adjustment of all the parameters. Our algorithm is best described us incremental bundle adjustment as it allows new information to be added to art existing non-linear least-squares solution.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937628",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 3,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Optimization methods",
                "Layout",
                "Robot vision systems",
                "Least squares methods",
                "Tensile stress",
                "Educational institutions",
                "Nonlinear filters",
                "Filtering",
                "Least squares approximation"
            ],
            "INSPEC: Controlled Indexing": [
                "computational complexity",
                "optimisation",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "innovative information",
                "nonlinear least-squares structure",
                "structure from motion",
                "scene structure",
                "least-squares solution"
            ]
        },
        "id": 133,
        "cited_by": []
    },
    {
        "title": "Uncalibrated motion capture exploiting articulated structure constraints",
        "authors": [
            "D. Liebowitz",
            "S. Carlsson"
        ],
        "abstract": "We present an algorithm for 3D reconstruction of dynamic articulated structures, such as humans, from uncalibrated multiple views. The reconstruction exploits constraints associated with a dynamic articulated structure, specifically the conservation over time of length between rotational joints. These constraints admit metric reconstruction from at least two different images in each of two uncalibrated parallel projection cameras. The algorithm is based on a stratified approach, starting with affine reconstruction from factorization, followed by rectification to metric structure using the articulated structure constraints. The exploitation of these specific constraints allows reconstruction and self-calibration with fewer feature paints and views compared to standard self-calibration. The method is extended to pairs of cameras that are zooming, Where calibration of the cameras allows compensation for the changing scale factor in a scaled orthographic camera. Results are presented in the form of stick figures and animated 3D reconstructions using pairs of sequences from broadcast television. The technique shows promise as a means of creating 3D animations of dynamic activities such as sports events.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937629",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 6,
            "total": 15
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Humans",
                "Image reconstruction",
                "Laboratories",
                "Animation",
                "Calibration",
                "Joints",
                "Computer vision",
                "Broadcasting",
                "Stability"
            ],
            "INSPEC: Controlled Indexing": [
                "computer animation",
                "image reconstruction",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion capture",
                "articulated structure constraints",
                "3D reconstruction",
                "uncalibrated multiple views",
                "dynamic articulated structures",
                "metric reconstruction",
                "parallel projection cameras",
                "reconstruction",
                "self-calibration",
                "orthographic camera",
                "3D animations"
            ]
        },
        "id": 134,
        "cited_by": []
    },
    {
        "title": "Affine 3-D reconstruction from two projective images of independently translating planes",
        "authors": [
            "L. Wolf",
            "A. Shashua"
        ],
        "abstract": "Consider two views of a multi-body scene consisting of k planar bodies moving in pure translation one relative to the other. We show that the fundamental matrices, one per body, live in a 3-dimensional subspace, which when represented as a step-3 extensor is the common transversal on the collection of extensors defined by the homograph matrices H/sub 1/,...,H/sub k/ of the moving planes. We show that as much as five bodies are necessary for recovering the common transversal from the homograph matrices, from which we show how to recover the fundamental matrices and the affine calibration between the two cameras.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937630",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 4,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Three dimensional displays",
                "Transmission line matrix methods",
                "Layout",
                "Cameras",
                "Symmetric matrices",
                "Calibration",
                "Computer science",
                "Geometry",
                "Image reconstruction",
                "Information resources"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "calibration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3-D reconstruction",
                "projective images",
                "independently translating planes",
                "multi-body scene",
                "homograph matrices",
                "affine calibration",
                "fundamental matrices"
            ]
        },
        "id": 135,
        "cited_by": []
    },
    {
        "title": "Harmonics extraction based on higher order statistics spectrum decomposition for a unified texture model",
        "authors": [
            "Yong Huang",
            "Kap Luk Chan",
            "Zhongyang Huang"
        ],
        "abstract": "By considering a texture being composed of two orthogonal components in a unified texture model, the deterministic component and the indeterministic component, a method of harmonics extraction from a Higher Order Statistics (HOS) based spectral decomposition is developed. The method estimates the power spectrum based on the diagonal slice of the fourth-order cumulants From this spectrum, the harmonic frequencies can be easily extracted even for noisy images. The simulation and experimental results indicate that this method is effective for texture decomposition and performs better than the traditional lower order statistics based decomposition methods.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937631",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Higher order statistics",
                "Frequency",
                "Gaussian noise",
                "Colored noise",
                "Computational modeling",
                "Image reconstruction",
                "Signal processing",
                "Random processes",
                "Spatial resolution",
                "Mathematical model"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "image texture"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "texture",
                "harmonics extraction",
                "Higher Order Statistics",
                "spectral decomposition",
                "noisy images",
                "texture decomposition"
            ]
        },
        "id": 136,
        "cited_by": []
    },
    {
        "title": "The Earth Mover's distance is the Mallows distance: some insights from statistics",
        "authors": [
            "E. Levina",
            "P. Bickel"
        ],
        "abstract": "The Earth Mover's distanc1e was first introduced as a purely empirical ways to measure texture and color similarities. We show that it has a rigorous probabilistic interpretation and is conceptually equivalent to the Mallows distance on probability distributions. The two distances are exactly the same when applied to probability distributions, but behave differently when applied to unnormalized distributions with different masses, called signatures. We discuss the advantages and disadvantages of both distances, and statistical issues involved in computing them from data. We also report some texture classification results for the Mallows distance applied to texture features and compare several ways of estimating feature distributions. In addition, we list some known probabilistic properties of this distance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937632",
        "reference_list": [],
        "citation": {
            "ieee": 65,
            "other": 42,
            "total": 107
        },
        "keywords": {
            "IEEE Keywords": [
                "Earth",
                "Statistics",
                "Probability distribution",
                "Image retrieval",
                "Image segmentation",
                "Humans",
                "Multidimensional systems",
                "Physics",
                "Fluid flow measurement",
                "Euclidean distance"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "probability",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Earth Mover's distance",
                "Mallows distance",
                "color similarities",
                "probability distributions",
                "texture classification"
            ]
        },
        "id": 137,
        "cited_by": []
    },
    {
        "title": "Physics-based model acquisition and identification in airborne spectral images",
        "authors": [
            "D. Slater",
            "G. Healey"
        ],
        "abstract": "We consider the problem of acquiring models for unknown materials in airborne 0.4 /spl mu/m-2.5 /spl mu/m hyperspectral imagery and using these models to identify the unknown materials an image data obtained under significantly different conditions. The material models are generated using an airborne sensor spectrum measured under unknown conditions and a physical model for spectral variability. For computational efficiency, the material models are represented using low-dimensional spectral subspaces. We demonstrate the effectiveness of the material models using a set of material tracking experiments in HYDICE images acquired in a forest environment over widely varying conditions. We show that techniques based on the new representation significantly outperform methods based on direct spectral matching.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937633",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Hyperspectral sensors",
                "Layout",
                "Reflectivity",
                "Atmospheric modeling",
                "Image sensors",
                "Computational efficiency",
                "Pixel",
                "Color",
                "Hyperspectral imaging",
                "Building materials"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image recognition",
                "remote sensing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model acquisition",
                "identification",
                "airborne spectral images",
                "hyperspectral imagery",
                "airborne sensor spectrum",
                "computational efficiency",
                "HYDICE images",
                "forest environment",
                "representation"
            ]
        },
        "id": 138,
        "cited_by": []
    },
    {
        "title": "Surface matching by 3D point's fingerprint",
        "authors": [
            "Y. Sun",
            "M.A. Abidi"
        ],
        "abstract": "This paper proposes a new efficient surface representation method for the application of surface matching. We generate a feature carrier for the surface point, which is a set of 2D contours that are the projection of geodesic circles onto the tangent plane. The carrier is named point's fingerprint because its pattern is similar to human fingerprint and discriminating for each point. Each point's fingerprint carries the information of the normal variation along geodesic circles. Corresponding points on surfaces from different views are found by comparing fingerprints of the points. This representation scheme includes more local geometry information than some previous works that only use one contour as the feature carrier. It is not histogram based so that it is able to carry more features to improve comparison accuracy. To speed up the matching, we use a novel candidate point selection method based on the shape irregularity of the projected local geodesic circle. The point's fingerprint is successfully used to register both synthetic and real 2 1/2 data.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937634",
        "reference_list": [],
        "citation": {
            "ieee": 37,
            "other": 13,
            "total": 50
        },
        "keywords": {
            "IEEE Keywords": [
                "Fingerprint recognition",
                "Histograms",
                "Application software",
                "Information geometry",
                "Sampling methods",
                "Humans",
                "Shape",
                "Registers",
                "Computer vision",
                "Closed-form solution"
            ],
            "INSPEC: Controlled Indexing": [
                "image matching",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface representation",
                "surface matching",
                "feature carrier",
                "geodesic circles",
                "point's fingerprint",
                "representation scheme",
                "local geometry information",
                "comparison accuracy",
                "candidate point selection",
                "shape irregularity",
                "matching"
            ]
        },
        "id": 139,
        "cited_by": [
            {
                "year": "2011",
                "id": 285
            }
        ]
    },
    {
        "title": "A Gabor feature classifier for face recognition",
        "authors": [
            "Chengjun Liu",
            "H. Wechsler"
        ],
        "abstract": "This paper describes a novel Gabor feature classifier (GFC) method for face recognition. The GFC method employs an enhanced Fisher discrimination model on an augmented Gabor feature vector, which is derived from the Gabor wavelet transformation of face images. The Gabor wavelets, whose kernels are similar to the 2D receptive field profiles of the mammalian cortical simple cells, exhibit desirable characteristics of spatial locality and orientation selectivity. As a result, the Gabor transformed face images produce salient local and discriminating features that are suitable for face recognition. The feasibility of the new GFC method has been successfully tested on face recognition using 600 FERET frontal face images, which involve different illumination and varied facial expressions of 200 subjects. The effectiveness of the novel GFC method is shown in terms of both absolute performance indices and comparative performance against some popular face recognition schemes such as the eigenfaces method and some other Gabor wavelet based classification methods. In particular, the novel GFC method achieves 100% recognition accuracy using only 62 features.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937635",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 4,
            "total": 25
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Kernel",
                "Computer science",
                "Testing",
                "Lighting",
                "Principal component analysis",
                "Linear discriminant analysis",
                "Independent component analysis",
                "Wavelet analysis",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "feature extraction",
                "wavelet transforms",
                "eigenvalues and eigenfunctions"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Gabor feature classifier",
                "face recognition",
                "enhanced Fisher discrimination model",
                "augmented Gabor feature vector",
                "Gabor wavelet transformation",
                "Gabor wavelets",
                "kernels",
                "mammalian cortical simple cells",
                "Gabor transformed face images",
                "eigenfaces method",
                "Gabor wavelet based classification"
            ]
        },
        "id": 140,
        "cited_by": []
    },
    {
        "title": "Recognizing large 3-D objects through next view planning using an uncalibrated camera",
        "authors": [
            "S.D. Roy",
            "S. Chaudhury",
            "S. Banerjee"
        ],
        "abstract": "We present a new on-line scheme for the recognition and pose estimation of a large isolated 3-D object, which may not entirely fit in a camera's field of view. We do not assume any knowledge of the internal parameters of the camera, or their constancy. We use a probabilistic reasoning framework for recognition and next view planning. We show results of successful recognition and pose estimation even in cases of a high degree of interpretation ambiguity associated with the initial view.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937636",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 2,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Object recognition",
                "Object detection",
                "Robot vision systems",
                "Computer vision",
                "Detectors",
                "Knowledge representation",
                "Costs"
            ],
            "INSPEC: Controlled Indexing": [
                "inference mechanisms",
                "object recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "large 3D objects recognition",
                "view planning",
                "uncalibrated camera",
                "online scheme",
                "pose recognition",
                "probabilistic reasoning framework"
            ]
        },
        "id": 141,
        "cited_by": []
    },
    {
        "title": "Pairwise face recognition",
        "authors": [
            "Guo-Dong Guo",
            "Hong-Jiang Zhang",
            "S.Z. Li"
        ],
        "abstract": "We develop a pairwise classification framework for face recognition, in which a C class face recognition problem is divided into a set of C(C-1)/2 two class problems. Such a problem decomposition not only leads to a set of simpler classification problems to be solved, thereby increasing overall classification accuracy, but also provides a framework for independent feature selection for each pair of classes. A simple feature ranking strategy is used to select a small subset of the features for each pair of classes. Furthermore, we evaluate two classification methods under the pairwise comparison framework: the Bayes classifier and the AdaBoost. Experiments on a large face database with 1079 face images of 137 individuals indicate that 20 features are enough to achieve a relatively high recognition accuracy, which demonstrates the effectiveness of the pairwise recognition framework.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937637",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 0,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Principal component analysis",
                "Spatial databases",
                "Image recognition",
                "Image databases",
                "Authentication",
                "Surveillance",
                "Pattern recognition",
                "Robustness",
                "Scattering"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "image classification",
                "feature extraction",
                "Bayes methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "pairwise face recognition",
                "pairwise classification framework",
                "classification problems",
                "feature selection",
                "feature ranking strategy",
                "classification methods",
                "pairwise comparison framework",
                "Bayes classifier",
                "AdaBoost",
                "large face database",
                "pairwise recognition framework"
            ]
        },
        "id": 142,
        "cited_by": [
            {
                "year": "2011",
                "id": 212
            }
        ]
    },
    {
        "title": "Separating appearance from deformation",
        "authors": [
            "N. Jojic",
            "P. Simard",
            "B.J. Frey",
            "D. Heckerman"
        ],
        "abstract": "By representing images and image prototypes by linear subspaces spanned by \"tangent vectors\" (derivatives of an image with respect to translation, rotation, etc.), impressive invariance to known types of uniform distortion can be built into feedforward discriminators. We describe a new probability model that can jointly cluster data and learn mixtures of nonuniform, smooth deformation fields. Our fields are based on low-frequency wavelets, so they use very few parameters to model a wide range of smooth deformations (unlike, e.g., factor analysis, which uses a large number of parameters to model deformations). In spirit, our ideas are most similar to the idea of separating content from style published by Tenenbaum and Freeman. However, our models do not need labeled data for training, and thus allow for unsupervised separation of appearance from deformation. We give results on handwritten digit recognition and face recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937638",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Deformable models",
                "Face recognition",
                "Video sequences",
                "Prototypes",
                "Handwriting recognition",
                "Image recognition",
                "Computer vision",
                "Image processing",
                "Character recognition",
                "Motion estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "image recognition",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image prototypes",
                "feedforward discriminators",
                "probability model",
                "smooth deformation fields",
                "digit recognition",
                "face recognition",
                "deformation"
            ]
        },
        "id": 143,
        "cited_by": []
    },
    {
        "title": "View-based clustering of object appearances based on independent subspace analysis",
        "authors": [
            "S.Z. Li",
            "Xiao Guang Lv",
            "Hong Jiang Zhang"
        ],
        "abstract": "In 3D object detection and recognition, an object of interest is subject to changes in view as well as in illumination and shape. For image classification purpose, it is desirable to derive a representation in which intrinsic characteristics of the object are captured in a low dimensional space while effects due to artifacts are reduced. In this paper, we propose a method for view-based unsupervised learning of object appearances. First, view-subspaces are learned from a view-unlabeled data set of multi-view appearances, using independent subspace analysis (ISA). A learned view-subspace provides a representation of appearances at that view, regardless of illumination effect. A measure, called view-subspace activity, is calculated thereby to provide a metric for view-based classification. View-based clustering is then performed by using maximum view-subspace activity (MVSA) criterion. This work is to the best of our knowledge the first devoted research on view-based clustering of images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937639",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 5,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Object detection",
                "Shape",
                "Principal component analysis",
                "Image classification",
                "Unsupervised learning",
                "Instruction sets",
                "Image analysis",
                "Image retrieval",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "object recognition",
                "image classification",
                "unsupervised learning",
                "pattern clustering"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "object detection",
                "recognition",
                "representation",
                "unsupervised learning",
                "object appearances",
                "view-subspace activity",
                "view-based clustering",
                "view-based classification"
            ]
        },
        "id": 144,
        "cited_by": []
    },
    {
        "title": "Do we really have to consider covariance matrices for image features?",
        "authors": [
            "Y. Kanazawa",
            "K. Kanatani"
        ],
        "abstract": "Many studies have been made in the past for optimization using covariance matrices of feature points. We first describe how to compute the covariance matrix of a feature point from the gray levels by integrating existing methods. Then, we experimentally examine if thus computed covariance matrices really reflect the accuracy of the feature points. To test this, we do subpixel template matching and compute the homography and the fundamental matrix. Our conclusion is rather surprising, pointing out important elements often overlooked.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937640",
        "reference_list": [],
        "citation": {
            "ieee": 28,
            "other": 17,
            "total": 45
        },
        "keywords": {
            "IEEE Keywords": [
                "Covariance matrix",
                "Uncertainty",
                "Computer vision",
                "Least squares approximation",
                "Knowledge engineering",
                "Computer science",
                "Testing",
                "Three dimensional displays",
                "Computational modeling",
                "Noise generators"
            ],
            "INSPEC: Controlled Indexing": [
                "covariance matrices",
                "image matching",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "covariance matrices",
                "image features",
                "optimization",
                "feature points",
                "gray levels",
                "template matching"
            ]
        },
        "id": 145,
        "cited_by": []
    },
    {
        "title": "3D object tracking using shape-encoded particle propagation",
        "authors": [
            "H. Moon",
            "R. Chellappa",
            "A. Rosenfeld"
        ],
        "abstract": "We present a comprehensive treatment of 3D object tracking by posing it as a nonlinear state estimation problem. The measurements are derived using the outputs of shape-encoded filters. The nonlinear state estimation is performed by solving the Zakai equation, and we use the branching particle propagation method for computing the solution. The unnormalized conditional density for the solution to the Zakai equation is realized by the weight of the particle. We first sample a set of particles approximating the initial distribution of the state vector conditioned on the observations, where each particle encodes the set of geometric parameters of the object. The weight of the particle represents geometric and temporal fit, which is computed bottom-up from the raw image using a shape-encoded filter. The particles branch so that the mean number of offspring is proportional to the weight. Time update is handled by employing a second-order motion model, combined with local stochastic search to minimize the prediction error. The prediction adjustment suggested by system identification theory is empirically verified to contribute to global stability. The amount of diffusion is effectively adjusted using a Kalman updating of the covariance matrix. WE have successfully applied this method to human head tracking, where we estimate head motion and compute structure using simple head and facial feature models.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937641",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 4,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Particle tracking",
                "Head",
                "State estimation",
                "Filters",
                "Nonlinear equations",
                "Shape measurement",
                "Predictive models",
                "Stochastic processes",
                "Error correction",
                "System identification"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "feature extraction",
                "object detection"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D object tracking",
                "nonlinear state estimation",
                "shape-encoded filters",
                "geometric parameters",
                "local stochastic search",
                "second-order motion model",
                "prediction error",
                "prediction adjustment",
                "human head tracking",
                "head motion",
                "facial feature models"
            ]
        },
        "id": 146,
        "cited_by": []
    },
    {
        "title": "Real-time tracking of highly articulated structures in the presence of noisy measurements",
        "authors": [
            "T. Drummond",
            "R. Cipolla"
        ],
        "abstract": "This paper presents a novel approach for model-based real-time tracking of highly articulated structures such as humans. This approach is based on an algorithm which efficiently propagates statistics of probability distributions through a kinematic chain to obtain maximum a posteriori estimates of the motion of the entire structure. This algorithm yields the least squares solution in linear time (in the number of components of the model) and can also be applied to non-Gaussian statistics using a simple but powerful trick. The resulting implementation runs in real-time on standard hardware without any pre-processing of the video data and can thus operate on live video. Results from experiments performed using this system are presented and discussed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937642",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 12,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Iterative algorithms",
                "Statistical distributions",
                "Time measurement",
                "Tracking",
                "Humans",
                "Kinematics",
                "Least squares methods",
                "Robustness",
                "Algebra",
                "Statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "tracking",
                "real-time systems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based real-time tracking",
                "articulated structures",
                "probability distributions",
                "a posteriori estimates",
                "least squares solution",
                "real-time",
                "live video",
                "kinematic chain"
            ]
        },
        "id": 147,
        "cited_by": []
    },
    {
        "title": "People tracking using hybrid Monte Carlo filtering",
        "authors": [
            "Kiam Choo",
            "D.J. Fleet"
        ],
        "abstract": "Particle filters are used for hidden state estimation with nonlinear dynamical systems. The inference of 3-D human motion is a natural application, given the nonlinear dynamics of the body and the nonlinear relation between states and image observations. However, the application of particle filters has been limited to cases where the number of state variables is relatively small, because the number of samples needed with high dimensional problems can be prohibitive. We describe a filter that uses hybrid Monte Carlo (HMC) to obtain samples in high dimensional spaces. It uses multiple Markov chains that use posterior gradients to rapidly explore the state space, yielding fair samples from the posterior. We find that the HMC filter is several thousand times faster than a conventional particle filter on a 28 D people tracking problem.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937643",
        "reference_list": [],
        "citation": {
            "ieee": 54,
            "other": 2,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Monte Carlo methods",
                "Particle filters",
                "State-space methods",
                "Filtering",
                "Humans",
                "Space exploration",
                "Layout",
                "Distributed computing",
                "Probability distribution",
                "Proposals"
            ],
            "INSPEC: Controlled Indexing": [
                "state estimation",
                "nonlinear dynamical systems",
                "tracking",
                "motion estimation",
                "Monte Carlo methods",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Monte Carlo filtering",
                "hidden state estimation",
                "nonlinear dynamical systems",
                "3-D human motion",
                "multiple Markov chains",
                "HMC filter",
                "people tracking"
            ]
        },
        "id": 148,
        "cited_by": [
            {
                "year": "2013",
                "id": 20
            },
            {
                "year": "2011",
                "id": 262
            },
            {
                "year": "2009",
                "id": 148
            },
            {
                "year": "2003",
                "id": 140
            }
        ]
    },
    {
        "title": "Improving AR using shadows arising from natural illumination distribution in video sequences",
        "authors": [
            "Taeone Kim",
            "Yong-Duck Seo",
            "Ki-Sang Hong"
        ],
        "abstract": "In this paper, we propose a method for generating realistic shadows of virtual objects inserted into a real video sequence. Our aim is to improve and extend the work of Sato, Sato, and Ikeuchi, (1999), which is based on a static camera, to the case of a video sequence. This extension consists of several procedures: calibration of a moving video camera and a graphic camera, removing false shadows occurring due to a shortcoming of the static camera approach for the estimation of an illumination distribution, and so on. The calibration of the moving camera is solved by camera self-calibration and, with it, we designed a flexible graphic world coordinate system embedding technique called \"match move\". We also show that the shortcoming of the previous static camera approach is overcome by using information from video sequence. Finally we present the experimental results of a real video sequence.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937644",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Video sequences",
                "Cameras",
                "Graphics",
                "Layout",
                "Calibration",
                "Image reconstruction",
                "Information processing",
                "Image sampling",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "augmented reality",
                "image sequences",
                "video signal processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "realistic shadows",
                "virtual objects",
                "video sequence",
                "calibration",
                "moving video camera",
                "graphic camera",
                "graphic world coordinate system",
                "match move"
            ]
        },
        "id": 149,
        "cited_by": [
            {
                "year": "2005",
                "id": 34
            }
        ]
    },
    {
        "title": "An EM algorithm for video summarization, generative model approach",
        "authors": [
            "X. Orriols",
            "X. Binefa"
        ],
        "abstract": "In this paper, we address the visual video summarization problem in a Bayesian framework in order to detect and describe the underlying temporal transformation symmetries in a video sequence. Given a set of time correlated frames, we attempt to extract a reduced number of image-like data structures which are semantically meaningful and that have the ability of representing the sequence evolution. To this end, we present a generative model which involves jointly the representation and the evolution of appearance. Applying Linear Dynamical System theory to this problem, we discuss how the temporal information is encoded yielding a manner of grouping the iconic representations of the video sequence in terms of invariance. The formulation of this problem is driven in terms of a probabilistic approach, which affords a measure of perceptual similarity taking both learned appearance and time evolution models into account.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937645",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Video sequences",
                "Computer vision",
                "Data mining",
                "Data structures",
                "Cameras",
                "Bayesian methods",
                "Information retrieval",
                "Content based retrieval",
                "Databases",
                "Application software"
            ],
            "INSPEC: Controlled Indexing": [
                "content-based retrieval",
                "video databases",
                "image sequences"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video summarization",
                "generative model",
                "Bayesian framework",
                "temporal transformation symmetries",
                "video sequence",
                "Linear Dynamical System",
                "perceptual similarity",
                "evolution models",
                "video data-bases",
                "retrieval"
            ]
        },
        "id": 150,
        "cited_by": [
            {
                "year": "2003",
                "id": 13
            }
        ]
    },
    {
        "title": "Video georegistration: algorithm and quantitative evaluation",
        "authors": [
            "R.P. Wildes",
            "D.J. Hirvonen",
            "S.C. Hsu",
            "R. Kumar",
            "W.B. Lehman",
            "B. Matei",
            "W.-Y. Zhao"
        ],
        "abstract": "An algorithm is presented for video georegistration, with a particular concern for aerial video, i.e., video captured from an airborne platform. The algorithm's input is a video stream with telemetry (camera model specification sufficient to define an initial estimate of the view) and geodetically calibrated reference imagery (coaligned digital orthoimage and elevation map). The output is a spatial registration of the video to the reference so that it inherits the available geodetic coordinates. The video is processed in a continuous fashion to yield a corresponding stream of georegistered results. Quantitative results of evaluating the developed approach with real world aerial video also are presented. The results suggest that the developed approach may provide valuable input to the analysis and interpretation of aerial video.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937646",
        "reference_list": [],
        "citation": {
            "ieee": 9,
            "other": 4,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Telemetry",
                "Streaming media",
                "Terrain mapping",
                "Digital cameras",
                "Information resources",
                "Image databases",
                "Robustness",
                "Rendering (computer graphics)"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "geographic information systems",
                "terrain mapping"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "video georegistration",
                "aerial video",
                "video stream",
                "spatial registration",
                "geodetic coordinates"
            ]
        },
        "id": 151,
        "cited_by": []
    },
    {
        "title": "Automatic segmentation and indexing in a database of bird images",
        "authors": [
            "M. Das",
            "R. Manmatha"
        ],
        "abstract": "The aim of this work is to index images in domain specific databases using colors computed from the object of interest only, instead of the whole image. The main problem in this task is the segmentation of the region of interest from the background. Viewing segmentation as a figure/ground segregation problem leads to a new approach-eliminating the background leaves the figure or object of interest. To find possible object colors, we first find background colors and eliminate them. We then use an edge image at an appropriate scale to eliminate those parts of the image which are not in focus and do not contain significant structures. The edge information is combined with the color-based background elimination to produce object (figure) regions. We test our approach on a database of bird images. We show that in 87% or 450 bird images tested, the segmentation is sufficient to determine the colors of the bird correctly for retrieval purposes. We also show that our approach provides improved retrieval performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937647",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 2,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Indexing",
                "Image databases",
                "Birds",
                "Image retrieval",
                "Information retrieval",
                "Multimedia databases",
                "Testing",
                "Focusing",
                "Image generation"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "database indexing",
                "visual databases",
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "segmentation",
                "indexing",
                "database",
                "bird images",
                "domain specific databases",
                "object colors",
                "edge image",
                "retrieval"
            ]
        },
        "id": 152,
        "cited_by": []
    },
    {
        "title": "Stripe boundary codes for real-time structured-light range scanning of moving objects",
        "authors": [
            "O. Hall-Holt",
            "S. Rusinkiewicz"
        ],
        "abstract": "We present a novel approach to real-time structured light range scanning. After an analysis of the underlying assumptions of existing structured light techniques, we derive a new set of illumination patterns based on coding the boundaries between projected stripes. These stripe boundary codes allow range scanning of moving objects, with only modest assumptions about scene continuity and reflectance. We describe an implementation that integrates these new codes with real-time algorithms for tracking stripe boundaries and determining depths. Our system uses a standard video camera and DLP projector, and produces dense range images at 60 Hz with 100 /spl mu/m accuracy over a 10 cm working volume. As an application, we demonstrate the creation of complete models of rigid objects: the objects are rotated in front of the scanner by hand, and successive range images are automatically aligned.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937648",
        "reference_list": [],
        "citation": {
            "ieee": 63,
            "other": 26,
            "total": 89
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Hardware",
                "Application software",
                "Computer graphics",
                "Costs",
                "Lighting",
                "Reflectivity",
                "Taxonomy",
                "Pattern analysis",
                "Shape"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "motion estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "light range scanning",
                "illumination patterns",
                "stripe boundary codes",
                "range scanning",
                "moving objects",
                "dense range images"
            ]
        },
        "id": 153,
        "cited_by": [
            {
                "year": "2011",
                "id": 242
            },
            {
                "year": "2009",
                "id": 228
            }
        ]
    },
    {
        "title": "Document restoration using 3D shape: a general deskewing algorithm for arbitrarily warped documents",
        "authors": [
            "M.S. Brown",
            "W.B. Seales"
        ],
        "abstract": "We present a framework for restoring arbitrarily warped and deformed documents to their original planar shape. The impetus for this work is the need for tools and techniques to help digitally preserve and restore fragile manuscripts. Current digitization is performed under the assumption that the documents are flat, with subsequent image-processing and restoration algorithms either relying on this assumption or attempting to overcome it without shape information. Although most manuscripts were originally flat, many become deformed from damage and deterioration. Physical flattening is not possible without risking further, possibly irreversible, damage. Our framework addresses this restoration problem with two primary contributions. First, we present a working 3D digitization setup that acquires a 3D model with accurate shape-to-texture registration under multiple lighting conditions. Second, we show how the 3D model and a mass-spring particle system can be used together as a framework for digital flattening. We show that this restoration process can correct document deformations and can significantly improve subsequent document analysis.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937649",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 8,
            "total": 43
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image restoration",
                "Optical materials",
                "Optical character recognition software",
                "Optical sensors",
                "Software libraries",
                "Optical distortion",
                "Facsimile",
                "Computer science",
                "Springs"
            ],
            "INSPEC: Controlled Indexing": [
                "document image processing",
                "image restoration",
                "image registration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "warped documents",
                "deskewing algorithm",
                "document restoration",
                "image-processing",
                "restoration",
                "3D digitization",
                "shape-to-texture registration",
                "multiple lighting",
                "document analysis"
            ]
        },
        "id": 154,
        "cited_by": [
            {
                "year": "2003",
                "id": 31
            }
        ]
    },
    {
        "title": "The KGBR viewpoint-lighting ambiguity and its resolution by generic constraints",
        "authors": [
            "A.L. Yuille",
            "J.M. Coughlan",
            "S. Konishi"
        ],
        "abstract": "We describe a novel viewpoint-lighting ambiguity which we call the KGBR. This ambiguity assumes orthographic projecting or an affine camera, and uses Lambertian reflectance functions including case/attached shadows and multiple light sources. A KGBR transform alters the geometry (by a three-dimensional affine transformation) and albedo properties of objects. If two objects are related by a KGBR transform then for any viewpoint and lighting of the first object there exists a corresponding viewpoint and lighting of the second object so that the images are identical up to an affine transformation. The Generalized Bas Relief (GBR) ambiguity is obtained as a special case of the KGBR. We describe generic viewpoint and lighting assumptions and show that either, or both, resolve this ambiguity by biasing towards objects with planar geometry.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937650",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Cameras",
                "Light sources",
                "Art",
                "Equations",
                "Reflectivity",
                "Photometry",
                "Ear",
                "Humans",
                "Painting"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "computer graphics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "viewpoint-lighting ambiguity",
                "KGBR",
                "orthographic projecting",
                "affine camera",
                "Lambertian reflectance functions",
                "albedo properties",
                "Generalized Bas Relief",
                "planar geometry"
            ]
        },
        "id": 155,
        "cited_by": []
    },
    {
        "title": "Lambertian reflectance and linear subspaces",
        "authors": [
            "R. Basri",
            "D. Jacobs"
        ],
        "abstract": "We prove that the set of all reflectance functions (the mapping from surface normals to intensities) produced by Lambertian objects under distant, isotropic lighting lies close to a 9D linear subspace. This implies that the images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately with a low-dimensional linear subspace, explaining prior empirical results. We also provide a simple analytic characterization of this linear space. We obtain these results by representing lighting using spherical harmonics and describing the effects of Lambertian materials as the analog of a convolution. These results allow us to construct algorithms for object recognition based on linear methods as well as algorithms that use convex optimization to enforce non-negative lighting functions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937651",
        "reference_list": [],
        "citation": {
            "ieee": 68,
            "other": 28,
            "total": 96
        },
        "keywords": {
            "IEEE Keywords": [
                "Reflectivity",
                "National electric code",
                "Convolution",
                "Object recognition",
                "Algorithm design and analysis",
                "Power harmonic filters",
                "Kernel",
                "Jacobian matrices",
                "Computer science",
                "Optimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "convex programming"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "reflectance functions",
                "Lambertian objects",
                "isotropic lighting",
                "convex Lambertian object",
                "spherical harmonics",
                "object recognition",
                "convex optimization"
            ]
        },
        "id": 156,
        "cited_by": [
            {
                "year": "2005",
                "id": 42
            },
            {
                "year": "2003",
                "id": 105
            },
            {
                "year": "2003",
                "id": 175
            },
            {
                "year": "2003",
                "id": 197
            }
        ]
    },
    {
        "title": "Beyond Lambert: reconstructing surfaces with arbitrary BRDFs",
        "authors": [
            "S. Magda",
            "D.J. Kriegman",
            "T. Zickler",
            "P.N. Belhumeur"
        ],
        "abstract": "We address an open and hitherto neglected problem in computer vision, how to reconstruct the geometry of objects with arbitrary and possibly anisotropic bidirectional reflectance distribution functions (BRDFs). Present reconstruction techniques, whether stereo vision, structure from motion, laser range finding, etc. make explicit or implicit assumptions about the BRDF. Here, we introduce two methods that were developed by re-examining the underlying image formation process; the methods make no assumptions about the object's shape, the presence or absence of shadowing, or the nature of the BRDF which may vary over the surface. The first method takes advantage of Helmholtz reciprocity, while the second method exploits the fact that the radiance along a ray of light is constant. In particular, the first method uses stereo pairs of images in which point light sources are co-located at the centers of projection of the stereo cameras. The second method is based on double covering a scene's incident light field; the depths of surface points are estimated using a large collection of images in which the viewpoint remains fixed and a point light source illuminates the object. Results from our implementations lend empirical support to both techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937652",
        "reference_list": [],
        "citation": {
            "ieee": 30,
            "other": 8,
            "total": 38
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Image reconstruction",
                "Stereo vision",
                "Light sources",
                "Computer vision",
                "Geometrical optics",
                "Computational geometry",
                "Anisotropic magnetoresistance",
                "Bidirectional control",
                "Distribution functions"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computer vision",
                "reconstructing surfaces",
                "arbitrary BRDFs",
                "reconstruction techniques",
                "image formation process",
                "incident light field"
            ]
        },
        "id": 157,
        "cited_by": [
            {
                "year": "2007",
                "id": 42
            },
            {
                "year": "2007",
                "id": 61
            },
            {
                "year": "2005",
                "id": 56
            },
            {
                "year": "2003",
                "id": 128
            },
            {
                "year": "2003",
                "id": 184
            }
        ]
    },
    {
        "title": "On the complexity of probabilistic image retrieval",
        "authors": [
            "N. Vasconcelos"
        ],
        "abstract": "Probabilistic image retrieval approaches can lead to significant gains over standard retrieval techniques. However, this occurs at the cost of a significant increase in computational complexity. In fact, closed-form solutions for probabilistic retrieval are currently available only for simple representations such as the Gaussian and the histogram. We analyze the case of mixture densities and exploit the asymptotic equivalence between likelihood and Kullback-Leibler divergence to derive solutions for these models. In particular, (1) we show that the divergence can be computed exactly for vector quantizers and, (2) has an approximate solution for Gaussian mixtures that introduces no significant degradation of the resulting similarity judgments. In both cases, the new solutions have closed-form and computational complexity equivalent to that of standard retrieval approaches, but significantly better retrieval performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937653",
        "reference_list": [],
        "citation": {
            "ieee": 22,
            "other": 5,
            "total": 27
        },
        "keywords": {
            "IEEE Keywords": [
                "Image retrieval",
                "Histograms",
                "Closed-form solution",
                "Content based retrieval",
                "Maximum likelihood estimation",
                "Laboratories",
                "Costs",
                "Image analysis",
                "Feature extraction",
                "Algorithm design and analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image retrieval",
                "computational complexity"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "complexity",
                "probabilistic image retrieval",
                "computational complexity",
                "probabilistic retrieval",
                "asymptotic equivalence",
                "retrieval"
            ]
        },
        "id": 158,
        "cited_by": [
            {
                "year": "2013",
                "id": 210
            },
            {
                "year": "2011",
                "id": 222
            },
            {
                "year": "2003",
                "id": 64
            }
        ]
    },
    {
        "title": "Learning the semantics of words and pictures",
        "authors": [
            "K. Barnard",
            "D. Forsyth"
        ],
        "abstract": "We present a statistical model for organizing image collections which integrates semantic information provided by associate text and visual information provided by image features. The model is very promising for information retrieval tasks such as database browsing and searching for images based on text and/or image features. Furthermore, since the model learns relationships between text and image features, it can be used for novel applications such as associating words with pictures, and unsupervised learning for object recognition.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937654",
        "reference_list": [],
        "citation": {
            "ieee": 116,
            "other": 94,
            "total": 210
        },
        "keywords": {
            "IEEE Keywords": [
                "Image databases",
                "Image retrieval",
                "Spatial databases",
                "Unsupervised learning",
                "Object recognition",
                "Statistics",
                "Predictive models",
                "Image segmentation",
                "Organizing",
                "Information retrieval"
            ],
            "INSPEC: Controlled Indexing": [
                "unsupervised learning",
                "information retrieval",
                "image retrieval",
                "visual databases"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image collections",
                "semantic information",
                "associate text",
                "visual information",
                "information retrieval tasks",
                "database browsing",
                "image features",
                "unsupervised learning",
                "object recognition"
            ]
        },
        "id": 159,
        "cited_by": [
            {
                "year": "2009",
                "id": 54
            },
            {
                "year": "2007",
                "id": 24
            },
            {
                "year": "2005",
                "id": 48
            }
        ]
    },
    {
        "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
        "authors": [
            "D. Martin",
            "C. Fowlkes",
            "D. Tal",
            "J. Malik"
        ],
        "abstract": "This paper presents a database containing 'ground truth' segmentations produced by humans for images of a wide variety of natural scenes. We define an error measure which quantifies the consistency between segmentations of differing granularities and find that different human segmentations of the same image are highly consistent. Use of this dataset is demonstrated in two applications: (1) evaluating the performance of segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937655",
        "reference_list": [],
        "citation": {
            "ieee": 1396,
            "other": 691,
            "total": 2087
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Image databases",
                "Humans",
                "Image recognition",
                "Layout",
                "Computer errors",
                "Testing",
                "Application software",
                "Electric variables measurement",
                "Statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "visual databases",
                "image segmentation",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image database",
                "ground truth",
                "natural scenes",
                "segmentations",
                "performance of segmentation",
                "probability distributions",
                "Gestalt grouping factors",
                "image region properties"
            ]
        },
        "id": 160,
        "cited_by": [
            {
                "year": "2017",
                "id": 23
            },
            {
                "year": "2017",
                "id": 110
            },
            {
                "year": "2017",
                "id": 112
            },
            {
                "year": "2017",
                "id": 380
            },
            {
                "year": "2017",
                "id": 456
            },
            {
                "year": "2017",
                "id": 476
            },
            {
                "year": "2017",
                "id": 477
            },
            {
                "year": "2017",
                "id": 504
            },
            {
                "year": "2017",
                "id": 507
            },
            {
                "year": "2017",
                "id": 577
            },
            {
                "year": "2015",
                "id": 41
            },
            {
                "year": "2015",
                "id": 44
            },
            {
                "year": "2015",
                "id": 45
            },
            {
                "year": "2015",
                "id": 51
            },
            {
                "year": "2015",
                "id": 67
            },
            {
                "year": "2015",
                "id": 180
            },
            {
                "year": "2015",
                "id": 181
            },
            {
                "year": "2013",
                "id": 69
            },
            {
                "year": "2013",
                "id": 207
            },
            {
                "year": "2013",
                "id": 218
            },
            {
                "year": "2013",
                "id": 229
            },
            {
                "year": "2013",
                "id": 272
            },
            {
                "year": "2013",
                "id": 279
            },
            {
                "year": "2013",
                "id": 311
            },
            {
                "year": "2013",
                "id": 353
            },
            {
                "year": "2013",
                "id": 440
            },
            {
                "year": "2011",
                "id": 46
            },
            {
                "year": "2011",
                "id": 56
            },
            {
                "year": "2011",
                "id": 100
            },
            {
                "year": "2011",
                "id": 125
            },
            {
                "year": "2011",
                "id": 268
            },
            {
                "year": "2011",
                "id": 332
            },
            {
                "year": "2009",
                "id": 59
            },
            {
                "year": "2009",
                "id": 71
            },
            {
                "year": "2009",
                "id": 72
            },
            {
                "year": "2009",
                "id": 84
            },
            {
                "year": "2009",
                "id": 104
            },
            {
                "year": "2009",
                "id": 288
            },
            {
                "year": "2009",
                "id": 294
            },
            {
                "year": "2009",
                "id": 306
            },
            {
                "year": "2007",
                "id": 70
            },
            {
                "year": "2007",
                "id": 80
            },
            {
                "year": "2007",
                "id": 81
            },
            {
                "year": "2007",
                "id": 90
            },
            {
                "year": "2007",
                "id": 140
            },
            {
                "year": "2007",
                "id": 144
            },
            {
                "year": "2005",
                "id": 5
            },
            {
                "year": "2003",
                "id": 1
            },
            {
                "year": "2003",
                "id": 2
            },
            {
                "year": "2003",
                "id": 42
            }
        ]
    },
    {
        "title": "Capturing natural hand articulation",
        "authors": [
            "Ying Wu",
            "J.Y. Lin",
            "T.S. Huang"
        ],
        "abstract": "Vision-based motion capturing of hand articulation is a challenging task, since the hand presents a motion of high degrees of freedom. Model-based approaches could be taken to approach this problem by searching in a high dimensional hand state space, and matching projections of a hand model and image observations. However, it is highly inefficient due to the curse of dimensionality. Fortunately, natural hand articulation is highly constrained, which largely reduces the dimensionality of hand state space. This paper presents a model-based method to capture hand articulation by learning hand natural constraints. Our study shows that natural hand articulation lies in a lower dimensional configurations space characterized by a union of liner manifolds spanned by a set of basis configurations. By integrating hand motion constraints, an efficient articulated motion-capturing algorithm is proposed based on sequential Monte Carlo techniques. Our experiments show that this algorithm is robust and accurate for tracking natural hand movements. This algorithm is easy to extend to other articulated motion capturing tasks.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937656",
        "reference_list": [],
        "citation": {
            "ieee": 40,
            "other": 2,
            "total": 42
        },
        "keywords": {
            "IEEE Keywords": [
                "Humans",
                "Robustness",
                "State-space methods",
                "Handicapped aids",
                "Fingers",
                "Uncertainty",
                "Search problems",
                "Shape",
                "Biological system modeling",
                "State estimation"
            ],
            "INSPEC: Controlled Indexing": [
                "gesture recognition",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hand articulation",
                "motion capturing",
                "matching projections",
                "model-based method",
                "Monte Carlo techniques",
                "motion capturing tasks"
            ]
        },
        "id": 161,
        "cited_by": [
            {
                "year": "2013",
                "id": 306
            },
            {
                "year": "2009",
                "id": 189
            },
            {
                "year": "2003",
                "id": 99
            },
            {
                "year": "2003",
                "id": 140
            },
            {
                "year": "2003",
                "id": 145
            }
        ]
    },
    {
        "title": "Example-based facial sketch generation with non-parametric sampling",
        "authors": [
            "Hong Chen",
            "Ying-Qing Xu",
            "Heung-Yeung Shum",
            "Song-Chun Zhu",
            "Nan-Ning Zheng"
        ],
        "abstract": "In this paper, we present an example-based facial sketch system. Our system automatically generates a sketch from an input image, by learning from example sketches drawn with a particular style by an artist. There are two key elements in our system: a non-parametric sampling method and a flexible sketch model. Given an input image pixel and its neighborhood, the conditional distribution of a sketch point is computed by querying the examples and finding all similar neighborhoods. An \"expected sketch image\" is then drawn from the distribution to reflect the drawing style. Finally, facial sketches are obtained by incorporating the sketch model. Experimental results demonstrate the effectiveness of our techniques.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937657",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 5,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Sampling methods",
                "Image sampling",
                "Face",
                "Pixel",
                "Distributed computing",
                "Psychology",
                "Decoding",
                "Humans",
                "Artificial intelligence",
                "Intelligent robots"
            ],
            "INSPEC: Controlled Indexing": [
                "image processing",
                "computer graphics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "facial sketch generation",
                "non-parametric sampling",
                "flexible sketch model",
                "input image pixel",
                "facial sketches"
            ]
        },
        "id": 162,
        "cited_by": []
    },
    {
        "title": "Dynamic textures",
        "authors": [
            "S. Soatto",
            "G. Doretto",
            "Ying Nian Wu"
        ],
        "abstract": "Dynamic textures are sequences of images of moving scenes that exhibit certain stationarity properties in time; these include sea-waves, smoke, foliage, whirlwind but also talking faces, traffic scenes etc. We present a novel characterization of dynamic textures that poses the problems of modelling, learning, recognizing and synthesizing dynamic textures on a firm analytical footing. We borrow tools from system identification to capture the \"essence\" of dynamic textures; we do so by learning (i.e. identifying) models that are optimal in the sense of maximum likelihood or minimum prediction error variance. For the special case of second-order stationary processes we identify the model in closed form. Once learned, a model has predictive power and can be used for extrapolating synthetic sequences to infinite length with negligible computational cost. We present experimental evidence that, within our framework, even low dimensional models can capture very complex visual phenomena.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937658",
        "reference_list": [],
        "citation": {
            "ieee": 77,
            "other": 33,
            "total": 110
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Image reconstruction",
                "Photometry",
                "Power system modeling",
                "Computer science",
                "Predictive models",
                "Shape",
                "Reflectivity",
                "Geometry",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image texture",
                "maximum likelihood estimation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "sequences of images",
                "moving scenes",
                "dynamic textures",
                "system identification",
                "learning",
                "maximum likelihood",
                "minimum prediction",
                "extrapolating synthetic sequences"
            ]
        },
        "id": 163,
        "cited_by": [
            {
                "year": "2015",
                "id": 76
            },
            {
                "year": "2003",
                "id": 5
            },
            {
                "year": "2003",
                "id": 145
            },
            {
                "year": "2003",
                "id": 161
            },
            {
                "year": "2003",
                "id": 170
            }
        ]
    },
    {
        "title": "Shape from texture and integrability",
        "authors": [
            "D.A. Forsyth"
        ],
        "abstract": "We describe a shape from texture method that constructs a maximum a posteriori estimate of surface coefficients using both the deformation of individual texture elements-as in local methods-and the overall distribution of elements-as in global methods. The method described applies to a much larger family of textures than any previous method, local or global. We demonstrate an analogy with shape from shading, and use this to produce a numerical method. Examples of reconstructions for synthetic images of surfaces are provided, and compared with ground truth. The method is defined for orthographic views, but can be generalised to perspective views simply.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937659",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 13,
            "total": 31
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface texture",
                "Surface reconstruction",
                "Computer vision",
                "Shape measurement",
                "Computer science",
                "Maximum a posteriori estimation",
                "Image reconstruction",
                "Face detection",
                "Surface waves",
                "Interpolation"
            ],
            "INSPEC: Controlled Indexing": [
                "image texture",
                "image reconstruction",
                "surface fitting"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "shape from texture",
                "a posteriori estimate",
                "surface coefficients",
                "texture elements",
                "local methods",
                "global methods",
                "shape from shading",
                "orthographic views",
                "reconstructions",
                "synthetic images"
            ]
        },
        "id": 164,
        "cited_by": [
            {
                "year": "2007",
                "id": 243
            },
            {
                "year": "2005",
                "id": 129
            }
        ]
    },
    {
        "title": "The impact of viewing geometry on vision through the atmosphere",
        "authors": [
            "Pei-Hsiu Suen",
            "G. Healey",
            "D. Slater"
        ],
        "abstract": "An increase in the off-nadir viewing angle for an airborne visible/near-infrared through short-wave infrared (VNIR/SWIR) imaging spectrometer leads to a decrease in upward atmospheric transmittance and an increase in line-of-sight scattered path radiance. These effects combine to reduce the spectral contrast between different materials in the sensed signal. We analyze the impact of viewing angle on material discriminability for 237 materials over a wide range of conditions. Material discriminability is quantified using a statistical algorithm that employs a subspace model to represent the set of spectra for a material as conditions vary. We show that reliable material discrimination is possible over a range of conditions even for large off-nadir viewing angles. We illustrate the performance of material identification over different viewing angles using simulated forest hyperspectral images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937660",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Geometry",
                "Atmosphere",
                "Infrared imaging",
                "Infrared spectra",
                "Optical imaging",
                "Spectroscopy",
                "Scattering",
                "Materials reliability",
                "Hyperspectral sensors",
                "Hyperspectral imaging"
            ],
            "INSPEC: Controlled Indexing": [
                "terrain mapping",
                "computational geometry",
                "infrared imaging",
                "image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "viewing geometry",
                "imaging spectrometer",
                "spectral contrast",
                "material discriminability",
                "off-nadir viewing angles",
                "material identification",
                "simulated forest hyperspectral images"
            ]
        },
        "id": 165,
        "cited_by": []
    },
    {
        "title": "BRDF/BTF measurement device",
        "authors": [
            "K.J. Dana"
        ],
        "abstract": "Capturing surface appearance is important for a large number of applications. Appearance of real world surfaces is difficult to model as it varies with the direction of illumination as well as the direction from which it is viewed. Consequently, measurements of the BRDF (bidirectional reflectance distribution function) have been important. In addition, many applications require measuring how the entire surface reflects light, i.e. spatially varying BRDF measurements are important as well. For compactness we refer to a spatially varying BRDF as a BTF (bidirectional texture function). Measurements of BRDF and/or BTF typically require significant resources in time and equipment. In this work, a device for BRDF/BTF measurement is presented that is compact, economical and convenient. The device uses the approach of curved mirrors to remove the need for hemispherical positioning of the camera and illumination source. Instead, simple planar translations of optical components are used to vary the illumination direction and to scan the surface. Furthermore, the measurement process is fast because the device enables simultaneous measurements of multiple viewing directions.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937661",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 16,
            "total": 34
        },
        "keywords": {
            "IEEE Keywords": [
                "Rough surfaces",
                "Surface roughness",
                "Lighting",
                "Surface texture",
                "Reflectivity",
                "Time measurement",
                "Mirrors",
                "Computer vision",
                "Predictive models",
                "Image texture"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "computer graphics"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surface appearance",
                "real world surfaces",
                "BRDF",
                "bidirectional reflectance distribution function",
                "bidirectional texture function",
                "multiple viewing directions",
                "simultaneous measurements"
            ]
        },
        "id": 166,
        "cited_by": [
            {
                "year": "2007",
                "id": 96
            },
            {
                "year": "2003",
                "id": 179
            }
        ]
    },
    {
        "title": "Self-calibration of a stereo rig using monocular epipolar geometry",
        "authors": [
            "F. Dornaika"
        ],
        "abstract": "This paper addresses the problem of self-calibration from one unknown motion of an uncalibrated stereo rig. Unlike the existing methods for stereo rig self-calibration, which have been focused on applying the autocalibration paradigm using both motion and stereo correspondences, our method does not require the recovery of stereo correspondences. Our method combines purely algebraic constraints with implicit geometric constraints. Assuming that the rotational part of the stereo geometry has two unknown degrees of freedom and the principle point of each camera is known, we show that the computation of the intrinsic and extrinsic parameters of the stereo rig can be recovered from the motion correspondences only, i.e. the monocular fundamental matrices. We provide a stability study for the method in the presence of image noise. Synthetic and real experiments demonstrate the feasibility and robustness of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937662",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 2,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Stability",
                "Shape",
                "Robot vision systems",
                "Calibration",
                "Computational geometry",
                "Noise robustness",
                "Robot sensing systems",
                "Orbital robotics"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "calibration"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "stereo rig",
                "monocular epipolar geometry",
                "self-calibration",
                "unknown motion",
                "uncalibrated stereo rig",
                "stereo geometry",
                "image noise",
                "robustness"
            ]
        },
        "id": 167,
        "cited_by": []
    },
    {
        "title": "4-sensor camera calibration for image representation invariant to shading, shadows, lighting, and specularities",
        "authors": [
            "G.D. Finlayson",
            "M.S. Drew"
        ],
        "abstract": "Most lighting can be accurately modeled using a simplified Planckian function. If we form logarithms of color ratios of camera sensor values, then in a Lambertian plus specular two-lobe model of reflection the temperature-dependent term is separate and is seen as a straight line: i.e., changing lighting amounts to changing each pixel value in a straight line, for a given camera. Here we use a 4-sensor camera. In this case, forming color ratios reduces the dimensionality to 3. Applying logarithms and projecting onto the plane in the 3D color space orthogonal to the light-change direction results in an image representation that is invariant to illumination change. For a given camera, the position of the specular point in the 2D plane is always the same, independent of the lighting. Thus a camera calibration produces illumination invariance at a single pixel. In the plane, matte surfaces reduce to points and specularities are almost straight lines. Extending each pixel value back to the matte position, postulated to be the maximum radius from the fixed specular point, at any angle in the 2D plane, removes specularity. Thus images are independent of shading (by forming ratios), independent of shadows (by making them independent of illumination temperature) and independent of specularities. The method is examined by forming 4D images from hyperspectral images, using real camera sensors, with encouraging results.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937663",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 6,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Calibration",
                "Image representation",
                "Temperature sensors",
                "Lighting",
                "Hyperspectral sensors",
                "Hyperspectral imaging",
                "Information systems",
                "Image sensors",
                "History"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "image representation",
                "image colour analysis"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "camera calibration",
                "image representation",
                "Planckian function",
                "color ratios",
                "camera sensor values",
                "Lambertian plus specular",
                "two-lobe model",
                "illumination change",
                "hyperspectral images"
            ]
        },
        "id": 168,
        "cited_by": []
    },
    {
        "title": "A statistical approach to background subtraction for surveillance systems",
        "authors": [
            "N. Ohta"
        ],
        "abstract": "Background subtraction is a commonly used process in surveillance systems. One difficult problem when using the process is maintaining a correct background image against changing illumination conditions. Most methods for maintaining the background image are based on intuitive definitions about the illumination change and are implemented as somewhat ad hoc algorithms. In contrast, we first define mathematical models representing the relation between the illumination intensity, a reflection index of objects and a pixel value. We also mathematically define an assumption about illumination, which requires that the distribution of the illumination intensity in a small region does not change. Then we formalize the background subtraction problem as a statistical test (/spl chi//sup 2/ test) based on the models and assumption. The experiments show that our models appropriately express the imaging process of a camera and our method provides stable detection performance for foreground objects.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937664",
        "reference_list": [],
        "citation": {
            "ieee": 54,
            "other": 9,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Surveillance",
                "Lighting",
                "Cameras",
                "Mathematical model",
                "Testing",
                "Object detection",
                "Pixel",
                "Computer science",
                "Image processing",
                "Layout"
            ],
            "INSPEC: Controlled Indexing": [
                "surveillance",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "statistical approach",
                "background subtraction",
                "surveillance systems",
                "mathematical models",
                "reflection index",
                "illumination intensity"
            ]
        },
        "id": 169,
        "cited_by": [
            {
                "year": "2003",
                "id": 170
            }
        ]
    },
    {
        "title": "Hierarchical pre-segmentation without prior knowledge",
        "authors": [
            "A. Kuijper",
            "L. Florack"
        ],
        "abstract": "A new method to pre-segment images by means of a hierarchical description is proposed. This description is obtained from an investigation of the deep structure of a scale space image-the input image and the Gaussian filtered ones simultaneously. We concentrate on scale space critical points-points with vanishing gradient with respect to both spatial and scale direction. We show that these points are always saddle points. They turn out to be extremely useful, since the iso-intensity manifolds through these points provide a scale space hierarchy tree and induce a segmentation without a priori knowledge. Moreover, together with the so-called catastrophe points, these scale space saddles form the critical points of the parameterised critical curves-the curves along which the spatial saddle points move in scale space. Experimental results with respect to the hierarchy and segmentation are given, based on an artificial image and a simulated MRI.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937665",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 7,
            "total": 9
        },
        "keywords": {
            "IEEE Keywords": [
                "Filters",
                "Image resolution",
                "Computer science",
                "Gaussian processes",
                "Image segmentation",
                "Magnetic resonance imaging",
                "Image analysis",
                "Spatial resolution",
                "Mathematics",
                "Joining processes"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "biomedical MRI"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "hierarchical pre-segmentation",
                "scale space image",
                "input image",
                "Gaussian filter",
                "scale space critical points",
                "saddle points",
                "scale space hierarchy tree",
                "parameterised critical curves",
                "artificial image",
                "simulated MRI"
            ]
        },
        "id": 170,
        "cited_by": []
    },
    {
        "title": "A curve evolution approach for image segmentation using adaptive flows",
        "authors": [
            "Haihua Feng",
            "D.A. Castanon",
            "W.C. Karl"
        ],
        "abstract": "In this paper, we develop a new active contour model for image segmentation using adaptive flows. This active contour model can be derived from minimizing a limiting form of the Mumford-Shah functional, where the segmented image is assumed to consist of piecewise constant regions. This paper is an extension of an active contour model developed by Chan-Vese. The segmentation method proposed in this paper adaptively estimates mean intensities for each separated region and uses a single curve to capture multiple regions with different intensities. The class of imagery that our new active model can handle is greater than the bimodal images. In particular, our method segments images with an arbitrary number of intensity levels and separated regions while avoiding the complexity of solving a full Mumford-Shah problem. The adaptive flow developed in this paper is easily formulated and solved using level set methods. We illustrate the performance of our segmentation methods on images generated by different modalities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937666",
        "reference_list": [],
        "citation": {
            "ieee": 4,
            "other": 0,
            "total": 4
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Active contours",
                "Image edge detection",
                "Level set",
                "Image generation",
                "Computer vision",
                "Application software",
                "Remote sensing",
                "Medical diagnosis",
                "Filtering"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "computational complexity",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "curve evolution approach",
                "image segmentation",
                "adaptive flows",
                "active contour model",
                "Mumford-Shah functional",
                "piecewise constant regions",
                "mean intensities"
            ]
        },
        "id": 171,
        "cited_by": []
    },
    {
        "title": "Segmentation and range sensing using a moving-aperture lens",
        "authors": [
            "A. Subramanian",
            "L.R. Iyer",
            "A.L. Abbott",
            "A.E. Bell"
        ],
        "abstract": "This paper is concerned with the use of a novel motorized lens to perform segmentation of image sequences. The lens has the effect of introducing small, repeating movements of the camera center, so that objects appear to translate in the image by an amount that depends on distance from the plane of focus. For a stationary scene, optical flow magnitudes and direction are therefore directly related to three-dimensional object distance from the observer. We describe a segmentation-procedure that exploits these controlled observer movements, and we present experimental results that demonstrate the successful extraction of objects at different depths. Potential applications of this approach include video compression, compositing, and passive range sensing.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937667",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 1,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Lenses",
                "Image segmentation",
                "Cameras",
                "Image sequences",
                "Layout",
                "Optical sensors",
                "Image motion analysis",
                "Video compression",
                "Optical computing",
                "Image edge detection"
            ],
            "INSPEC: Controlled Indexing": [
                "data compression",
                "video coding",
                "image sequences",
                "image segmentation",
                "distance measurement"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "range sensing",
                "image segmentation",
                "moving-aperture lens",
                "motorized lens",
                "image sequences",
                "optical flow magnitudes",
                "three-dimensional object distance",
                "video compression",
                "passive range sensing"
            ]
        },
        "id": 172,
        "cited_by": []
    },
    {
        "title": "Computing visual correspondence with occlusions using graph cuts",
        "authors": [
            "V. Kolmogorov",
            "R. Zabih"
        ],
        "abstract": "Several new algorithms for visual correspondence based on graph cuts have recently been developed. While these methods give very strong results in practice, they do not handle occlusions properly. Specifically, they treat the two input images asymmetrically, and they do not ensure that a pixel corresponds to at most one pixel in the other image. In this paper, we present a new method which properly addresses occlusions, while preserving the advantages of graph cut algorithms. We give experimental results for stereo as well as motion, which demonstrate that our method performs well both at detecting occlusions and computing disparities.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937668",
        "reference_list": [],
        "citation": {
            "ieee": 372,
            "other": 128,
            "total": 500
        },
        "keywords": {
            "IEEE Keywords": [
                "Pixel",
                "Computer science",
                "Motion detection",
                "Minimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "computer graphics",
                "computer vision",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "visual correspondence",
                "occlusions",
                "graph cuts",
                "disparities"
            ]
        },
        "id": 173,
        "cited_by": [
            {
                "year": "2009",
                "id": 164
            },
            {
                "year": "2007",
                "id": 162
            }
        ]
    },
    {
        "title": "Finding \"anomalies\" in an arbitrary image",
        "authors": [
            "T. Honda",
            "S.K. Nayar"
        ],
        "abstract": "A fast and general method to extract \"anomalies\" in an arbitrary image is proposed. The basic idea is to compute a probability density for sub-regions in an image, conditioned upon the areas surrounding the sub-regions. Linear estimation and Independent Component Analysis (ICA) are combined to obtain the probability estimates. Pseudo non-parametric correlation is used to group sets of similar surrounding patterns, from which a probability for the occurrence of a given sub-region is derived. A carefully designed multi-dimensional histogram, based on compressed vector representations, enables efficient and high-resolution extraction of anomalies from the image. Our current (unoptimized) implementation performs anomaly extraction in about 30 seconds for a 640/spl times/480 image using a 700 MHz PC. Experimental results are included that demonstrate the performance of the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937669",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 2,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Independent component analysis",
                "Layout",
                "Nonlinear filters",
                "Frequency synthesizers",
                "Production engineering",
                "Laboratories",
                "Computer science",
                "Ear",
                "Histograms",
                "Image coding"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "data compression",
                "image coding"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "anomalies",
                "arbitrary image",
                "probability density",
                "independent component analysis",
                "probability estimates",
                "multi-dimensional histogram",
                "compressed vector representations"
            ]
        },
        "id": 174,
        "cited_by": []
    },
    {
        "title": "JetStream: probabilistic contour extraction with particles",
        "authors": [
            "P. Perez",
            "A. Blake",
            "M. Gangnet"
        ],
        "abstract": "The problem of extracting continuous structures from noisy or cluttered images is a difficult one. Successful extraction depends critically on the ability to balance prior constraints on continuity and smoothness against evidence garnered from image analysis. Exact, deterministic optimisation algorithms, based on discretized functionals, suffer from severe limitations on the form of prior constraint that can be imposed tractably. This paper proposes a sequential Monte-Carlo technique, termed JetStream, that enables constraints on curvature, corners, and contour parallelism. To be mobilized, all of which are infeasible under exact optimization. The power of JetStream is demonstrated in two contexts: (1) interactive cut-out in photo-editing applications, and (2) the recovery of roads in aerial photographs.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937670",
        "reference_list": [],
        "citation": {
            "ieee": 39,
            "other": 13,
            "total": 52
        },
        "keywords": {
            "IEEE Keywords": [
                "Roads",
                "Cost function",
                "Data mining",
                "Image edge detection",
                "Stochastic processes",
                "Image analysis",
                "Image processing",
                "Image segmentation",
                "Satellites",
                "Streaming media"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "computational geometry",
                "clutter",
                "deterministic algorithms",
                "Monte Carlo methods"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "JetStream",
                "probabilistic contour extraction",
                "continuous structures extraction",
                "cluttered images",
                "prior constraints",
                "image analysis",
                "deterministic optimisation algorithms",
                "discretized functionals",
                "sequential Monte-Carlo technique",
                "contour parallelism",
                "aerial photographs"
            ]
        },
        "id": 175,
        "cited_by": [
            {
                "year": "2005",
                "id": 39
            }
        ]
    },
    {
        "title": "A framework for segmentation of talk and game shows",
        "authors": [
            "O. Javed",
            "Z. Rasheed",
            "M. Shah"
        ],
        "abstract": "In this paper, we present a method to remove commercials from talk and game show videos and to segment these videos into host and guest shots. In our approach, we mainly rely on information contained in shot transitions, rather than analyzing the scene content of individual frames. We utilize the inherent differences in scene structure of commercials and talk shows to differentiate between them. Similarly, we make use of the well-defined structure of talk shows, which can be exploited to classify shots as host or guest shots. The entire show is first segmented into camera shots based on color histogram. Then, we construct a data-structure (shot connectivity graph) which links similar shots over time. Analysis of the shot connectivity graph helps us to automatically separate commercials from program segments. This is done by first detecting stories, and then assigning a weight to each story based on its likelihood of being a commercial. Further analysis on stories is done to distinguish shots of the hosts from shots of the guests. We have tested our approach on several full-length shows (including commercials) and have achieved video segmentation with high accuracy. The whole scheme is fast and works even on low quality video (160/spl times/120 pixel images at 5 Hz).",
        "ieee_link": "https://ieeexplore.ieee.org/document/937671",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Games",
                "Testing",
                "Image segmentation",
                "Software libraries",
                "Digital video broadcasting",
                "Computer vision",
                "Computer science",
                "Information analysis",
                "Layout",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "video coding"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "talk and game shows segmentation",
                "camera shots",
                "color histogram",
                "data structure",
                "shot connectivity graph",
                "program segments",
                "video segmentation"
            ]
        },
        "id": 176,
        "cited_by": []
    },
    {
        "title": "A robust interest points matching algorithm",
        "authors": [
            "Il-Kyun Jung",
            "S. Lacroix"
        ],
        "abstract": "This paper presents an algorithm that matches interest points detected on a pair of grey level images taken from arbitrary points of view. First matching hypotheses are generated using a similarity measure of the interest points. Hypotheses are confirmed using local groups of interest paints: group matches are based on a measure defined on an affine transformation estimate and on a correlation coefficient computed on the intensity of the interest points. Once a reliable match has been determined for a given interest point and the corresponding local group, new group matches are found by propagating the estimated affine transformation. The algorithm has been widely tested under various image transformations: it provides dense matches and is very robust to outliers, i.e. interest points generated by noise or present in only one image because of occlusions or non overlap.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937672",
        "reference_list": [],
        "citation": {
            "ieee": 15,
            "other": 1,
            "total": 16
        },
        "keywords": {
            "IEEE Keywords": [
                "Noise robustness",
                "Information retrieval",
                "Image retrieval",
                "Image databases",
                "Detectors",
                "Autocorrelation",
                "Testing",
                "Noise generators",
                "Computer vision",
                "Feature extraction"
            ],
            "INSPEC: Controlled Indexing": [
                "object recognition",
                "image retrieval",
                "image matching"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "robust interest points matching algorithm",
                "grey level images",
                "arbitrary points",
                "similarity measure",
                "affine transformation estimate"
            ]
        },
        "id": 177,
        "cited_by": [
            {
                "year": "2009",
                "id": 269
            }
        ]
    },
    {
        "title": "A simple and efficient template matching algorithm",
        "authors": [
            "F. Jurie",
            "M. Dhome"
        ],
        "abstract": "We propose a general framework for object tracking in video images. It consists of low-order parametric models for the image motion of a target region. These models are used to predict the movement and to track the target. The difference of intensity between the pixels belonging to the current region and the pixels of the selected target (learnt during an off-line stage) allows a straightforward prediction of the region position in the current image. The proposed algorithm allows to track in real time (less than 10 ms) any planar textured target under homographic motions. This algorithm is very simple (a few lines of code) and very efficient (less than 10 ms on a 150 MHz hardware).",
        "ieee_link": "https://ieeexplore.ieee.org/document/937673",
        "reference_list": [],
        "citation": {
            "ieee": 18,
            "other": 5,
            "total": 23
        },
        "keywords": {
            "IEEE Keywords": [
                "Target tracking",
                "Predictive models",
                "Image reconstruction",
                "Pixel",
                "Parametric statistics",
                "Hardware",
                "Image representation",
                "Robustness",
                "Image segmentation",
                "Uncertainty"
            ],
            "INSPEC: Controlled Indexing": [
                "image representation",
                "image matching",
                "minimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "template matching algorithm",
                "object tracking",
                "video images",
                "low-order parametric models",
                "image motion",
                "homographic motions"
            ]
        },
        "id": 178,
        "cited_by": []
    },
    {
        "title": "Empirical filter estimation for subpixel interpolation and matching",
        "authors": [
            "B. Triggs"
        ],
        "abstract": "We study the low-level problem of predicting pixel intensities after subpixel image translations. This is a basic subroutine for image warping and super-resolution, and it has a critical influence on the accuracy of subpixel matching by image correlation. Rather than using traditional frequency-space filtering theory or ad hoc interpolators such as splines, we take an empirical approach, finding optimal subpixel interpolation filters by direct numerical optimization over a large set of training examples. The training set is generated by subsampling larger images at different translations, using subsamplers that mimic the spatial response functions of real pixels. We argue that this gives realistic results, and design filters of various different parametric forms under traditional and robust prediction error metrics. We systematically study the performance of the resulting filters, paying particular attention to the influence of the underlying image sampling regime and the effects of aliasing (\"jaggies\"). We summarize the results and give practical advice for obtaining subpixel accuracy.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937674",
        "reference_list": [],
        "citation": {
            "ieee": 8,
            "other": 4,
            "total": 12
        },
        "keywords": {
            "IEEE Keywords": [
                "Matched filters",
                "Interpolation",
                "Pixel",
                "Filtering theory",
                "Algorithms",
                "Spatial resolution",
                "Image resolution",
                "Frequency",
                "Robustness",
                "Image sampling"
            ],
            "INSPEC: Controlled Indexing": [
                "interpolation",
                "filtering theory",
                "optimisation",
                "image sampling",
                "image matching",
                "splines (mathematics)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "empirical filter estimation",
                "subpixel interpolation",
                "subpixel matching",
                "low-level problem",
                "pixel intensities",
                "subpixel image translations",
                "image warping",
                "subroutine",
                "image correlation",
                "frequency-space filtering",
                "ad hoc interpolators",
                "splines",
                "optimal subpixel interpolation filters",
                "spatial response functions",
                "image sampling"
            ]
        },
        "id": 179,
        "cited_by": []
    },
    {
        "title": "Performance evaluation of stereo for tele-presence",
        "authors": [
            "J. Mulligan",
            "V. Isler",
            "K. Daniilidis"
        ],
        "abstract": "In an immersive tele-presence environment a 3D remote real scene is projected from the viewpoint of the local user. This 3D world is acquired through stereo reconstruction at the remote site. In this paper we start a performance analysis of stereo algorithms with respect to the task of immersive visualization. As opposed to usual monocular image based rendering, we are also interested in the depth error in novel views because our rendering is stereoscopic. We describe an evaluation test-bed which provides a world-wide first available set of registered dense \"ground-truth\" laser data and image data from multiple views. We establish metrics for novel depth views that reflect discrepancies both in the image and in 3D-space. It is well known that stereo performance is affected by both erroneous matching as well as incorrect depth triangulation. We experimentally study the effects of occlusion and low texture on the distributions of the error metrics. Then, we algebraically predict the behavior of depth and novel projection error as a function of the camera set-up and the error in the disparity. These are first steps towards building a laboratory for psychophysical judgement of depth estimates which is the ultimate performance test of tele-presence stereo.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937675",
        "reference_list": [],
        "citation": {
            "ieee": 14,
            "other": 4,
            "total": 18
        },
        "keywords": {
            "IEEE Keywords": [
                "Rendering (computer graphics)",
                "Testing",
                "Layout",
                "Stereo image processing",
                "Image reconstruction",
                "Performance analysis",
                "Data visualization",
                "Cameras",
                "Laboratories",
                "Psychology"
            ],
            "INSPEC: Controlled Indexing": [
                "performance evaluation",
                "telecontrol",
                "virtual reality",
                "rendering (computer graphics)",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "performance evaluation",
                "telepresence environment",
                "3D remote real scene",
                "3D world",
                "stereo reconstruction",
                "performance analysis",
                "stereo algorithms",
                "immersive visualization",
                "monocular image based rendering",
                "depth error",
                "discrepancies",
                "occlusion",
                "error metrics"
            ]
        },
        "id": 180,
        "cited_by": [
            {
                "year": "2015",
                "id": 236
            }
        ]
    },
    {
        "title": "Template matching approach to content based image indexing by low dimensional Euclidean embedding",
        "authors": [
            "H. Schweitzer"
        ],
        "abstract": "Content based indexing is computed from input that consists of matching values between images and templates. The key idea is to embed both images and templates in a low-dimensional Euclidean space so that matching between embedded images and embedded templates approximates the given input. It is shown that such embedding can be computed by means of a singular value decomposition of the input matrix. Classic principal component analysis is shown to be a special case of the proposed technique, corresponding to the case where the templates and the images are the same.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937676",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Indexing",
                "Embedded computing",
                "Principal component analysis",
                "Application software",
                "Covariance matrix",
                "Computer applications"
            ],
            "INSPEC: Controlled Indexing": [
                "principal component analysis",
                "singular value decomposition",
                "content-based retrieval",
                "image matching",
                "database indexing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "template matching approach",
                "content based image indexing",
                "low dimensional Euclidean embedding",
                "content based indexing",
                "images",
                "templates",
                "low-dimensional Euclidean space",
                "embedded images",
                "embedded templates",
                "singular value decomposition",
                "principal component analysis"
            ]
        },
        "id": 181,
        "cited_by": [
            {
                "year": "2015",
                "id": 251
            }
        ]
    },
    {
        "title": "Euclidean reconstruction and auto-calibration from continuous motion",
        "authors": [
            "F. Kahl",
            "A. Heyden"
        ],
        "abstract": "This paper deals with the problem of incorporating natural regularity conditions on the motion in an MAP estimator for structure and motion recovery from uncalibrated image sequences. The purpose of incorporating these constraints is to increase performance and robustness. Auto-calibration and structure and motion algorithms are known to have problems with (i) the frequently occurring critical camera motions, (ii) local minima in the non-linear optimization and (iii) the high correlation between different intrinsic and extrinsic parameters of the camera, e.g. the coupling between focal length and camera position. The camera motion (both intrinsic and extrinsic parameters) is modelled as a random walk process, where the inter-frame motions are assumed to be independently normally distributed. The proposed scheme is demonstrated on both simulated and real data showing the increased performance.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937677",
        "reference_list": [],
        "citation": {
            "ieee": 7,
            "other": 3,
            "total": 10
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Image reconstruction",
                "Image sequences",
                "Motion estimation",
                "Layout",
                "Maximum likelihood estimation",
                "Robustness",
                "Calibration",
                "Feature extraction",
                "Reconstruction algorithms"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "image reconstruction",
                "computational geometry"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Euclidean reconstruction",
                "auto-calibration",
                "continuous motion",
                "natural regularity conditions",
                "MAP estimator",
                "uncalibrated image sequences",
                "local minima",
                "focal length",
                "camera position",
                "camera motion",
                "random walk process",
                "inter-frame motions"
            ]
        },
        "id": 182,
        "cited_by": [
            {
                "year": "2003",
                "id": 134
            }
        ]
    },
    {
        "title": "A versatile method for trifocal tensor estimation",
        "authors": [
            "B. Matei",
            "B. Georgescu",
            "P. Meer"
        ],
        "abstract": "Reliable estimation of the trifocal tensor is crucial for 3D reconstruction from uncalibrated cameras. The estimation process is based on minimizing the geometric distances between the measurements and the corrected data points, the underlying nonlinear optimization problem being most often solved with the Levenberg-Marquardt (LM) algorithm. We employ for this task the heteroscedastic errors-in-variables (HEIV) estimator and take into account both the singularity of the multivariate tensor constraint and the bifurcation which can appear for noisy data. In comparison to the Gold Standard method, the new approach is significantly faster while having the same performance, and it is less sensitive to initialization when the data is close to degenerate. Analytical expressions for the covariances of the parameter and corrected image point estimates are available for the HEIV estimator and thus the confidence regions of the corrected measurements can be delineated in the images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937678",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 2,
            "total": 7
        },
        "keywords": {
            "IEEE Keywords": [
                "Tensile stress",
                "Cameras",
                "Robustness",
                "Image reconstruction",
                "Bifurcation",
                "Gold",
                "Books",
                "Optimization methods"
            ],
            "INSPEC: Controlled Indexing": [
                "motion estimation",
                "computer vision",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "versatile method",
                "trifocal tensor estimation",
                "3D reconstruction",
                "uncalibrated cameras",
                "geometric distances",
                "nonlinear optimization",
                "Levenberg-Marquardt algorithm",
                "heteroscedastic errors-in-variables estimator",
                "multivariate tensor constraint",
                "bifurcation"
            ]
        },
        "id": 183,
        "cited_by": []
    },
    {
        "title": "Motion segmentation by subspace separation and model selection",
        "authors": [
            "K. Kanatani"
        ],
        "abstract": "Reformulating the Costeira-Kanade algorithm as a pure mathematical theorem independent of the Tomasi-Kanade factorization, we present a robust segmentation algorithm by incorporating such techniques as dimension correction, model selection using the geometric AIC, and least-median fitting. Doing numerical simulations, we demonstrate that oar algorithm dramatically outperforms existing methods. It does not involve any parameters which need to be adjusted empirically.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937679",
        "reference_list": [],
        "citation": {
            "ieee": 114,
            "other": 53,
            "total": 167
        },
        "keywords": {
            "IEEE Keywords": [
                "Motion segmentation",
                "Computer vision",
                "Numerical simulation",
                "Image segmentation",
                "Tracking",
                "Gears",
                "Gaussian noise",
                "Cameras",
                "Information technology",
                "Robustness"
            ],
            "INSPEC: Controlled Indexing": [
                "image segmentation",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "motion segmentation",
                "subspace separation",
                "model selection",
                "Costeira-Kanade algorithm",
                "pure mathematical theorem",
                "Tomasi-Kanade factorization",
                "robust segmentation algorithm",
                "dimension correction",
                "least-median fitting",
                "numerical simulations"
            ]
        },
        "id": 184,
        "cited_by": [
            {
                "year": "2015",
                "id": 473
            },
            {
                "year": "2015",
                "id": 523
            },
            {
                "year": "2013",
                "id": 170
            },
            {
                "year": "2013",
                "id": 250
            },
            {
                "year": "2009",
                "id": 86
            },
            {
                "year": "2009",
                "id": 105
            },
            {
                "year": "2009",
                "id": 110
            },
            {
                "year": "2009",
                "id": 156
            },
            {
                "year": "2007",
                "id": 124
            }
        ]
    },
    {
        "title": "Multi-frame infinitesimal motion model for the reconstruction of (dynamic) scenes with multiple linearly moving objects",
        "authors": [
            "A. Shashua",
            "A. Levin"
        ],
        "abstract": "We introduce new small-motion multi-frame equations applicable to the reconstruction of dynamic scenes in which points are allowed to move along straight-line paths with constant velocity. The motion equations apply to both static and dynamic points, thus prior segmentation is not necessary. We present a reconstruction algorithm of camera motion, scene structure, and point trajectories embedded into a multi-frame factorization principle which requires the minimum of 11 images and 7 points (out of which at feast 3 are dynamic).",
        "ieee_link": "https://ieeexplore.ieee.org/document/937680",
        "reference_list": [],
        "citation": {
            "ieee": 11,
            "other": 6,
            "total": 17
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Cameras",
                "Image reconstruction",
                "Transmission line matrix methods",
                "Equations",
                "Image segmentation",
                "Image sequences",
                "Computer science",
                "Reconstruction algorithms",
                "Image analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "image segmentation",
                "motion estimation",
                "matrix algebra"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "multi-frame infinitesimal motion model",
                "scenes reconstruction",
                "multiple linearly moving objects",
                "small-motion multi-frame equations",
                "straight-line paths",
                "reconstruction algorithm",
                "camera motion",
                "scene structure",
                "point trajectories",
                "multi-frame factorization principle"
            ]
        },
        "id": 185,
        "cited_by": [
            {
                "year": "2005",
                "id": 0
            },
            {
                "year": "2005",
                "id": 20
            }
        ]
    },
    {
        "title": "Colour photometric stereo: simultaneous reconstruction of local gradient and colour of rough textured surfaces",
        "authors": [
            "S. Barsky",
            "M. Petrou"
        ],
        "abstract": "Classification of a rough 3D surface from 2D images may be difficult due to directional effects introduced by illumination. One possible way of dealing with the problem is to extract the local albedo and gradient surface information which do not depend on the illumination, and classify the texture directly using these intrinsic characteristics. In this paper we present an algorithm for simultaneous recovery of local gradient and colour using multiple photometric images. The algorithm is proven to be optimal in the least squares error sense. Experimental results with real images and comparison with other approaches are also presented.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937681",
        "reference_list": [],
        "citation": {
            "ieee": 6,
            "other": 2,
            "total": 8
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Rough surfaces",
                "Surface roughness",
                "Surface reconstruction",
                "Lighting",
                "Stereo image processing",
                "Image reconstruction",
                "Data mining",
                "Surface texture",
                "Least squares methods"
            ],
            "INSPEC: Controlled Indexing": [
                "image reconstruction",
                "stereo image processing",
                "image classification",
                "least squares approximations"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "colour photometric stereo",
                "simultaneous reconstruction",
                "local gradient",
                "rough textured surfaces",
                "rough 3D surface classification",
                "gradient surface information",
                "intrinsic characteristics",
                "multiple photometric images",
                "least squares error sense"
            ]
        },
        "id": 186,
        "cited_by": []
    },
    {
        "title": "Reconstructing surfaces using anisotropic basis functions",
        "authors": [
            "Huong Quynh Dinh",
            "G. Turk",
            "G. Slabaugh"
        ],
        "abstract": "Point sets obtained from computer vision techniques are often noisy and non-uniform. We present a new method of surface reconstruction that can handle such data sets using anisotropic basis functions. Our reconstruction algorithm draws upon the work in variational implicit surfaces for constructing smooth and seamless 3D surfaces. Implicit functions are often formulated as a sum of weighted basis functions that are radially symmetric. Using radially symmetric basis functions inherently assumes, however that the surface to be reconstructed is, everywhere, locally symmetric. Such an assumption is true only at planar regions, and hence, reconstruction using isotropic basis is insufficient to recover objects that exhibit sharp features. We preserve sharp features using anisotropic basis that allow the surface to vary locally. The reconstructed surface is sharper along edges and at corner points. We determine the direction of anisotropy at a point by performing principal component analysis of the data points in a small neighborhood. The resulting field of principle directions across the surface is smoothed through tensor filtering. We have applied the anisotropic basis functions to reconstruct surfaces from noisy synthetic 3D data and from real range data obtained from space carving.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937682",
        "reference_list": [],
        "citation": {
            "ieee": 10,
            "other": 3,
            "total": 13
        },
        "keywords": {
            "IEEE Keywords": [
                "Surface reconstruction",
                "Anisotropic magnetoresistance",
                "Image reconstruction",
                "Surface fitting",
                "Optical noise",
                "Shape",
                "Polynomials",
                "Computer vision",
                "Surface emitting lasers",
                "Space technology"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "principal component analysis",
                "image reconstruction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "surfaces reconstruction",
                "anisotropic basis functions",
                "point sets",
                "computer vision",
                "weighted basis functions",
                "isotropic basis",
                "tensor filtering",
                "real range data",
                "space carving"
            ]
        },
        "id": 187,
        "cited_by": [
            {
                "year": "2009",
                "id": 100
            },
            {
                "year": "2009",
                "id": 229
            }
        ]
    },
    {
        "title": "Structure from motion using sequential Monte Carlo methods",
        "authors": [
            "Gang Qian",
            "R. Chellappa"
        ],
        "abstract": "In this paper the structure from motion (SfM) problem is addressed using sequential Monte Carlo methods. A new SfM algorithm based on random sampling is derived to estimate the posterior distributions of camera camera motion and scene structure for the perspective projection camera model. Experimental results show that challenging issues in solving the structure from motion problem including errors in feature tracking, feature occlusion, motion/structure ambiguity, processing mixed-domain sequences and handling mismatched features can be well modeled and effectively addressed using the proposed method.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937683",
        "reference_list": [],
        "citation": {
            "ieee": 5,
            "other": 1,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Sliding mode control",
                "Layout",
                "Nonlinear dynamical systems",
                "Monte Carlo methods",
                "Optical sensors",
                "Optical noise",
                "Motion estimation",
                "Optical computing",
                "State-space methods"
            ],
            "INSPEC: Controlled Indexing": [
                "Monte Carlo methods",
                "image motion analysis",
                "feature extraction",
                "tracking"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "structure from motion",
                "sequential Monte Carlo methods",
                "SfM algorithm",
                "random sampling",
                "posterior distributions",
                "camera camera motion",
                "scene structure",
                "feature tracking",
                "feature occlusion",
                "mixed-domain sequences",
                "mismatched features"
            ]
        },
        "id": 188,
        "cited_by": []
    },
    {
        "title": "Noise in bilinear problems",
        "authors": [
            "J.A. Haddon",
            "D.A. Forsyth"
        ],
        "abstract": "Despite the wide application of bilinear problems to problems both in computer vision and in other fields, their behaviour under the effects of noise is still poorly understood. In this paper, we show analytically that marginal distributions on the solution components of a bilinear problem can be bimodal, even with Gaussian measurement error. We demonstrate and compare three different methods of estimating the covariance of a solution. We show that the Hessian at the mode substantially underestimates covariance. Many problems in computer vision can be posed as bilinear problems: i.e. one must find a solution to a set of equations of the form.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937684",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Gaussian noise",
                "Optical scattering",
                "Computer vision",
                "Noise measurement",
                "Image reconstruction",
                "Computer science",
                "Application software",
                "Computer errors",
                "Equations",
                "Motion analysis"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "Gaussian noise",
                "bilinear systems"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "bilinear problems",
                "computer vision",
                "solution components",
                "Gaussian measurement error",
                "Hessian",
                "covariance"
            ]
        },
        "id": 189,
        "cited_by": []
    },
    {
        "title": "Plan-view trajectory estimation with dense stereo background models",
        "authors": [
            "T. Darrell",
            "D. Demirdjian",
            "N. Checka",
            "P. Felzenszwalb"
        ],
        "abstract": "In a known environment, objects may be tracked in multiple views using a set of background models. Stereo-based models can be illumination-invariant, but often have undefined values which inevitably lead to foreground classification errors. We derive dense stereo models for object tracking using long-term, extended dynamic-range imagery, and by detecting and interpolating uniform but unoccluded planar regions. Foreground points are detected quickly in new images using pruned disparity search. We adopt a \"late-segmentation\" strategy, using an integrated plan-view density representation. Foreground points are segmented into object regions only when a trajectory is finally estimated, using a dynamic programming-based method. Object entry and exit are optimally determined and are not restricted to special spatial zones.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937685",
        "reference_list": [],
        "citation": {
            "ieee": 35,
            "other": 12,
            "total": 47
        },
        "keywords": {
            "IEEE Keywords": [
                "Image segmentation",
                "Stereo vision",
                "Object detection",
                "Lighting",
                "Layout",
                "Trajectory",
                "Artificial intelligence",
                "Computer vision",
                "Shape",
                "Brightness"
            ],
            "INSPEC: Controlled Indexing": [
                "computer vision",
                "stereo image processing",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "plan-view trajectory estimation",
                "dense stereo background models",
                "multiple views",
                "stereo-based models",
                "foreground classification errors",
                "dense stereo models",
                "object tracking",
                "dynamic-range imagery",
                "unoccluded planar regions",
                "integrated plan-view density representation"
            ]
        },
        "id": 190,
        "cited_by": [
            {
                "year": "2003",
                "id": 141
            }
        ]
    },
    {
        "title": "Viewpoint invariant texture matching and wide baseline stereo",
        "authors": [
            "F. Schaffalitzky",
            "A. Zisserman"
        ],
        "abstract": "We describe and demonstrate a texture region descriptor which is invariant to affine geometric and photometric transformations, and insensitive to the shape of the texture region. It is applicable to texture patches which are locally planar and have stationary statistics. The novelty of the descriptor is that it is based on statistics aggregated over the region, resulting in richer and more stable descriptors than those computed at a point. Two texture matching applications of this descriptor are demonstrated: (1) it is used to automatically identify, regions of the same type of texture, but with varying surface pose, within a single image; (2) it is used to support wide baseline stereo, i.e. to enable the automatic computation of the epipolar geometry between two images acquired from quite separated viewpoints. Results are presented on several sets of real images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937686",
        "reference_list": [],
        "citation": {
            "ieee": 50,
            "other": 38,
            "total": 88
        },
        "keywords": {
            "IEEE Keywords": [
                "Computational geometry",
                "Statistics",
                "Surface texture",
                "Photometry",
                "Robustness",
                "Shape",
                "Image segmentation",
                "Layout",
                "Least squares approximation",
                "Cameras"
            ],
            "INSPEC: Controlled Indexing": [
                "stereo image processing",
                "image matching",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "viewpoint invariant texture matching",
                "wide baseline stereo",
                "texture region descriptor",
                "photometric transformations",
                "texture patches",
                "texture matching",
                "baseline stereo",
                "epipolar geometry",
                "real images"
            ]
        },
        "id": 191,
        "cited_by": [
            {
                "year": "2015",
                "id": 248
            },
            {
                "year": "2007",
                "id": 37
            },
            {
                "year": "2005",
                "id": 192
            },
            {
                "year": "2003",
                "id": 35
            },
            {
                "year": "2003",
                "id": 86
            }
        ]
    },
    {
        "title": "Model-based bundle adjustment with application to face modeling",
        "authors": [
            "Ying Shan",
            "Zicheng Liu",
            "Zhengyou Zhang"
        ],
        "abstract": "We present a new model-based bundle adjustment algorithm to recover the 3D model of a scene/object from a sequence of images with unknown motions. Instead of representing scene/object by a collection of isolated 3D features (usually points), our algorithm uses a surface controlled by a small set of parameters. Compared with previous model-based approaches, our approach has the following advantages. First instead of using the model space as a regularizer we directly use it as our search space, thus resulting in a more elegant formulation with fewer unknowns and fewer equations. Second, our algorithm automatically associates tracked points with their correct locations on the surfaces, thereby eliminating the need for a prior 2D-to-3D association. Third, regarding face modeling, we use a very small set of face metrics (meaningful deformations) to parameterize the face geometry, resulting in a smaller search space and a better posed system. Experiments with both synthetic and real data show that this new algorithm is faster, more accurate and more stable than existing ones.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937687",
        "reference_list": [],
        "citation": {
            "ieee": 21,
            "other": 3,
            "total": 24
        },
        "keywords": {
            "IEEE Keywords": [
                "Layout",
                "Clouds",
                "Cameras",
                "Robustness",
                "Equations",
                "Deformable models",
                "Extraterrestrial measurements",
                "USA Councils",
                "Solid modeling",
                "Statistics"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "computational geometry",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "model-based bundle adjustment",
                "face modeling",
                "3D model",
                "images sequences",
                "isolated 3D features",
                "model-based approaches",
                "face metrics",
                "face geometry"
            ]
        },
        "id": 192,
        "cited_by": [
            {
                "year": "2007",
                "id": 158
            },
            {
                "year": "2005",
                "id": 156
            },
            {
                "year": "2001",
                "id": 212
            }
        ]
    },
    {
        "title": "Photometric image-based rendering for image generation in arbitrary illumination",
        "authors": [
            "Y. Mukaigawa",
            "H. Miyaki",
            "S. Mihashi",
            "T. Shakunaga"
        ],
        "abstract": "A Photometric Image-Based Rendering (PIER) concept is proposed that facilitates the generation of an image with an arbitrary illumination. Based on this concept, we aim to generate both diffuse and specular reflections. It is not necessary to explicitly recover 3D shape and reflection properties of the scene. In order to control appearance changes caused by modifications in the lighting conditions, we utilize a set of real images taken, in a variety of lighting conditions. Since the diffuse and specular reflection components have different characteristics, we separate these components and apply different methods to each. A photometric linearization is introduced to control diffuse reflections as well as for separating the other components. This also facilitates the treatment of attached shadows as a part of the diffuse reflection. A morphing technique is utilized to generate specular reflections. This is an effective technique for dealing with glossy objects, even when the light shape is clearly observed in the specular reflection. Experimental results show that realistic images can be successfully generated using this technique.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937688",
        "reference_list": [],
        "citation": {
            "ieee": 12,
            "other": 2,
            "total": 14
        },
        "keywords": {
            "IEEE Keywords": [
                "Photometry",
                "Rendering (computer graphics)",
                "Image generation",
                "Optical reflection",
                "Shape",
                "Brain modeling",
                "Surface reconstruction",
                "Layout",
                "Lighting control",
                "Image reconstruction"
            ],
            "INSPEC: Controlled Indexing": [
                "rendering (computer graphics)",
                "realistic images",
                "image morphing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "photometric image-based rendering",
                "image generation",
                "arbitrary illumination",
                "3D shape",
                "real images",
                "photometric linearization",
                "diffuse reflections",
                "morphing technique"
            ]
        },
        "id": 193,
        "cited_by": [
            {
                "year": "2003",
                "id": 23
            }
        ]
    },
    {
        "title": "Learning spectral calibration parameters for color inspection",
        "authors": [
            "P. Carvalho",
            "A. Santos",
            "A. Dourado",
            "B. Ribeiro"
        ],
        "abstract": "Light sensor spectral calibration is an ill-defined problem. For the identification problem one needs a priori knowledge of the characteristics of the sensor which is difficult to get in most situations. A new methodology is presented in this paper that does not rely on any a priori knowledge of the sensor's characteristics. The method uses an extended generalized cross-validation function to measure predictability of the identified sensor's spectral behavior. The prediction error is minimized with a hybrid genetic algorithm. Further an extended image formation model is introduced to model changes in additive and multiplicative errors. The calibration problem is formulated to be independent of these changes by previously identifying and removing them from the images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937689",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 0,
            "total": 3
        },
        "keywords": {
            "IEEE Keywords": [
                "Calibration",
                "Inspection",
                "Sensor phenomena and characterization",
                "Lighting",
                "Informatics",
                "Color",
                "Sampling methods",
                "Colored noise",
                "Lenses",
                "Genetics"
            ],
            "INSPEC: Controlled Indexing": [
                "calibration",
                "colour vision",
                "computer vision",
                "genetic algorithms"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "spectral calibration parameters learning",
                "color inspection",
                "light sensor spectral calibration",
                "extended generalized cross-validation function",
                "prediction error",
                "hybrid genetic algorithm",
                "image formation model",
                "calibration problem"
            ]
        },
        "id": 194,
        "cited_by": []
    },
    {
        "title": "Image detection under varying illumination and pose",
        "authors": [
            "M. Osadchy",
            "D. Keren"
        ],
        "abstract": "This paper focuses on the detection of objects with Lambertian surface under both varying illumination and pose. We offer to apply a novel detection method that proceeds by modeling the different illuminations from a small number of images in the training set; this automatically voids the illumination effects, allowing fast illumination invariant detection, without having to create a large training set. It is demonstrated that the method \"fits in\" nicely with previous work about the modeling of the set of object appearances under varying illumination. In the experiments, an object was correctly detected under image plane rotations in a 15-degrees range, and a wide variety of different illuminations.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937690",
        "reference_list": [],
        "citation": {
            "ieee": 3,
            "other": 3,
            "total": 6
        },
        "keywords": {
            "IEEE Keywords": [
                "Lighting",
                "Light sources",
                "Object detection",
                "H infinity control",
                "Pixel",
                "Shadow mapping",
                "Computer science",
                "Reflectivity",
                "Singular value decomposition"
            ],
            "INSPEC: Controlled Indexing": [
                "object detection",
                "image recognition",
                "computer vision"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image detection",
                "illumination",
                "pose",
                "objects detection",
                "Lambertian surface",
                "training set",
                "illumination effects",
                "image plane rotations"
            ]
        },
        "id": 195,
        "cited_by": [
            {
                "year": "2003",
                "id": 106
            }
        ]
    },
    {
        "title": "Kernel machine based learning for multi-view face detection and pose estimation",
        "authors": [
            "S.Z. Li",
            "Qingdong Fu",
            "Lie Gu",
            "B. Scholkopf",
            "Yimin Cheng",
            "Hongjiag Zhang"
        ],
        "abstract": "Face images are subject to changes in view and illumination. Such changes cause data distribution to be highly nonlinear and complex in the image space. It is desirable to learn a nonlinear mapping from the image space to a low dimensional space such that the distribution becomes simpler tighter and therefore more predictable for better modeling effaces. In this paper we present a kernel machine based approach for learning such nonlinear mappings. The aim is to provide an effective view-based representation for multi-view face detection and pose estimation. Assuming that the view is partitioned into a number of distinct ranges, one nonlinear view-subspace is learned for each (range of) view from a set of example face images of that view (range), by using kernel principal component analysis (KPCA). Projections of the data onto the view-subspaces are then computed as view-based nonlinear features. Multi-view face detection and pose estimation are performed by classifying a face into one of the facial views or into the nonface class, by using a multi-class kernel support vector classifier (KSVC). Experimental results show that fusion of evidences from multi-views can produce better results than using the result from a single view; and that our approach yields high detection and low false alarm rates in face detection and good accuracy in pose estimation, in comparison with the linear counterpart composed of linear principal component analysis (PCA) feature extraction and Fisher linear discriminant based classification (FLDC).",
        "ieee_link": "https://ieeexplore.ieee.org/document/937691",
        "reference_list": [],
        "citation": {
            "ieee": 44,
            "other": 12,
            "total": 56
        },
        "keywords": {
            "IEEE Keywords": [
                "Kernel",
                "Machine learning",
                "Face detection",
                "Principal component analysis",
                "Space technology",
                "Feature extraction",
                "Face recognition",
                "Lighting",
                "Support vector machines",
                "Predictive models"
            ],
            "INSPEC: Controlled Indexing": [
                "principal component analysis",
                "feature extraction",
                "learning (artificial intelligence)",
                "face recognition",
                "motion estimation",
                "image classification"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "kernel machine based learning",
                "multi-view face detection",
                "pose estimation",
                "data distribution",
                "nonlinear mapping",
                "image space",
                "low dimensional space",
                "view-based representation",
                "principal component analysis",
                "face detection",
                "Fisher linear discriminant based classification"
            ]
        },
        "id": 196,
        "cited_by": [
            {
                "year": "2005",
                "id": 17
            },
            {
                "year": "2005",
                "id": 176
            },
            {
                "year": "2003",
                "id": 35
            }
        ]
    },
    {
        "title": "Markov face models",
        "authors": [
            "S.C. Dass",
            "A.K. Jain"
        ],
        "abstract": "The spatial distribution of gray level intensities in an image can be naturally modeled using Markov random field (MRF) models. We develop and investigate the performance of face detection algorithms derived from MRF considerations. For enhanced detection, the MRF models are defined for every permutation of site indices (pixels) in the image. We find the optimal permutation that provides maximum discriminatory power to identify faces from nonfaces. The methodology presented here is a generalization of the face detection algorithm described previously where a most discriminating Markov chain model was used. The MRF models successfully detect faces in a number of test images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937692",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 3,
            "total": 5
        },
        "keywords": {
            "IEEE Keywords": [
                "Face detection",
                "Markov random fields",
                "Testing",
                "Statistical distributions",
                "Probability",
                "Computer science",
                "Pixel",
                "Simulated annealing",
                "Lattices",
                "Neural networks"
            ],
            "INSPEC: Controlled Indexing": [
                "feature extraction",
                "computational complexity",
                "face recognition",
                "Markov processes"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "Markov face models",
                "spatial distribution",
                "gray level intensities",
                "Markov random field models",
                "face detection algorithms",
                "optimal permutation"
            ]
        },
        "id": 197,
        "cited_by": []
    },
    {
        "title": "Face recognition with support vector machines: global versus component-based approach",
        "authors": [
            "B. Heisele",
            "P. Ho",
            "T. Poggio"
        ],
        "abstract": "We present a component-based method and two global methods for face recognition and evaluate them with respect to robustness against pose changes. In the component system we first locate facial components, extract them and combine them into a single feature vector which is classified by a Support Vector Machine (SVM). The two global systems recognize faces by classifying a single feature vector consisting of the gray values of the whole face image. In the first global system we trained a single SVM classifier for each person in the database. The second system consists of sets of viewpoint-specific SVM classifiers and involves clustering during training. We performed extensive tests on a database which included faces rotated up to about 40/spl deg/ in depth. The component system clearly outperformed both global systems on all tests.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937693",
        "reference_list": [],
        "citation": {
            "ieee": 112,
            "other": 28,
            "total": 140
        },
        "keywords": {
            "IEEE Keywords": [
                "Face recognition",
                "Support vector machines",
                "Support vector machine classification",
                "Robustness",
                "Image databases",
                "Mouth",
                "Active shape model",
                "Solid modeling",
                "Biology computing",
                "Image recognition"
            ],
            "INSPEC: Controlled Indexing": [
                "face recognition",
                "learning automata",
                "feature extraction"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "face recognition",
                "support vector machines",
                "component-based approach",
                "global methods",
                "facial components",
                "feature vector",
                "SVM classifier",
                "clustering"
            ]
        },
        "id": 198,
        "cited_by": [
            {
                "year": "2009",
                "id": 136
            },
            {
                "year": "2007",
                "id": 220
            }
        ]
    },
    {
        "title": "Computationally efficient face detection",
        "authors": [
            "S. Romdhani",
            "P. Torr",
            "B. Scholkopf",
            "A. Blake"
        ],
        "abstract": "This paper describes an algorithm for finding faces within an image. The basis of the algorithm is to run an observation window at all possible positions, scales and orientation within the image. A non-linear support vector machine is used to determine whether or not a face is contained within the observation window. The non-linear support vector machine operates by comparing the input patch to a set of support vectors (which can be thought of as face and anti-face templates). Each support vector is scored by some nonlinear function against the observation window and if the resulting sum is over some threshold a face is indicated. Because of the huge search space that is considered, it is imperative to investigate ways to speed up the support vector machine. Within this paper we suggest a method of speeding up the non-linear support vector machine. A set of reduced set vectors (RVs) are calculated from the support vectors. By considering the RV's sequentially, and if at any point a face is deemed too unlikely to cease the sequential evaluation, obviating the need to evaluate the remaining RVs. The idea being that we only need to apply a subset of the RVs to eliminate things that are obviously not a face (thus reducing the computation). The key then is to explore the RVs in the right order and a method for this is proposed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937694",
        "reference_list": [],
        "citation": {
            "ieee": 57,
            "other": 37,
            "total": 94
        },
        "keywords": {
            "IEEE Keywords": [
                "Face detection",
                "Support vector machines",
                "Support vector machine classification",
                "Neural networks",
                "Runtime",
                "Image databases",
                "Internet",
                "Maximum likelihood detection",
                "Histograms",
                "Pixel"
            ],
            "INSPEC: Controlled Indexing": [
                "learning automata",
                "face recognition"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "computationally efficient face detection",
                "orientation",
                "nonlinear support vector machine",
                "reduced set vectors"
            ]
        },
        "id": 199,
        "cited_by": [
            {
                "year": "2011",
                "id": 133
            },
            {
                "year": "2007",
                "id": 281
            },
            {
                "year": "2003",
                "id": 47
            }
        ]
    },
    {
        "title": "A data-driven model for monocular face tracking",
        "authors": [
            "S.B. Gokturk",
            "J.-Y. Bouguet",
            "R. Grzeszczuk"
        ],
        "abstract": "This paper describes a two-stage system for 3D tracking of pose and deformation of the human face in monocular image sequences without the use of special markers. The first stage of the system learns the space of all possible facial deformations by applying principal component analysis on real stereo tracking data. The resulting model approximates any generic shape as a linear combination of shape basis vectors. The second stage of the system uses this low-complexity deformable model for simultaneous tracking of pose and deformation of the face from a single image sequence. This stage is known as model-based monocular tracking. There are three main contributions of this paper. First we demonstrate that a data-driven approach for model construction is suitable for tracking non rigid objects and offers an elegant and practical alternative to the task of manual construction of models using 3D scanners or CAD modelers. Second, we show that such a method exhibits good tracking accuracy (errors less than 5 mm) and robustness characteristics. Third, we demonstrate that our system exhibits very promising generalization properties in enabling tracking of multiple persons with the same 3D model.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937695",
        "reference_list": [],
        "citation": {
            "ieee": 17,
            "other": 13,
            "total": 30
        },
        "keywords": {
            "IEEE Keywords": [
                "Shape",
                "Image motion analysis",
                "Deformable models",
                "Humans",
                "Principal component analysis",
                "Stereo vision",
                "Face detection",
                "Image sequences",
                "Cameras",
                "Databases"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "principal component analysis",
                "face recognition",
                "stereo image processing"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "data-driven model",
                "monocular face tracking",
                "two-stage system",
                "3D tracking",
                "monocular image sequences",
                "facial deformations",
                "principal component analysis",
                "real stereo tracking data",
                "deformable model"
            ]
        },
        "id": 200,
        "cited_by": [
            {
                "year": "2005",
                "id": 50
            },
            {
                "year": "2003",
                "id": 146
            }
        ]
    },
    {
        "title": "Learning image statistics for Bayesian tracking",
        "authors": [
            "H. Sidenbladh",
            "M.J. Black"
        ],
        "abstract": "This paper describes a framework for learning probabilistic models of objects and scenes and for exploiting these models for tracking complex, deformable, or articulated objects in image sequences. We focus on the probabilistic tracking of people and learn models of how they appear and move in images. In particular we learn the likelihood of observing various spatial and temporal filter responses corresponding to edges, ridges, and motion differences given a model of the person. Similarly, we learn probability distributions over filter responses for general scenes that define a likelihood of observing the filter responses for arbitrary backgrounds. We then derive a probabilistic model for tracking that exploits the ratio between the likelihood that image pixels corresponding to the foreground (person) were generated by an actual person or by some unknown background. The paper extends previous work on learning image statistics and combines it with Bayesian tracking using particle filtering. By combining multiple image cues, and by using learned likelihood models, we demonstrate improved robustness and accuracy when tracking complex objects such as people in monocular image sequences with cluttered scene and a moving camera.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937696",
        "reference_list": [],
        "citation": {
            "ieee": 46,
            "other": 17,
            "total": 63
        },
        "keywords": {
            "IEEE Keywords": [
                "Statistics",
                "Bayesian methods",
                "Layout",
                "Filters",
                "Deformable models",
                "Image sequences",
                "Probability distribution",
                "Pixel",
                "Particle tracking",
                "Filtering"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "probability",
                "Bayes methods",
                "learning (artificial intelligence)"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "image statistics learning",
                "Bayesian tracking",
                "probabilistic models",
                "articulated objects",
                "image sequences",
                "probabilistic tracking",
                "temporal filter responses",
                "probability distributions",
                "image pixels",
                "image statistics",
                "cluttered scene"
            ]
        },
        "id": 201,
        "cited_by": [
            {
                "year": "2003",
                "id": 110
            }
        ]
    },
    {
        "title": "Trust-region methods for real-time tracking",
        "authors": [
            "Hwann-Tzong Chen",
            "Tyng-Luh Liu"
        ],
        "abstract": "Optimization methods based on iterative schemes can be divided into two classes: linesearch methods and trust-region methods. While linesearch techniques are commonly found in various vision applications, not much attention is paid to trust-region methods. Motivated by the fact that linesearch methods can be considered as special cases of trust-region methods, we propose to apply trust-region methods to visual tracking problems. Our approach integrates trust-region methods with the Kullback Leibler distance to track a rigid or non-rigid object in real-time. If not limited by the speed of a camera, the algorithm can achieve frame rate above 60 fps. To justify our method, a variety of experiments/comparisons are carried out for the trust-region tracker and a linesearch-based mean-shift tracker with same initial conditions. The experimental results support our conjecture that a trust-region tracker should perform superiorly to a linesearch one.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937697",
        "reference_list": [],
        "citation": {
            "ieee": 20,
            "other": 1,
            "total": 21
        },
        "keywords": {
            "IEEE Keywords": [
                "Optimization methods",
                "Cameras",
                "Shape",
                "Target tracking",
                "Information science",
                "Iterative methods",
                "Color",
                "Pixel",
                "Image reconstruction",
                "Humans"
            ],
            "INSPEC: Controlled Indexing": [
                "iterative methods",
                "tracking",
                "computer vision",
                "optimisation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "trust-region methods",
                "real-time tracking",
                "optimization methods",
                "iterative schemes",
                "linesearch methods",
                "visual tracking",
                "Kullback Leibler distance",
                "trust-region tracker"
            ]
        },
        "id": 202,
        "cited_by": [
            {
                "year": "2003",
                "id": 54
            }
        ]
    },
    {
        "title": "3D LAMP: a new layered panoramic representation",
        "authors": [
            "Zhigang Zhu",
            "A.R. Hanson"
        ],
        "abstract": "In this paper a compact representation, 3D Layered, Adaptive-resolution and Multi-perspective Panorama (LAMP), is proposed for representing large scale and 3D scenes with occlusion. Two kinds of 3D LAMP representations are constructed, i.e. the relief-like LAMP and the image-based LAMP, both of which concisely represent almost all the information from a long image sequence. The relief-like LAMP is basically a single extended multi-perspective panoramic view image with both texture and depth values, but each pixel has multiple values to represent results of occlusion recovery and resolution enhancement. The image-based LAMP, on the other hand, consists of a set of multi-perspective layers, each of which has both texture and depth maps, with adaptive time-sampling scales depending on depths of scene points. Several examples of 3D LAMP construction for real image sequences are given. The 3D LAMP is a concise and powerful representation for image-based rendering.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937698",
        "reference_list": [
            {
                "year": "2001",
                "id": 47
            }
        ],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Lamps",
                "Layout",
                "Cameras",
                "Image sequences",
                "Stereo vision",
                "Large-scale systems",
                "Rendering (computer graphics)",
                "Computer vision",
                "Motion estimation",
                "Computer science"
            ],
            "INSPEC: Controlled Indexing": [
                "image sequences",
                "rendering (computer graphics)",
                "image representation"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "3D LAMP",
                "layered panoramic representation",
                "compact representation",
                "3D scenes",
                "occlusion",
                "image-based LAMP",
                "image sequence",
                "multi-perspective panoramic view image",
                "occlusion recovery",
                "multi-perspective layers",
                "depth maps",
                "time-sampling scales",
                "real image sequences",
                "image-based rendering"
            ]
        },
        "id": 203,
        "cited_by": []
    },
    {
        "title": "Automatic registration of 2-D with 3-D imagery in urban environments",
        "authors": [
            "I. Stamos",
            "P.K. Alien"
        ],
        "abstract": "We are building a system that can automatically acquire 3D range scans and 2D images to build geometrically correct, texture mapped 3D models of urban environments. This paper deals with the problem of automatically registering the 3D range scans with images acquired at other times and with unknown camera calibration and location. The method involves the utilization of parallelism and orthogonality constraints that naturally exist in urban environments. We present results for building a texture mapped 3-D model of an urban building.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937699",
        "reference_list": [],
        "citation": {
            "ieee": 26,
            "other": 11,
            "total": 37
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Layout",
                "Solid modeling",
                "Sensor systems",
                "Image sensors",
                "Parallel processing",
                "Robot vision systems",
                "Computer science",
                "Calibration",
                "Image registration"
            ],
            "INSPEC: Controlled Indexing": [
                "image registration",
                "calibration",
                "solid modelling"
            ],
            "INSPEC: Non-Controlled Indexing": [
                "automatic registration",
                "3-D imagery",
                "2-D imagery",
                "urban environments",
                "3D range scans",
                "2D images",
                "texture mapped 3D models",
                "camera calibration",
                "orthogonality constraints",
                "texture mapped 3-D model"
            ]
        },
        "id": 204,
        "cited_by": [
            {
                "year": "2007",
                "id": 384
            }
        ]
    },
    {
        "title": "3D-mode: a 3D modeling and measurement system using a few photos",
        "authors": [
            "Gang Xu",
            "T. Nakayama",
            "T. Kajikawa",
            "J. Terai"
        ],
        "abstract": "We demonstrate \"3D-Mode,\" a software system that builds 3D models of objects and scenes by taking a few (minimally 2) photographs using a digital camera at possibly largely separated positions. It has recently been commercialized by 3D Media Co., Ltd. 3D-Mode has the following steps: (1) taking photos; (2) manually obtaining and matching a few feature points on the object; (3) automatically computing 3D structure and motion; (4) automatic Delaunay triangulation and manual modification; (5) automatic acquisition of texture for each triangular patch; (6) if necessary, assign a new coordinate system and scale of space. The system does not need input of camera parameters, but requires manual input of feature points in a special manner such that matching information is input simultaneously. Once epipolar lines are available, they are used to help locate corresponding points in other images.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937700",
        "reference_list": [],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Transmission line matrix methods",
                "Computer vision",
                "Topology",
                "Surface texture",
                "Computer graphics",
                "Surface fitting",
                "Computer science",
                "Software systems"
            ]
        },
        "id": 205,
        "cited_by": []
    },
    {},
    {},
    {},
    {},
    {},
    {},
    {
        "title": "Cloning your own face with a desktop camera",
        "authors": [
            "Zhengyou Zhang",
            "Zicheng Liu",
            "D. Adler",
            "M. Cohen",
            "E. Hanson",
            "Ying Shan"
        ],
        "abstract": "We have developed an easy and cost-effective system that constructs textured 3D animated face models from videos with minimal user interaction. Our system first takes, with an ordinary video camera, images of a face of a person sitting in front of the camera turning the head from one side to the other. After five manual clicks on two images to tell the system where the eye corners, nose top and mouth corners are, the system automatically generates a realistic looking 3D human head model and the constructed model can be animated immediately (different poses, facial expressions and talking). A user, with a PC and a video camera, can use our system to generate hisher face model in a few minutes. The face model can then be imported in hisher favorite game, and the user sees themselves and their friends take part in the game they are playing. We will demonstrate the system on a laptop computer live at the conference, and participants can try it to model their own faces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937707",
        "reference_list": [
            {
                "year": "2001",
                "id": 192
            }
        ],
        "citation": {
            "ieee": 0,
            "other": 0,
            "total": 0
        },
        "keywords": {
            "IEEE Keywords": [
                "Cloning",
                "Cameras",
                "Facial animation",
                "Videos",
                "Face detection",
                "Games",
                "Solid modeling",
                "Computer vision",
                "Magnetic heads",
                "Humans"
            ],
            "Author Keywords": [
                "face modeling",
                "facial animation",
                "geometric modeling",
                "3D computer vision"
            ]
        },
        "id": 212,
        "cited_by": []
    },
    {
        "title": "Control of home appliances using face and hand sign recognition",
        "authors": [
            "H. Watanabe",
            "H. Hongo",
            "M. Yasumoto",
            "Y. Niwa",
            "K. Yamamoto"
        ],
        "abstract": "This article discusses face and hand sign recognition and its applications. We have already proposed a method that can recognize faces and hand signs using hierarchical discriminant analysis. Here, we implement this method on a laptop computer and develop a real-time system that can control home appliances by recognizing faces and hand signs. Our system can control various appliances in the home by remote control. Moreover, since our system can recognize the user, it can assign distinct commands to the hand signs of each user and limit the appliances and functions that users can control.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937708",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 1,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Home appliances",
                "Face recognition",
                "Face detection",
                "Linear discriminant analysis",
                "Portable computers",
                "Control systems",
                "Vectors",
                "Universal Serial Bus",
                "Cameras"
            ]
        },
        "id": 213,
        "cited_by": []
    },
    {},
    {
        "title": "Self-calibrating camera-projector systems for interactive displays and presentations",
        "authors": [
            "Rahul Sukthankar",
            "Tat-Jen Cham",
            "G. Sukthankar",
            "J. Rehg",
            "D. Hsu",
            "T. Leung"
        ],
        "abstract": "The authors demonstrate a self-calibrating system that employs uncalibrated cameras and microportable projectors to create novel interactive displays and presentations. Three benefits of ther system are detailed.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937710",
        "reference_list": [
            {
                "year": "2001",
                "id": 33
            }
        ],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Laboratories",
                "Computer displays",
                "Image generation",
                "Application software",
                "Motion pictures",
                "Pressing",
                "Automatic control",
                "Optical control"
            ]
        },
        "id": 215,
        "cited_by": []
    },
    {},
    {
        "title": "Tele-graffiti: a pen and paper-based remote sketching system",
        "authors": [
            "N. Takao",
            "Jianbo Shi",
            "S. Baker",
            "I. Matthews",
            "B. Nabbe"
        ],
        "abstract": "Tele-Graffiti is a system allowing two.or more users to communicate remotely via hand-drawn sketches. What one person writes at one site is captured using a video camera, transmitted to the other site(s), and displayed there using an LCD projector. The advantage of our system over other intelligent desktops and white-boards is that the users are free to move the pieces of paper on which they are writing. In Tele-Graffiti, paper detection and tracking is based on real-time paper boundary detection.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937712",
        "reference_list": [],
        "citation": {
            "ieee": 2,
            "other": 0,
            "total": 2
        },
        "keywords": {
            "IEEE Keywords": [
                "Cameras",
                "Application software",
                "Image edge detection",
                "Image coding",
                "Robot vision systems",
                "Computer science education",
                "Computer displays",
                "Liquid crystal displays",
                "Web pages",
                "Intelligent systems"
            ]
        },
        "id": 217,
        "cited_by": []
    },
    {
        "title": "Real-time automated concurrent visual tracking of many animals and subsequent behavioural compilation",
        "authors": [
            "J.S. Zelek",
            "D. Bullock"
        ],
        "abstract": "One of our major research focus areas is real-time visual tracking and monitoring of moving and static objects in a video sequence. In particular we are interested in (1) object localization (also referred to as the focus of attention) which involves identifying the object of interest, (2) tracking the object using a model of the object which was initiated in step 1, and (3) understanding the accumulation of movements of the object over time (i.e., behavior). The objects of interiest for the purpose of this proposal are pigs. Automatically monitoring pigs via a non-invasively placed camera in their pens is interesting because the pigs are monitored in their natural habitat. Visual tracking involves modelling the object of interest and keeping track of its position and orientation through time. Issues include tracker recovery from error and preventing the tracker from jumping to other pigs. we have been able to demonstrate tracking pigs at about 10-15 Hz, however, the tracker tends to drift off the target eventually. We have only experimented with a single pig but our initial tests indicate that we can probably track at least 10 pigs simultaneously. Some unknowns include determining how quickly the pigs move and the type of motions, including quick jerky movements. Our preliminary investigations revealed that a blob tracker is insufficient for producing accurate traces.",
        "ieee_link": "https://ieeexplore.ieee.org/document/937713",
        "reference_list": [],
        "citation": {
            "ieee": 1,
            "other": 0,
            "total": 1
        },
        "keywords": {
            "IEEE Keywords": [
                "Animals",
                "Target tracking",
                "Computerized monitoring",
                "Robustness",
                "Computer vision",
                "Streaming media",
                "Video sequences",
                "Object recognition",
                "Proposals",
                "Cameras"
            ]
        },
        "id": 218,
        "cited_by": []
    }
]